################################################################################
                      [1m Learning iteration 0/10000 [0m                      

                       Computation: 10624 steps/s (collection: 8.996s, learning 0.257s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 44.0508
                       Mean reward: 0.01
               Mean episode length: 21.21
    Episode_Reward/reaching_object: 0.0014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.25s
                      Time elapsed: 00:00:09
                               ETA: 01:42:07

################################################################################
                      [1m Learning iteration 1/10000 [0m                      

                       Computation: 14792 steps/s (collection: 6.458s, learning 0.187s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 44.2097
                       Mean reward: 0.01
               Mean episode length: 45.83
    Episode_Reward/reaching_object: 0.0041
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.65s
                      Time elapsed: 00:00:15
                               ETA: 22:04:44

################################################################################
                      [1m Learning iteration 2/10000 [0m                      

                       Computation: 14991 steps/s (collection: 6.422s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.2878
                       Mean reward: 0.02
               Mean episode length: 69.78
    Episode_Reward/reaching_object: 0.0068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.56s
                      Time elapsed: 00:00:22
                               ETA: 20:47:16

################################################################################
                      [1m Learning iteration 3/10000 [0m                      

                       Computation: 14870 steps/s (collection: 6.479s, learning 0.132s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 44.3444
                       Mean reward: 0.04
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.0104
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.61s
                      Time elapsed: 00:00:29
                               ETA: 20:10:43

################################################################################
                      [1m Learning iteration 4/10000 [0m                      

                       Computation: 14842 steps/s (collection: 6.473s, learning 0.150s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 44.3770
                       Mean reward: 0.05
               Mean episode length: 117.74
    Episode_Reward/reaching_object: 0.0143
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.62s
                      Time elapsed: 00:00:35
                               ETA: 19:49:09

################################################################################
                      [1m Learning iteration 5/10000 [0m                      

                       Computation: 15065 steps/s (collection: 6.367s, learning 0.158s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 44.4080
                       Mean reward: 0.07
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.0194
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.53s
                      Time elapsed: 00:00:42
                               ETA: 19:32:02

################################################################################
                      [1m Learning iteration 6/10000 [0m                      

                       Computation: 14621 steps/s (collection: 6.590s, learning 0.133s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 44.4329
                       Mean reward: 0.09
               Mean episode length: 165.77
    Episode_Reward/reaching_object: 0.0249
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.72s
                      Time elapsed: 00:00:48
                               ETA: 19:24:28

################################################################################
                      [1m Learning iteration 7/10000 [0m                      

                       Computation: 14575 steps/s (collection: 6.613s, learning 0.132s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 44.4782
                       Mean reward: 0.12
               Mean episode length: 189.36
    Episode_Reward/reaching_object: 0.0318
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.74s
                      Time elapsed: 00:00:55
                               ETA: 19:19:13

################################################################################
                      [1m Learning iteration 8/10000 [0m                      

                       Computation: 18014 steps/s (collection: 5.345s, learning 0.112s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 44.5055
                       Mean reward: 0.14
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 0.0397
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.46s
                      Time elapsed: 00:01:01
                               ETA: 18:51:17

################################################################################
                      [1m Learning iteration 9/10000 [0m                      

                       Computation: 53815 steps/s (collection: 1.729s, learning 0.098s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 44.5797
                       Mean reward: 0.21
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.0524
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.83s
                      Time elapsed: 00:01:02
                               ETA: 17:28:29

################################################################################
                     [1m Learning iteration 10/10000 [0m                      

                       Computation: 57274 steps/s (collection: 1.626s, learning 0.090s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 44.6688
                       Mean reward: 0.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0715
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.72s
                      Time elapsed: 00:01:04
                               ETA: 16:19:03

################################################################################
                     [1m Learning iteration 11/10000 [0m                      

                       Computation: 56483 steps/s (collection: 1.649s, learning 0.092s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 44.7259
                       Mean reward: 0.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0816
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.74s
                      Time elapsed: 00:01:06
                               ETA: 15:21:31

################################################################################
                     [1m Learning iteration 12/10000 [0m                      

                       Computation: 56488 steps/s (collection: 1.627s, learning 0.113s)
             Mean action noise std: 1.03
          Mean value_function loss: 2.4736
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.8681
                       Mean reward: -1.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0978
     Episode_Reward/lifting_object: -0.1296
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.74s
                      Time elapsed: 00:01:08
                               ETA: 14:32:49

################################################################################
                     [1m Learning iteration 13/10000 [0m                      

                       Computation: 55700 steps/s (collection: 1.666s, learning 0.099s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.8233
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.9767
                       Mean reward: 0.61
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.1201
     Episode_Reward/lifting_object: -0.0625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.76s
                      Time elapsed: 00:01:09
                               ETA: 13:51:23

################################################################################
                     [1m Learning iteration 14/10000 [0m                      

                       Computation: 55294 steps/s (collection: 1.669s, learning 0.109s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.7622
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.2558
                       Mean reward: -1.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1327
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.78s
                      Time elapsed: 00:01:11
                               ETA: 13:15:36

################################################################################
                     [1m Learning iteration 15/10000 [0m                      

                       Computation: 54781 steps/s (collection: 1.679s, learning 0.116s)
             Mean action noise std: 1.05
          Mean value_function loss: 7.6193
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.4413
                       Mean reward: -1.23
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.1742
     Episode_Reward/lifting_object: -0.2234
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.79s
                      Time elapsed: 00:01:13
                               ETA: 12:44:28

################################################################################
                     [1m Learning iteration 16/10000 [0m                      

                       Computation: 52341 steps/s (collection: 1.768s, learning 0.110s)
             Mean action noise std: 1.06
          Mean value_function loss: 10.7116
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.6219
                       Mean reward: -1.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2039
     Episode_Reward/lifting_object: -0.8477
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.88s
                      Time elapsed: 00:01:15
                               ETA: 12:17:49

################################################################################
                     [1m Learning iteration 17/10000 [0m                      

                       Computation: 49040 steps/s (collection: 1.890s, learning 0.115s)
             Mean action noise std: 1.06
          Mean value_function loss: 13.5432
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.7662
                       Mean reward: -2.22
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.2274
     Episode_Reward/lifting_object: -1.0335
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.00s
                      Time elapsed: 00:01:17
                               ETA: 11:55:17

################################################################################
                     [1m Learning iteration 18/10000 [0m                      

                       Computation: 53867 steps/s (collection: 1.713s, learning 0.112s)
             Mean action noise std: 1.07
          Mean value_function loss: 7.8835
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.9235
                       Mean reward: 0.04
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.2456
     Episode_Reward/lifting_object: -0.8806
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.82s
                      Time elapsed: 00:01:19
                               ETA: 11:33:33

################################################################################
                     [1m Learning iteration 19/10000 [0m                      

                       Computation: 52984 steps/s (collection: 1.758s, learning 0.097s)
             Mean action noise std: 1.07
          Mean value_function loss: 7.9933
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.0919
                       Mean reward: -2.20
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.2505
     Episode_Reward/lifting_object: -0.4349
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.86s
                      Time elapsed: 00:01:21
                               ETA: 11:14:14

################################################################################
                     [1m Learning iteration 20/10000 [0m                      

                       Computation: 53051 steps/s (collection: 1.759s, learning 0.094s)
             Mean action noise std: 1.08
          Mean value_function loss: 3.4543
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2766
                       Mean reward: -0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2586
     Episode_Reward/lifting_object: -0.4587
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.85s
                      Time elapsed: 00:01:22
                               ETA: 10:56:44

################################################################################
                     [1m Learning iteration 21/10000 [0m                      

                       Computation: 54395 steps/s (collection: 1.711s, learning 0.097s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2453
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.4619
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2497
     Episode_Reward/lifting_object: -0.1140
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.81s
                      Time elapsed: 00:01:24
                               ETA: 10:40:29

################################################################################
                     [1m Learning iteration 22/10000 [0m                      

                       Computation: 55869 steps/s (collection: 1.668s, learning 0.092s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2544
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.6166
                       Mean reward: 0.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2350
     Episode_Reward/lifting_object: -0.0541
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.76s
                      Time elapsed: 00:01:26
                               ETA: 10:25:18

################################################################################
                     [1m Learning iteration 23/10000 [0m                      

                       Computation: 55537 steps/s (collection: 1.662s, learning 0.108s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.4728
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.7546
                       Mean reward: 0.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2155
     Episode_Reward/lifting_object: -0.0279
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.77s
                      Time elapsed: 00:01:28
                               ETA: 10:11:27

################################################################################
                     [1m Learning iteration 24/10000 [0m                      

                       Computation: 56341 steps/s (collection: 1.634s, learning 0.111s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.6638
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.8811
                       Mean reward: -0.17
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.1937
     Episode_Reward/lifting_object: -0.2061
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.74s
                      Time elapsed: 00:01:29
                               ETA: 09:58:32

################################################################################
                     [1m Learning iteration 25/10000 [0m                      

                       Computation: 56781 steps/s (collection: 1.638s, learning 0.093s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.1573
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.0262
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1717
     Episode_Reward/lifting_object: -0.0400
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.73s
                      Time elapsed: 00:01:31
                               ETA: 09:46:32

################################################################################
                     [1m Learning iteration 26/10000 [0m                      

                       Computation: 55789 steps/s (collection: 1.671s, learning 0.091s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.2743
                       Mean reward: 0.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1492
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.76s
                      Time elapsed: 00:01:33
                               ETA: 09:35:36

################################################################################
                     [1m Learning iteration 27/10000 [0m                      

                       Computation: 57054 steps/s (collection: 1.613s, learning 0.110s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 47.5014
                       Mean reward: 0.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1460
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.72s
                      Time elapsed: 00:01:35
                               ETA: 09:25:13

################################################################################
                     [1m Learning iteration 28/10000 [0m                      

                       Computation: 56403 steps/s (collection: 1.643s, learning 0.100s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 47.5425
                       Mean reward: 0.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1357
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.74s
                      Time elapsed: 00:01:36
                               ETA: 09:15:39

################################################################################
                     [1m Learning iteration 29/10000 [0m                      

                       Computation: 56212 steps/s (collection: 1.639s, learning 0.110s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0625
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.6734
                       Mean reward: 0.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1262
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.75s
                      Time elapsed: 00:01:38
                               ETA: 09:06:46

################################################################################
                     [1m Learning iteration 30/10000 [0m                      

                       Computation: 55908 steps/s (collection: 1.644s, learning 0.115s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 47.7246
                       Mean reward: 0.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1212
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.76s
                      Time elapsed: 00:01:40
                               ETA: 08:58:30

################################################################################
                     [1m Learning iteration 31/10000 [0m                      

                       Computation: 57545 steps/s (collection: 1.596s, learning 0.112s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 47.8654
                       Mean reward: 0.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1220
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.71s
                      Time elapsed: 00:01:42
                               ETA: 08:50:29

################################################################################
                     [1m Learning iteration 32/10000 [0m                      

                       Computation: 55935 steps/s (collection: 1.653s, learning 0.104s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 47.8878
                       Mean reward: 0.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1134
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.76s
                      Time elapsed: 00:01:43
                               ETA: 08:43:12

################################################################################
                     [1m Learning iteration 33/10000 [0m                      

                       Computation: 56668 steps/s (collection: 1.646s, learning 0.089s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 48.0080
                       Mean reward: 0.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1226
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.73s
                      Time elapsed: 00:01:45
                               ETA: 08:36:15

################################################################################
                     [1m Learning iteration 34/10000 [0m                      

                       Computation: 53920 steps/s (collection: 1.724s, learning 0.099s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.7137
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.1041
                       Mean reward: -1.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1251
     Episode_Reward/lifting_object: -0.0877
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.82s
                      Time elapsed: 00:01:47
                               ETA: 08:30:06

################################################################################
                     [1m Learning iteration 35/10000 [0m                      

                       Computation: 55158 steps/s (collection: 1.626s, learning 0.156s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 48.1610
                       Mean reward: 0.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1379
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.78s
                      Time elapsed: 00:01:49
                               ETA: 08:24:06

################################################################################
                     [1m Learning iteration 36/10000 [0m                      

                       Computation: 53123 steps/s (collection: 1.758s, learning 0.092s)
             Mean action noise std: 1.15
          Mean value_function loss: 2.1473
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.2881
                       Mean reward: 0.58
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.1687
     Episode_Reward/lifting_object: -0.0235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.85s
                      Time elapsed: 00:01:51
                               ETA: 08:18:44

################################################################################
                     [1m Learning iteration 37/10000 [0m                      

                       Computation: 53937 steps/s (collection: 1.723s, learning 0.100s)
             Mean action noise std: 1.15
          Mean value_function loss: 2.5661
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.3548
                       Mean reward: 0.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1834
     Episode_Reward/lifting_object: -0.1200
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.82s
                      Time elapsed: 00:01:52
                               ETA: 08:13:31

################################################################################
                     [1m Learning iteration 38/10000 [0m                      

                       Computation: 52436 steps/s (collection: 1.785s, learning 0.090s)
             Mean action noise std: 1.16
          Mean value_function loss: 8.0394
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4816
                       Mean reward: -4.52
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.2142
     Episode_Reward/lifting_object: -0.7791
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.87s
                      Time elapsed: 00:01:54
                               ETA: 08:08:48

################################################################################
                     [1m Learning iteration 39/10000 [0m                      

                       Computation: 52375 steps/s (collection: 1.774s, learning 0.103s)
             Mean action noise std: 1.16
          Mean value_function loss: 3.5336
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.5773
                       Mean reward: -1.10
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.2260
     Episode_Reward/lifting_object: -0.4531
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.88s
                      Time elapsed: 00:01:56
                               ETA: 08:04:19

################################################################################
                     [1m Learning iteration 40/10000 [0m                      

                       Computation: 54066 steps/s (collection: 1.732s, learning 0.086s)
             Mean action noise std: 1.17
          Mean value_function loss: 1.2593
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.6931
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2531
     Episode_Reward/lifting_object: -0.1391
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.82s
                      Time elapsed: 00:01:58
                               ETA: 07:59:49

################################################################################
                     [1m Learning iteration 41/10000 [0m                      

                       Computation: 54313 steps/s (collection: 1.710s, learning 0.100s)
             Mean action noise std: 1.17
          Mean value_function loss: 2.6644
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8015
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2532
     Episode_Reward/lifting_object: -0.3973
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.81s
                      Time elapsed: 00:02:00
                               ETA: 07:55:30

################################################################################
                     [1m Learning iteration 42/10000 [0m                      

                       Computation: 51774 steps/s (collection: 1.801s, learning 0.098s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.7980
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.9107
                       Mean reward: 0.88
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: -0.2203
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.90s
                      Time elapsed: 00:02:02
                               ETA: 07:51:44

################################################################################
                     [1m Learning iteration 43/10000 [0m                      

                       Computation: 53801 steps/s (collection: 1.721s, learning 0.106s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.6048
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.0296
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2574
     Episode_Reward/lifting_object: -0.1012
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.83s
                      Time elapsed: 00:02:04
                               ETA: 07:47:51

################################################################################
                     [1m Learning iteration 44/10000 [0m                      

                       Computation: 54382 steps/s (collection: 1.705s, learning 0.103s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.4146
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.1306
                       Mean reward: 1.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2529
     Episode_Reward/lifting_object: -0.1528
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.81s
                      Time elapsed: 00:02:05
                               ETA: 07:44:04

################################################################################
                     [1m Learning iteration 45/10000 [0m                      

                       Computation: 55677 steps/s (collection: 1.670s, learning 0.096s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 49.2728
                       Mean reward: 0.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2065
     Episode_Reward/lifting_object: -0.0448
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.77s
                      Time elapsed: 00:02:07
                               ETA: 07:40:18

################################################################################
                     [1m Learning iteration 46/10000 [0m                      

                       Computation: 54978 steps/s (collection: 1.675s, learning 0.113s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 49.4182
                       Mean reward: 0.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2168
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.79s
                      Time elapsed: 00:02:09
                               ETA: 07:36:47

################################################################################
                     [1m Learning iteration 47/10000 [0m                      

                       Computation: 54492 steps/s (collection: 1.689s, learning 0.115s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0265
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.4676
                       Mean reward: 0.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1808
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.80s
                      Time elapsed: 00:02:11
                               ETA: 07:33:27

################################################################################
                     [1m Learning iteration 48/10000 [0m                      

                       Computation: 56313 steps/s (collection: 1.641s, learning 0.105s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 49.4971
                       Mean reward: 0.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1651
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.75s
                      Time elapsed: 00:02:12
                               ETA: 07:30:04

################################################################################
                     [1m Learning iteration 49/10000 [0m                      

                       Computation: 54284 steps/s (collection: 1.700s, learning 0.111s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 49.6103
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1510
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.81s
                      Time elapsed: 00:02:14
                               ETA: 07:27:01

################################################################################
                     [1m Learning iteration 50/10000 [0m                      

                       Computation: 55903 steps/s (collection: 1.664s, learning 0.095s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 49.6891
                       Mean reward: 0.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1545
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.76s
                      Time elapsed: 00:02:16
                               ETA: 07:23:56

################################################################################
                     [1m Learning iteration 51/10000 [0m                      

                       Computation: 55054 steps/s (collection: 1.682s, learning 0.104s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0279
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 49.7438
                       Mean reward: 0.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1651
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.79s
                      Time elapsed: 00:02:18
                               ETA: 07:21:03

################################################################################
                     [1m Learning iteration 52/10000 [0m                      

                       Computation: 55218 steps/s (collection: 1.686s, learning 0.094s)
             Mean action noise std: 1.21
          Mean value_function loss: 2.8155
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.7865
                       Mean reward: 0.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1674
     Episode_Reward/lifting_object: -0.1426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.78s
                      Time elapsed: 00:02:20
                               ETA: 07:18:15

################################################################################
                     [1m Learning iteration 53/10000 [0m                      

                       Computation: 53881 steps/s (collection: 1.735s, learning 0.090s)
             Mean action noise std: 1.21
          Mean value_function loss: 5.4879
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.9064
                       Mean reward: 1.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1989
     Episode_Reward/lifting_object: -0.2365
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.82s
                      Time elapsed: 00:02:21
                               ETA: 07:15:41

################################################################################
                     [1m Learning iteration 54/10000 [0m                      

                       Computation: 52653 steps/s (collection: 1.778s, learning 0.089s)
             Mean action noise std: 1.22
          Mean value_function loss: 21.2296
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.0022
                       Mean reward: -7.28
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.2156
     Episode_Reward/lifting_object: -1.0363
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.87s
                      Time elapsed: 00:02:23
                               ETA: 07:13:21

################################################################################
                     [1m Learning iteration 55/10000 [0m                      

                       Computation: 51819 steps/s (collection: 1.787s, learning 0.110s)
             Mean action noise std: 1.22
          Mean value_function loss: 16.5828
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.1049
                       Mean reward: -0.76
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.2536
     Episode_Reward/lifting_object: -0.7694
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.90s
                      Time elapsed: 00:02:25
                               ETA: 07:11:11

################################################################################
                     [1m Learning iteration 56/10000 [0m                      

                       Computation: 51781 steps/s (collection: 1.787s, learning 0.111s)
             Mean action noise std: 1.22
          Mean value_function loss: 14.9550
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.1839
                       Mean reward: -1.53
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.2927
     Episode_Reward/lifting_object: -1.5888
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.90s
                      Time elapsed: 00:02:27
                               ETA: 07:09:06

################################################################################
                     [1m Learning iteration 57/10000 [0m                      

                       Computation: 51585 steps/s (collection: 1.808s, learning 0.098s)
             Mean action noise std: 1.23
          Mean value_function loss: 9.3156
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.2641
                       Mean reward: -8.31
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.3111
     Episode_Reward/lifting_object: -1.1127
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.91s
                      Time elapsed: 00:02:29
                               ETA: 07:07:06

################################################################################
                     [1m Learning iteration 58/10000 [0m                      

                       Computation: 51318 steps/s (collection: 1.809s, learning 0.106s)
             Mean action noise std: 1.23
          Mean value_function loss: 6.8002
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.3465
                       Mean reward: -2.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3233
     Episode_Reward/lifting_object: -0.8588
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.92s
                      Time elapsed: 00:02:31
                               ETA: 07:05:12

################################################################################
                     [1m Learning iteration 59/10000 [0m                      

                       Computation: 54160 steps/s (collection: 1.715s, learning 0.100s)
             Mean action noise std: 1.23
          Mean value_function loss: 2.1016
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.4206
                       Mean reward: 0.15
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.3494
     Episode_Reward/lifting_object: -0.3625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.82s
                      Time elapsed: 00:02:33
                               ETA: 07:03:05

################################################################################
                     [1m Learning iteration 60/10000 [0m                      

                       Computation: 54237 steps/s (collection: 1.724s, learning 0.089s)
             Mean action noise std: 1.24
          Mean value_function loss: 5.1233
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.5208
                       Mean reward: -2.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3290
     Episode_Reward/lifting_object: -0.5412
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.81s
                      Time elapsed: 00:02:35
                               ETA: 07:01:02

################################################################################
                     [1m Learning iteration 61/10000 [0m                      

                       Computation: 53877 steps/s (collection: 1.724s, learning 0.101s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0384
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.6078
                       Mean reward: 1.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3097
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.82s
                      Time elapsed: 00:02:36
                               ETA: 06:59:04

################################################################################
                     [1m Learning iteration 62/10000 [0m                      

                       Computation: 55321 steps/s (collection: 1.683s, learning 0.094s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2811
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 50.7844
                       Mean reward: 1.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2846
     Episode_Reward/lifting_object: -0.0352
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.78s
                      Time elapsed: 00:02:38
                               ETA: 06:57:03

################################################################################
                     [1m Learning iteration 63/10000 [0m                      

                       Computation: 55789 steps/s (collection: 1.650s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8448
                       Mean reward: 0.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2350
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.76s
                      Time elapsed: 00:02:40
                               ETA: 06:55:03

################################################################################
                     [1m Learning iteration 64/10000 [0m                      

                       Computation: 56007 steps/s (collection: 1.653s, learning 0.103s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 51.0092
                       Mean reward: 0.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.76s
                      Time elapsed: 00:02:42
                               ETA: 06:53:06

################################################################################
                     [1m Learning iteration 65/10000 [0m                      

                       Computation: 55708 steps/s (collection: 1.672s, learning 0.093s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 51.0493
                       Mean reward: 0.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1658
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.76s
                      Time elapsed: 00:02:43
                               ETA: 06:51:13

################################################################################
                     [1m Learning iteration 66/10000 [0m                      

                       Computation: 56270 steps/s (collection: 1.646s, learning 0.101s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 51.1663
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1664
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.75s
                      Time elapsed: 00:02:45
                               ETA: 06:49:22

################################################################################
                     [1m Learning iteration 67/10000 [0m                      

                       Computation: 55952 steps/s (collection: 1.656s, learning 0.101s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 51.2284
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1350
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.76s
                      Time elapsed: 00:02:47
                               ETA: 06:47:35

################################################################################
                     [1m Learning iteration 68/10000 [0m                      

                       Computation: 55791 steps/s (collection: 1.648s, learning 0.114s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 51.3012
                       Mean reward: 0.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1281
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.76s
                      Time elapsed: 00:02:49
                               ETA: 06:45:51

################################################################################
                     [1m Learning iteration 69/10000 [0m                      

                       Computation: 56478 steps/s (collection: 1.648s, learning 0.093s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 51.3723
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1296
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.74s
                      Time elapsed: 00:02:50
                               ETA: 06:44:08

################################################################################
                     [1m Learning iteration 70/10000 [0m                      

                       Computation: 55526 steps/s (collection: 1.667s, learning 0.104s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 51.4477
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1491
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.77s
                      Time elapsed: 00:02:52
                               ETA: 06:42:32

################################################################################
                     [1m Learning iteration 71/10000 [0m                      

                       Computation: 55556 steps/s (collection: 1.659s, learning 0.111s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1971
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.5267
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1544
     Episode_Reward/lifting_object: -0.0209
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.77s
                      Time elapsed: 00:02:54
                               ETA: 06:40:58

################################################################################
                     [1m Learning iteration 72/10000 [0m                      

                       Computation: 52331 steps/s (collection: 1.762s, learning 0.117s)
             Mean action noise std: 1.28
          Mean value_function loss: 9.4642
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.5710
                       Mean reward: 0.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1912
     Episode_Reward/lifting_object: -0.2042
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.88s
                      Time elapsed: 00:02:56
                               ETA: 06:39:41

################################################################################
                     [1m Learning iteration 73/10000 [0m                      

                       Computation: 50568 steps/s (collection: 1.832s, learning 0.112s)
             Mean action noise std: 1.28
          Mean value_function loss: 21.6613
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.6656
                       Mean reward: -9.84
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.2379
     Episode_Reward/lifting_object: -0.9851
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.94s
                      Time elapsed: 00:02:58
                               ETA: 06:38:36

################################################################################
                     [1m Learning iteration 74/10000 [0m                      

                       Computation: 51545 steps/s (collection: 1.811s, learning 0.096s)
             Mean action noise std: 1.29
          Mean value_function loss: 23.9486
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.7396
                       Mean reward: -4.80
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.2965
     Episode_Reward/lifting_object: -1.2102
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.91s
                      Time elapsed: 00:03:00
                               ETA: 06:37:27

################################################################################
                     [1m Learning iteration 75/10000 [0m                      

                       Computation: 50430 steps/s (collection: 1.861s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 29.3971
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.8020
                       Mean reward: -10.99
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.3344
     Episode_Reward/lifting_object: -2.0723
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.95s
                      Time elapsed: 00:03:02
                               ETA: 06:36:25

################################################################################
                     [1m Learning iteration 76/10000 [0m                      

                       Computation: 52389 steps/s (collection: 1.784s, learning 0.092s)
             Mean action noise std: 1.29
          Mean value_function loss: 14.5807
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.8753
                       Mean reward: -1.93
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: -2.1771
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.88s
                      Time elapsed: 00:03:04
                               ETA: 06:35:16

################################################################################
                     [1m Learning iteration 77/10000 [0m                      

                       Computation: 51352 steps/s (collection: 1.818s, learning 0.097s)
             Mean action noise std: 1.29
          Mean value_function loss: 5.8844
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.9463
                       Mean reward: 0.81
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.3881
     Episode_Reward/lifting_object: -0.9713
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.91s
                      Time elapsed: 00:03:05
                               ETA: 06:34:13

################################################################################
                     [1m Learning iteration 78/10000 [0m                      

                       Computation: 52289 steps/s (collection: 1.783s, learning 0.097s)
             Mean action noise std: 1.30
          Mean value_function loss: 2.8254
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.0198
                       Mean reward: -6.94
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4045
     Episode_Reward/lifting_object: -0.5694
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.88s
                      Time elapsed: 00:03:07
                               ETA: 06:33:07

################################################################################
                     [1m Learning iteration 79/10000 [0m                      

                       Computation: 53509 steps/s (collection: 1.714s, learning 0.123s)
             Mean action noise std: 1.30
          Mean value_function loss: 2.8237
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.0842
                       Mean reward: 1.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3941
     Episode_Reward/lifting_object: -0.1627
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.84s
                      Time elapsed: 00:03:09
                               ETA: 06:31:58

################################################################################
                     [1m Learning iteration 80/10000 [0m                      

                       Computation: 54236 steps/s (collection: 1.701s, learning 0.112s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.2140
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.1582
                       Mean reward: -1.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3672
     Episode_Reward/lifting_object: -0.3986
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.81s
                      Time elapsed: 00:03:11
                               ETA: 06:30:47

################################################################################
                     [1m Learning iteration 81/10000 [0m                      

                       Computation: 54433 steps/s (collection: 1.697s, learning 0.109s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 52.2181
                       Mean reward: 1.29
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.81s
                      Time elapsed: 00:03:13
                               ETA: 06:29:37

################################################################################
                     [1m Learning iteration 82/10000 [0m                      

                       Computation: 51852 steps/s (collection: 1.768s, learning 0.128s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 52.3161
                       Mean reward: 0.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2322
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.90s
                      Time elapsed: 00:03:15
                               ETA: 06:28:40

################################################################################
                     [1m Learning iteration 83/10000 [0m                      

                       Computation: 52366 steps/s (collection: 1.787s, learning 0.090s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 52.4677
                       Mean reward: 0.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1798
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.88s
                      Time elapsed: 00:03:17
                               ETA: 06:27:42

################################################################################
                     [1m Learning iteration 84/10000 [0m                      

                       Computation: 51717 steps/s (collection: 1.809s, learning 0.092s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 52.5989
                       Mean reward: 0.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1404
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.90s
                      Time elapsed: 00:03:18
                               ETA: 06:26:47

################################################################################
                     [1m Learning iteration 85/10000 [0m                      

                       Computation: 52944 steps/s (collection: 1.746s, learning 0.111s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 52.7255
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.86s
                      Time elapsed: 00:03:20
                               ETA: 06:25:49

################################################################################
                     [1m Learning iteration 86/10000 [0m                      

                       Computation: 50823 steps/s (collection: 1.810s, learning 0.125s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 52.8387
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0778
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.93s
                      Time elapsed: 00:03:22
                               ETA: 06:25:01

################################################################################
                     [1m Learning iteration 87/10000 [0m                      

                       Computation: 52658 steps/s (collection: 1.731s, learning 0.136s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 52.9505
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0611
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.87s
                      Time elapsed: 00:03:24
                               ETA: 06:24:07

################################################################################
                     [1m Learning iteration 88/10000 [0m                      

                       Computation: 52513 steps/s (collection: 1.751s, learning 0.121s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 52.9994
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0492
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.87s
                      Time elapsed: 00:03:26
                               ETA: 06:23:14

################################################################################
                     [1m Learning iteration 89/10000 [0m                      

                       Computation: 52725 steps/s (collection: 1.749s, learning 0.116s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 53.0642
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0411
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.86s
                      Time elapsed: 00:03:28
                               ETA: 06:22:22

################################################################################
                     [1m Learning iteration 90/10000 [0m                      

                       Computation: 50254 steps/s (collection: 1.827s, learning 0.130s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 53.0713
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0320
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.96s
                      Time elapsed: 00:03:30
                               ETA: 06:21:40

################################################################################
                     [1m Learning iteration 91/10000 [0m                      

                       Computation: 51885 steps/s (collection: 1.778s, learning 0.117s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 53.1110
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0285
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.89s
                      Time elapsed: 00:03:32
                               ETA: 06:20:53

################################################################################
                     [1m Learning iteration 92/10000 [0m                      

                       Computation: 52609 steps/s (collection: 1.768s, learning 0.101s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 53.1030
                       Mean reward: -0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0233
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.87s
                      Time elapsed: 00:03:34
                               ETA: 06:20:04

################################################################################
                     [1m Learning iteration 93/10000 [0m                      

                       Computation: 49903 steps/s (collection: 1.856s, learning 0.114s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 53.1310
                       Mean reward: -0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0209
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.97s
                      Time elapsed: 00:03:36
                               ETA: 06:19:27

################################################################################
                     [1m Learning iteration 94/10000 [0m                      

                       Computation: 53776 steps/s (collection: 1.695s, learning 0.133s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 53.1075
                       Mean reward: -0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0183
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.83s
                      Time elapsed: 00:03:37
                               ETA: 06:18:36

################################################################################
                     [1m Learning iteration 95/10000 [0m                      

                       Computation: 52898 steps/s (collection: 1.727s, learning 0.132s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 53.1035
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0161
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.86s
                      Time elapsed: 00:03:39
                               ETA: 06:17:48

################################################################################
                     [1m Learning iteration 96/10000 [0m                      

                       Computation: 52433 steps/s (collection: 1.760s, learning 0.115s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 53.0656
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0156
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.87s
                      Time elapsed: 00:03:41
                               ETA: 06:17:04

################################################################################
                     [1m Learning iteration 97/10000 [0m                      

                       Computation: 52537 steps/s (collection: 1.765s, learning 0.106s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 53.0301
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0154
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.87s
                      Time elapsed: 00:03:43
                               ETA: 06:16:20

################################################################################
                     [1m Learning iteration 98/10000 [0m                      

                       Computation: 50041 steps/s (collection: 1.856s, learning 0.108s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 52.9970
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0146
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.96s
                      Time elapsed: 00:03:45
                               ETA: 06:15:46

################################################################################
                     [1m Learning iteration 99/10000 [0m                      

                       Computation: 52353 steps/s (collection: 1.777s, learning 0.100s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 52.9532
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0143
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.88s
                      Time elapsed: 00:03:47
                               ETA: 06:15:04

################################################################################
                     [1m Learning iteration 100/10000 [0m                     

                       Computation: 52435 steps/s (collection: 1.762s, learning 0.113s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 52.9258
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0145
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.87s
                      Time elapsed: 00:03:49
                               ETA: 06:14:23

################################################################################
                     [1m Learning iteration 101/10000 [0m                     

                       Computation: 51676 steps/s (collection: 1.813s, learning 0.089s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 52.9069
                       Mean reward: -0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0165
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.90s
                      Time elapsed: 00:03:51
                               ETA: 06:13:45

################################################################################
                     [1m Learning iteration 102/10000 [0m                     

                       Computation: 54128 steps/s (collection: 1.685s, learning 0.131s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 52.8978
                       Mean reward: -0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0173
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.82s
                      Time elapsed: 00:03:52
                               ETA: 06:13:00

################################################################################
                     [1m Learning iteration 103/10000 [0m                     

                       Computation: 55002 steps/s (collection: 1.679s, learning 0.109s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 52.8971
                       Mean reward: -0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0181
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.79s
                      Time elapsed: 00:03:54
                               ETA: 06:12:12

################################################################################
                     [1m Learning iteration 104/10000 [0m                     

                       Computation: 55100 steps/s (collection: 1.698s, learning 0.086s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 52.9107
                       Mean reward: -0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0221
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.78s
                      Time elapsed: 00:03:56
                               ETA: 06:11:25

################################################################################
                     [1m Learning iteration 105/10000 [0m                     

                       Computation: 54558 steps/s (collection: 1.692s, learning 0.110s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 52.9287
                       Mean reward: 0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0269
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.80s
                      Time elapsed: 00:03:58
                               ETA: 06:10:41

################################################################################
                     [1m Learning iteration 106/10000 [0m                     

                       Computation: 53292 steps/s (collection: 1.756s, learning 0.089s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 52.9889
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0315
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.84s
                      Time elapsed: 00:04:00
                               ETA: 06:10:02

################################################################################
                     [1m Learning iteration 107/10000 [0m                     

                       Computation: 52472 steps/s (collection: 1.757s, learning 0.116s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 53.0386
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0365
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.87s
                      Time elapsed: 00:04:01
                               ETA: 06:09:25

################################################################################
                     [1m Learning iteration 108/10000 [0m                     

                       Computation: 53037 steps/s (collection: 1.714s, learning 0.139s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 53.0741
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0478
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.85s
                      Time elapsed: 00:04:03
                               ETA: 06:08:48

################################################################################
                     [1m Learning iteration 109/10000 [0m                     

                       Computation: 54598 steps/s (collection: 1.704s, learning 0.096s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 53.1379
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0596
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.80s
                      Time elapsed: 00:04:05
                               ETA: 06:08:07

################################################################################
                     [1m Learning iteration 110/10000 [0m                     

                       Computation: 53656 steps/s (collection: 1.746s, learning 0.086s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 53.1879
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0836
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.83s
                      Time elapsed: 00:04:07
                               ETA: 06:07:29

################################################################################
                     [1m Learning iteration 111/10000 [0m                     

                       Computation: 51108 steps/s (collection: 1.797s, learning 0.126s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.3409
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.2669
                       Mean reward: -0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1044
     Episode_Reward/lifting_object: -0.0694
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.92s
                      Time elapsed: 00:04:09
                               ETA: 06:06:59

################################################################################
                     [1m Learning iteration 112/10000 [0m                     

                       Computation: 52471 steps/s (collection: 1.784s, learning 0.090s)
             Mean action noise std: 1.35
          Mean value_function loss: 2.1874
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.3105
                       Mean reward: 0.66
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.1512
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.87s
                      Time elapsed: 00:04:11
                               ETA: 06:06:26

################################################################################
                     [1m Learning iteration 113/10000 [0m                     

                       Computation: 51360 steps/s (collection: 1.814s, learning 0.100s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.1151
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.4118
                       Mean reward: -1.50
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.1946
     Episode_Reward/lifting_object: -0.3185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.91s
                      Time elapsed: 00:04:13
                               ETA: 06:05:57

################################################################################
                     [1m Learning iteration 114/10000 [0m                     

                       Computation: 51805 steps/s (collection: 1.783s, learning 0.114s)
             Mean action noise std: 1.36
          Mean value_function loss: 3.9575
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.5041
                       Mean reward: 0.34
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.2372
     Episode_Reward/lifting_object: -0.1336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.90s
                      Time elapsed: 00:04:15
                               ETA: 06:05:27

################################################################################
                     [1m Learning iteration 115/10000 [0m                     

                       Computation: 49773 steps/s (collection: 1.868s, learning 0.107s)
             Mean action noise std: 1.37
          Mean value_function loss: 9.1906
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.6147
                       Mean reward: -2.80
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.2913
     Episode_Reward/lifting_object: -0.7962
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.98s
                      Time elapsed: 00:04:17
                               ETA: 06:05:04

################################################################################
                     [1m Learning iteration 116/10000 [0m                     

                       Computation: 50036 steps/s (collection: 1.855s, learning 0.110s)
             Mean action noise std: 1.37
          Mean value_function loss: 4.9667
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.7083
                       Mean reward: -1.14
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.3340
     Episode_Reward/lifting_object: -0.4343
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.96s
                      Time elapsed: 00:04:19
                               ETA: 06:04:41

################################################################################
                     [1m Learning iteration 117/10000 [0m                     

                       Computation: 50841 steps/s (collection: 1.832s, learning 0.101s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.4434
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.7917
                       Mean reward: 1.52
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.3432
     Episode_Reward/lifting_object: -0.2844
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.93s
                      Time elapsed: 00:04:20
                               ETA: 06:04:15

################################################################################
                     [1m Learning iteration 118/10000 [0m                     

                       Computation: 51760 steps/s (collection: 1.771s, learning 0.128s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 53.8747
                       Mean reward: 1.77
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.3803
     Episode_Reward/lifting_object: -0.1642
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.90s
                      Time elapsed: 00:04:22
                               ETA: 06:03:47

################################################################################
                     [1m Learning iteration 119/10000 [0m                     

                       Computation: 53270 steps/s (collection: 1.751s, learning 0.094s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3562
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.9860
                       Mean reward: 1.55
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.3387
     Episode_Reward/lifting_object: -0.0583
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.85s
                      Time elapsed: 00:04:24
                               ETA: 06:03:15

################################################################################
                     [1m Learning iteration 120/10000 [0m                     

                       Computation: 53143 steps/s (collection: 1.756s, learning 0.094s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.0294
                       Mean reward: 1.42
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.3517
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.85s
                      Time elapsed: 00:04:26
                               ETA: 06:02:43

################################################################################
                     [1m Learning iteration 121/10000 [0m                     

                       Computation: 53922 steps/s (collection: 1.723s, learning 0.100s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1066
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 54.1753
                       Mean reward: 1.36
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.3124
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.82s
                      Time elapsed: 00:04:28
                               ETA: 06:02:11

################################################################################
                     [1m Learning iteration 122/10000 [0m                     

                       Computation: 55135 steps/s (collection: 1.692s, learning 0.091s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 54.2160
                       Mean reward: 1.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.78s
                      Time elapsed: 00:04:30
                               ETA: 06:01:35

################################################################################
                     [1m Learning iteration 123/10000 [0m                     

                       Computation: 54195 steps/s (collection: 1.708s, learning 0.106s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 54.2620
                       Mean reward: 1.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2257
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.81s
                      Time elapsed: 00:04:31
                               ETA: 06:01:02

################################################################################
                     [1m Learning iteration 124/10000 [0m                     

                       Computation: 54983 steps/s (collection: 1.691s, learning 0.097s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 54.2916
                       Mean reward: 0.70
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.1816
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.79s
                      Time elapsed: 00:04:33
                               ETA: 06:00:28

################################################################################
                     [1m Learning iteration 125/10000 [0m                     

                       Computation: 54864 steps/s (collection: 1.691s, learning 0.101s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 54.3081
                       Mean reward: 0.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1497
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.79s
                      Time elapsed: 00:04:35
                               ETA: 05:59:55

################################################################################
                     [1m Learning iteration 126/10000 [0m                     

                       Computation: 55177 steps/s (collection: 1.678s, learning 0.104s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 54.3206
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1388
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.78s
                      Time elapsed: 00:04:37
                               ETA: 05:59:21

################################################################################
                     [1m Learning iteration 127/10000 [0m                     

                       Computation: 53287 steps/s (collection: 1.740s, learning 0.105s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1422
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.3514
                       Mean reward: 0.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1324
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.84s
                      Time elapsed: 00:04:39
                               ETA: 05:58:53

################################################################################
                     [1m Learning iteration 128/10000 [0m                     

                       Computation: 52295 steps/s (collection: 1.763s, learning 0.117s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0618
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.3871
                       Mean reward: -0.43
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.1414
     Episode_Reward/lifting_object: -0.0521
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.88s
                      Time elapsed: 00:04:41
                               ETA: 05:58:27

################################################################################
                     [1m Learning iteration 129/10000 [0m                     

                       Computation: 52482 steps/s (collection: 1.785s, learning 0.088s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 54.4734
                       Mean reward: 0.66
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.1454
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.87s
                      Time elapsed: 00:04:42
                               ETA: 05:58:02

################################################################################
                     [1m Learning iteration 130/10000 [0m                     

                       Computation: 51430 steps/s (collection: 1.818s, learning 0.094s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.1867
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.5623
                       Mean reward: 0.60
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.1733
     Episode_Reward/lifting_object: -0.1482
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.91s
                      Time elapsed: 00:04:44
                               ETA: 05:57:40

################################################################################
                     [1m Learning iteration 131/10000 [0m                     

                       Computation: 51777 steps/s (collection: 1.807s, learning 0.092s)
             Mean action noise std: 1.41
          Mean value_function loss: 7.1158
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.6059
                       Mean reward: -0.39
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.2198
     Episode_Reward/lifting_object: -0.2318
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.90s
                      Time elapsed: 00:04:46
                               ETA: 05:57:17

################################################################################
                     [1m Learning iteration 132/10000 [0m                     

                       Computation: 50205 steps/s (collection: 1.872s, learning 0.086s)
             Mean action noise std: 1.42
          Mean value_function loss: 13.2209
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 54.6926
                       Mean reward: -3.53
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.2869
     Episode_Reward/lifting_object: -0.8738
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.96s
                      Time elapsed: 00:04:48
                               ETA: 05:56:59

################################################################################
                     [1m Learning iteration 133/10000 [0m                     

                       Computation: 50378 steps/s (collection: 1.853s, learning 0.099s)
             Mean action noise std: 1.42
          Mean value_function loss: 6.0683
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.7657
                       Mean reward: -2.25
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3219
     Episode_Reward/lifting_object: -1.1362
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.95s
                      Time elapsed: 00:04:50
                               ETA: 05:56:41

################################################################################
                     [1m Learning iteration 134/10000 [0m                     

                       Computation: 51328 steps/s (collection: 1.828s, learning 0.088s)
             Mean action noise std: 1.42
          Mean value_function loss: 7.2451
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.8441
                       Mean reward: -0.88
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 0.3653
     Episode_Reward/lifting_object: -0.8999
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.92s
                      Time elapsed: 00:04:52
                               ETA: 05:56:20

################################################################################
                     [1m Learning iteration 135/10000 [0m                     

                       Computation: 51141 steps/s (collection: 1.812s, learning 0.110s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.3390
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.9307
                       Mean reward: 1.81
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.3885
     Episode_Reward/lifting_object: -0.3235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.92s
                      Time elapsed: 00:04:54
                               ETA: 05:56:00

################################################################################
                     [1m Learning iteration 136/10000 [0m                     

                       Computation: 49494 steps/s (collection: 1.872s, learning 0.114s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.7312
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.0192
                       Mean reward: 0.51
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.3959
     Episode_Reward/lifting_object: -0.4937
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.99s
                      Time elapsed: 00:04:56
                               ETA: 05:55:45

################################################################################
                     [1m Learning iteration 137/10000 [0m                     

                       Computation: 51443 steps/s (collection: 1.786s, learning 0.125s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1306
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.1201
                       Mean reward: 2.02
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.4081
     Episode_Reward/lifting_object: -0.0493
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.91s
                      Time elapsed: 00:04:58
                               ETA: 05:55:25

################################################################################
                     [1m Learning iteration 138/10000 [0m                     

                       Computation: 48541 steps/s (collection: 1.882s, learning 0.143s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0354
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2176
                       Mean reward: 1.37
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.3911
     Episode_Reward/lifting_object: -0.0233
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.03s
                      Time elapsed: 00:05:00
                               ETA: 05:55:13

################################################################################
                     [1m Learning iteration 139/10000 [0m                     

                       Computation: 52891 steps/s (collection: 1.765s, learning 0.094s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.1626
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.3932
                       Mean reward: 1.68
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.3564
     Episode_Reward/lifting_object: -0.0702
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.86s
                      Time elapsed: 00:05:02
                               ETA: 05:54:49

################################################################################
                     [1m Learning iteration 140/10000 [0m                     

                       Computation: 53411 steps/s (collection: 1.723s, learning 0.117s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.4493
                       Mean reward: 1.15
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.84s
                      Time elapsed: 00:05:04
                               ETA: 05:54:25

################################################################################
                     [1m Learning iteration 141/10000 [0m                     

                       Computation: 54742 steps/s (collection: 1.697s, learning 0.099s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 55.5165
                       Mean reward: 1.09
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.2796
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.80s
                      Time elapsed: 00:05:05
                               ETA: 05:53:58

################################################################################
                     [1m Learning iteration 142/10000 [0m                     

                       Computation: 53634 steps/s (collection: 1.735s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 55.5388
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2106
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.83s
                      Time elapsed: 00:05:07
                               ETA: 05:53:33

################################################################################
                     [1m Learning iteration 143/10000 [0m                     

                       Computation: 53978 steps/s (collection: 1.733s, learning 0.088s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 55.5451
                       Mean reward: 0.71
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.1750
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.82s
                      Time elapsed: 00:05:09
                               ETA: 05:53:09

################################################################################
                     [1m Learning iteration 144/10000 [0m                     

                       Computation: 50712 steps/s (collection: 1.847s, learning 0.091s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 55.5518
                       Mean reward: 0.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1525
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.94s
                      Time elapsed: 00:05:11
                               ETA: 05:52:52

################################################################################
                     [1m Learning iteration 145/10000 [0m                     

                       Computation: 54016 steps/s (collection: 1.718s, learning 0.102s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 55.5764
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1455
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.82s
                      Time elapsed: 00:05:13
                               ETA: 05:52:28

################################################################################
                     [1m Learning iteration 146/10000 [0m                     

                       Computation: 51699 steps/s (collection: 1.759s, learning 0.143s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 55.6004
                       Mean reward: 0.64
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.1487
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.90s
                      Time elapsed: 00:05:15
                               ETA: 05:52:09

################################################################################
                     [1m Learning iteration 147/10000 [0m                     

                       Computation: 49735 steps/s (collection: 1.843s, learning 0.134s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.9856
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.6329
                       Mean reward: -1.14
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.1804
     Episode_Reward/lifting_object: -0.0949
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.98s
                      Time elapsed: 00:05:17
                               ETA: 05:51:56

################################################################################
                     [1m Learning iteration 148/10000 [0m                     

                       Computation: 50058 steps/s (collection: 1.866s, learning 0.098s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.7958
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.6767
                       Mean reward: -1.00
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.2253
     Episode_Reward/lifting_object: -0.2787
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.96s
                      Time elapsed: 00:05:19
                               ETA: 05:51:42

################################################################################
                     [1m Learning iteration 149/10000 [0m                     

                       Computation: 50607 steps/s (collection: 1.852s, learning 0.091s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.0605
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.7696
                       Mean reward: -1.86
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.2706
     Episode_Reward/lifting_object: -0.3944
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.94s
                      Time elapsed: 00:05:21
                               ETA: 05:51:27

################################################################################
                     [1m Learning iteration 150/10000 [0m                     

                       Computation: 49959 steps/s (collection: 1.864s, learning 0.104s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.8588
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.8339
                       Mean reward: 0.84
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 0.3211
     Episode_Reward/lifting_object: -0.1712
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.97s
                      Time elapsed: 00:05:23
                               ETA: 05:51:13

################################################################################
                     [1m Learning iteration 151/10000 [0m                     

                       Computation: 47167 steps/s (collection: 1.903s, learning 0.181s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.4240
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.9328
                       Mean reward: 1.10
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 0.3624
     Episode_Reward/lifting_object: -0.2930
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.08s
                      Time elapsed: 00:05:25
                               ETA: 05:51:08

################################################################################
                     [1m Learning iteration 152/10000 [0m                     

                       Computation: 47593 steps/s (collection: 1.945s, learning 0.120s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1405
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.0032
                       Mean reward: 0.70
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.3951
     Episode_Reward/lifting_object: -0.1330
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.07s
                      Time elapsed: 00:05:27
                               ETA: 05:51:01

################################################################################
                     [1m Learning iteration 153/10000 [0m                     

                       Computation: 50501 steps/s (collection: 1.839s, learning 0.108s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1524
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 56.1519
                       Mean reward: 2.15
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 0.4333
     Episode_Reward/lifting_object: -0.0478
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.95s
                      Time elapsed: 00:05:29
                               ETA: 05:50:46

################################################################################
                     [1m Learning iteration 154/10000 [0m                     

                       Computation: 50606 steps/s (collection: 1.848s, learning 0.095s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.9966
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.1970
                       Mean reward: 1.99
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 0.4334
     Episode_Reward/lifting_object: -0.1130
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.94s
                      Time elapsed: 00:05:31
                               ETA: 05:50:32

################################################################################
                     [1m Learning iteration 155/10000 [0m                     

                       Computation: 50366 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.7554
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.2828
                       Mean reward: 1.64
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.4493
     Episode_Reward/lifting_object: -0.1558
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.95s
                      Time elapsed: 00:05:33
                               ETA: 05:50:18

################################################################################
                     [1m Learning iteration 156/10000 [0m                     

                       Computation: 50026 steps/s (collection: 1.862s, learning 0.103s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.5231
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.3552
                       Mean reward: 1.75
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4372
     Episode_Reward/lifting_object: -0.0518
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.97s
                      Time elapsed: 00:05:35
                               ETA: 05:50:05

################################################################################
                     [1m Learning iteration 157/10000 [0m                     

                       Computation: 52526 steps/s (collection: 1.777s, learning 0.095s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2693
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.4468
                       Mean reward: 1.77
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.4216
     Episode_Reward/lifting_object: -0.0908
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.87s
                      Time elapsed: 00:05:36
                               ETA: 05:49:47

################################################################################
                     [1m Learning iteration 158/10000 [0m                     

                       Computation: 52756 steps/s (collection: 1.757s, learning 0.106s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3390
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.5393
                       Mean reward: 1.57
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: -0.0484
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.86s
                      Time elapsed: 00:05:38
                               ETA: 05:49:28

################################################################################
                     [1m Learning iteration 159/10000 [0m                     

                       Computation: 46343 steps/s (collection: 1.902s, learning 0.219s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1883
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.6138
                       Mean reward: 1.68
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3655
     Episode_Reward/lifting_object: -0.0639
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.12s
                      Time elapsed: 00:05:40
                               ETA: 05:49:25

################################################################################
                     [1m Learning iteration 160/10000 [0m                     

                       Computation: 44950 steps/s (collection: 2.083s, learning 0.104s)
             Mean action noise std: 1.51
          Mean value_function loss: 1.6047
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.6980
                       Mean reward: -0.01
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: -0.1057
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.19s
                      Time elapsed: 00:05:43
                               ETA: 05:49:27

################################################################################
                     [1m Learning iteration 161/10000 [0m                     

                       Computation: 49222 steps/s (collection: 1.897s, learning 0.100s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.7815
                       Mean reward: 1.31
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.3264
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.00s
                      Time elapsed: 00:05:45
                               ETA: 05:49:16

################################################################################
                     [1m Learning iteration 162/10000 [0m                     

                       Computation: 51324 steps/s (collection: 1.798s, learning 0.117s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.0273
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.8371
                       Mean reward: 0.60
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.3362
     Episode_Reward/lifting_object: -0.1557
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.92s
                      Time elapsed: 00:05:46
                               ETA: 05:49:01

################################################################################
                     [1m Learning iteration 163/10000 [0m                     

                       Computation: 50095 steps/s (collection: 1.858s, learning 0.105s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0503
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9038
                       Mean reward: 1.34
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.3325
     Episode_Reward/lifting_object: -0.0471
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.96s
                      Time elapsed: 00:05:48
                               ETA: 05:48:49

################################################################################
                     [1m Learning iteration 164/10000 [0m                     

                       Computation: 50633 steps/s (collection: 1.829s, learning 0.113s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.7365
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 57.0418
                       Mean reward: 0.56
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.3391
     Episode_Reward/lifting_object: -0.1072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.94s
                      Time elapsed: 00:05:50
                               ETA: 05:48:36

################################################################################
                     [1m Learning iteration 165/10000 [0m                     

                       Computation: 50455 steps/s (collection: 1.839s, learning 0.109s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0576
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.0876
                       Mean reward: 1.38
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.3316
     Episode_Reward/lifting_object: -0.0402
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.95s
                      Time elapsed: 00:05:52
                               ETA: 05:48:23

################################################################################
                     [1m Learning iteration 166/10000 [0m                     

                       Computation: 49971 steps/s (collection: 1.879s, learning 0.089s)
             Mean action noise std: 1.53
          Mean value_function loss: 1.4679
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.1618
                       Mean reward: 0.60
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: -0.1796
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.97s
                      Time elapsed: 00:05:54
                               ETA: 05:48:12

################################################################################
                     [1m Learning iteration 167/10000 [0m                     

                       Computation: 46253 steps/s (collection: 1.967s, learning 0.158s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0793
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.2111
                       Mean reward: 1.80
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.3723
     Episode_Reward/lifting_object: -0.0235
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.13s
                      Time elapsed: 00:05:56
                               ETA: 05:48:10

################################################################################
                     [1m Learning iteration 168/10000 [0m                     

                       Computation: 51224 steps/s (collection: 1.831s, learning 0.088s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.5427
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.3375
                       Mean reward: 1.28
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.92s
                      Time elapsed: 00:05:58
                               ETA: 05:47:56

################################################################################
                     [1m Learning iteration 169/10000 [0m                     

                       Computation: 47648 steps/s (collection: 1.950s, learning 0.113s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0477
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.3760
                       Mean reward: 1.30
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.3696
     Episode_Reward/lifting_object: -0.0823
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.06s
                      Time elapsed: 00:06:00
                               ETA: 05:47:50

################################################################################
                     [1m Learning iteration 170/10000 [0m                     

                       Computation: 50539 steps/s (collection: 1.853s, learning 0.093s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.3412
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.4521
                       Mean reward: 1.66
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 0.3761
     Episode_Reward/lifting_object: -0.0677
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.95s
                      Time elapsed: 00:06:02
                               ETA: 05:47:38

################################################################################
                     [1m Learning iteration 171/10000 [0m                     

                       Computation: 51591 steps/s (collection: 1.815s, learning 0.091s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1686
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.5079
                       Mean reward: 1.28
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.3806
     Episode_Reward/lifting_object: -0.0333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.91s
                      Time elapsed: 00:06:04
                               ETA: 05:47:23

################################################################################
                     [1m Learning iteration 172/10000 [0m                     

                       Computation: 51421 steps/s (collection: 1.809s, learning 0.103s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2440
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.5846
                       Mean reward: 1.47
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.3609
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.91s
                      Time elapsed: 00:06:06
                               ETA: 05:47:09

################################################################################
                     [1m Learning iteration 173/10000 [0m                     

                       Computation: 50145 steps/s (collection: 1.857s, learning 0.103s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.3543
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.6389
                       Mean reward: 1.65
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3637
     Episode_Reward/lifting_object: -0.1574
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.96s
                      Time elapsed: 00:06:08
                               ETA: 05:46:58

################################################################################
                     [1m Learning iteration 174/10000 [0m                     

                       Computation: 50883 steps/s (collection: 1.816s, learning 0.116s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.4932
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.7049
                       Mean reward: 0.51
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 0.3512
     Episode_Reward/lifting_object: -0.1516
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.93s
                      Time elapsed: 00:06:10
                               ETA: 05:46:45

################################################################################
                     [1m Learning iteration 175/10000 [0m                     

                       Computation: 50688 steps/s (collection: 1.835s, learning 0.105s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.7536
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.7576
                       Mean reward: 0.84
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.3546
     Episode_Reward/lifting_object: -0.1137
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.94s
                      Time elapsed: 00:06:12
                               ETA: 05:46:33

################################################################################
                     [1m Learning iteration 176/10000 [0m                     

                       Computation: 50303 steps/s (collection: 1.814s, learning 0.141s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1411
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.8265
                       Mean reward: 1.56
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: -0.0935
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.95s
                      Time elapsed: 00:06:14
                               ETA: 05:46:22

################################################################################
                     [1m Learning iteration 177/10000 [0m                     

                       Computation: 51918 steps/s (collection: 1.801s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3214
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8886
                       Mean reward: 1.68
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.89s
                      Time elapsed: 00:06:16
                               ETA: 05:46:08

################################################################################
                     [1m Learning iteration 178/10000 [0m                     

                       Computation: 51786 steps/s (collection: 1.797s, learning 0.101s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.4652
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.9740
                       Mean reward: 1.73
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 0.3506
     Episode_Reward/lifting_object: -0.0733
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.90s
                      Time elapsed: 00:06:18
                               ETA: 05:45:54

################################################################################
                     [1m Learning iteration 179/10000 [0m                     

                       Computation: 50732 steps/s (collection: 1.821s, learning 0.117s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.5524
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.0610
                       Mean reward: 0.83
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.3348
     Episode_Reward/lifting_object: -0.1027
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.94s
                      Time elapsed: 00:06:20
                               ETA: 05:45:42

################################################################################
                     [1m Learning iteration 180/10000 [0m                     

                       Computation: 51161 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.4034
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.1235
                       Mean reward: 1.53
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 0.3561
     Episode_Reward/lifting_object: -0.1134
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.92s
                      Time elapsed: 00:06:22
                               ETA: 05:45:30

################################################################################
                     [1m Learning iteration 181/10000 [0m                     

                       Computation: 50532 steps/s (collection: 1.841s, learning 0.104s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.5485
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.1974
                       Mean reward: 0.83
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.3504
     Episode_Reward/lifting_object: -0.0850
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.95s
                      Time elapsed: 00:06:24
                               ETA: 05:45:19

################################################################################
                     [1m Learning iteration 182/10000 [0m                     

                       Computation: 47491 steps/s (collection: 1.930s, learning 0.140s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2629
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.2537
                       Mean reward: 1.09
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 0.3658
     Episode_Reward/lifting_object: -0.0976
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.07s
                      Time elapsed: 00:06:26
                               ETA: 05:45:15

################################################################################
                     [1m Learning iteration 183/10000 [0m                     

                       Computation: 51191 steps/s (collection: 1.826s, learning 0.095s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1245
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.3120
                       Mean reward: 1.54
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.3578
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.92s
                      Time elapsed: 00:06:28
                               ETA: 05:45:02

################################################################################
                     [1m Learning iteration 184/10000 [0m                     

                       Computation: 47525 steps/s (collection: 1.843s, learning 0.225s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.5927
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.3845
                       Mean reward: 0.17
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: -0.1258
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.07s
                      Time elapsed: 00:06:30
                               ETA: 05:44:58

################################################################################
                     [1m Learning iteration 185/10000 [0m                     

                       Computation: 46699 steps/s (collection: 1.989s, learning 0.116s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4174
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.4450
                       Mean reward: 0.93
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.3540
     Episode_Reward/lifting_object: -0.0735
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.11s
                      Time elapsed: 00:06:32
                               ETA: 05:44:56

################################################################################
                     [1m Learning iteration 186/10000 [0m                     

                       Computation: 49721 steps/s (collection: 1.859s, learning 0.118s)
             Mean action noise std: 1.60
          Mean value_function loss: 1.0603
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.5064
                       Mean reward: 0.51
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 0.3977
     Episode_Reward/lifting_object: -0.1250
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.98s
                      Time elapsed: 00:06:34
                               ETA: 05:44:47

################################################################################
                     [1m Learning iteration 187/10000 [0m                     

                       Computation: 50131 steps/s (collection: 1.856s, learning 0.105s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4724
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.5603
                       Mean reward: 1.72
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 0.3937
     Episode_Reward/lifting_object: -0.1738
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.96s
                      Time elapsed: 00:06:36
                               ETA: 05:44:37

################################################################################
                     [1m Learning iteration 188/10000 [0m                     

                       Computation: 49885 steps/s (collection: 1.865s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3351
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.6276
                       Mean reward: 1.27
               Mean episode length: 233.63
    Episode_Reward/reaching_object: 0.4002
     Episode_Reward/lifting_object: -0.0907
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.97s
                      Time elapsed: 00:06:38
                               ETA: 05:44:28

################################################################################
                     [1m Learning iteration 189/10000 [0m                     

                       Computation: 50072 steps/s (collection: 1.860s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.6941
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6843
                       Mean reward: 1.25
               Mean episode length: 227.46
    Episode_Reward/reaching_object: 0.3908
     Episode_Reward/lifting_object: -0.1772
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.96s
                      Time elapsed: 00:06:40
                               ETA: 05:44:18

################################################################################
                     [1m Learning iteration 190/10000 [0m                     

                       Computation: 49248 steps/s (collection: 1.898s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.0462
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.7495
                       Mean reward: 1.85
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 0.4053
     Episode_Reward/lifting_object: -0.1621
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.00s
                      Time elapsed: 00:06:42
                               ETA: 05:44:11

################################################################################
                     [1m Learning iteration 191/10000 [0m                     

                       Computation: 45507 steps/s (collection: 1.988s, learning 0.172s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.6482
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.8068
                       Mean reward: 1.10
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: -0.1730
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.16s
                      Time elapsed: 00:06:44
                               ETA: 05:44:11

################################################################################
                     [1m Learning iteration 192/10000 [0m                     

                       Computation: 45991 steps/s (collection: 2.027s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3643
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.8852
                       Mean reward: 2.05
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 0.4305
     Episode_Reward/lifting_object: -0.0346
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.14s
                      Time elapsed: 00:06:46
                               ETA: 05:44:11

################################################################################
                     [1m Learning iteration 193/10000 [0m                     

                       Computation: 50144 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6107
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.9944
                       Mean reward: 0.93
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 0.4518
     Episode_Reward/lifting_object: -0.0917
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.96s
                      Time elapsed: 00:06:48
                               ETA: 05:44:01

################################################################################
                     [1m Learning iteration 194/10000 [0m                     

                       Computation: 49717 steps/s (collection: 1.864s, learning 0.114s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.5419
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.0378
                       Mean reward: 1.66
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.4565
     Episode_Reward/lifting_object: -0.0593
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.98s
                      Time elapsed: 00:06:50
                               ETA: 05:43:53

################################################################################
                     [1m Learning iteration 195/10000 [0m                     

                       Computation: 50341 steps/s (collection: 1.854s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.7307
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.1132
                       Mean reward: 0.77
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.4450
     Episode_Reward/lifting_object: -0.1668
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.95s
                      Time elapsed: 00:06:52
                               ETA: 05:43:43

################################################################################
                     [1m Learning iteration 196/10000 [0m                     

                       Computation: 50281 steps/s (collection: 1.861s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.6429
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.1723
                       Mean reward: 1.23
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 0.4508
     Episode_Reward/lifting_object: -0.2002
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.96s
                      Time elapsed: 00:06:54
                               ETA: 05:43:34

################################################################################
                     [1m Learning iteration 197/10000 [0m                     

                       Computation: 47417 steps/s (collection: 1.981s, learning 0.093s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.0448
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.2316
                       Mean reward: 1.90
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 0.4383
     Episode_Reward/lifting_object: -0.0477
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.07s
                      Time elapsed: 00:06:56
                               ETA: 05:43:30

################################################################################
                     [1m Learning iteration 198/10000 [0m                     

                       Computation: 51798 steps/s (collection: 1.808s, learning 0.090s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.2542
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.3287
                       Mean reward: 1.48
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 0.4317
     Episode_Reward/lifting_object: -0.0418
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.90s
                      Time elapsed: 00:06:58
                               ETA: 05:43:18

################################################################################
                     [1m Learning iteration 199/10000 [0m                     

                       Computation: 49053 steps/s (collection: 1.887s, learning 0.117s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.1353
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.3604
                       Mean reward: 1.65
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.4272
     Episode_Reward/lifting_object: -0.0570
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.00s
                      Time elapsed: 00:07:00
                               ETA: 05:43:11

################################################################################
                     [1m Learning iteration 200/10000 [0m                     

                       Computation: 49632 steps/s (collection: 1.887s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.8997
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.4142
                       Mean reward: 0.27
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 0.4193
     Episode_Reward/lifting_object: -0.2172
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.98s
                      Time elapsed: 00:07:02
                               ETA: 05:43:03

################################################################################
                     [1m Learning iteration 201/10000 [0m                     

                       Computation: 49727 steps/s (collection: 1.889s, learning 0.088s)
             Mean action noise std: 1.65
          Mean value_function loss: 2.3104
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.4581
                       Mean reward: -0.79
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 0.4309
     Episode_Reward/lifting_object: -0.1604
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.98s
                      Time elapsed: 00:07:04
                               ETA: 05:42:55

################################################################################
                     [1m Learning iteration 202/10000 [0m                     

                       Computation: 50989 steps/s (collection: 1.825s, learning 0.103s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3157
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.5161
                       Mean reward: 1.29
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.4243
     Episode_Reward/lifting_object: -0.0996
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.93s
                      Time elapsed: 00:07:06
                               ETA: 05:42:45

################################################################################
                     [1m Learning iteration 203/10000 [0m                     

                       Computation: 50025 steps/s (collection: 1.847s, learning 0.118s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4252
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.5859
                       Mean reward: 1.13
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 0.4445
     Episode_Reward/lifting_object: -0.0810
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.97s
                      Time elapsed: 00:07:08
                               ETA: 05:42:36

################################################################################
                     [1m Learning iteration 204/10000 [0m                     

                       Computation: 50198 steps/s (collection: 1.835s, learning 0.124s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4988
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6421
                       Mean reward: 1.87
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 0.4298
     Episode_Reward/lifting_object: -0.1255
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.96s
                      Time elapsed: 00:07:10
                               ETA: 05:42:27

################################################################################
                     [1m Learning iteration 205/10000 [0m                     

                       Computation: 49544 steps/s (collection: 1.818s, learning 0.167s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.9955
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6990
                       Mean reward: 2.14
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 0.4487
     Episode_Reward/lifting_object: -0.0769
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.98s
                      Time elapsed: 00:07:11
                               ETA: 05:42:20

################################################################################
                     [1m Learning iteration 206/10000 [0m                     

                       Computation: 49657 steps/s (collection: 1.801s, learning 0.179s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.3155
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.7486
                       Mean reward: 0.54
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: -0.1891
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.98s
                      Time elapsed: 00:07:13
                               ETA: 05:42:12

################################################################################
                     [1m Learning iteration 207/10000 [0m                     

                       Computation: 50101 steps/s (collection: 1.848s, learning 0.115s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.7218
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.8018
                       Mean reward: 1.57
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 0.4175
     Episode_Reward/lifting_object: -0.0902
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.96s
                      Time elapsed: 00:07:15
                               ETA: 05:42:04

################################################################################
                     [1m Learning iteration 208/10000 [0m                     

                       Computation: 49940 steps/s (collection: 1.841s, learning 0.127s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.1094
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.8436
                       Mean reward: 1.22
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 0.4367
     Episode_Reward/lifting_object: -0.0770
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.97s
                      Time elapsed: 00:07:17
                               ETA: 05:41:56

################################################################################
                     [1m Learning iteration 209/10000 [0m                     

                       Computation: 49999 steps/s (collection: 1.864s, learning 0.102s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.1917
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.9281
                       Mean reward: 1.80
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 0.4289
     Episode_Reward/lifting_object: -0.0816
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.97s
                      Time elapsed: 00:07:19
                               ETA: 05:41:47

################################################################################
                     [1m Learning iteration 210/10000 [0m                     

                       Computation: 50617 steps/s (collection: 1.832s, learning 0.110s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.8472
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.9829
                       Mean reward: 1.84
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 0.4584
     Episode_Reward/lifting_object: -0.1156
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.94s
                      Time elapsed: 00:07:21
                               ETA: 05:41:38

################################################################################
                     [1m Learning iteration 211/10000 [0m                     

                       Computation: 49682 steps/s (collection: 1.862s, learning 0.117s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2592
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.0227
                       Mean reward: 1.84
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 0.4240
     Episode_Reward/lifting_object: -0.0406
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.98s
                      Time elapsed: 00:07:23
                               ETA: 05:41:31

################################################################################
                     [1m Learning iteration 212/10000 [0m                     

                       Computation: 50632 steps/s (collection: 1.846s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.0569
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 60.0933
                       Mean reward: 2.10
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 0.4469
     Episode_Reward/lifting_object: -0.0192
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.94s
                      Time elapsed: 00:07:25
                               ETA: 05:41:22

################################################################################
                     [1m Learning iteration 213/10000 [0m                     

                       Computation: 50183 steps/s (collection: 1.856s, learning 0.103s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1797
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.2060
                       Mean reward: 2.00
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 0.4456
     Episode_Reward/lifting_object: -0.0292
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.96s
                      Time elapsed: 00:07:27
                               ETA: 05:41:14

################################################################################
                     [1m Learning iteration 214/10000 [0m                     

                       Computation: 50485 steps/s (collection: 1.850s, learning 0.098s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.1684
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.2428
                       Mean reward: 1.89
               Mean episode length: 216.60
    Episode_Reward/reaching_object: 0.4357
     Episode_Reward/lifting_object: -0.1478
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.95s
                      Time elapsed: 00:07:29
                               ETA: 05:41:05

################################################################################
                     [1m Learning iteration 215/10000 [0m                     

                       Computation: 51528 steps/s (collection: 1.814s, learning 0.094s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.5489
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.2947
                       Mean reward: 0.14
               Mean episode length: 220.67
    Episode_Reward/reaching_object: 0.4481
     Episode_Reward/lifting_object: -0.2391
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.91s
                      Time elapsed: 00:07:31
                               ETA: 05:40:55

################################################################################
                     [1m Learning iteration 216/10000 [0m                     

                       Computation: 51244 steps/s (collection: 1.826s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.1257
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.3504
                       Mean reward: 2.05
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 0.4728
     Episode_Reward/lifting_object: -0.0596
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.92s
                      Time elapsed: 00:07:33
                               ETA: 05:40:45

################################################################################
                     [1m Learning iteration 217/10000 [0m                     

                       Computation: 51139 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.4991
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.4314
                       Mean reward: 1.23
               Mean episode length: 221.19
    Episode_Reward/reaching_object: 0.4407
     Episode_Reward/lifting_object: -0.0508
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.92s
                      Time elapsed: 00:07:35
                               ETA: 05:40:35

################################################################################
                     [1m Learning iteration 218/10000 [0m                     

                       Computation: 50575 steps/s (collection: 1.836s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.1969
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.4685
                       Mean reward: 1.21
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 0.4641
     Episode_Reward/lifting_object: -0.0608
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.94s
                      Time elapsed: 00:07:37
                               ETA: 05:40:27

################################################################################
                     [1m Learning iteration 219/10000 [0m                     

                       Computation: 50325 steps/s (collection: 1.839s, learning 0.114s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1291
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.5800
                       Mean reward: 1.73
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 0.4635
     Episode_Reward/lifting_object: -0.0379
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.95s
                      Time elapsed: 00:07:39
                               ETA: 05:40:18

################################################################################
                     [1m Learning iteration 220/10000 [0m                     

                       Computation: 49561 steps/s (collection: 1.862s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.3757
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.7364
                       Mean reward: 1.12
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 0.4774
     Episode_Reward/lifting_object: -0.0427
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.98s
                      Time elapsed: 00:07:41
                               ETA: 05:40:12

################################################################################
                     [1m Learning iteration 221/10000 [0m                     

                       Computation: 50059 steps/s (collection: 1.845s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.5405
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.7698
                       Mean reward: 1.87
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 0.4556
     Episode_Reward/lifting_object: -0.0572
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.96s
                      Time elapsed: 00:07:43
                               ETA: 05:40:04

################################################################################
                     [1m Learning iteration 222/10000 [0m                     

                       Computation: 49821 steps/s (collection: 1.845s, learning 0.128s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.0948
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.8187
                       Mean reward: 1.70
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 0.4822
     Episode_Reward/lifting_object: -0.0966
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.97s
                      Time elapsed: 00:07:45
                               ETA: 05:39:57

################################################################################
                     [1m Learning iteration 223/10000 [0m                     

                       Computation: 50494 steps/s (collection: 1.796s, learning 0.151s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.0719
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.9196
                       Mean reward: 2.20
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 0.4912
     Episode_Reward/lifting_object: -0.0294
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.95s
                      Time elapsed: 00:07:47
                               ETA: 05:39:49

################################################################################
                     [1m Learning iteration 224/10000 [0m                     

                       Computation: 51047 steps/s (collection: 1.820s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.1121
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.9899
                       Mean reward: 2.19
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 0.4910
     Episode_Reward/lifting_object: -0.0215
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.93s
                      Time elapsed: 00:07:49
                               ETA: 05:39:40

################################################################################
                     [1m Learning iteration 225/10000 [0m                     

                       Computation: 50062 steps/s (collection: 1.857s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.7164
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.0534
                       Mean reward: 1.30
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 0.4668
     Episode_Reward/lifting_object: -0.1625
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.96s
                      Time elapsed: 00:07:51
                               ETA: 05:39:33

################################################################################
                     [1m Learning iteration 226/10000 [0m                     

                       Computation: 50077 steps/s (collection: 1.839s, learning 0.124s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.0806
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.0867
                       Mean reward: 2.37
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 0.4920
     Episode_Reward/lifting_object: -0.0658
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.96s
                      Time elapsed: 00:07:52
                               ETA: 05:39:25

################################################################################
                     [1m Learning iteration 227/10000 [0m                     

                       Computation: 50291 steps/s (collection: 1.848s, learning 0.107s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.4994
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.1798
                       Mean reward: 2.33
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 0.4904
     Episode_Reward/lifting_object: -0.1091
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.95s
                      Time elapsed: 00:07:54
                               ETA: 05:39:18

################################################################################
                     [1m Learning iteration 228/10000 [0m                     

                       Computation: 50888 steps/s (collection: 1.827s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.1295
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.2144
                       Mean reward: 2.20
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: -0.0494
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.93s
                      Time elapsed: 00:07:56
                               ETA: 05:39:09

################################################################################
                     [1m Learning iteration 229/10000 [0m                     

                       Computation: 50878 steps/s (collection: 1.833s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.4333
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.3033
                       Mean reward: 2.05
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 0.5308
     Episode_Reward/lifting_object: -0.0917
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.93s
                      Time elapsed: 00:07:58
                               ETA: 05:39:01

################################################################################
                     [1m Learning iteration 230/10000 [0m                     

                       Computation: 49274 steps/s (collection: 1.848s, learning 0.147s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.0626
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.3293
                       Mean reward: 2.28
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 0.5283
     Episode_Reward/lifting_object: -0.0376
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.00s
                      Time elapsed: 00:08:00
                               ETA: 05:38:55

################################################################################
                     [1m Learning iteration 231/10000 [0m                     

                       Computation: 50710 steps/s (collection: 1.842s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.0701
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 61.4079
                       Mean reward: 1.75
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 0.5378
     Episode_Reward/lifting_object: -0.0490
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.94s
                      Time elapsed: 00:08:02
                               ETA: 05:38:47

################################################################################
                     [1m Learning iteration 232/10000 [0m                     

                       Computation: 50693 steps/s (collection: 1.843s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.8674
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 61.5283
                       Mean reward: 1.27
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 0.5292
     Episode_Reward/lifting_object: -0.0897
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.94s
                      Time elapsed: 00:08:04
                               ETA: 05:38:39

################################################################################
                     [1m Learning iteration 233/10000 [0m                     

                       Computation: 51500 steps/s (collection: 1.821s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3992
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.5565
                       Mean reward: 2.42
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 0.5548
     Episode_Reward/lifting_object: -0.0581
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.91s
                      Time elapsed: 00:08:06
                               ETA: 05:38:30

################################################################################
                     [1m Learning iteration 234/10000 [0m                     

                       Computation: 47718 steps/s (collection: 1.963s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 2.4219
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.5983
                       Mean reward: -0.33
               Mean episode length: 216.96
    Episode_Reward/reaching_object: 0.5334
     Episode_Reward/lifting_object: -0.1895
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.06s
                      Time elapsed: 00:08:08
                               ETA: 05:38:27

################################################################################
                     [1m Learning iteration 235/10000 [0m                     

                       Computation: 50661 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.4596
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6347
                       Mean reward: 2.41
               Mean episode length: 216.15
    Episode_Reward/reaching_object: 0.5427
     Episode_Reward/lifting_object: -0.0549
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.94s
                      Time elapsed: 00:08:10
                               ETA: 05:38:19

################################################################################
                     [1m Learning iteration 236/10000 [0m                     

                       Computation: 49625 steps/s (collection: 1.886s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.2598
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.6854
                       Mean reward: 2.24
               Mean episode length: 219.25
    Episode_Reward/reaching_object: 0.5242
     Episode_Reward/lifting_object: -0.1181
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.98s
                      Time elapsed: 00:08:12
                               ETA: 05:38:13

################################################################################
                     [1m Learning iteration 237/10000 [0m                     

                       Computation: 50692 steps/s (collection: 1.840s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4034
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.7221
                       Mean reward: 1.92
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 0.5405
     Episode_Reward/lifting_object: -0.1622
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.94s
                      Time elapsed: 00:08:14
                               ETA: 05:38:05

################################################################################
                     [1m Learning iteration 238/10000 [0m                     

                       Computation: 51396 steps/s (collection: 1.810s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.3363
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.7839
                       Mean reward: 1.59
               Mean episode length: 223.18
    Episode_Reward/reaching_object: 0.5552
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.91s
                      Time elapsed: 00:08:16
                               ETA: 05:37:56

################################################################################
                     [1m Learning iteration 239/10000 [0m                     

                       Computation: 50592 steps/s (collection: 1.834s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.2120
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.8979
                       Mean reward: 2.61
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 0.5773
     Episode_Reward/lifting_object: -0.0513
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.94s
                      Time elapsed: 00:08:18
                               ETA: 05:37:49

################################################################################
                     [1m Learning iteration 240/10000 [0m                     

                       Computation: 50007 steps/s (collection: 1.831s, learning 0.135s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.2772
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.9501
                       Mean reward: 2.50
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: -0.0042
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.97s
                      Time elapsed: 00:08:20
                               ETA: 05:37:42

################################################################################
                     [1m Learning iteration 241/10000 [0m                     

                       Computation: 49647 steps/s (collection: 1.867s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.8841
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.9917
                       Mean reward: 0.26
               Mean episode length: 219.99
    Episode_Reward/reaching_object: 0.5607
     Episode_Reward/lifting_object: -0.1674
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.98s
                      Time elapsed: 00:08:22
                               ETA: 05:37:36

################################################################################
                     [1m Learning iteration 242/10000 [0m                     

                       Computation: 49902 steps/s (collection: 1.831s, learning 0.139s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.2892
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.0240
                       Mean reward: 1.98
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 0.5803
     Episode_Reward/lifting_object: -0.0466
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.97s
                      Time elapsed: 00:08:24
                               ETA: 05:37:30

################################################################################
                     [1m Learning iteration 243/10000 [0m                     

                       Computation: 50327 steps/s (collection: 1.857s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.1223
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.1314
                       Mean reward: 2.43
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 0.5794
     Episode_Reward/lifting_object: -0.0439
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.95s
                      Time elapsed: 00:08:26
                               ETA: 05:37:23

################################################################################
                     [1m Learning iteration 244/10000 [0m                     

                       Computation: 49778 steps/s (collection: 1.884s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.0361
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.2416
                       Mean reward: 1.82
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 0.5865
     Episode_Reward/lifting_object: -0.0938
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.97s
                      Time elapsed: 00:08:28
                               ETA: 05:37:17

################################################################################
                     [1m Learning iteration 245/10000 [0m                     

                       Computation: 51098 steps/s (collection: 1.832s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.2314
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.2730
                       Mean reward: 2.27
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 0.5759
     Episode_Reward/lifting_object: -0.1014
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.92s
                      Time elapsed: 00:08:30
                               ETA: 05:37:09

################################################################################
                     [1m Learning iteration 246/10000 [0m                     

                       Computation: 51250 steps/s (collection: 1.827s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.7679
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.3604
                       Mean reward: 2.53
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 0.5925
     Episode_Reward/lifting_object: -0.0375
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.92s
                      Time elapsed: 00:08:32
                               ETA: 05:37:01

################################################################################
                     [1m Learning iteration 247/10000 [0m                     

                       Computation: 51275 steps/s (collection: 1.814s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.0667
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.3884
                       Mean reward: 2.94
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 0.6154
     Episode_Reward/lifting_object: -0.0725
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.92s
                      Time elapsed: 00:08:33
                               ETA: 05:36:52

################################################################################
                     [1m Learning iteration 248/10000 [0m                     

                       Computation: 51200 steps/s (collection: 1.820s, learning 0.100s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.2620
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.4464
                       Mean reward: 3.09
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.6398
     Episode_Reward/lifting_object: -0.0519
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.92s
                      Time elapsed: 00:08:35
                               ETA: 05:36:44

################################################################################
                     [1m Learning iteration 249/10000 [0m                     

                       Computation: 50506 steps/s (collection: 1.833s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4972
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.4844
                       Mean reward: 2.32
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: -0.0693
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.95s
                      Time elapsed: 00:08:37
                               ETA: 05:36:37

################################################################################
                     [1m Learning iteration 250/10000 [0m                     

                       Computation: 49992 steps/s (collection: 1.851s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.5568
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.5146
                       Mean reward: 1.97
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: -0.1185
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.97s
                      Time elapsed: 00:08:39
                               ETA: 05:36:31

################################################################################
                     [1m Learning iteration 251/10000 [0m                     

                       Computation: 49611 steps/s (collection: 1.873s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.5046
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.5852
                       Mean reward: 2.96
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 0.6361
     Episode_Reward/lifting_object: -0.0678
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.98s
                      Time elapsed: 00:08:41
                               ETA: 05:36:26

################################################################################
                     [1m Learning iteration 252/10000 [0m                     

                       Computation: 51060 steps/s (collection: 1.832s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.2888
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.6110
                       Mean reward: 2.02
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 0.6462
     Episode_Reward/lifting_object: -0.2848
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.93s
                      Time elapsed: 00:08:43
                               ETA: 05:36:18

################################################################################
                     [1m Learning iteration 253/10000 [0m                     

                       Computation: 51971 steps/s (collection: 1.797s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.2478
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.6432
                       Mean reward: 2.35
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 0.6797
     Episode_Reward/lifting_object: -0.1537
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.89s
                      Time elapsed: 00:08:45
                               ETA: 05:36:09

################################################################################
                     [1m Learning iteration 254/10000 [0m                     

                       Computation: 50839 steps/s (collection: 1.830s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.0311
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.7160
                       Mean reward: 2.67
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 0.7108
     Episode_Reward/lifting_object: -0.0564
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.93s
                      Time elapsed: 00:08:47
                               ETA: 05:36:02

################################################################################
                     [1m Learning iteration 255/10000 [0m                     

                       Computation: 49791 steps/s (collection: 1.868s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.0373
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 62.7479
                       Mean reward: 3.22
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: -0.0176
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.97s
                      Time elapsed: 00:08:49
                               ETA: 05:35:56

################################################################################
                     [1m Learning iteration 256/10000 [0m                     

                       Computation: 50398 steps/s (collection: 1.848s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.4850
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.7749
                       Mean reward: 3.11
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 0.6860
     Episode_Reward/lifting_object: -0.0979
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.95s
                      Time elapsed: 00:08:51
                               ETA: 05:35:50

################################################################################
                     [1m Learning iteration 257/10000 [0m                     

                       Computation: 50773 steps/s (collection: 1.828s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.8898
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.7964
                       Mean reward: 3.47
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 0.7329
     Episode_Reward/lifting_object: -0.1564
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.94s
                      Time elapsed: 00:08:53
                               ETA: 05:35:43

################################################################################
                     [1m Learning iteration 258/10000 [0m                     

                       Computation: 51272 steps/s (collection: 1.808s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.6276
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.8317
                       Mean reward: 3.18
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 0.7177
     Episode_Reward/lifting_object: -0.3204
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.92s
                      Time elapsed: 00:08:55
                               ETA: 05:35:35

################################################################################
                     [1m Learning iteration 259/10000 [0m                     

                       Computation: 52290 steps/s (collection: 1.780s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.1631
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.9253
                       Mean reward: 2.77
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: -0.0571
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.88s
                      Time elapsed: 00:08:57
                               ETA: 05:35:26

################################################################################
                     [1m Learning iteration 260/10000 [0m                     

                       Computation: 51849 steps/s (collection: 1.810s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.9208
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.9684
                       Mean reward: 3.34
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.90s
                      Time elapsed: 00:08:59
                               ETA: 05:35:17

################################################################################
                     [1m Learning iteration 261/10000 [0m                     

                       Computation: 51758 steps/s (collection: 1.805s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4405
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.9916
                       Mean reward: 2.72
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 0.7112
     Episode_Reward/lifting_object: -0.0720
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.90s
                      Time elapsed: 00:09:00
                               ETA: 05:35:09

################################################################################
                     [1m Learning iteration 262/10000 [0m                     

                       Computation: 51417 steps/s (collection: 1.809s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.8813
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.0563
                       Mean reward: 3.42
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: -0.1488
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 1.91s
                      Time elapsed: 00:09:02
                               ETA: 05:35:01

################################################################################
                     [1m Learning iteration 263/10000 [0m                     

                       Computation: 50561 steps/s (collection: 1.835s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.3656
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.0831
                       Mean reward: 3.21
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: -0.0776
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.94s
                      Time elapsed: 00:09:04
                               ETA: 05:34:55

################################################################################
                     [1m Learning iteration 264/10000 [0m                     

                       Computation: 50950 steps/s (collection: 1.835s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.5186
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.1486
                       Mean reward: 2.80
               Mean episode length: 219.55
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: -0.2147
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.93s
                      Time elapsed: 00:09:06
                               ETA: 05:34:48

################################################################################
                     [1m Learning iteration 265/10000 [0m                     

                       Computation: 50438 steps/s (collection: 1.846s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.6483
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.1747
                       Mean reward: 2.29
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: -0.1961
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 1.95s
                      Time elapsed: 00:09:08
                               ETA: 05:34:42

################################################################################
                     [1m Learning iteration 266/10000 [0m                     

                       Computation: 50112 steps/s (collection: 1.854s, learning 0.108s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.8316
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.2405
                       Mean reward: 2.43
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: -0.1765
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.96s
                      Time elapsed: 00:09:10
                               ETA: 05:34:36

################################################################################
                     [1m Learning iteration 267/10000 [0m                     

                       Computation: 50840 steps/s (collection: 1.824s, learning 0.109s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.4166
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.2665
                       Mean reward: 3.53
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.7971
     Episode_Reward/lifting_object: -0.0816
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.93s
                      Time elapsed: 00:09:12
                               ETA: 05:34:29

################################################################################
                     [1m Learning iteration 268/10000 [0m                     

                       Computation: 51446 steps/s (collection: 1.819s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 63.3334
                       Mean reward: 2.97
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: -0.0512
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 1.91s
                      Time elapsed: 00:09:14
                               ETA: 05:34:22

################################################################################
                     [1m Learning iteration 269/10000 [0m                     

                       Computation: 51246 steps/s (collection: 1.824s, learning 0.095s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.7319
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.3883
                       Mean reward: 2.55
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 0.8283
     Episode_Reward/lifting_object: -0.0999
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.92s
                      Time elapsed: 00:09:16
                               ETA: 05:34:14

################################################################################
                     [1m Learning iteration 270/10000 [0m                     

                       Computation: 50974 steps/s (collection: 1.827s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.9980
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.4083
                       Mean reward: 3.61
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 0.8548
     Episode_Reward/lifting_object: -0.0435
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 1.93s
                      Time elapsed: 00:09:18
                               ETA: 05:34:08

################################################################################
                     [1m Learning iteration 271/10000 [0m                     

                       Computation: 50709 steps/s (collection: 1.822s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.3115
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.4456
                       Mean reward: 2.07
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 0.8417
     Episode_Reward/lifting_object: -0.1021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.94s
                      Time elapsed: 00:09:20
                               ETA: 05:34:01

################################################################################
                     [1m Learning iteration 272/10000 [0m                     

                       Computation: 49925 steps/s (collection: 1.860s, learning 0.109s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.1882
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.5203
                       Mean reward: 3.47
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: -0.0234
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.97s
                      Time elapsed: 00:09:22
                               ETA: 05:33:56

################################################################################
                     [1m Learning iteration 273/10000 [0m                     

                       Computation: 49295 steps/s (collection: 1.878s, learning 0.117s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.0097
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.5646
                       Mean reward: 3.95
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.8402
     Episode_Reward/lifting_object: -0.0830
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.99s
                      Time elapsed: 00:09:24
                               ETA: 05:33:51

################################################################################
                     [1m Learning iteration 274/10000 [0m                     

                       Computation: 49159 steps/s (collection: 1.884s, learning 0.116s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.0788
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.5948
                       Mean reward: 2.79
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 0.8103
     Episode_Reward/lifting_object: -0.1854
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.00s
                      Time elapsed: 00:09:26
                               ETA: 05:33:47

################################################################################
                     [1m Learning iteration 275/10000 [0m                     

                       Computation: 48013 steps/s (collection: 1.932s, learning 0.116s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.5946
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.6217
                       Mean reward: 3.46
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: -0.1745
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.05s
                      Time elapsed: 00:09:28
                               ETA: 05:33:45

################################################################################
                     [1m Learning iteration 276/10000 [0m                     

                       Computation: 49955 steps/s (collection: 1.853s, learning 0.115s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.2081
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.6670
                       Mean reward: 3.80
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: -0.1094
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.97s
                      Time elapsed: 00:09:30
                               ETA: 05:33:40

################################################################################
                     [1m Learning iteration 277/10000 [0m                     

                       Computation: 50933 steps/s (collection: 1.840s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.0818
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.7541
                       Mean reward: 2.96
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.8486
     Episode_Reward/lifting_object: -0.1086
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.93s
                      Time elapsed: 00:09:32
                               ETA: 05:33:33

################################################################################
                     [1m Learning iteration 278/10000 [0m                     

                       Computation: 50696 steps/s (collection: 1.825s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.1234
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.8473
                       Mean reward: 2.85
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 0.8601
     Episode_Reward/lifting_object: -0.1026
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.94s
                      Time elapsed: 00:09:34
                               ETA: 05:33:27

################################################################################
                     [1m Learning iteration 279/10000 [0m                     

                       Computation: 49544 steps/s (collection: 1.864s, learning 0.120s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.5163
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.8987
                       Mean reward: 4.12
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 0.8839
     Episode_Reward/lifting_object: -0.0999
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.98s
                      Time elapsed: 00:09:36
                               ETA: 05:33:22

################################################################################
                     [1m Learning iteration 280/10000 [0m                     

                       Computation: 49159 steps/s (collection: 1.879s, learning 0.121s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.1842
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.9396
                       Mean reward: 3.56
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 0.8695
     Episode_Reward/lifting_object: -0.1309
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.00s
                      Time elapsed: 00:09:38
                               ETA: 05:33:18

################################################################################
                     [1m Learning iteration 281/10000 [0m                     

                       Computation: 49574 steps/s (collection: 1.868s, learning 0.115s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.5479
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.9627
                       Mean reward: 3.23
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.8664
     Episode_Reward/lifting_object: -0.2110
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.98s
                      Time elapsed: 00:09:40
                               ETA: 05:33:13

################################################################################
                     [1m Learning iteration 282/10000 [0m                     

                       Computation: 49471 steps/s (collection: 1.881s, learning 0.106s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.1571
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.0280
                       Mean reward: 4.48
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.9456
     Episode_Reward/lifting_object: -0.0638
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 1.99s
                      Time elapsed: 00:09:42
                               ETA: 05:33:09

################################################################################
                     [1m Learning iteration 283/10000 [0m                     

                       Computation: 48982 steps/s (collection: 1.897s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.7610
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.0832
                       Mean reward: 4.66
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.9692
     Episode_Reward/lifting_object: -0.1065
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.01s
                      Time elapsed: 00:09:44
                               ETA: 05:33:05

################################################################################
                     [1m Learning iteration 284/10000 [0m                     

                       Computation: 50434 steps/s (collection: 1.827s, learning 0.123s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.3054
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.1059
                       Mean reward: 2.43
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.9214
     Episode_Reward/lifting_object: -0.2091
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.95s
                      Time elapsed: 00:09:46
                               ETA: 05:33:00

################################################################################
                     [1m Learning iteration 285/10000 [0m                     

                       Computation: 50651 steps/s (collection: 1.831s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.0367
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.1673
                       Mean reward: 4.16
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.9136
     Episode_Reward/lifting_object: -0.0617
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 1.94s
                      Time elapsed: 00:09:48
                               ETA: 05:32:54

################################################################################
                     [1m Learning iteration 286/10000 [0m                     

                       Computation: 51374 steps/s (collection: 1.806s, learning 0.107s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.2763
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.2277
                       Mean reward: 3.95
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 0.9314
     Episode_Reward/lifting_object: -0.0765
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.91s
                      Time elapsed: 00:09:49
                               ETA: 05:32:47

################################################################################
                     [1m Learning iteration 287/10000 [0m                     

                       Computation: 50145 steps/s (collection: 1.843s, learning 0.117s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4654
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.2441
                       Mean reward: 4.17
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 0.9591
     Episode_Reward/lifting_object: -0.0997
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.96s
                      Time elapsed: 00:09:51
                               ETA: 05:32:41

################################################################################
                     [1m Learning iteration 288/10000 [0m                     

                       Computation: 49218 steps/s (collection: 1.874s, learning 0.123s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.6750
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.2844
                       Mean reward: 2.83
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 0.9094
     Episode_Reward/lifting_object: -0.2570
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.00s
                      Time elapsed: 00:09:53
                               ETA: 05:32:37

################################################################################
                     [1m Learning iteration 289/10000 [0m                     

                       Computation: 51423 steps/s (collection: 1.808s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4796
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.3017
                       Mean reward: 4.43
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.9419
     Episode_Reward/lifting_object: -0.0450
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.91s
                      Time elapsed: 00:09:55
                               ETA: 05:32:31

################################################################################
                     [1m Learning iteration 290/10000 [0m                     

                       Computation: 51674 steps/s (collection: 1.797s, learning 0.106s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.6737
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 64.3417
                       Mean reward: 4.06
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.9888
     Episode_Reward/lifting_object: -0.0638
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.90s
                      Time elapsed: 00:09:57
                               ETA: 05:32:23

################################################################################
                     [1m Learning iteration 291/10000 [0m                     

                       Computation: 52410 steps/s (collection: 1.785s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.3531
                       Mean reward: 4.86
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.9825
     Episode_Reward/lifting_object: -0.0102
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.88s
                      Time elapsed: 00:09:59
                               ETA: 05:32:15

################################################################################
                     [1m Learning iteration 292/10000 [0m                     

                       Computation: 52113 steps/s (collection: 1.790s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.3149
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.3764
                       Mean reward: 4.04
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: -0.0931
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.89s
                      Time elapsed: 00:10:01
                               ETA: 05:32:08

################################################################################
                     [1m Learning iteration 293/10000 [0m                     

                       Computation: 50751 steps/s (collection: 1.850s, learning 0.087s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.8418
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.4017
                       Mean reward: 3.38
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 0.9355
     Episode_Reward/lifting_object: -0.1651
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.94s
                      Time elapsed: 00:10:03
                               ETA: 05:32:02

################################################################################
                     [1m Learning iteration 294/10000 [0m                     

                       Computation: 50274 steps/s (collection: 1.855s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.2288
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.4319
                       Mean reward: 4.28
               Mean episode length: 228.11
    Episode_Reward/reaching_object: 0.9558
     Episode_Reward/lifting_object: -0.0776
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.96s
                      Time elapsed: 00:10:05
                               ETA: 05:31:57

################################################################################
                     [1m Learning iteration 295/10000 [0m                     

                       Computation: 49880 steps/s (collection: 1.878s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.9220
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.4879
                       Mean reward: 2.61
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 0.9707
     Episode_Reward/lifting_object: -0.1526
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.97s
                      Time elapsed: 00:10:07
                               ETA: 05:31:52

################################################################################
                     [1m Learning iteration 296/10000 [0m                     

                       Computation: 51005 steps/s (collection: 1.830s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.8546
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.5226
                       Mean reward: 3.22
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 0.9826
     Episode_Reward/lifting_object: -0.0858
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.93s
                      Time elapsed: 00:10:09
                               ETA: 05:31:46

################################################################################
                     [1m Learning iteration 297/10000 [0m                     

                       Computation: 50861 steps/s (collection: 1.823s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.2484
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.5585
                       Mean reward: 3.02
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 0.9408
     Episode_Reward/lifting_object: -0.1034
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.93s
                      Time elapsed: 00:10:11
                               ETA: 05:31:40

################################################################################
                     [1m Learning iteration 298/10000 [0m                     

                       Computation: 50493 steps/s (collection: 1.832s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.4514
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.5998
                       Mean reward: 3.79
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: -0.2073
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.95s
                      Time elapsed: 00:10:13
                               ETA: 05:31:35

################################################################################
                     [1m Learning iteration 299/10000 [0m                     

                       Computation: 50395 steps/s (collection: 1.849s, learning 0.102s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.4789
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.6707
                       Mean reward: 4.76
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: -0.0898
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 1.95s
                      Time elapsed: 00:10:15
                               ETA: 05:31:29

################################################################################
                     [1m Learning iteration 300/10000 [0m                     

                       Computation: 51338 steps/s (collection: 1.820s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.2559
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.7175
                       Mean reward: 4.79
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.0135
     Episode_Reward/lifting_object: -0.0520
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.91s
                      Time elapsed: 00:10:16
                               ETA: 05:31:23

################################################################################
                     [1m Learning iteration 301/10000 [0m                     

                       Computation: 49603 steps/s (collection: 1.879s, learning 0.103s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.2370
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.7865
                       Mean reward: 3.56
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.0318
     Episode_Reward/lifting_object: -0.0742
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 1.98s
                      Time elapsed: 00:10:18
                               ETA: 05:31:19

################################################################################
                     [1m Learning iteration 302/10000 [0m                     

                       Computation: 50540 steps/s (collection: 1.830s, learning 0.115s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.2121
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.8070
                       Mean reward: 5.15
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.1077
     Episode_Reward/lifting_object: -0.2025
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.95s
                      Time elapsed: 00:10:20
                               ETA: 05:31:13

################################################################################
                     [1m Learning iteration 303/10000 [0m                     

                       Computation: 50601 steps/s (collection: 1.822s, learning 0.121s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.1588
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.8618
                       Mean reward: 3.99
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.0775
     Episode_Reward/lifting_object: -0.1614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 1.94s
                      Time elapsed: 00:10:22
                               ETA: 05:31:08

################################################################################
                     [1m Learning iteration 304/10000 [0m                     

                       Computation: 50342 steps/s (collection: 1.846s, learning 0.106s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.6104
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.9267
                       Mean reward: 5.32
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: -0.0229
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 1.95s
                      Time elapsed: 00:10:24
                               ETA: 05:31:03

################################################################################
                     [1m Learning iteration 305/10000 [0m                     

                       Computation: 49712 steps/s (collection: 1.883s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.3741
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.9781
                       Mean reward: 4.32
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.0681
     Episode_Reward/lifting_object: -0.0773
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 1.98s
                      Time elapsed: 00:10:26
                               ETA: 05:30:58

################################################################################
                     [1m Learning iteration 306/10000 [0m                     

                       Computation: 49881 steps/s (collection: 1.877s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.1076
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.0577
                       Mean reward: 4.73
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.0278
     Episode_Reward/lifting_object: -0.0732
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 1.97s
                      Time elapsed: 00:10:28
                               ETA: 05:30:54

################################################################################
                     [1m Learning iteration 307/10000 [0m                     

                       Computation: 50030 steps/s (collection: 1.872s, learning 0.093s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.2400
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.0813
                       Mean reward: 5.08
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: -0.0396
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 1.96s
                      Time elapsed: 00:10:30
                               ETA: 05:30:49

################################################################################
                     [1m Learning iteration 308/10000 [0m                     

                       Computation: 50101 steps/s (collection: 1.873s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.0777
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.1453
                       Mean reward: 4.29
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.0180
     Episode_Reward/lifting_object: -0.0848
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.96s
                      Time elapsed: 00:10:32
                               ETA: 05:30:44

################################################################################
                     [1m Learning iteration 309/10000 [0m                     

                       Computation: 49735 steps/s (collection: 1.884s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.2587
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.2002
                       Mean reward: 4.56
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.0723
     Episode_Reward/lifting_object: -0.1944
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.98s
                      Time elapsed: 00:10:34
                               ETA: 05:30:40

################################################################################
                     [1m Learning iteration 310/10000 [0m                     

                       Computation: 50003 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.1650
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.2433
                       Mean reward: 4.74
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 1.0405
     Episode_Reward/lifting_object: -0.1516
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.97s
                      Time elapsed: 00:10:36
                               ETA: 05:30:36

################################################################################
                     [1m Learning iteration 311/10000 [0m                     

                       Computation: 49379 steps/s (collection: 1.878s, learning 0.113s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.4096
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.2653
                       Mean reward: 4.64
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: -0.0815
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 1.99s
                      Time elapsed: 00:10:38
                               ETA: 05:30:32

################################################################################
                     [1m Learning iteration 312/10000 [0m                     

                       Computation: 49621 steps/s (collection: 1.874s, learning 0.107s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.9126
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.2812
                       Mean reward: 3.64
               Mean episode length: 216.21
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: -0.1913
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 1.98s
                      Time elapsed: 00:10:40
                               ETA: 05:30:28

################################################################################
                     [1m Learning iteration 313/10000 [0m                     

                       Computation: 50667 steps/s (collection: 1.846s, learning 0.094s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.1544
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.2996
                       Mean reward: 5.64
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 1.1481
     Episode_Reward/lifting_object: -0.1416
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 1.94s
                      Time elapsed: 00:10:42
                               ETA: 05:30:22

################################################################################
                     [1m Learning iteration 314/10000 [0m                     

                       Computation: 50592 steps/s (collection: 1.854s, learning 0.090s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.0657
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.3593
                       Mean reward: 4.93
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 1.1402
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 1.94s
                      Time elapsed: 00:10:44
                               ETA: 05:30:17

################################################################################
                     [1m Learning iteration 315/10000 [0m                     

                       Computation: 50978 steps/s (collection: 1.833s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.0594
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.4453
                       Mean reward: 5.82
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 1.1254
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 1.93s
                      Time elapsed: 00:10:46
                               ETA: 05:30:12

################################################################################
                     [1m Learning iteration 316/10000 [0m                     

                       Computation: 50560 steps/s (collection: 1.846s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.1656
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.5113
                       Mean reward: 5.28
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.1438
     Episode_Reward/lifting_object: -0.1874
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 1.94s
                      Time elapsed: 00:10:48
                               ETA: 05:30:06

################################################################################
                     [1m Learning iteration 317/10000 [0m                     

                       Computation: 49757 steps/s (collection: 1.861s, learning 0.115s)
             Mean action noise std: 2.01
          Mean value_function loss: 1.3500
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.5706
                       Mean reward: 4.40
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 1.0946
     Episode_Reward/lifting_object: -0.1427
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 1.98s
                      Time elapsed: 00:10:50
                               ETA: 05:30:02

################################################################################
                     [1m Learning iteration 318/10000 [0m                     

                       Computation: 48588 steps/s (collection: 1.902s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.6978
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.5892
                       Mean reward: 4.40
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 1.1390
     Episode_Reward/lifting_object: -0.1436
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.02s
                      Time elapsed: 00:10:52
                               ETA: 05:29:59

################################################################################
                     [1m Learning iteration 319/10000 [0m                     

                       Computation: 49417 steps/s (collection: 1.881s, learning 0.109s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.7223
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.6282
                       Mean reward: 4.40
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 1.1360
     Episode_Reward/lifting_object: -0.0892
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 1.99s
                      Time elapsed: 00:10:54
                               ETA: 05:29:56

################################################################################
                     [1m Learning iteration 320/10000 [0m                     

                       Computation: 49714 steps/s (collection: 1.882s, learning 0.095s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.3950
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.6455
                       Mean reward: 5.70
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 1.1620
     Episode_Reward/lifting_object: -0.0369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 1.98s
                      Time elapsed: 00:10:56
                               ETA: 05:29:52

################################################################################
                     [1m Learning iteration 321/10000 [0m                     

                       Computation: 50256 steps/s (collection: 1.851s, learning 0.105s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.9794
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.6865
                       Mean reward: 5.09
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 1.1681
     Episode_Reward/lifting_object: -0.0768
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 1.96s
                      Time elapsed: 00:10:58
                               ETA: 05:29:47

################################################################################
                     [1m Learning iteration 322/10000 [0m                     

                       Computation: 50318 steps/s (collection: 1.845s, learning 0.109s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.0966
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.7235
                       Mean reward: 5.11
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 1.1720
     Episode_Reward/lifting_object: -0.0637
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 1.95s
                      Time elapsed: 00:11:00
                               ETA: 05:29:42

################################################################################
                     [1m Learning iteration 323/10000 [0m                     

                       Computation: 49629 steps/s (collection: 1.867s, learning 0.114s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.1644
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.8073
                       Mean reward: 5.08
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 1.1523
     Episode_Reward/lifting_object: -0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 1.98s
                      Time elapsed: 00:11:02
                               ETA: 05:29:38

################################################################################
                     [1m Learning iteration 324/10000 [0m                     

                       Computation: 49011 steps/s (collection: 1.892s, learning 0.113s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.5153
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.8716
                       Mean reward: 5.45
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.1791
     Episode_Reward/lifting_object: -0.1047
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.01s
                      Time elapsed: 00:11:04
                               ETA: 05:29:35

################################################################################
                     [1m Learning iteration 325/10000 [0m                     

                       Computation: 49177 steps/s (collection: 1.881s, learning 0.118s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.7283
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.8916
                       Mean reward: 5.83
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.1612
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.00s
                      Time elapsed: 00:11:06
                               ETA: 05:29:32

################################################################################
                     [1m Learning iteration 326/10000 [0m                     

                       Computation: 48835 steps/s (collection: 1.903s, learning 0.110s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.5195
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.9198
                       Mean reward: 5.02
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.2115
     Episode_Reward/lifting_object: -0.1292
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.01s
                      Time elapsed: 00:11:08
                               ETA: 05:29:29

################################################################################
                     [1m Learning iteration 327/10000 [0m                     

                       Computation: 49351 steps/s (collection: 1.881s, learning 0.111s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.5539
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.9776
                       Mean reward: 5.34
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 1.1893
     Episode_Reward/lifting_object: -0.1334
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 1.99s
                      Time elapsed: 00:11:10
                               ETA: 05:29:25

################################################################################
                     [1m Learning iteration 328/10000 [0m                     

                       Computation: 49384 steps/s (collection: 1.892s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.1997
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.0277
                       Mean reward: 6.07
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.2350
     Episode_Reward/lifting_object: -0.0371
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 1.99s
                      Time elapsed: 00:11:12
                               ETA: 05:29:22

################################################################################
                     [1m Learning iteration 329/10000 [0m                     

                       Computation: 49015 steps/s (collection: 1.911s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 1.0698
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.0834
                       Mean reward: 6.09
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 1.2122
     Episode_Reward/lifting_object: -0.0934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.01s
                      Time elapsed: 00:11:14
                               ETA: 05:29:18

################################################################################
                     [1m Learning iteration 330/10000 [0m                     

                       Computation: 48264 steps/s (collection: 1.916s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.2460
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.1342
                       Mean reward: 4.77
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.2231
     Episode_Reward/lifting_object: -0.0377
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.04s
                      Time elapsed: 00:11:16
                               ETA: 05:29:16

################################################################################
                     [1m Learning iteration 331/10000 [0m                     

                       Computation: 48379 steps/s (collection: 1.936s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 1.6053
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.1834
                       Mean reward: 4.12
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 1.1792
     Episode_Reward/lifting_object: -0.1939
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.03s
                      Time elapsed: 00:11:18
                               ETA: 05:29:14

################################################################################
                     [1m Learning iteration 332/10000 [0m                     

                       Computation: 47843 steps/s (collection: 1.959s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.8275
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.2054
                       Mean reward: 3.76
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.1983
     Episode_Reward/lifting_object: -0.1895
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.05s
                      Time elapsed: 00:11:20
                               ETA: 05:29:12

################################################################################
                     [1m Learning iteration 333/10000 [0m                     

                       Computation: 19425 steps/s (collection: 4.915s, learning 0.146s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.4494
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.2233
                       Mean reward: 5.38
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: -0.1043
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.06s
                      Time elapsed: 00:11:25
                               ETA: 05:30:37

################################################################################
                     [1m Learning iteration 334/10000 [0m                     

                       Computation: 14251 steps/s (collection: 6.783s, learning 0.115s)
             Mean action noise std: 2.06
          Mean value_function loss: 1.8575
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.2573
                       Mean reward: 4.91
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.1918
     Episode_Reward/lifting_object: -0.1127
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.90s
                      Time elapsed: 00:11:32
                               ETA: 05:32:55

################################################################################
                     [1m Learning iteration 335/10000 [0m                     

                       Computation: 14406 steps/s (collection: 6.682s, learning 0.141s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.1701
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.2683
                       Mean reward: 5.72
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: -0.0465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.82s
                      Time elapsed: 00:11:39
                               ETA: 05:35:10

################################################################################
                     [1m Learning iteration 336/10000 [0m                     

                       Computation: 14650 steps/s (collection: 6.597s, learning 0.113s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.0625
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.2979
                       Mean reward: 6.05
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 1.2590
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.71s
                      Time elapsed: 00:11:45
                               ETA: 05:37:21

################################################################################
                     [1m Learning iteration 337/10000 [0m                     

                       Computation: 14563 steps/s (collection: 6.632s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.2677
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.3496
                       Mean reward: 3.84
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.2299
     Episode_Reward/lifting_object: -0.1226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.75s
                      Time elapsed: 00:11:52
                               ETA: 05:39:32

################################################################################
                     [1m Learning iteration 338/10000 [0m                     

                       Computation: 14714 steps/s (collection: 6.564s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.3188
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.4006
                       Mean reward: 5.90
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 1.2562
     Episode_Reward/lifting_object: -0.0623
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.68s
                      Time elapsed: 00:11:59
                               ETA: 05:41:40

################################################################################
                     [1m Learning iteration 339/10000 [0m                     

                       Computation: 14697 steps/s (collection: 6.562s, learning 0.127s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.5834
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.4450
                       Mean reward: 5.77
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 1.1984
     Episode_Reward/lifting_object: -0.0648
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.69s
                      Time elapsed: 00:12:05
                               ETA: 05:43:47

################################################################################
                     [1m Learning iteration 340/10000 [0m                     

                       Computation: 14502 steps/s (collection: 6.652s, learning 0.126s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.2159
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.4904
                       Mean reward: 5.08
               Mean episode length: 242.58
    Episode_Reward/reaching_object: 1.1929
     Episode_Reward/lifting_object: -0.0463
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.78s
                      Time elapsed: 00:12:12
                               ETA: 05:45:57

################################################################################
                     [1m Learning iteration 341/10000 [0m                     

                       Computation: 13842 steps/s (collection: 6.997s, learning 0.105s)
             Mean action noise std: 2.08
          Mean value_function loss: 1.3078
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.5361
                       Mean reward: 5.73
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 1.2255
     Episode_Reward/lifting_object: -0.0403
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.10s
                      Time elapsed: 00:12:19
                               ETA: 05:48:15

################################################################################
                     [1m Learning iteration 342/10000 [0m                     

                       Computation: 50386 steps/s (collection: 1.859s, learning 0.092s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.2890
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.5822
                       Mean reward: 6.02
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: -0.0285
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 1.95s
                      Time elapsed: 00:12:21
                               ETA: 05:48:06

################################################################################
                     [1m Learning iteration 343/10000 [0m                     

                       Computation: 49744 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.08
          Mean value_function loss: 1.1206
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.6562
                       Mean reward: 5.44
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: -0.0302
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.98s
                      Time elapsed: 00:12:23
                               ETA: 05:47:59

################################################################################
                     [1m Learning iteration 344/10000 [0m                     

                       Computation: 49888 steps/s (collection: 1.876s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.3145
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 66.7240
                       Mean reward: 5.75
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.2045
     Episode_Reward/lifting_object: -0.1448
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.97s
                      Time elapsed: 00:12:25
                               ETA: 05:47:52

################################################################################
                     [1m Learning iteration 345/10000 [0m                     

                       Computation: 49643 steps/s (collection: 1.890s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.7296
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.8209
                       Mean reward: 5.02
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 1.98s
                      Time elapsed: 00:12:27
                               ETA: 05:47:44

################################################################################
                     [1m Learning iteration 346/10000 [0m                     

                       Computation: 48953 steps/s (collection: 1.905s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.8477
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.8419
                       Mean reward: 5.38
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 1.2410
     Episode_Reward/lifting_object: -0.0487
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.01s
                      Time elapsed: 00:12:29
                               ETA: 05:47:38

################################################################################
                     [1m Learning iteration 347/10000 [0m                     

                       Computation: 49003 steps/s (collection: 1.903s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.8188
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.8778
                       Mean reward: 5.63
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.2644
     Episode_Reward/lifting_object: -0.0265
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.01s
                      Time elapsed: 00:12:31
                               ETA: 05:47:31

################################################################################
                     [1m Learning iteration 348/10000 [0m                     

                       Computation: 49250 steps/s (collection: 1.870s, learning 0.126s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.7595
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.9121
                       Mean reward: 5.70
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.2843
     Episode_Reward/lifting_object: -0.0644
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.00s
                      Time elapsed: 00:12:33
                               ETA: 05:47:25

################################################################################
                     [1m Learning iteration 349/10000 [0m                     

                       Computation: 43747 steps/s (collection: 2.146s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.4587
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.9698
                       Mean reward: 6.13
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 1.2491
     Episode_Reward/lifting_object: -0.1229
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.25s
                      Time elapsed: 00:12:35
                               ETA: 05:47:25

################################################################################
                     [1m Learning iteration 350/10000 [0m                     

                       Computation: 48578 steps/s (collection: 1.918s, learning 0.106s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.4063
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.0266
                       Mean reward: 6.34
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.2842
     Episode_Reward/lifting_object: -0.0544
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.02s
                      Time elapsed: 00:12:38
                               ETA: 05:47:19

################################################################################
                     [1m Learning iteration 351/10000 [0m                     

                       Computation: 47618 steps/s (collection: 1.979s, learning 0.086s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.2918
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.0963
                       Mean reward: 6.05
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 1.3276
     Episode_Reward/lifting_object: -0.0852
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.06s
                      Time elapsed: 00:12:40
                               ETA: 05:47:14

################################################################################
                     [1m Learning iteration 352/10000 [0m                     

                       Computation: 49588 steps/s (collection: 1.896s, learning 0.086s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.5601
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.1770
                       Mean reward: 6.37
               Mean episode length: 224.29
    Episode_Reward/reaching_object: 1.2740
     Episode_Reward/lifting_object: -0.0302
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.98s
                      Time elapsed: 00:12:42
                               ETA: 05:47:07

################################################################################
                     [1m Learning iteration 353/10000 [0m                     

                       Computation: 47419 steps/s (collection: 1.975s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 1.6882
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.2222
                       Mean reward: 5.01
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.2547
     Episode_Reward/lifting_object: -0.0675
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.07s
                      Time elapsed: 00:12:44
                               ETA: 05:47:03

################################################################################
                     [1m Learning iteration 354/10000 [0m                     

                       Computation: 47312 steps/s (collection: 1.926s, learning 0.152s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.9427
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.2338
                       Mean reward: 5.57
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 1.2721
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.08s
                      Time elapsed: 00:12:46
                               ETA: 05:46:59

################################################################################
                     [1m Learning iteration 355/10000 [0m                     

                       Computation: 49250 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.3146
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.2528
                       Mean reward: 5.62
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 1.3095
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.00s
                      Time elapsed: 00:12:48
                               ETA: 05:46:52

################################################################################
                     [1m Learning iteration 356/10000 [0m                     

                       Computation: 49714 steps/s (collection: 1.873s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.3009
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.3060
                       Mean reward: 6.46
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 1.2837
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.98s
                      Time elapsed: 00:12:50
                               ETA: 05:46:45

################################################################################
                     [1m Learning iteration 357/10000 [0m                     

                       Computation: 49278 steps/s (collection: 1.906s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.7219
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.3689
                       Mean reward: 6.42
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.3071
     Episode_Reward/lifting_object: 0.0706
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.99s
                      Time elapsed: 00:12:52
                               ETA: 05:46:38

################################################################################
                     [1m Learning iteration 358/10000 [0m                     

                       Computation: 48860 steps/s (collection: 1.911s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.2860
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.3844
                       Mean reward: 6.03
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.3229
     Episode_Reward/lifting_object: -0.0021
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.01s
                      Time elapsed: 00:12:54
                               ETA: 05:46:32

################################################################################
                     [1m Learning iteration 359/10000 [0m                     

                       Computation: 49626 steps/s (collection: 1.888s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.3978
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.4401
                       Mean reward: 6.23
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.3033
     Episode_Reward/lifting_object: -0.0486
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.98s
                      Time elapsed: 00:12:56
                               ETA: 05:46:25

################################################################################
                     [1m Learning iteration 360/10000 [0m                     

                       Computation: 47924 steps/s (collection: 1.952s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.4070
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.5207
                       Mean reward: 5.46
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 1.2626
     Episode_Reward/lifting_object: -0.0230
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.05s
                      Time elapsed: 00:12:58
                               ETA: 05:46:21

################################################################################
                     [1m Learning iteration 361/10000 [0m                     

                       Computation: 48734 steps/s (collection: 1.907s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.2167
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.5781
                       Mean reward: 6.55
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 1.3164
     Episode_Reward/lifting_object: 0.0630
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.02s
                      Time elapsed: 00:13:00
                               ETA: 05:46:15

################################################################################
                     [1m Learning iteration 362/10000 [0m                     

                       Computation: 50167 steps/s (collection: 1.867s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.2876
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.6437
                       Mean reward: 6.55
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.2777
     Episode_Reward/lifting_object: -0.0003
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.96s
                      Time elapsed: 00:13:02
                               ETA: 05:46:07

################################################################################
                     [1m Learning iteration 363/10000 [0m                     

                       Computation: 48242 steps/s (collection: 1.949s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.4368
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.7104
                       Mean reward: 7.14
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 1.3067
     Episode_Reward/lifting_object: 0.0082
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.04s
                      Time elapsed: 00:13:04
                               ETA: 05:46:02

################################################################################
                     [1m Learning iteration 364/10000 [0m                     

                       Computation: 48843 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.4577
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.7534
                       Mean reward: 6.59
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 1.3051
     Episode_Reward/lifting_object: 0.0087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.01s
                      Time elapsed: 00:13:06
                               ETA: 05:45:56

################################################################################
                     [1m Learning iteration 365/10000 [0m                     

                       Computation: 48562 steps/s (collection: 1.928s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.4478
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.7729
                       Mean reward: 6.07
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.3048
     Episode_Reward/lifting_object: 0.0433
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.02s
                      Time elapsed: 00:13:08
                               ETA: 05:45:51

################################################################################
                     [1m Learning iteration 366/10000 [0m                     

                       Computation: 47868 steps/s (collection: 1.929s, learning 0.125s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.3023
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.8143
                       Mean reward: 6.30
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 0.0264
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.05s
                      Time elapsed: 00:13:10
                               ETA: 05:45:46

################################################################################
                     [1m Learning iteration 367/10000 [0m                     

                       Computation: 47962 steps/s (collection: 1.926s, learning 0.124s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.4560
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.8612
                       Mean reward: 7.35
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 1.3615
     Episode_Reward/lifting_object: 0.0627
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.05s
                      Time elapsed: 00:13:12
                               ETA: 05:45:41

################################################################################
                     [1m Learning iteration 368/10000 [0m                     

                       Computation: 48562 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.5170
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.9190
                       Mean reward: 7.01
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.3298
     Episode_Reward/lifting_object: 0.0465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.02s
                      Time elapsed: 00:13:14
                               ETA: 05:45:35

################################################################################
                     [1m Learning iteration 369/10000 [0m                     

                       Computation: 48197 steps/s (collection: 1.946s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 1.5890
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.9662
                       Mean reward: 6.26
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 1.3033
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.04s
                      Time elapsed: 00:13:16
                               ETA: 05:45:30

################################################################################
                     [1m Learning iteration 370/10000 [0m                     

                       Computation: 48760 steps/s (collection: 1.928s, learning 0.089s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.7006
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 68.0201
                       Mean reward: 6.34
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.3628
     Episode_Reward/lifting_object: 0.0659
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.02s
                      Time elapsed: 00:13:18
                               ETA: 05:45:25

################################################################################
                     [1m Learning iteration 371/10000 [0m                     

                       Computation: 48971 steps/s (collection: 1.896s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.2711
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 68.0877
                       Mean reward: 6.75
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 1.3105
     Episode_Reward/lifting_object: 0.0037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.01s
                      Time elapsed: 00:13:20
                               ETA: 05:45:19

################################################################################
                     [1m Learning iteration 372/10000 [0m                     

                       Computation: 48026 steps/s (collection: 1.956s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 2.4011
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.1356
                       Mean reward: 3.64
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: -0.0998
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.05s
                      Time elapsed: 00:13:22
                               ETA: 05:45:14

################################################################################
                     [1m Learning iteration 373/10000 [0m                     

                       Computation: 48012 steps/s (collection: 1.935s, learning 0.113s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.2299
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.1459
                       Mean reward: 6.48
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 1.3266
     Episode_Reward/lifting_object: 0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.05s
                      Time elapsed: 00:13:24
                               ETA: 05:45:09

################################################################################
                     [1m Learning iteration 374/10000 [0m                     

                       Computation: 47481 steps/s (collection: 1.954s, learning 0.116s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.4948
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.1827
                       Mean reward: 5.37
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.3186
     Episode_Reward/lifting_object: 0.0198
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.07s
                      Time elapsed: 00:13:26
                               ETA: 05:45:05

################################################################################
                     [1m Learning iteration 375/10000 [0m                     

                       Computation: 48191 steps/s (collection: 1.929s, learning 0.111s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.5117
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.2475
                       Mean reward: 6.49
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.3242
     Episode_Reward/lifting_object: 0.0622
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.04s
                      Time elapsed: 00:13:28
                               ETA: 05:45:00

################################################################################
                     [1m Learning iteration 376/10000 [0m                     

                       Computation: 49637 steps/s (collection: 1.893s, learning 0.087s)
             Mean action noise std: 2.20
          Mean value_function loss: 2.9885
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.2851
                       Mean reward: 5.87
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 1.3197
     Episode_Reward/lifting_object: -0.0512
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.98s
                      Time elapsed: 00:13:30
                               ETA: 05:44:53

################################################################################
                     [1m Learning iteration 377/10000 [0m                     

                       Computation: 48652 steps/s (collection: 1.920s, learning 0.100s)
             Mean action noise std: 2.20
          Mean value_function loss: 1.0537
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.2953
                       Mean reward: 5.11
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.2587
     Episode_Reward/lifting_object: -0.0690
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.02s
                      Time elapsed: 00:13:32
                               ETA: 05:44:48

################################################################################
                     [1m Learning iteration 378/10000 [0m                     

                       Computation: 49459 steps/s (collection: 1.896s, learning 0.092s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.6023
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.3257
                       Mean reward: 6.29
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.2454
     Episode_Reward/lifting_object: 0.0357
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.99s
                      Time elapsed: 00:13:34
                               ETA: 05:44:42

################################################################################
                     [1m Learning iteration 379/10000 [0m                     

                       Computation: 48003 steps/s (collection: 1.931s, learning 0.117s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.5202
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 68.3893
                       Mean reward: 6.85
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 1.2729
     Episode_Reward/lifting_object: 0.0398
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.05s
                      Time elapsed: 00:13:36
                               ETA: 05:44:37

################################################################################
                     [1m Learning iteration 380/10000 [0m                     

                       Computation: 46613 steps/s (collection: 1.998s, learning 0.111s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.7128
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.4557
                       Mean reward: 7.07
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 1.3139
     Episode_Reward/lifting_object: 0.0999
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.11s
                      Time elapsed: 00:13:38
                               ETA: 05:44:34

################################################################################
                     [1m Learning iteration 381/10000 [0m                     

                       Computation: 48937 steps/s (collection: 1.904s, learning 0.105s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.5548
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 68.5097
                       Mean reward: 6.63
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 0.0367
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.01s
                      Time elapsed: 00:13:40
                               ETA: 05:44:28

################################################################################
                     [1m Learning iteration 382/10000 [0m                     

                       Computation: 48809 steps/s (collection: 1.902s, learning 0.112s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.4040
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.5769
                       Mean reward: 6.55
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 1.3157
     Episode_Reward/lifting_object: 0.1383
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.01s
                      Time elapsed: 00:13:42
                               ETA: 05:44:23

################################################################################
                     [1m Learning iteration 383/10000 [0m                     

                       Computation: 47907 steps/s (collection: 1.940s, learning 0.112s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.6873
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.6344
                       Mean reward: 6.32
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.3001
     Episode_Reward/lifting_object: 0.0331
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.05s
                      Time elapsed: 00:13:44
                               ETA: 05:44:18

################################################################################
                     [1m Learning iteration 384/10000 [0m                     

                       Computation: 47859 steps/s (collection: 1.941s, learning 0.113s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.7106
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 68.7143
                       Mean reward: 6.24
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.2613
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.05s
                      Time elapsed: 00:13:46
                               ETA: 05:44:13

################################################################################
                     [1m Learning iteration 385/10000 [0m                     

                       Computation: 47022 steps/s (collection: 1.973s, learning 0.118s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.6512
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 68.7616
                       Mean reward: 6.81
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.2358
     Episode_Reward/lifting_object: 0.0467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.09s
                      Time elapsed: 00:13:49
                               ETA: 05:44:10

################################################################################
                     [1m Learning iteration 386/10000 [0m                     

                       Computation: 49159 steps/s (collection: 1.895s, learning 0.105s)
             Mean action noise std: 2.23
          Mean value_function loss: 3.2208
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 68.8205
                       Mean reward: 5.48
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 1.2057
     Episode_Reward/lifting_object: 0.0132
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.00s
                      Time elapsed: 00:13:51
                               ETA: 05:44:04

################################################################################
                     [1m Learning iteration 387/10000 [0m                     

                       Computation: 48876 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.7144
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.8402
                       Mean reward: 7.06
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.2565
     Episode_Reward/lifting_object: 0.0670
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.01s
                      Time elapsed: 00:13:53
                               ETA: 05:43:59

################################################################################
                     [1m Learning iteration 388/10000 [0m                     

                       Computation: 48344 steps/s (collection: 1.934s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.5109
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 68.8826
                       Mean reward: 6.80
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 1.2408
     Episode_Reward/lifting_object: 0.0513
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.03s
                      Time elapsed: 00:13:55
                               ETA: 05:43:54

################################################################################
                     [1m Learning iteration 389/10000 [0m                     

                       Computation: 47329 steps/s (collection: 1.975s, learning 0.102s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.8743
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 68.9155
                       Mean reward: 7.02
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.2448
     Episode_Reward/lifting_object: 0.2304
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.08s
                      Time elapsed: 00:13:57
                               ETA: 05:43:50

################################################################################
                     [1m Learning iteration 390/10000 [0m                     

                       Computation: 48229 steps/s (collection: 1.942s, learning 0.096s)
             Mean action noise std: 2.25
          Mean value_function loss: 6.2829
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.9624
                       Mean reward: 8.05
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.2375
     Episode_Reward/lifting_object: 0.1052
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.04s
                      Time elapsed: 00:13:59
                               ETA: 05:43:45

################################################################################
                     [1m Learning iteration 391/10000 [0m                     

                       Computation: 47960 steps/s (collection: 1.962s, learning 0.088s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.1073
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.9818
                       Mean reward: 6.54
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.1973
     Episode_Reward/lifting_object: -0.0026
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.05s
                      Time elapsed: 00:14:01
                               ETA: 05:43:40

################################################################################
                     [1m Learning iteration 392/10000 [0m                     

                       Computation: 47176 steps/s (collection: 1.985s, learning 0.099s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.0851
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 69.0249
                       Mean reward: 5.98
               Mean episode length: 223.24
    Episode_Reward/reaching_object: 1.1269
     Episode_Reward/lifting_object: 0.2650
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.08s
                      Time elapsed: 00:14:03
                               ETA: 05:43:37

################################################################################
                     [1m Learning iteration 393/10000 [0m                     

                       Computation: 46504 steps/s (collection: 1.988s, learning 0.126s)
             Mean action noise std: 2.25
          Mean value_function loss: 0.9068
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.0892
                       Mean reward: 6.53
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.1883
     Episode_Reward/lifting_object: 0.2747
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.11s
                      Time elapsed: 00:14:05
                               ETA: 05:43:34

################################################################################
                     [1m Learning iteration 394/10000 [0m                     

                       Computation: 47518 steps/s (collection: 1.941s, learning 0.128s)
             Mean action noise std: 2.26
          Mean value_function loss: 1.6732
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.1254
                       Mean reward: 4.54
               Mean episode length: 217.51
    Episode_Reward/reaching_object: 1.1544
     Episode_Reward/lifting_object: 0.0243
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.07s
                      Time elapsed: 00:14:07
                               ETA: 05:43:30

################################################################################
                     [1m Learning iteration 395/10000 [0m                     

                       Computation: 48136 steps/s (collection: 1.937s, learning 0.105s)
             Mean action noise std: 2.26
          Mean value_function loss: 2.1473
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.1747
                       Mean reward: 5.95
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.1968
     Episode_Reward/lifting_object: 0.1512
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.04s
                      Time elapsed: 00:14:09
                               ETA: 05:43:25

################################################################################
                     [1m Learning iteration 396/10000 [0m                     

                       Computation: 48110 steps/s (collection: 1.890s, learning 0.154s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.2319
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.2220
                       Mean reward: 6.56
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.1233
     Episode_Reward/lifting_object: 0.2826
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.04s
                      Time elapsed: 00:14:11
                               ETA: 05:43:21

################################################################################
                     [1m Learning iteration 397/10000 [0m                     

                       Computation: 48771 steps/s (collection: 1.917s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.6304
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.2867
                       Mean reward: 5.43
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 1.1229
     Episode_Reward/lifting_object: 0.2491
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.02s
                      Time elapsed: 00:14:13
                               ETA: 05:43:15

################################################################################
                     [1m Learning iteration 398/10000 [0m                     

                       Computation: 48376 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 2.27
          Mean value_function loss: 3.0532
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.3458
                       Mean reward: 7.20
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.0783
     Episode_Reward/lifting_object: 0.1521
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.03s
                      Time elapsed: 00:14:15
                               ETA: 05:43:10

################################################################################
                     [1m Learning iteration 399/10000 [0m                     

                       Computation: 49274 steps/s (collection: 1.905s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.0002
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.4010
                       Mean reward: 5.86
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.0785
     Episode_Reward/lifting_object: 0.2210
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.00s
                      Time elapsed: 00:14:17
                               ETA: 05:43:05

################################################################################
                     [1m Learning iteration 400/10000 [0m                     

                       Computation: 48890 steps/s (collection: 1.920s, learning 0.091s)
             Mean action noise std: 2.28
          Mean value_function loss: 3.8971
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.4549
                       Mean reward: 7.32
               Mean episode length: 216.44
    Episode_Reward/reaching_object: 1.0075
     Episode_Reward/lifting_object: 0.2267
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.01s
                      Time elapsed: 00:14:19
                               ETA: 05:42:59

################################################################################
                     [1m Learning iteration 401/10000 [0m                     

                       Computation: 48932 steps/s (collection: 1.922s, learning 0.087s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.4646
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.5001
                       Mean reward: 6.30
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 1.0191
     Episode_Reward/lifting_object: 0.0679
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.01s
                      Time elapsed: 00:14:21
                               ETA: 05:42:54

################################################################################
                     [1m Learning iteration 402/10000 [0m                     

                       Computation: 49178 steps/s (collection: 1.907s, learning 0.092s)
             Mean action noise std: 2.29
          Mean value_function loss: 1.2640
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 69.5614
                       Mean reward: 6.84
               Mean episode length: 214.73
    Episode_Reward/reaching_object: 1.0158
     Episode_Reward/lifting_object: 0.2517
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.00s
                      Time elapsed: 00:14:23
                               ETA: 05:42:48

################################################################################
                     [1m Learning iteration 403/10000 [0m                     

                       Computation: 48651 steps/s (collection: 1.922s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.5557
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.6090
                       Mean reward: 3.30
               Mean episode length: 210.26
    Episode_Reward/reaching_object: 0.9841
     Episode_Reward/lifting_object: 0.1174
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.02s
                      Time elapsed: 00:14:25
                               ETA: 05:42:43

################################################################################
                     [1m Learning iteration 404/10000 [0m                     

                       Computation: 48689 steps/s (collection: 1.903s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.8686
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.6349
                       Mean reward: 6.31
               Mean episode length: 220.11
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 0.1962
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.02s
                      Time elapsed: 00:14:27
                               ETA: 05:42:38

################################################################################
                     [1m Learning iteration 405/10000 [0m                     

                       Computation: 48757 steps/s (collection: 1.915s, learning 0.102s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.2780
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 69.6572
                       Mean reward: 6.68
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 0.2415
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.02s
                      Time elapsed: 00:14:29
                               ETA: 05:42:33

################################################################################
                     [1m Learning iteration 406/10000 [0m                     

                       Computation: 49435 steps/s (collection: 1.882s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.4187
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.7076
                       Mean reward: 5.84
               Mean episode length: 217.66
    Episode_Reward/reaching_object: 1.0352
     Episode_Reward/lifting_object: 0.2879
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.99s
                      Time elapsed: 00:14:31
                               ETA: 05:42:27

################################################################################
                     [1m Learning iteration 407/10000 [0m                     

                       Computation: 48782 steps/s (collection: 1.887s, learning 0.129s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.3235
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 69.7619
                       Mean reward: 7.03
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.0274
     Episode_Reward/lifting_object: 0.2923
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.02s
                      Time elapsed: 00:14:33
                               ETA: 05:42:22

################################################################################
                     [1m Learning iteration 408/10000 [0m                     

                       Computation: 49263 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 1.8191
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 69.8036
                       Mean reward: 8.16
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.0429
     Episode_Reward/lifting_object: 0.5027
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.00s
                      Time elapsed: 00:14:35
                               ETA: 05:42:17

################################################################################
                     [1m Learning iteration 409/10000 [0m                     

                       Computation: 49086 steps/s (collection: 1.911s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 2.1257
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.8194
                       Mean reward: 7.10
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.0837
     Episode_Reward/lifting_object: 0.2699
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.00s
                      Time elapsed: 00:14:37
                               ETA: 05:42:11

################################################################################
                     [1m Learning iteration 410/10000 [0m                     

                       Computation: 48508 steps/s (collection: 1.934s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 1.7797
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 69.8623
                       Mean reward: 6.04
               Mean episode length: 210.22
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 0.3396
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.03s
                      Time elapsed: 00:14:39
                               ETA: 05:42:06

################################################################################
                     [1m Learning iteration 411/10000 [0m                     

                       Computation: 49522 steps/s (collection: 1.882s, learning 0.104s)
             Mean action noise std: 2.32
          Mean value_function loss: 1.8412
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 69.9121
                       Mean reward: 7.04
               Mean episode length: 220.44
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 0.3199
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.99s
                      Time elapsed: 00:14:41
                               ETA: 05:42:01

################################################################################
                     [1m Learning iteration 412/10000 [0m                     

                       Computation: 48521 steps/s (collection: 1.929s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 1.1823
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 69.9732
                       Mean reward: 5.89
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 0.2621
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.03s
                      Time elapsed: 00:14:43
                               ETA: 05:41:56

################################################################################
                     [1m Learning iteration 413/10000 [0m                     

                       Computation: 49498 steps/s (collection: 1.894s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 2.6393
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.0120
                       Mean reward: 5.72
               Mean episode length: 208.42
    Episode_Reward/reaching_object: 1.0134
     Episode_Reward/lifting_object: 0.4461
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.99s
                      Time elapsed: 00:14:45
                               ETA: 05:41:50

################################################################################
                     [1m Learning iteration 414/10000 [0m                     

                       Computation: 46501 steps/s (collection: 2.003s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 2.5295
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.0381
                       Mean reward: 4.66
               Mean episode length: 201.44
    Episode_Reward/reaching_object: 1.0052
     Episode_Reward/lifting_object: 0.2419
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.11s
                      Time elapsed: 00:14:47
                               ETA: 05:41:48

################################################################################
                     [1m Learning iteration 415/10000 [0m                     

                       Computation: 48862 steps/s (collection: 1.919s, learning 0.093s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.9341
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 70.0975
                       Mean reward: 6.57
               Mean episode length: 215.41
    Episode_Reward/reaching_object: 1.0137
     Episode_Reward/lifting_object: 0.2472
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.01s
                      Time elapsed: 00:14:49
                               ETA: 05:41:42

################################################################################
                     [1m Learning iteration 416/10000 [0m                     

                       Computation: 49863 steps/s (collection: 1.884s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 3.0373
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.1410
                       Mean reward: 4.22
               Mean episode length: 205.58
    Episode_Reward/reaching_object: 0.9644
     Episode_Reward/lifting_object: 0.2779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.97s
                      Time elapsed: 00:14:51
                               ETA: 05:41:36

################################################################################
                     [1m Learning iteration 417/10000 [0m                     

                       Computation: 48846 steps/s (collection: 1.924s, learning 0.089s)
             Mean action noise std: 2.34
          Mean value_function loss: 4.2209
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.1723
                       Mean reward: 7.99
               Mean episode length: 214.17
    Episode_Reward/reaching_object: 0.9917
     Episode_Reward/lifting_object: 0.3439
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.01s
                      Time elapsed: 00:14:53
                               ETA: 05:41:31

################################################################################
                     [1m Learning iteration 418/10000 [0m                     

                       Computation: 48286 steps/s (collection: 1.946s, learning 0.090s)
             Mean action noise std: 2.34
          Mean value_function loss: 2.5195
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.2355
                       Mean reward: 5.30
               Mean episode length: 210.65
    Episode_Reward/reaching_object: 0.9541
     Episode_Reward/lifting_object: 0.2168
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.04s
                      Time elapsed: 00:14:55
                               ETA: 05:41:27

################################################################################
                     [1m Learning iteration 419/10000 [0m                     

                       Computation: 47412 steps/s (collection: 1.964s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 3.1242
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 70.2827
                       Mean reward: 5.23
               Mean episode length: 208.83
    Episode_Reward/reaching_object: 0.9864
     Episode_Reward/lifting_object: 0.4562
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.07s
                      Time elapsed: 00:14:57
                               ETA: 05:41:23

################################################################################
                     [1m Learning iteration 420/10000 [0m                     

                       Computation: 47603 steps/s (collection: 1.952s, learning 0.113s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.8447
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.3267
                       Mean reward: 4.53
               Mean episode length: 217.80
    Episode_Reward/reaching_object: 0.9659
     Episode_Reward/lifting_object: 0.3503
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.07s
                      Time elapsed: 00:15:00
                               ETA: 05:41:20

################################################################################
                     [1m Learning iteration 421/10000 [0m                     

                       Computation: 47787 steps/s (collection: 1.966s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 3.1951
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.3452
                       Mean reward: 7.38
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 0.9665
     Episode_Reward/lifting_object: 0.3046
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.06s
                      Time elapsed: 00:15:02
                               ETA: 05:41:16

################################################################################
                     [1m Learning iteration 422/10000 [0m                     

                       Computation: 48810 steps/s (collection: 1.919s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.4970
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 70.3825
                       Mean reward: 6.47
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 0.9799
     Episode_Reward/lifting_object: 0.3330
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.01s
                      Time elapsed: 00:15:04
                               ETA: 05:41:11

################################################################################
                     [1m Learning iteration 423/10000 [0m                     

                       Computation: 48966 steps/s (collection: 1.913s, learning 0.095s)
             Mean action noise std: 2.36
          Mean value_function loss: 1.9411
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 70.4362
                       Mean reward: 9.10
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 0.9928
     Episode_Reward/lifting_object: 0.5387
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.01s
                      Time elapsed: 00:15:06
                               ETA: 05:41:06

################################################################################
                     [1m Learning iteration 424/10000 [0m                     

                       Computation: 47939 steps/s (collection: 1.951s, learning 0.100s)
             Mean action noise std: 2.36
          Mean value_function loss: 1.6060
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 70.4928
                       Mean reward: 7.71
               Mean episode length: 203.56
    Episode_Reward/reaching_object: 0.9695
     Episode_Reward/lifting_object: 0.3489
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.05s
                      Time elapsed: 00:15:08
                               ETA: 05:41:01

################################################################################
                     [1m Learning iteration 425/10000 [0m                     

                       Computation: 48448 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 2.36
          Mean value_function loss: 8.2530
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.5348
                       Mean reward: 4.39
               Mean episode length: 200.75
    Episode_Reward/reaching_object: 0.9334
     Episode_Reward/lifting_object: 0.0787
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.03s
                      Time elapsed: 00:15:10
                               ETA: 05:40:57

################################################################################
                     [1m Learning iteration 426/10000 [0m                     

                       Computation: 48398 steps/s (collection: 1.936s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 2.0855
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 70.5638
                       Mean reward: 8.29
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 0.9295
     Episode_Reward/lifting_object: 0.4331
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.03s
                      Time elapsed: 00:15:12
                               ETA: 05:40:52

################################################################################
                     [1m Learning iteration 427/10000 [0m                     

                       Computation: 49410 steps/s (collection: 1.891s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 3.0183
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 70.6125
                       Mean reward: 7.76
               Mean episode length: 216.21
    Episode_Reward/reaching_object: 0.9826
     Episode_Reward/lifting_object: 0.6261
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.99s
                      Time elapsed: 00:15:14
                               ETA: 05:40:47

################################################################################
                     [1m Learning iteration 428/10000 [0m                     

                       Computation: 49336 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 2.37
          Mean value_function loss: 2.0479
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 70.6699
                       Mean reward: 6.45
               Mean episode length: 208.06
    Episode_Reward/reaching_object: 0.9394
     Episode_Reward/lifting_object: 0.4168
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 1.99s
                      Time elapsed: 00:15:16
                               ETA: 05:40:42

################################################################################
                     [1m Learning iteration 429/10000 [0m                     

                       Computation: 46521 steps/s (collection: 2.003s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 4.1162
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 70.7287
                       Mean reward: 7.39
               Mean episode length: 204.98
    Episode_Reward/reaching_object: 0.9889
     Episode_Reward/lifting_object: 0.5191
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.11s
                      Time elapsed: 00:15:18
                               ETA: 05:40:39

################################################################################
                     [1m Learning iteration 430/10000 [0m                     

                       Computation: 46201 steps/s (collection: 2.013s, learning 0.115s)
             Mean action noise std: 2.38
          Mean value_function loss: 3.2755
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 70.7695
                       Mean reward: 5.91
               Mean episode length: 207.86
    Episode_Reward/reaching_object: 0.9480
     Episode_Reward/lifting_object: 0.2086
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.13s
                      Time elapsed: 00:15:20
                               ETA: 05:40:37

################################################################################
                     [1m Learning iteration 431/10000 [0m                     

                       Computation: 47889 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 2.38
          Mean value_function loss: 9.5165
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 70.8124
                       Mean reward: 7.83
               Mean episode length: 212.96
    Episode_Reward/reaching_object: 0.9771
     Episode_Reward/lifting_object: 0.3835
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.05s
                      Time elapsed: 00:15:22
                               ETA: 05:40:33

################################################################################
                     [1m Learning iteration 432/10000 [0m                     

                       Computation: 47357 steps/s (collection: 1.959s, learning 0.117s)
             Mean action noise std: 2.39
          Mean value_function loss: 1.7588
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 70.8516
                       Mean reward: 7.03
               Mean episode length: 207.94
    Episode_Reward/reaching_object: 0.9819
     Episode_Reward/lifting_object: 0.1623
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.08s
                      Time elapsed: 00:15:24
                               ETA: 05:40:29

################################################################################
                     [1m Learning iteration 433/10000 [0m                     

                       Computation: 46157 steps/s (collection: 1.998s, learning 0.131s)
             Mean action noise std: 2.39
          Mean value_function loss: 2.0546
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 70.9071
                       Mean reward: 4.73
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 0.9659
     Episode_Reward/lifting_object: 0.2472
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.13s
                      Time elapsed: 00:15:26
                               ETA: 05:40:27

################################################################################
                     [1m Learning iteration 434/10000 [0m                     

                       Computation: 46769 steps/s (collection: 1.964s, learning 0.138s)
             Mean action noise std: 2.39
          Mean value_function loss: 6.4641
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.9419
                       Mean reward: 5.96
               Mean episode length: 213.77
    Episode_Reward/reaching_object: 0.9538
     Episode_Reward/lifting_object: 0.2906
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.10s
                      Time elapsed: 00:15:28
                               ETA: 05:40:24

################################################################################
                     [1m Learning iteration 435/10000 [0m                     

                       Computation: 47269 steps/s (collection: 1.938s, learning 0.142s)
             Mean action noise std: 2.40
          Mean value_function loss: 2.1200
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.9945
                       Mean reward: 6.06
               Mean episode length: 204.52
    Episode_Reward/reaching_object: 0.9394
     Episode_Reward/lifting_object: 0.4043
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.08s
                      Time elapsed: 00:15:30
                               ETA: 05:40:21

################################################################################
                     [1m Learning iteration 436/10000 [0m                     

                       Computation: 46834 steps/s (collection: 1.979s, learning 0.120s)
             Mean action noise std: 2.40
          Mean value_function loss: 2.3117
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 71.0341
                       Mean reward: 6.28
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 0.3728
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.10s
                      Time elapsed: 00:15:32
                               ETA: 05:40:18

################################################################################
                     [1m Learning iteration 437/10000 [0m                     

                       Computation: 47308 steps/s (collection: 1.965s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 2.5228
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.0904
                       Mean reward: 5.06
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 0.9475
     Episode_Reward/lifting_object: 0.4558
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.08s
                      Time elapsed: 00:15:35
                               ETA: 05:40:15

################################################################################
                     [1m Learning iteration 438/10000 [0m                     

                       Computation: 46960 steps/s (collection: 1.978s, learning 0.116s)
             Mean action noise std: 2.41
          Mean value_function loss: 6.1637
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.1052
                       Mean reward: 6.60
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 0.9637
     Episode_Reward/lifting_object: 0.3293
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.09s
                      Time elapsed: 00:15:37
                               ETA: 05:40:11

################################################################################
                     [1m Learning iteration 439/10000 [0m                     

                       Computation: 46611 steps/s (collection: 1.983s, learning 0.126s)
             Mean action noise std: 2.41
          Mean value_function loss: 4.6582
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.1433
                       Mean reward: 6.43
               Mean episode length: 215.26
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 0.3544
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.11s
                      Time elapsed: 00:15:39
                               ETA: 05:40:09

################################################################################
                     [1m Learning iteration 440/10000 [0m                     

                       Computation: 47834 steps/s (collection: 1.959s, learning 0.097s)
             Mean action noise std: 2.42
          Mean value_function loss: 2.0070
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 71.2154
                       Mean reward: 6.03
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 0.4352
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.06s
                      Time elapsed: 00:15:41
                               ETA: 05:40:05

################################################################################
                     [1m Learning iteration 441/10000 [0m                     

                       Computation: 47340 steps/s (collection: 1.976s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 4.3073
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 71.2679
                       Mean reward: 4.89
               Mean episode length: 200.35
    Episode_Reward/reaching_object: 0.9061
     Episode_Reward/lifting_object: 0.1971
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.08s
                      Time elapsed: 00:15:43
                               ETA: 05:40:02

################################################################################
                     [1m Learning iteration 442/10000 [0m                     

                       Computation: 46769 steps/s (collection: 2.009s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 8.2918
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.3162
                       Mean reward: 4.56
               Mean episode length: 201.76
    Episode_Reward/reaching_object: 0.8960
     Episode_Reward/lifting_object: 0.3987
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.10s
                      Time elapsed: 00:15:45
                               ETA: 05:39:59

################################################################################
                     [1m Learning iteration 443/10000 [0m                     

                       Computation: 47422 steps/s (collection: 1.963s, learning 0.110s)
             Mean action noise std: 2.43
          Mean value_function loss: 4.6959
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 71.3639
                       Mean reward: 8.33
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 0.9494
     Episode_Reward/lifting_object: 0.2578
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.07s
                      Time elapsed: 00:15:47
                               ETA: 05:39:55

################################################################################
                     [1m Learning iteration 444/10000 [0m                     

                       Computation: 46564 steps/s (collection: 1.966s, learning 0.145s)
             Mean action noise std: 2.43
          Mean value_function loss: 5.3297
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.4387
                       Mean reward: 5.69
               Mean episode length: 204.19
    Episode_Reward/reaching_object: 0.8537
     Episode_Reward/lifting_object: 0.3059
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.11s
                      Time elapsed: 00:15:49
                               ETA: 05:39:53

################################################################################
                     [1m Learning iteration 445/10000 [0m                     

                       Computation: 47517 steps/s (collection: 1.952s, learning 0.117s)
             Mean action noise std: 2.44
          Mean value_function loss: 5.0663
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.4688
                       Mean reward: 5.63
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 0.9188
     Episode_Reward/lifting_object: 0.6085
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.07s
                      Time elapsed: 00:15:51
                               ETA: 05:39:49

################################################################################
                     [1m Learning iteration 446/10000 [0m                     

                       Computation: 46658 steps/s (collection: 1.971s, learning 0.135s)
             Mean action noise std: 2.44
          Mean value_function loss: 7.4302
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.5085
                       Mean reward: 3.45
               Mean episode length: 208.42
    Episode_Reward/reaching_object: 0.8768
     Episode_Reward/lifting_object: 0.0838
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.11s
                      Time elapsed: 00:15:53
                               ETA: 05:39:46

################################################################################
                     [1m Learning iteration 447/10000 [0m                     

                       Computation: 47453 steps/s (collection: 1.984s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 7.2138
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.5363
                       Mean reward: 3.91
               Mean episode length: 201.56
    Episode_Reward/reaching_object: 0.8661
     Episode_Reward/lifting_object: 0.3388
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.07s
                      Time elapsed: 00:15:55
                               ETA: 05:39:43

################################################################################
                     [1m Learning iteration 448/10000 [0m                     

                       Computation: 48665 steps/s (collection: 1.930s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 2.9201
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.5571
                       Mean reward: 6.62
               Mean episode length: 210.76
    Episode_Reward/reaching_object: 0.8823
     Episode_Reward/lifting_object: 0.3797
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.02s
                      Time elapsed: 00:15:57
                               ETA: 05:39:38

################################################################################
                     [1m Learning iteration 449/10000 [0m                     

                       Computation: 47639 steps/s (collection: 1.959s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 5.9925
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.6061
                       Mean reward: 4.97
               Mean episode length: 201.00
    Episode_Reward/reaching_object: 0.8934
     Episode_Reward/lifting_object: 0.2868
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.06s
                      Time elapsed: 00:15:59
                               ETA: 05:39:35

################################################################################
                     [1m Learning iteration 450/10000 [0m                     

                       Computation: 48750 steps/s (collection: 1.926s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 2.8611
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.6509
                       Mean reward: 7.01
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 0.8411
     Episode_Reward/lifting_object: 0.3435
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.02s
                      Time elapsed: 00:16:02
                               ETA: 05:39:30

################################################################################
                     [1m Learning iteration 451/10000 [0m                     

                       Computation: 48274 steps/s (collection: 1.946s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 6.1831
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.6894
                       Mean reward: 4.41
               Mean episode length: 198.37
    Episode_Reward/reaching_object: 0.8792
     Episode_Reward/lifting_object: 0.3505
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.04s
                      Time elapsed: 00:16:04
                               ETA: 05:39:26

################################################################################
                     [1m Learning iteration 452/10000 [0m                     

                       Computation: 47344 steps/s (collection: 1.971s, learning 0.105s)
             Mean action noise std: 2.46
          Mean value_function loss: 4.9275
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.7332
                       Mean reward: 4.19
               Mean episode length: 191.80
    Episode_Reward/reaching_object: 0.8785
     Episode_Reward/lifting_object: 0.3883
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.08s
                      Time elapsed: 00:16:06
                               ETA: 05:39:23

################################################################################
                     [1m Learning iteration 453/10000 [0m                     

                       Computation: 47159 steps/s (collection: 1.983s, learning 0.101s)
             Mean action noise std: 2.46
          Mean value_function loss: 5.9483
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 71.7606
                       Mean reward: 7.22
               Mean episode length: 192.81
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 0.6123
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.08s
                      Time elapsed: 00:16:08
                               ETA: 05:39:19

################################################################################
                     [1m Learning iteration 454/10000 [0m                     

                       Computation: 46982 steps/s (collection: 1.998s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 3.4655
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.7852
                       Mean reward: 6.06
               Mean episode length: 190.72
    Episode_Reward/reaching_object: 0.8495
     Episode_Reward/lifting_object: 0.4132
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.09s
                      Time elapsed: 00:16:10
                               ETA: 05:39:17

################################################################################
                     [1m Learning iteration 455/10000 [0m                     

                       Computation: 46358 steps/s (collection: 2.008s, learning 0.113s)
             Mean action noise std: 2.46
          Mean value_function loss: 4.0760
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.8218
                       Mean reward: 6.41
               Mean episode length: 204.81
    Episode_Reward/reaching_object: 0.8234
     Episode_Reward/lifting_object: 0.5434
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.12s
                      Time elapsed: 00:16:12
                               ETA: 05:39:14

################################################################################
                     [1m Learning iteration 456/10000 [0m                     

                       Computation: 47260 steps/s (collection: 1.986s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 3.4657
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 71.8569
                       Mean reward: 7.30
               Mean episode length: 215.32
    Episode_Reward/reaching_object: 0.9205
     Episode_Reward/lifting_object: 0.7692
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.08s
                      Time elapsed: 00:16:14
                               ETA: 05:39:11

################################################################################
                     [1m Learning iteration 457/10000 [0m                     

                       Computation: 46467 steps/s (collection: 2.015s, learning 0.101s)
             Mean action noise std: 2.47
          Mean value_function loss: 2.5037
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.9145
                       Mean reward: 5.60
               Mean episode length: 202.40
    Episode_Reward/reaching_object: 0.8831
     Episode_Reward/lifting_object: 0.5980
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.12s
                      Time elapsed: 00:16:16
                               ETA: 05:39:08

################################################################################
                     [1m Learning iteration 458/10000 [0m                     

                       Computation: 47653 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 5.0151
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.9476
                       Mean reward: 7.85
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 0.9178
     Episode_Reward/lifting_object: 0.5859
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.06s
                      Time elapsed: 00:16:18
                               ETA: 05:39:05

################################################################################
                     [1m Learning iteration 459/10000 [0m                     

                       Computation: 47287 steps/s (collection: 1.973s, learning 0.106s)
             Mean action noise std: 2.48
          Mean value_function loss: 3.0559
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 71.9832
                       Mean reward: 6.34
               Mean episode length: 212.74
    Episode_Reward/reaching_object: 0.9302
     Episode_Reward/lifting_object: 0.7412
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.08s
                      Time elapsed: 00:16:20
                               ETA: 05:39:02

################################################################################
                     [1m Learning iteration 460/10000 [0m                     

                       Computation: 47317 steps/s (collection: 1.960s, learning 0.118s)
             Mean action noise std: 2.48
          Mean value_function loss: 3.1992
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.0430
                       Mean reward: 6.65
               Mean episode length: 212.71
    Episode_Reward/reaching_object: 0.9056
     Episode_Reward/lifting_object: 0.3705
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.08s
                      Time elapsed: 00:16:22
                               ETA: 05:38:58

################################################################################
                     [1m Learning iteration 461/10000 [0m                     

                       Computation: 45937 steps/s (collection: 1.991s, learning 0.148s)
             Mean action noise std: 2.49
          Mean value_function loss: 10.9372
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.0978
                       Mean reward: 6.24
               Mean episode length: 193.22
    Episode_Reward/reaching_object: 0.8981
     Episode_Reward/lifting_object: 0.6335
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.14s
                      Time elapsed: 00:16:24
                               ETA: 05:38:56

################################################################################
                     [1m Learning iteration 462/10000 [0m                     

                       Computation: 47085 steps/s (collection: 1.980s, learning 0.108s)
             Mean action noise std: 2.49
          Mean value_function loss: 6.2456
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.1481
                       Mean reward: 4.35
               Mean episode length: 205.29
    Episode_Reward/reaching_object: 0.8869
     Episode_Reward/lifting_object: 0.1155
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.09s
                      Time elapsed: 00:16:27
                               ETA: 05:38:53

################################################################################
                     [1m Learning iteration 463/10000 [0m                     

                       Computation: 47933 steps/s (collection: 1.956s, learning 0.095s)
             Mean action noise std: 2.49
          Mean value_function loss: 6.1609
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.1979
                       Mean reward: 6.75
               Mean episode length: 209.82
    Episode_Reward/reaching_object: 0.9007
     Episode_Reward/lifting_object: 0.6352
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.05s
                      Time elapsed: 00:16:29
                               ETA: 05:38:50

################################################################################
                     [1m Learning iteration 464/10000 [0m                     

                       Computation: 47676 steps/s (collection: 1.971s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 6.6858
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.2425
                       Mean reward: 6.76
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 0.8998
     Episode_Reward/lifting_object: 0.3890
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.06s
                      Time elapsed: 00:16:31
                               ETA: 05:38:46

################################################################################
                     [1m Learning iteration 465/10000 [0m                     

                       Computation: 47791 steps/s (collection: 1.965s, learning 0.092s)
             Mean action noise std: 2.50
          Mean value_function loss: 6.0059
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.2791
                       Mean reward: 6.14
               Mean episode length: 193.22
    Episode_Reward/reaching_object: 0.8795
     Episode_Reward/lifting_object: 0.5628
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.06s
                      Time elapsed: 00:16:33
                               ETA: 05:38:42

################################################################################
                     [1m Learning iteration 466/10000 [0m                     

                       Computation: 46562 steps/s (collection: 2.017s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 4.0333
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.3234
                       Mean reward: 5.98
               Mean episode length: 194.72
    Episode_Reward/reaching_object: 0.8822
     Episode_Reward/lifting_object: 0.3659
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.11s
                      Time elapsed: 00:16:35
                               ETA: 05:38:40

################################################################################
                     [1m Learning iteration 467/10000 [0m                     

                       Computation: 47104 steps/s (collection: 1.985s, learning 0.102s)
             Mean action noise std: 2.51
          Mean value_function loss: 6.3270
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.3671
                       Mean reward: 7.33
               Mean episode length: 194.64
    Episode_Reward/reaching_object: 0.8583
     Episode_Reward/lifting_object: 0.6955
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.09s
                      Time elapsed: 00:16:37
                               ETA: 05:38:37

################################################################################
                     [1m Learning iteration 468/10000 [0m                     

                       Computation: 46650 steps/s (collection: 2.018s, learning 0.090s)
             Mean action noise std: 2.51
          Mean value_function loss: 3.7613
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.4150
                       Mean reward: 6.41
               Mean episode length: 191.35
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 0.6122
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.11s
                      Time elapsed: 00:16:39
                               ETA: 05:38:34

################################################################################
                     [1m Learning iteration 469/10000 [0m                     

                       Computation: 46677 steps/s (collection: 2.017s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 7.3945
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.4569
                       Mean reward: 4.72
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 0.8703
     Episode_Reward/lifting_object: 0.5758
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.11s
                      Time elapsed: 00:16:41
                               ETA: 05:38:31

################################################################################
                     [1m Learning iteration 470/10000 [0m                     

                       Computation: 44171 steps/s (collection: 2.100s, learning 0.125s)
             Mean action noise std: 2.52
          Mean value_function loss: 3.6295
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 72.5030
                       Mean reward: 8.59
               Mean episode length: 216.46
    Episode_Reward/reaching_object: 0.8711
     Episode_Reward/lifting_object: 0.7224
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.23s
                      Time elapsed: 00:16:43
                               ETA: 05:38:31

################################################################################
                     [1m Learning iteration 471/10000 [0m                     

                       Computation: 45555 steps/s (collection: 2.047s, learning 0.111s)
             Mean action noise std: 2.52
          Mean value_function loss: 4.9026
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.5508
                       Mean reward: 7.49
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 0.1973
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.16s
                      Time elapsed: 00:16:46
                               ETA: 05:38:30

################################################################################
                     [1m Learning iteration 472/10000 [0m                     

                       Computation: 46756 steps/s (collection: 1.996s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 8.4131
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 72.5966
                       Mean reward: 7.11
               Mean episode length: 208.58
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: 0.7133
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.10s
                      Time elapsed: 00:16:48
                               ETA: 05:38:27

################################################################################
                     [1m Learning iteration 473/10000 [0m                     

                       Computation: 47212 steps/s (collection: 1.986s, learning 0.096s)
             Mean action noise std: 2.53
          Mean value_function loss: 14.2969
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.6064
                       Mean reward: 4.52
               Mean episode length: 198.87
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: 0.6128
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.08s
                      Time elapsed: 00:16:50
                               ETA: 05:38:24

################################################################################
                     [1m Learning iteration 474/10000 [0m                     

                       Computation: 44631 steps/s (collection: 2.033s, learning 0.170s)
             Mean action noise std: 2.53
          Mean value_function loss: 12.0554
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.6256
                       Mean reward: 7.04
               Mean episode length: 205.92
    Episode_Reward/reaching_object: 0.8817
     Episode_Reward/lifting_object: 0.3920
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.20s
                      Time elapsed: 00:16:52
                               ETA: 05:38:23

################################################################################
                     [1m Learning iteration 475/10000 [0m                     

                       Computation: 46994 steps/s (collection: 1.998s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 3.6922
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.6408
                       Mean reward: 8.00
               Mean episode length: 208.57
    Episode_Reward/reaching_object: 0.8141
     Episode_Reward/lifting_object: 0.4433
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.09s
                      Time elapsed: 00:16:54
                               ETA: 05:38:20

################################################################################
                     [1m Learning iteration 476/10000 [0m                     

                       Computation: 47488 steps/s (collection: 1.967s, learning 0.103s)
             Mean action noise std: 2.53
          Mean value_function loss: 5.2642
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.6707
                       Mean reward: 4.50
               Mean episode length: 187.00
    Episode_Reward/reaching_object: 0.8389
     Episode_Reward/lifting_object: 0.6354
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.07s
                      Time elapsed: 00:16:56
                               ETA: 05:38:17

################################################################################
                     [1m Learning iteration 477/10000 [0m                     

                       Computation: 45886 steps/s (collection: 2.040s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 2.4731
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.7131
                       Mean reward: 8.55
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 0.8707
     Episode_Reward/lifting_object: 0.6984
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.14s
                      Time elapsed: 00:16:58
                               ETA: 05:38:15

################################################################################
                     [1m Learning iteration 478/10000 [0m                     

                       Computation: 46153 steps/s (collection: 2.024s, learning 0.106s)
             Mean action noise std: 2.54
          Mean value_function loss: 3.3224
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 72.7530
                       Mean reward: 6.72
               Mean episode length: 188.73
    Episode_Reward/reaching_object: 0.8019
     Episode_Reward/lifting_object: 0.5782
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.13s
                      Time elapsed: 00:17:00
                               ETA: 05:38:13

################################################################################
                     [1m Learning iteration 479/10000 [0m                     

                       Computation: 46833 steps/s (collection: 1.982s, learning 0.117s)
             Mean action noise std: 2.54
          Mean value_function loss: 7.1316
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.7907
                       Mean reward: 7.51
               Mean episode length: 184.82
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 0.6198
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.10s
                      Time elapsed: 00:17:02
                               ETA: 05:38:10

################################################################################
                     [1m Learning iteration 480/10000 [0m                     

                       Computation: 45424 steps/s (collection: 2.021s, learning 0.144s)
             Mean action noise std: 2.55
          Mean value_function loss: 4.5530
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.8284
                       Mean reward: 7.40
               Mean episode length: 196.65
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 0.6619
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.16s
                      Time elapsed: 00:17:05
                               ETA: 05:38:09

################################################################################
                     [1m Learning iteration 481/10000 [0m                     

                       Computation: 46755 steps/s (collection: 1.984s, learning 0.119s)
             Mean action noise std: 2.55
          Mean value_function loss: 8.2772
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.8645
                       Mean reward: 6.36
               Mean episode length: 182.92
    Episode_Reward/reaching_object: 0.8137
     Episode_Reward/lifting_object: 0.5607
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.10s
                      Time elapsed: 00:17:07
                               ETA: 05:38:06

################################################################################
                     [1m Learning iteration 482/10000 [0m                     

                       Computation: 46483 steps/s (collection: 2.006s, learning 0.109s)
             Mean action noise std: 2.55
          Mean value_function loss: 4.9759
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.8965
                       Mean reward: 7.60
               Mean episode length: 188.87
    Episode_Reward/reaching_object: 0.8228
     Episode_Reward/lifting_object: 0.5701
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.11s
                      Time elapsed: 00:17:09
                               ETA: 05:38:03

################################################################################
                     [1m Learning iteration 483/10000 [0m                     

                       Computation: 45437 steps/s (collection: 2.031s, learning 0.132s)
             Mean action noise std: 2.56
          Mean value_function loss: 4.0794
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 72.9532
                       Mean reward: 5.57
               Mean episode length: 188.20
    Episode_Reward/reaching_object: 0.8110
     Episode_Reward/lifting_object: 0.6051
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.16s
                      Time elapsed: 00:17:11
                               ETA: 05:38:02

################################################################################
                     [1m Learning iteration 484/10000 [0m                     

                       Computation: 45564 steps/s (collection: 2.039s, learning 0.118s)
             Mean action noise std: 2.56
          Mean value_function loss: 4.5873
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.9959
                       Mean reward: 8.48
               Mean episode length: 181.47
    Episode_Reward/reaching_object: 0.8018
     Episode_Reward/lifting_object: 0.6723
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.16s
                      Time elapsed: 00:17:13
                               ETA: 05:38:00

################################################################################
                     [1m Learning iteration 485/10000 [0m                     

                       Computation: 46952 steps/s (collection: 1.994s, learning 0.100s)
             Mean action noise std: 2.56
          Mean value_function loss: 5.3866
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.0309
                       Mean reward: 10.10
               Mean episode length: 195.43
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 1.0150
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.09s
                      Time elapsed: 00:17:15
                               ETA: 05:37:57

################################################################################
                     [1m Learning iteration 486/10000 [0m                     

                       Computation: 46618 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 5.6046
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.0542
                       Mean reward: 7.65
               Mean episode length: 197.33
    Episode_Reward/reaching_object: 0.8238
     Episode_Reward/lifting_object: 0.6576
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.11s
                      Time elapsed: 00:17:17
                               ETA: 05:37:55

################################################################################
                     [1m Learning iteration 487/10000 [0m                     

                       Computation: 46191 steps/s (collection: 2.036s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 12.1388
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 73.0672
                       Mean reward: 8.77
               Mean episode length: 186.65
    Episode_Reward/reaching_object: 0.8090
     Episode_Reward/lifting_object: 0.5773
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.13s
                      Time elapsed: 00:17:19
                               ETA: 05:37:53

################################################################################
                     [1m Learning iteration 488/10000 [0m                     

                       Computation: 46584 steps/s (collection: 1.991s, learning 0.120s)
             Mean action noise std: 2.56
          Mean value_function loss: 9.1773
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.0763
                       Mean reward: 3.05
               Mean episode length: 195.71
    Episode_Reward/reaching_object: 0.8121
     Episode_Reward/lifting_object: 0.5389
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.11s
                      Time elapsed: 00:17:22
                               ETA: 05:37:50

################################################################################
                     [1m Learning iteration 489/10000 [0m                     

                       Computation: 45017 steps/s (collection: 2.022s, learning 0.162s)
             Mean action noise std: 2.57
          Mean value_function loss: 5.3717
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.1084
                       Mean reward: 1.77
               Mean episode length: 195.68
    Episode_Reward/reaching_object: 0.8119
     Episode_Reward/lifting_object: 0.6253
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.18s
                      Time elapsed: 00:17:24
                               ETA: 05:37:49

################################################################################
                     [1m Learning iteration 490/10000 [0m                     

                       Computation: 46090 steps/s (collection: 2.015s, learning 0.118s)
             Mean action noise std: 2.57
          Mean value_function loss: 2.8775
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 73.1615
                       Mean reward: 5.51
               Mean episode length: 205.71
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 0.7018
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.13s
                      Time elapsed: 00:17:26
                               ETA: 05:37:47

################################################################################
                     [1m Learning iteration 491/10000 [0m                     

                       Computation: 47423 steps/s (collection: 1.978s, learning 0.095s)
             Mean action noise std: 2.58
          Mean value_function loss: 5.0199
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.2020
                       Mean reward: 7.95
               Mean episode length: 212.86
    Episode_Reward/reaching_object: 0.8284
     Episode_Reward/lifting_object: 0.6946
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.07s
                      Time elapsed: 00:17:28
                               ETA: 05:37:44

################################################################################
                     [1m Learning iteration 492/10000 [0m                     

                       Computation: 46834 steps/s (collection: 2.006s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 11.4017
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 73.2218
                       Mean reward: 9.74
               Mean episode length: 199.85
    Episode_Reward/reaching_object: 0.8059
     Episode_Reward/lifting_object: 0.7137
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.10s
                      Time elapsed: 00:17:30
                               ETA: 05:37:41

################################################################################
                     [1m Learning iteration 493/10000 [0m                     

                       Computation: 46889 steps/s (collection: 2.003s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 6.0554
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 73.2487
                       Mean reward: 5.19
               Mean episode length: 194.46
    Episode_Reward/reaching_object: 0.7971
     Episode_Reward/lifting_object: 0.4911
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.10s
                      Time elapsed: 00:17:32
                               ETA: 05:37:38

################################################################################
                     [1m Learning iteration 494/10000 [0m                     

                       Computation: 46918 steps/s (collection: 1.995s, learning 0.100s)
             Mean action noise std: 2.58
          Mean value_function loss: 4.0915
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.2853
                       Mean reward: 8.86
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 0.9380
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.10s
                      Time elapsed: 00:17:34
                               ETA: 05:37:35

################################################################################
                     [1m Learning iteration 495/10000 [0m                     

                       Computation: 46465 steps/s (collection: 2.014s, learning 0.102s)
             Mean action noise std: 2.59
          Mean value_function loss: 5.5530
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.3181
                       Mean reward: 6.73
               Mean episode length: 177.65
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 0.7554
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.12s
                      Time elapsed: 00:17:36
                               ETA: 05:37:33

################################################################################
                     [1m Learning iteration 496/10000 [0m                     

                       Computation: 47093 steps/s (collection: 1.988s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 6.6561
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.3667
                       Mean reward: 5.61
               Mean episode length: 185.37
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 0.7715
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.09s
                      Time elapsed: 00:17:38
                               ETA: 05:37:30

################################################################################
                     [1m Learning iteration 497/10000 [0m                     

                       Computation: 46731 steps/s (collection: 2.003s, learning 0.101s)
             Mean action noise std: 2.59
          Mean value_function loss: 17.1537
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.4111
                       Mean reward: 6.00
               Mean episode length: 188.14
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 0.6413
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.10s
                      Time elapsed: 00:17:41
                               ETA: 05:37:27

################################################################################
                     [1m Learning iteration 498/10000 [0m                     

                       Computation: 46569 steps/s (collection: 2.002s, learning 0.109s)
             Mean action noise std: 2.60
          Mean value_function loss: 16.4127
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.4532
                       Mean reward: 8.15
               Mean episode length: 181.14
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 0.6062
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.11s
                      Time elapsed: 00:17:43
                               ETA: 05:37:25

################################################################################
                     [1m Learning iteration 499/10000 [0m                     

                       Computation: 46441 steps/s (collection: 2.018s, learning 0.099s)
             Mean action noise std: 2.60
          Mean value_function loss: 8.9999
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4965
                       Mean reward: 2.65
               Mean episode length: 183.56
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 0.4664
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.12s
                      Time elapsed: 00:17:45
                               ETA: 05:37:22

################################################################################
                     [1m Learning iteration 500/10000 [0m                     

                       Computation: 46199 steps/s (collection: 2.012s, learning 0.116s)
             Mean action noise std: 2.60
          Mean value_function loss: 8.1887
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.5380
                       Mean reward: 7.96
               Mean episode length: 185.34
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 0.8118
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.13s
                      Time elapsed: 00:17:47
                               ETA: 05:37:20

################################################################################
                     [1m Learning iteration 501/10000 [0m                     

                       Computation: 45861 steps/s (collection: 2.025s, learning 0.119s)
             Mean action noise std: 2.61
          Mean value_function loss: 5.5002
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 73.5843
                       Mean reward: 6.63
               Mean episode length: 185.95
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 0.7266
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.14s
                      Time elapsed: 00:17:49
                               ETA: 05:37:18

################################################################################
                     [1m Learning iteration 502/10000 [0m                     

                       Computation: 46771 steps/s (collection: 2.003s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 4.7895
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.6290
                       Mean reward: 6.46
               Mean episode length: 183.63
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 0.6343
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.10s
                      Time elapsed: 00:17:51
                               ETA: 05:37:16

################################################################################
                     [1m Learning iteration 503/10000 [0m                     

                       Computation: 46585 steps/s (collection: 1.994s, learning 0.117s)
             Mean action noise std: 2.61
          Mean value_function loss: 5.9934
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.6534
                       Mean reward: 8.35
               Mean episode length: 175.91
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 0.9303
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.11s
                      Time elapsed: 00:17:53
                               ETA: 05:37:13

################################################################################
                     [1m Learning iteration 504/10000 [0m                     

                       Computation: 47047 steps/s (collection: 1.996s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 6.0370
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.6986
                       Mean reward: 5.84
               Mean episode length: 196.87
    Episode_Reward/reaching_object: 0.8193
     Episode_Reward/lifting_object: 0.8972
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.09s
                      Time elapsed: 00:17:55
                               ETA: 05:37:10

################################################################################
                     [1m Learning iteration 505/10000 [0m                     

                       Computation: 47166 steps/s (collection: 1.991s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 4.0171
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 73.7423
                       Mean reward: 7.50
               Mean episode length: 190.92
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 0.6382
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.08s
                      Time elapsed: 00:17:57
                               ETA: 05:37:07

################################################################################
                     [1m Learning iteration 506/10000 [0m                     

                       Computation: 46232 steps/s (collection: 2.031s, learning 0.096s)
             Mean action noise std: 2.63
          Mean value_function loss: 9.4874
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.7923
                       Mean reward: 8.02
               Mean episode length: 184.85
    Episode_Reward/reaching_object: 0.8036
     Episode_Reward/lifting_object: 1.0007
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.13s
                      Time elapsed: 00:18:00
                               ETA: 05:37:05

################################################################################
                     [1m Learning iteration 507/10000 [0m                     

                       Computation: 46906 steps/s (collection: 1.994s, learning 0.102s)
             Mean action noise std: 2.63
          Mean value_function loss: 9.0440
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8402
                       Mean reward: 8.54
               Mean episode length: 183.26
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 0.7173
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.10s
                      Time elapsed: 00:18:02
                               ETA: 05:37:02

################################################################################
                     [1m Learning iteration 508/10000 [0m                     

                       Computation: 46666 steps/s (collection: 2.011s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 6.1224
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 73.8775
                       Mean reward: 7.61
               Mean episode length: 191.89
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 0.8359
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.11s
                      Time elapsed: 00:18:04
                               ETA: 05:37:00

################################################################################
                     [1m Learning iteration 509/10000 [0m                     

                       Computation: 46337 steps/s (collection: 2.021s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 5.2471
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.9192
                       Mean reward: 8.83
               Mean episode length: 183.51
    Episode_Reward/reaching_object: 0.7903
     Episode_Reward/lifting_object: 1.2211
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.12s
                      Time elapsed: 00:18:06
                               ETA: 05:36:57

################################################################################
                     [1m Learning iteration 510/10000 [0m                     

                       Computation: 47477 steps/s (collection: 1.979s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 48.1672
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 73.9463
                       Mean reward: 5.92
               Mean episode length: 184.51
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 0.6500
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.07s
                      Time elapsed: 00:18:08
                               ETA: 05:36:54

################################################################################
                     [1m Learning iteration 511/10000 [0m                     

                       Computation: 46758 steps/s (collection: 2.011s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 11.7783
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.9535
                       Mean reward: 5.96
               Mean episode length: 181.11
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 0.7781
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.10s
                      Time elapsed: 00:18:10
                               ETA: 05:36:51

################################################################################
                     [1m Learning iteration 512/10000 [0m                     

                       Computation: 46094 steps/s (collection: 2.009s, learning 0.124s)
             Mean action noise std: 2.64
          Mean value_function loss: 4.8589
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 73.9865
                       Mean reward: 8.23
               Mean episode length: 166.97
    Episode_Reward/reaching_object: 0.7120
     Episode_Reward/lifting_object: 0.8238
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.13s
                      Time elapsed: 00:18:12
                               ETA: 05:36:49

################################################################################
                     [1m Learning iteration 513/10000 [0m                     

                       Computation: 45790 steps/s (collection: 2.035s, learning 0.112s)
             Mean action noise std: 2.64
          Mean value_function loss: 16.4626
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.0259
                       Mean reward: 7.88
               Mean episode length: 178.86
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 0.5985
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.15s
                      Time elapsed: 00:18:14
                               ETA: 05:36:48

################################################################################
                     [1m Learning iteration 514/10000 [0m                     

                       Computation: 45938 steps/s (collection: 2.026s, learning 0.114s)
             Mean action noise std: 2.65
          Mean value_function loss: 4.7153
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 74.0496
                       Mean reward: 7.73
               Mean episode length: 171.97
    Episode_Reward/reaching_object: 0.6906
     Episode_Reward/lifting_object: 0.6117
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.14s
                      Time elapsed: 00:18:16
                               ETA: 05:36:46

################################################################################
                     [1m Learning iteration 515/10000 [0m                     

                       Computation: 46514 steps/s (collection: 2.015s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 12.7147
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.0891
                       Mean reward: 4.73
               Mean episode length: 175.00
    Episode_Reward/reaching_object: 0.6832
     Episode_Reward/lifting_object: 0.8606
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.11s
                      Time elapsed: 00:18:19
                               ETA: 05:36:43

################################################################################
                     [1m Learning iteration 516/10000 [0m                     

                       Computation: 46019 steps/s (collection: 2.005s, learning 0.132s)
             Mean action noise std: 2.65
          Mean value_function loss: 15.5460
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1198
                       Mean reward: 8.45
               Mean episode length: 180.74
    Episode_Reward/reaching_object: 0.7017
     Episode_Reward/lifting_object: 0.8899
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.14s
                      Time elapsed: 00:18:21
                               ETA: 05:36:41

################################################################################
                     [1m Learning iteration 517/10000 [0m                     

                       Computation: 46689 steps/s (collection: 2.000s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 10.3161
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.1500
                       Mean reward: 7.75
               Mean episode length: 182.42
    Episode_Reward/reaching_object: 0.7071
     Episode_Reward/lifting_object: 0.7622
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.11s
                      Time elapsed: 00:18:23
                               ETA: 05:36:39

################################################################################
                     [1m Learning iteration 518/10000 [0m                     

                       Computation: 46278 steps/s (collection: 2.023s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 12.1002
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.1829
                       Mean reward: 9.14
               Mean episode length: 198.98
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 0.9641
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.12s
                      Time elapsed: 00:18:25
                               ETA: 05:36:36

################################################################################
                     [1m Learning iteration 519/10000 [0m                     

                       Computation: 47260 steps/s (collection: 1.986s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 6.4710
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 74.2246
                       Mean reward: 3.21
               Mean episode length: 183.59
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 0.6023
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.08s
                      Time elapsed: 00:18:27
                               ETA: 05:36:33

################################################################################
                     [1m Learning iteration 520/10000 [0m                     

                       Computation: 46228 steps/s (collection: 2.022s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 11.4413
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2710
                       Mean reward: 6.81
               Mean episode length: 182.18
    Episode_Reward/reaching_object: 0.6960
     Episode_Reward/lifting_object: 0.8011
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.13s
                      Time elapsed: 00:18:29
                               ETA: 05:36:31

################################################################################
                     [1m Learning iteration 521/10000 [0m                     

                       Computation: 46479 steps/s (collection: 2.020s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 7.5802
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.3156
                       Mean reward: 6.44
               Mean episode length: 171.91
    Episode_Reward/reaching_object: 0.6765
     Episode_Reward/lifting_object: 0.6193
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.12s
                      Time elapsed: 00:18:31
                               ETA: 05:36:29

################################################################################
                     [1m Learning iteration 522/10000 [0m                     

                       Computation: 47124 steps/s (collection: 1.991s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 18.1110
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.3525
                       Mean reward: 6.35
               Mean episode length: 182.82
    Episode_Reward/reaching_object: 0.7096
     Episode_Reward/lifting_object: 0.8974
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.09s
                      Time elapsed: 00:18:33
                               ETA: 05:36:26

################################################################################
                     [1m Learning iteration 523/10000 [0m                     

                       Computation: 46202 steps/s (collection: 2.010s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 5.9696
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 74.3920
                       Mean reward: 8.75
               Mean episode length: 183.23
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 0.9084
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.13s
                      Time elapsed: 00:18:36
                               ETA: 05:36:24

################################################################################
                     [1m Learning iteration 524/10000 [0m                     

                       Computation: 46403 steps/s (collection: 2.024s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 8.4595
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.4415
                       Mean reward: 5.13
               Mean episode length: 190.89
    Episode_Reward/reaching_object: 0.7161
     Episode_Reward/lifting_object: 0.8089
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.12s
                      Time elapsed: 00:18:38
                               ETA: 05:36:21

################################################################################
                     [1m Learning iteration 525/10000 [0m                     

                       Computation: 46279 steps/s (collection: 2.022s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 6.5607
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.4774
                       Mean reward: 8.33
               Mean episode length: 186.16
    Episode_Reward/reaching_object: 0.7221
     Episode_Reward/lifting_object: 1.1367
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.12s
                      Time elapsed: 00:18:40
                               ETA: 05:36:19

################################################################################
                     [1m Learning iteration 526/10000 [0m                     

                       Computation: 46038 steps/s (collection: 2.022s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 6.9906
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 74.5000
                       Mean reward: 11.37
               Mean episode length: 174.73
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 1.1505
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.14s
                      Time elapsed: 00:18:42
                               ETA: 05:36:17

################################################################################
                     [1m Learning iteration 527/10000 [0m                     

                       Computation: 45686 steps/s (collection: 2.039s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 6.1703
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.5243
                       Mean reward: 7.05
               Mean episode length: 172.60
    Episode_Reward/reaching_object: 0.7129
     Episode_Reward/lifting_object: 0.7557
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.15s
                      Time elapsed: 00:18:44
                               ETA: 05:36:15

################################################################################
                     [1m Learning iteration 528/10000 [0m                     

                       Computation: 46334 steps/s (collection: 2.011s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 13.9112
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.5547
                       Mean reward: 11.46
               Mean episode length: 177.27
    Episode_Reward/reaching_object: 0.7400
     Episode_Reward/lifting_object: 0.7186
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.12s
                      Time elapsed: 00:18:46
                               ETA: 05:36:13

################################################################################
                     [1m Learning iteration 529/10000 [0m                     

                       Computation: 46202 steps/s (collection: 2.024s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 5.2464
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 74.5827
                       Mean reward: 9.20
               Mean episode length: 188.60
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 0.7720
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.13s
                      Time elapsed: 00:18:48
                               ETA: 05:36:11

################################################################################
                     [1m Learning iteration 530/10000 [0m                     

                       Computation: 46497 steps/s (collection: 1.999s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 9.1937
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.6144
                       Mean reward: 8.19
               Mean episode length: 186.83
    Episode_Reward/reaching_object: 0.8000
     Episode_Reward/lifting_object: 1.0160
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.11s
                      Time elapsed: 00:18:50
                               ETA: 05:36:08

################################################################################
                     [1m Learning iteration 531/10000 [0m                     

                       Computation: 46394 steps/s (collection: 2.012s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 9.8350
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.6516
                       Mean reward: 8.71
               Mean episode length: 187.18
    Episode_Reward/reaching_object: 0.8118
     Episode_Reward/lifting_object: 1.0282
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.12s
                      Time elapsed: 00:18:53
                               ETA: 05:36:06

################################################################################
                     [1m Learning iteration 532/10000 [0m                     

                       Computation: 46648 steps/s (collection: 2.007s, learning 0.100s)
             Mean action noise std: 2.70
          Mean value_function loss: 11.6305
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.6834
                       Mean reward: 6.86
               Mean episode length: 187.35
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 0.9780
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.11s
                      Time elapsed: 00:18:55
                               ETA: 05:36:04

################################################################################
                     [1m Learning iteration 533/10000 [0m                     

                       Computation: 46747 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 9.4721
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.7195
                       Mean reward: 5.52
               Mean episode length: 192.02
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 1.0925
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.10s
                      Time elapsed: 00:18:57
                               ETA: 05:36:01

################################################################################
                     [1m Learning iteration 534/10000 [0m                     

                       Computation: 46582 steps/s (collection: 2.009s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 7.7419
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 74.7544
                       Mean reward: 9.17
               Mean episode length: 197.26
    Episode_Reward/reaching_object: 0.7995
     Episode_Reward/lifting_object: 1.1179
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.11s
                      Time elapsed: 00:18:59
                               ETA: 05:35:58

################################################################################
                     [1m Learning iteration 535/10000 [0m                     

                       Computation: 47209 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 7.3006
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.7834
                       Mean reward: 10.66
               Mean episode length: 195.36
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 0.9799
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.08s
                      Time elapsed: 00:19:01
                               ETA: 05:35:56

################################################################################
                     [1m Learning iteration 536/10000 [0m                     

                       Computation: 46519 steps/s (collection: 2.008s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 15.4130
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.8178
                       Mean reward: 6.61
               Mean episode length: 203.34
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 1.1087
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.11s
                      Time elapsed: 00:19:03
                               ETA: 05:35:53

################################################################################
                     [1m Learning iteration 537/10000 [0m                     

                       Computation: 47002 steps/s (collection: 1.994s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 5.9720
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 74.8572
                       Mean reward: 10.41
               Mean episode length: 202.65
    Episode_Reward/reaching_object: 0.8244
     Episode_Reward/lifting_object: 1.1036
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.09s
                      Time elapsed: 00:19:05
                               ETA: 05:35:50

################################################################################
                     [1m Learning iteration 538/10000 [0m                     

                       Computation: 46971 steps/s (collection: 1.992s, learning 0.101s)
             Mean action noise std: 2.72
          Mean value_function loss: 9.5176
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.8862
                       Mean reward: 9.24
               Mean episode length: 201.38
    Episode_Reward/reaching_object: 0.8310
     Episode_Reward/lifting_object: 1.4282
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.09s
                      Time elapsed: 00:19:07
                               ETA: 05:35:48

################################################################################
                     [1m Learning iteration 539/10000 [0m                     

                       Computation: 46941 steps/s (collection: 2.002s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 11.7976
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.9065
                       Mean reward: 8.52
               Mean episode length: 194.67
    Episode_Reward/reaching_object: 0.7920
     Episode_Reward/lifting_object: 0.9056
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.09s
                      Time elapsed: 00:19:09
                               ETA: 05:35:45

################################################################################
                     [1m Learning iteration 540/10000 [0m                     

                       Computation: 46547 steps/s (collection: 2.005s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 6.3310
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.9342
                       Mean reward: 6.11
               Mean episode length: 199.42
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 0.8276
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.11s
                      Time elapsed: 00:19:11
                               ETA: 05:35:42

################################################################################
                     [1m Learning iteration 541/10000 [0m                     

                       Computation: 46626 steps/s (collection: 2.007s, learning 0.101s)
             Mean action noise std: 2.73
          Mean value_function loss: 5.5432
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.9620
                       Mean reward: 9.74
               Mean episode length: 205.07
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 1.1157
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.11s
                      Time elapsed: 00:19:14
                               ETA: 05:35:40

################################################################################
                     [1m Learning iteration 542/10000 [0m                     

                       Computation: 46663 steps/s (collection: 1.994s, learning 0.113s)
             Mean action noise std: 2.73
          Mean value_function loss: 8.1813
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.9924
                       Mean reward: 6.44
               Mean episode length: 202.55
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 0.9411
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.11s
                      Time elapsed: 00:19:16
                               ETA: 05:35:37

################################################################################
                     [1m Learning iteration 543/10000 [0m                     

                       Computation: 46683 steps/s (collection: 1.996s, learning 0.110s)
             Mean action noise std: 2.73
          Mean value_function loss: 12.2200
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 75.0237
                       Mean reward: 9.99
               Mean episode length: 204.27
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 1.1779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.11s
                      Time elapsed: 00:19:18
                               ETA: 05:35:35

################################################################################
                     [1m Learning iteration 544/10000 [0m                     

                       Computation: 47269 steps/s (collection: 1.984s, learning 0.096s)
             Mean action noise std: 2.74
          Mean value_function loss: 6.3528
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.0654
                       Mean reward: 7.59
               Mean episode length: 199.76
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: 0.9562
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.08s
                      Time elapsed: 00:19:20
                               ETA: 05:35:32

################################################################################
                     [1m Learning iteration 545/10000 [0m                     

                       Computation: 46170 steps/s (collection: 2.016s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 9.8283
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.0849
                       Mean reward: 10.01
               Mean episode length: 200.70
    Episode_Reward/reaching_object: 0.8085
     Episode_Reward/lifting_object: 1.3761
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.13s
                      Time elapsed: 00:19:22
                               ETA: 05:35:30

################################################################################
                     [1m Learning iteration 546/10000 [0m                     

                       Computation: 46386 steps/s (collection: 2.006s, learning 0.114s)
             Mean action noise std: 2.74
          Mean value_function loss: 6.5949
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.1154
                       Mean reward: 7.04
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 0.7993
     Episode_Reward/lifting_object: 1.1429
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.12s
                      Time elapsed: 00:19:24
                               ETA: 05:35:27

################################################################################
                     [1m Learning iteration 547/10000 [0m                     

                       Computation: 46391 steps/s (collection: 1.988s, learning 0.131s)
             Mean action noise std: 2.74
          Mean value_function loss: 15.7788
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.1358
                       Mean reward: 9.63
               Mean episode length: 201.37
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: 1.4731
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.12s
                      Time elapsed: 00:19:26
                               ETA: 05:35:25

################################################################################
                     [1m Learning iteration 548/10000 [0m                     

                       Computation: 45782 steps/s (collection: 2.024s, learning 0.124s)
             Mean action noise std: 2.74
          Mean value_function loss: 9.6734
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.1574
                       Mean reward: 9.87
               Mean episode length: 207.05
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 1.3426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.15s
                      Time elapsed: 00:19:28
                               ETA: 05:35:23

################################################################################
                     [1m Learning iteration 549/10000 [0m                     

                       Computation: 46161 steps/s (collection: 2.033s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 5.4649
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 75.1911
                       Mean reward: 9.54
               Mean episode length: 173.01
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 1.2889
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.13s
                      Time elapsed: 00:19:30
                               ETA: 05:35:21

################################################################################
                     [1m Learning iteration 550/10000 [0m                     

                       Computation: 46867 steps/s (collection: 2.002s, learning 0.096s)
             Mean action noise std: 2.75
          Mean value_function loss: 16.8013
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.2215
                       Mean reward: 10.29
               Mean episode length: 187.78
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 1.4326
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.10s
                      Time elapsed: 00:19:33
                               ETA: 05:35:18

################################################################################
                     [1m Learning iteration 551/10000 [0m                     

                       Computation: 46284 steps/s (collection: 2.022s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 14.8890
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.2541
                       Mean reward: 5.60
               Mean episode length: 174.58
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 0.8862
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.12s
                      Time elapsed: 00:19:35
                               ETA: 05:35:16

################################################################################
                     [1m Learning iteration 552/10000 [0m                     

                       Computation: 46890 steps/s (collection: 2.002s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 11.2388
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.2988
                       Mean reward: 8.57
               Mean episode length: 185.66
    Episode_Reward/reaching_object: 0.7187
     Episode_Reward/lifting_object: 0.8069
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.10s
                      Time elapsed: 00:19:37
                               ETA: 05:35:14

################################################################################
                     [1m Learning iteration 553/10000 [0m                     

                       Computation: 46795 steps/s (collection: 2.006s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 8.5177
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 75.3362
                       Mean reward: 8.32
               Mean episode length: 198.32
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 1.0525
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.10s
                      Time elapsed: 00:19:39
                               ETA: 05:35:11

################################################################################
                     [1m Learning iteration 554/10000 [0m                     

                       Computation: 47100 steps/s (collection: 1.989s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 16.9774
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 75.3706
                       Mean reward: 10.96
               Mean episode length: 186.15
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 1.4972
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.09s
                      Time elapsed: 00:19:41
                               ETA: 05:35:08

################################################################################
                     [1m Learning iteration 555/10000 [0m                     

                       Computation: 46707 steps/s (collection: 1.983s, learning 0.122s)
             Mean action noise std: 2.76
          Mean value_function loss: 5.8799
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.3819
                       Mean reward: 10.89
               Mean episode length: 188.10
    Episode_Reward/reaching_object: 0.7079
     Episode_Reward/lifting_object: 0.8189
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.10s
                      Time elapsed: 00:19:43
                               ETA: 05:35:06

################################################################################
                     [1m Learning iteration 556/10000 [0m                     

                       Computation: 47053 steps/s (collection: 1.972s, learning 0.117s)
             Mean action noise std: 2.77
          Mean value_function loss: 8.8479
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.4069
                       Mean reward: 6.51
               Mean episode length: 198.57
    Episode_Reward/reaching_object: 0.6933
     Episode_Reward/lifting_object: 1.2697
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.09s
                      Time elapsed: 00:19:45
                               ETA: 05:35:03

################################################################################
                     [1m Learning iteration 557/10000 [0m                     

                       Computation: 46686 steps/s (collection: 1.996s, learning 0.110s)
             Mean action noise std: 2.77
          Mean value_function loss: 6.1048
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.4405
                       Mean reward: 8.86
               Mean episode length: 196.05
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 1.3503
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.11s
                      Time elapsed: 00:19:47
                               ETA: 05:35:00

################################################################################
                     [1m Learning iteration 558/10000 [0m                     

                       Computation: 46214 steps/s (collection: 2.027s, learning 0.100s)
             Mean action noise std: 2.77
          Mean value_function loss: 11.0540
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.4677
                       Mean reward: 5.53
               Mean episode length: 187.70
    Episode_Reward/reaching_object: 0.7167
     Episode_Reward/lifting_object: 1.3641
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.13s
                      Time elapsed: 00:19:49
                               ETA: 05:34:58

################################################################################
                     [1m Learning iteration 559/10000 [0m                     

                       Computation: 46155 steps/s (collection: 2.028s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 10.5020
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.5082
                       Mean reward: 13.73
               Mean episode length: 177.70
    Episode_Reward/reaching_object: 0.6850
     Episode_Reward/lifting_object: 1.4155
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.13s
                      Time elapsed: 00:19:52
                               ETA: 05:34:56

################################################################################
                     [1m Learning iteration 560/10000 [0m                     

                       Computation: 46205 steps/s (collection: 2.028s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 6.3751
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 75.5495
                       Mean reward: 10.40
               Mean episode length: 186.82
    Episode_Reward/reaching_object: 0.6809
     Episode_Reward/lifting_object: 1.4330
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.13s
                      Time elapsed: 00:19:54
                               ETA: 05:34:54

################################################################################
                     [1m Learning iteration 561/10000 [0m                     

                       Computation: 46144 steps/s (collection: 2.020s, learning 0.110s)
             Mean action noise std: 2.78
          Mean value_function loss: 14.7190
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 75.5647
                       Mean reward: 6.72
               Mean episode length: 179.12
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 1.2257
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.13s
                      Time elapsed: 00:19:56
                               ETA: 05:34:52

################################################################################
                     [1m Learning iteration 562/10000 [0m                     

                       Computation: 45992 steps/s (collection: 2.022s, learning 0.115s)
             Mean action noise std: 2.78
          Mean value_function loss: 11.3444
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 75.5698
                       Mean reward: 8.85
               Mean episode length: 174.01
    Episode_Reward/reaching_object: 0.6718
     Episode_Reward/lifting_object: 1.2729
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.14s
                      Time elapsed: 00:19:58
                               ETA: 05:34:50

################################################################################
                     [1m Learning iteration 563/10000 [0m                     

                       Computation: 45993 steps/s (collection: 2.015s, learning 0.122s)
             Mean action noise std: 2.78
          Mean value_function loss: 14.2147
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 75.5717
                       Mean reward: 5.41
               Mean episode length: 166.73
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 0.9967
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.14s
                      Time elapsed: 00:20:00
                               ETA: 05:34:48

################################################################################
                     [1m Learning iteration 564/10000 [0m                     

                       Computation: 46442 steps/s (collection: 2.021s, learning 0.096s)
             Mean action noise std: 2.78
          Mean value_function loss: 11.1808
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 75.5792
                       Mean reward: 9.53
               Mean episode length: 164.72
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 1.4159
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.12s
                      Time elapsed: 00:20:02
                               ETA: 05:34:45

################################################################################
                     [1m Learning iteration 565/10000 [0m                     

                       Computation: 45700 steps/s (collection: 2.059s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 14.8917
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.5900
                       Mean reward: 9.59
               Mean episode length: 160.74
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 0.9750
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.15s
                      Time elapsed: 00:20:04
                               ETA: 05:34:44

################################################################################
                     [1m Learning iteration 566/10000 [0m                     

                       Computation: 45598 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 10.3218
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.6102
                       Mean reward: 6.84
               Mean episode length: 159.68
    Episode_Reward/reaching_object: 0.6061
     Episode_Reward/lifting_object: 0.8360
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.16s
                      Time elapsed: 00:20:06
                               ETA: 05:34:42

################################################################################
                     [1m Learning iteration 567/10000 [0m                     

                       Computation: 46188 steps/s (collection: 2.033s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 25.6625
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 75.6371
                       Mean reward: 10.08
               Mean episode length: 160.92
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 1.2875
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.13s
                      Time elapsed: 00:20:09
                               ETA: 05:34:40

################################################################################
                     [1m Learning iteration 568/10000 [0m                     

                       Computation: 46777 steps/s (collection: 2.005s, learning 0.097s)
             Mean action noise std: 2.79
          Mean value_function loss: 9.5580
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 75.6709
                       Mean reward: 10.42
               Mean episode length: 166.48
    Episode_Reward/reaching_object: 0.6381
     Episode_Reward/lifting_object: 1.1584
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.10s
                      Time elapsed: 00:20:11
                               ETA: 05:34:37

################################################################################
                     [1m Learning iteration 569/10000 [0m                     

                       Computation: 46457 steps/s (collection: 2.021s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 41.5076
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.6909
                       Mean reward: 12.36
               Mean episode length: 189.68
    Episode_Reward/reaching_object: 0.6713
     Episode_Reward/lifting_object: 1.6443
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.12s
                      Time elapsed: 00:20:13
                               ETA: 05:34:35

################################################################################
                     [1m Learning iteration 570/10000 [0m                     

                       Computation: 45957 steps/s (collection: 2.025s, learning 0.114s)
             Mean action noise std: 2.79
          Mean value_function loss: 72.4111
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 75.7063
                       Mean reward: 7.86
               Mean episode length: 176.70
    Episode_Reward/reaching_object: 0.6544
     Episode_Reward/lifting_object: 1.1449
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.14s
                      Time elapsed: 00:20:15
                               ETA: 05:34:33

################################################################################
                     [1m Learning iteration 571/10000 [0m                     

                       Computation: 45761 steps/s (collection: 2.031s, learning 0.117s)
             Mean action noise std: 2.80
          Mean value_function loss: 12.0309
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.7255
                       Mean reward: 8.65
               Mean episode length: 177.56
    Episode_Reward/reaching_object: 0.6458
     Episode_Reward/lifting_object: 1.4799
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.15s
                      Time elapsed: 00:20:17
                               ETA: 05:34:31

################################################################################
                     [1m Learning iteration 572/10000 [0m                     

                       Computation: 45804 steps/s (collection: 2.033s, learning 0.113s)
             Mean action noise std: 2.80
          Mean value_function loss: 9.3666
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 75.7604
                       Mean reward: 10.17
               Mean episode length: 175.48
    Episode_Reward/reaching_object: 0.6362
     Episode_Reward/lifting_object: 1.6501
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.15s
                      Time elapsed: 00:20:19
                               ETA: 05:34:29

################################################################################
                     [1m Learning iteration 573/10000 [0m                     

                       Computation: 46351 steps/s (collection: 1.992s, learning 0.129s)
             Mean action noise std: 2.80
          Mean value_function loss: 22.9526
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.7836
                       Mean reward: 13.73
               Mean episode length: 193.10
    Episode_Reward/reaching_object: 0.6626
     Episode_Reward/lifting_object: 1.6315
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.12s
                      Time elapsed: 00:20:21
                               ETA: 05:34:27

################################################################################
                     [1m Learning iteration 574/10000 [0m                     

                       Computation: 45747 steps/s (collection: 2.031s, learning 0.118s)
             Mean action noise std: 2.80
          Mean value_function loss: 9.8390
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 75.8116
                       Mean reward: 8.16
               Mean episode length: 174.89
    Episode_Reward/reaching_object: 0.6566
     Episode_Reward/lifting_object: 1.1684
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.15s
                      Time elapsed: 00:20:24
                               ETA: 05:34:25

################################################################################
                     [1m Learning iteration 575/10000 [0m                     

                       Computation: 46214 steps/s (collection: 2.033s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 11.5325
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.8454
                       Mean reward: 10.91
               Mean episode length: 179.16
    Episode_Reward/reaching_object: 0.6554
     Episode_Reward/lifting_object: 1.5030
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.13s
                      Time elapsed: 00:20:26
                               ETA: 05:34:23

################################################################################
                     [1m Learning iteration 576/10000 [0m                     

                       Computation: 46081 steps/s (collection: 2.027s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 9.2207
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 75.8800
                       Mean reward: 11.13
               Mean episode length: 165.39
    Episode_Reward/reaching_object: 0.6580
     Episode_Reward/lifting_object: 1.4037
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.13s
                      Time elapsed: 00:20:28
                               ETA: 05:34:21

################################################################################
                     [1m Learning iteration 577/10000 [0m                     

                       Computation: 45827 steps/s (collection: 2.045s, learning 0.100s)
             Mean action noise std: 2.81
          Mean value_function loss: 9.4977
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.9059
                       Mean reward: 10.70
               Mean episode length: 187.13
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 1.5976
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.15s
                      Time elapsed: 00:20:30
                               ETA: 05:34:19

################################################################################
                     [1m Learning iteration 578/10000 [0m                     

                       Computation: 46225 steps/s (collection: 2.036s, learning 0.091s)
             Mean action noise std: 2.81
          Mean value_function loss: 12.3262
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.9211
                       Mean reward: 11.49
               Mean episode length: 163.21
    Episode_Reward/reaching_object: 0.6709
     Episode_Reward/lifting_object: 1.8537
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.13s
                      Time elapsed: 00:20:32
                               ETA: 05:34:17

################################################################################
                     [1m Learning iteration 579/10000 [0m                     

                       Computation: 46641 steps/s (collection: 2.011s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 20.1156
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.9432
                       Mean reward: 11.17
               Mean episode length: 166.95
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 1.5333
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.11s
                      Time elapsed: 00:20:34
                               ETA: 05:34:15

################################################################################
                     [1m Learning iteration 580/10000 [0m                     

                       Computation: 45589 steps/s (collection: 2.024s, learning 0.132s)
             Mean action noise std: 2.82
          Mean value_function loss: 22.0668
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.9708
                       Mean reward: 10.37
               Mean episode length: 171.33
    Episode_Reward/reaching_object: 0.6422
     Episode_Reward/lifting_object: 1.4629
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.16s
                      Time elapsed: 00:20:36
                               ETA: 05:34:13

################################################################################
                     [1m Learning iteration 581/10000 [0m                     

                       Computation: 38738 steps/s (collection: 2.422s, learning 0.116s)
             Mean action noise std: 2.82
          Mean value_function loss: 14.9969
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 75.9960
                       Mean reward: 14.19
               Mean episode length: 187.98
    Episode_Reward/reaching_object: 0.6937
     Episode_Reward/lifting_object: 1.4154
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.54s
                      Time elapsed: 00:20:39
                               ETA: 05:34:17

################################################################################
                     [1m Learning iteration 582/10000 [0m                     

                       Computation: 46798 steps/s (collection: 1.980s, learning 0.121s)
             Mean action noise std: 2.83
          Mean value_function loss: 14.1952
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 76.0380
                       Mean reward: 13.43
               Mean episode length: 187.32
    Episode_Reward/reaching_object: 0.6841
     Episode_Reward/lifting_object: 1.6811
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.10s
                      Time elapsed: 00:20:41
                               ETA: 05:34:15

################################################################################
                     [1m Learning iteration 583/10000 [0m                     

                       Computation: 46193 steps/s (collection: 2.024s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 14.9086
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 76.0719
                       Mean reward: 10.81
               Mean episode length: 190.08
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 1.7644
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.13s
                      Time elapsed: 00:20:43
                               ETA: 05:34:13

################################################################################
                     [1m Learning iteration 584/10000 [0m                     

                       Computation: 47191 steps/s (collection: 1.968s, learning 0.116s)
             Mean action noise std: 2.83
          Mean value_function loss: 14.8519
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.0985
                       Mean reward: 17.03
               Mean episode length: 192.44
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 1.9363
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.08s
                      Time elapsed: 00:20:45
                               ETA: 05:34:10

################################################################################
                     [1m Learning iteration 585/10000 [0m                     

                       Computation: 47204 steps/s (collection: 1.958s, learning 0.124s)
             Mean action noise std: 2.83
          Mean value_function loss: 30.1726
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 76.1244
                       Mean reward: 14.33
               Mean episode length: 188.73
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 1.9505
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.08s
                      Time elapsed: 00:20:47
                               ETA: 05:34:07

################################################################################
                     [1m Learning iteration 586/10000 [0m                     

                       Computation: 47526 steps/s (collection: 1.954s, learning 0.115s)
             Mean action noise std: 2.83
          Mean value_function loss: 44.2531
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 76.1460
                       Mean reward: 6.91
               Mean episode length: 197.16
    Episode_Reward/reaching_object: 0.6786
     Episode_Reward/lifting_object: 1.0676
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.07s
                      Time elapsed: 00:20:49
                               ETA: 05:34:04

################################################################################
                     [1m Learning iteration 587/10000 [0m                     

                       Computation: 47311 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 2.84
          Mean value_function loss: 11.4569
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 76.1840
                       Mean reward: 12.00
               Mean episode length: 177.93
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 1.8437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.08s
                      Time elapsed: 00:20:51
                               ETA: 05:34:01

################################################################################
                     [1m Learning iteration 588/10000 [0m                     

                       Computation: 47812 steps/s (collection: 1.951s, learning 0.105s)
             Mean action noise std: 2.84
          Mean value_function loss: 12.8327
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.2157
                       Mean reward: 10.43
               Mean episode length: 190.03
    Episode_Reward/reaching_object: 0.7152
     Episode_Reward/lifting_object: 2.1427
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.06s
                      Time elapsed: 00:20:53
                               ETA: 05:33:58

################################################################################
                     [1m Learning iteration 589/10000 [0m                     

                       Computation: 47263 steps/s (collection: 1.982s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 12.9626
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.2356
                       Mean reward: 11.68
               Mean episode length: 182.07
    Episode_Reward/reaching_object: 0.6730
     Episode_Reward/lifting_object: 1.9433
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.08s
                      Time elapsed: 00:20:56
                               ETA: 05:33:55

################################################################################
                     [1m Learning iteration 590/10000 [0m                     

                       Computation: 47867 steps/s (collection: 1.960s, learning 0.094s)
             Mean action noise std: 2.84
          Mean value_function loss: 24.0837
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 76.2486
                       Mean reward: 12.70
               Mean episode length: 190.39
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: 1.8973
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.05s
                      Time elapsed: 00:20:58
                               ETA: 05:33:51

################################################################################
                     [1m Learning iteration 591/10000 [0m                     

                       Computation: 47971 steps/s (collection: 1.959s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 20.2118
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.2696
                       Mean reward: 11.13
               Mean episode length: 187.78
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 1.6819
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.05s
                      Time elapsed: 00:21:00
                               ETA: 05:33:48

################################################################################
                     [1m Learning iteration 592/10000 [0m                     

                       Computation: 48204 steps/s (collection: 1.941s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 19.3585
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 76.2933
                       Mean reward: 11.93
               Mean episode length: 202.19
    Episode_Reward/reaching_object: 0.7133
     Episode_Reward/lifting_object: 2.1760
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.04s
                      Time elapsed: 00:21:02
                               ETA: 05:33:44

################################################################################
                     [1m Learning iteration 593/10000 [0m                     

                       Computation: 48240 steps/s (collection: 1.943s, learning 0.095s)
             Mean action noise std: 2.85
          Mean value_function loss: 12.1102
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.3216
                       Mean reward: 9.00
               Mean episode length: 194.24
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 1.4767
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.04s
                      Time elapsed: 00:21:04
                               ETA: 05:33:41

################################################################################
                     [1m Learning iteration 594/10000 [0m                     

                       Computation: 47878 steps/s (collection: 1.958s, learning 0.096s)
             Mean action noise std: 2.85
          Mean value_function loss: 13.7679
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.3469
                       Mean reward: 14.10
               Mean episode length: 187.98
    Episode_Reward/reaching_object: 0.6921
     Episode_Reward/lifting_object: 2.0448
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.05s
                      Time elapsed: 00:21:06
                               ETA: 05:33:37

################################################################################
                     [1m Learning iteration 595/10000 [0m                     

                       Computation: 47807 steps/s (collection: 1.956s, learning 0.101s)
             Mean action noise std: 2.86
          Mean value_function loss: 8.8484
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 76.3766
                       Mean reward: 17.39
               Mean episode length: 184.24
    Episode_Reward/reaching_object: 0.7046
     Episode_Reward/lifting_object: 2.2747
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.06s
                      Time elapsed: 00:21:08
                               ETA: 05:33:34

################################################################################
                     [1m Learning iteration 596/10000 [0m                     

                       Computation: 47078 steps/s (collection: 1.996s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 22.0255
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.4082
                       Mean reward: 13.87
               Mean episode length: 186.72
    Episode_Reward/reaching_object: 0.6742
     Episode_Reward/lifting_object: 2.1598
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.09s
                      Time elapsed: 00:21:10
                               ETA: 05:33:31

################################################################################
                     [1m Learning iteration 597/10000 [0m                     

                       Computation: 47302 steps/s (collection: 1.961s, learning 0.117s)
             Mean action noise std: 2.86
          Mean value_function loss: 15.1020
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.4493
                       Mean reward: 12.31
               Mean episode length: 183.42
    Episode_Reward/reaching_object: 0.6644
     Episode_Reward/lifting_object: 1.7472
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.08s
                      Time elapsed: 00:21:12
                               ETA: 05:33:29

################################################################################
                     [1m Learning iteration 598/10000 [0m                     

                       Computation: 47895 steps/s (collection: 1.944s, learning 0.108s)
             Mean action noise std: 2.86
          Mean value_function loss: 13.9703
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.4710
                       Mean reward: 14.48
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.6888
     Episode_Reward/lifting_object: 1.9490
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.05s
                      Time elapsed: 00:21:14
                               ETA: 05:33:25

################################################################################
                     [1m Learning iteration 599/10000 [0m                     

                       Computation: 47479 steps/s (collection: 1.958s, learning 0.113s)
             Mean action noise std: 2.87
          Mean value_function loss: 17.4681
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.4958
                       Mean reward: 14.43
               Mean episode length: 181.54
    Episode_Reward/reaching_object: 0.6778
     Episode_Reward/lifting_object: 2.0739
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.07s
                      Time elapsed: 00:21:16
                               ETA: 05:33:22

################################################################################
                     [1m Learning iteration 600/10000 [0m                     

                       Computation: 46416 steps/s (collection: 2.003s, learning 0.115s)
             Mean action noise std: 2.87
          Mean value_function loss: 11.7331
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 76.5304
                       Mean reward: 10.83
               Mean episode length: 191.07
    Episode_Reward/reaching_object: 0.6835
     Episode_Reward/lifting_object: 2.0298
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.12s
                      Time elapsed: 00:21:18
                               ETA: 05:33:20

################################################################################
                     [1m Learning iteration 601/10000 [0m                     

                       Computation: 46513 steps/s (collection: 2.003s, learning 0.111s)
             Mean action noise std: 2.87
          Mean value_function loss: 20.4483
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.5586
                       Mean reward: 14.42
               Mean episode length: 168.31
    Episode_Reward/reaching_object: 0.6581
     Episode_Reward/lifting_object: 1.8869
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.11s
                      Time elapsed: 00:21:20
                               ETA: 05:33:18

################################################################################
                     [1m Learning iteration 602/10000 [0m                     

                       Computation: 46511 steps/s (collection: 2.003s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 13.0108
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 76.5847
                       Mean reward: 14.55
               Mean episode length: 180.13
    Episode_Reward/reaching_object: 0.6762
     Episode_Reward/lifting_object: 2.0712
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.11s
                      Time elapsed: 00:21:22
                               ETA: 05:33:15

################################################################################
                     [1m Learning iteration 603/10000 [0m                     

                       Computation: 46768 steps/s (collection: 1.992s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 21.8700
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 76.6035
                       Mean reward: 14.63
               Mean episode length: 172.30
    Episode_Reward/reaching_object: 0.6458
     Episode_Reward/lifting_object: 2.0068
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.10s
                      Time elapsed: 00:21:25
                               ETA: 05:33:13

################################################################################
                     [1m Learning iteration 604/10000 [0m                     

                       Computation: 47257 steps/s (collection: 1.965s, learning 0.115s)
             Mean action noise std: 2.88
          Mean value_function loss: 17.8218
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 76.6149
                       Mean reward: 11.65
               Mean episode length: 190.95
    Episode_Reward/reaching_object: 0.6658
     Episode_Reward/lifting_object: 2.0559
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.08s
                      Time elapsed: 00:21:27
                               ETA: 05:33:10

################################################################################
                     [1m Learning iteration 605/10000 [0m                     

                       Computation: 46061 steps/s (collection: 2.023s, learning 0.112s)
             Mean action noise std: 2.88
          Mean value_function loss: 26.0772
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 76.6333
                       Mean reward: 13.00
               Mean episode length: 189.91
    Episode_Reward/reaching_object: 0.6287
     Episode_Reward/lifting_object: 1.7633
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.13s
                      Time elapsed: 00:21:29
                               ETA: 05:33:08

################################################################################
                     [1m Learning iteration 606/10000 [0m                     

                       Computation: 42077 steps/s (collection: 2.221s, learning 0.115s)
             Mean action noise std: 2.88
          Mean value_function loss: 12.3698
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 76.6526
                       Mean reward: 17.80
               Mean episode length: 178.66
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: 1.7702
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.34s
                      Time elapsed: 00:21:31
                               ETA: 05:33:09

################################################################################
                     [1m Learning iteration 607/10000 [0m                     

                       Computation: 46391 steps/s (collection: 2.010s, learning 0.109s)
             Mean action noise std: 2.88
          Mean value_function loss: 19.0530
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.6705
                       Mean reward: 12.26
               Mean episode length: 182.16
    Episode_Reward/reaching_object: 0.6306
     Episode_Reward/lifting_object: 1.7571
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.12s
                      Time elapsed: 00:21:33
                               ETA: 05:33:07

################################################################################
                     [1m Learning iteration 608/10000 [0m                     

                       Computation: 46137 steps/s (collection: 2.018s, learning 0.113s)
             Mean action noise std: 2.89
          Mean value_function loss: 16.7844
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 76.6939
                       Mean reward: 20.03
               Mean episode length: 163.81
    Episode_Reward/reaching_object: 0.6107
     Episode_Reward/lifting_object: 1.9663
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.13s
                      Time elapsed: 00:21:35
                               ETA: 05:33:05

################################################################################
                     [1m Learning iteration 609/10000 [0m                     

                       Computation: 45790 steps/s (collection: 1.994s, learning 0.153s)
             Mean action noise std: 2.89
          Mean value_function loss: 13.7315
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.7326
                       Mean reward: 8.97
               Mean episode length: 169.41
    Episode_Reward/reaching_object: 0.6208
     Episode_Reward/lifting_object: 1.8495
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.15s
                      Time elapsed: 00:21:38
                               ETA: 05:33:03

################################################################################
                     [1m Learning iteration 610/10000 [0m                     

                       Computation: 43852 steps/s (collection: 2.117s, learning 0.125s)
             Mean action noise std: 2.89
          Mean value_function loss: 23.6316
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.7703
                       Mean reward: 10.50
               Mean episode length: 160.60
    Episode_Reward/reaching_object: 0.5824
     Episode_Reward/lifting_object: 1.7990
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.24s
                      Time elapsed: 00:21:40
                               ETA: 05:33:02

################################################################################
                     [1m Learning iteration 611/10000 [0m                     

                       Computation: 46485 steps/s (collection: 2.006s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 27.0559
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 76.7854
                       Mean reward: 12.51
               Mean episode length: 161.77
    Episode_Reward/reaching_object: 0.6219
     Episode_Reward/lifting_object: 1.8287
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.11s
                      Time elapsed: 00:21:42
                               ETA: 05:33:00

################################################################################
                     [1m Learning iteration 612/10000 [0m                     

                       Computation: 47182 steps/s (collection: 1.986s, learning 0.098s)
             Mean action noise std: 2.89
          Mean value_function loss: 35.8113
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 76.7877
                       Mean reward: 9.59
               Mean episode length: 151.20
    Episode_Reward/reaching_object: 0.6071
     Episode_Reward/lifting_object: 2.0491
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.08s
                      Time elapsed: 00:21:44
                               ETA: 05:32:57

################################################################################
                     [1m Learning iteration 613/10000 [0m                     

                       Computation: 45578 steps/s (collection: 2.043s, learning 0.114s)
             Mean action noise std: 2.89
          Mean value_function loss: 63.8526
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 76.7934
                       Mean reward: 12.82
               Mean episode length: 163.05
    Episode_Reward/reaching_object: 0.5996
     Episode_Reward/lifting_object: 1.9397
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.16s
                      Time elapsed: 00:21:46
                               ETA: 05:32:56

################################################################################
                     [1m Learning iteration 614/10000 [0m                     

                       Computation: 47583 steps/s (collection: 1.972s, learning 0.094s)
             Mean action noise std: 2.90
          Mean value_function loss: 38.4913
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 76.8036
                       Mean reward: 9.56
               Mean episode length: 159.88
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 1.5043
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.07s
                      Time elapsed: 00:21:48
                               ETA: 05:32:52

################################################################################
                     [1m Learning iteration 615/10000 [0m                     

                       Computation: 47977 steps/s (collection: 1.953s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 15.1575
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.8279
                       Mean reward: 10.83
               Mean episode length: 179.85
    Episode_Reward/reaching_object: 0.6148
     Episode_Reward/lifting_object: 1.6589
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.05s
                      Time elapsed: 00:21:50
                               ETA: 05:32:49

################################################################################
                     [1m Learning iteration 616/10000 [0m                     

                       Computation: 46466 steps/s (collection: 2.015s, learning 0.101s)
             Mean action noise std: 2.90
          Mean value_function loss: 13.3733
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 76.8608
                       Mean reward: 13.82
               Mean episode length: 172.82
    Episode_Reward/reaching_object: 0.6331
     Episode_Reward/lifting_object: 2.2405
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.12s
                      Time elapsed: 00:21:52
                               ETA: 05:32:47

################################################################################
                     [1m Learning iteration 617/10000 [0m                     

                       Computation: 47890 steps/s (collection: 1.954s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 35.1621
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 76.8854
                       Mean reward: 16.62
               Mean episode length: 181.32
    Episode_Reward/reaching_object: 0.6587
     Episode_Reward/lifting_object: 2.2632
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.05s
                      Time elapsed: 00:21:54
                               ETA: 05:32:44

################################################################################
                     [1m Learning iteration 618/10000 [0m                     

                       Computation: 45040 steps/s (collection: 2.010s, learning 0.173s)
             Mean action noise std: 2.91
          Mean value_function loss: 39.0424
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 76.9093
                       Mean reward: 18.43
               Mean episode length: 175.77
    Episode_Reward/reaching_object: 0.6299
     Episode_Reward/lifting_object: 1.9592
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.18s
                      Time elapsed: 00:21:57
                               ETA: 05:32:42

################################################################################
                     [1m Learning iteration 619/10000 [0m                     

                       Computation: 47170 steps/s (collection: 1.942s, learning 0.142s)
             Mean action noise std: 2.91
          Mean value_function loss: 71.3391
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.9313
                       Mean reward: 15.18
               Mean episode length: 163.35
    Episode_Reward/reaching_object: 0.6178
     Episode_Reward/lifting_object: 1.6704
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.08s
                      Time elapsed: 00:21:59
                               ETA: 05:32:39

################################################################################
                     [1m Learning iteration 620/10000 [0m                     

                       Computation: 46656 steps/s (collection: 2.012s, learning 0.095s)
             Mean action noise std: 2.91
          Mean value_function loss: 66.3680
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 76.9617
                       Mean reward: 14.66
               Mean episode length: 159.58
    Episode_Reward/reaching_object: 0.5962
     Episode_Reward/lifting_object: 2.1068
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.11s
                      Time elapsed: 00:22:01
                               ETA: 05:32:37

################################################################################
                     [1m Learning iteration 621/10000 [0m                     

                       Computation: 46676 steps/s (collection: 2.006s, learning 0.101s)
             Mean action noise std: 2.91
          Mean value_function loss: 15.5452
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 76.9930
                       Mean reward: 16.14
               Mean episode length: 166.49
    Episode_Reward/reaching_object: 0.6012
     Episode_Reward/lifting_object: 2.2149
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.11s
                      Time elapsed: 00:22:03
                               ETA: 05:32:35

################################################################################
                     [1m Learning iteration 622/10000 [0m                     

                       Computation: 45901 steps/s (collection: 2.050s, learning 0.092s)
             Mean action noise std: 2.92
          Mean value_function loss: 22.5213
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.0215
                       Mean reward: 10.79
               Mean episode length: 150.15
    Episode_Reward/reaching_object: 0.5985
     Episode_Reward/lifting_object: 2.1107
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.14s
                      Time elapsed: 00:22:05
                               ETA: 05:32:33

################################################################################
                     [1m Learning iteration 623/10000 [0m                     

                       Computation: 47072 steps/s (collection: 1.997s, learning 0.091s)
             Mean action noise std: 2.92
          Mean value_function loss: 17.4416
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.0381
                       Mean reward: 7.16
               Mean episode length: 155.38
    Episode_Reward/reaching_object: 0.6071
     Episode_Reward/lifting_object: 2.0997
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.09s
                      Time elapsed: 00:22:07
                               ETA: 05:32:30

################################################################################
                     [1m Learning iteration 624/10000 [0m                     

                       Computation: 47120 steps/s (collection: 1.994s, learning 0.093s)
             Mean action noise std: 2.92
          Mean value_function loss: 21.6116
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.0601
                       Mean reward: 13.62
               Mean episode length: 147.55
    Episode_Reward/reaching_object: 0.5954
     Episode_Reward/lifting_object: 2.0936
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.09s
                      Time elapsed: 00:22:09
                               ETA: 05:32:27

################################################################################
                     [1m Learning iteration 625/10000 [0m                     

                       Computation: 46678 steps/s (collection: 2.008s, learning 0.098s)
             Mean action noise std: 2.92
          Mean value_function loss: 29.6871
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.0848
                       Mean reward: 13.76
               Mean episode length: 157.50
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: 2.1483
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.11s
                      Time elapsed: 00:22:11
                               ETA: 05:32:25

################################################################################
                     [1m Learning iteration 626/10000 [0m                     

                       Computation: 46424 steps/s (collection: 1.971s, learning 0.147s)
             Mean action noise std: 2.92
          Mean value_function loss: 20.9935
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.1061
                       Mean reward: 5.68
               Mean episode length: 173.85
    Episode_Reward/reaching_object: 0.6001
     Episode_Reward/lifting_object: 1.8115
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.12s
                      Time elapsed: 00:22:13
                               ETA: 05:32:22

################################################################################
                     [1m Learning iteration 627/10000 [0m                     

                       Computation: 46543 steps/s (collection: 1.986s, learning 0.126s)
             Mean action noise std: 2.93
          Mean value_function loss: 12.8858
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.1250
                       Mean reward: 13.00
               Mean episode length: 175.09
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 2.0423
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.11s
                      Time elapsed: 00:22:16
                               ETA: 05:32:20

################################################################################
                     [1m Learning iteration 628/10000 [0m                     

                       Computation: 45851 steps/s (collection: 1.999s, learning 0.145s)
             Mean action noise std: 2.93
          Mean value_function loss: 33.2655
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.1482
                       Mean reward: 11.96
               Mean episode length: 169.23
    Episode_Reward/reaching_object: 0.6040
     Episode_Reward/lifting_object: 2.3219
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.14s
                      Time elapsed: 00:22:18
                               ETA: 05:32:18

################################################################################
                     [1m Learning iteration 629/10000 [0m                     

                       Computation: 46192 steps/s (collection: 2.020s, learning 0.108s)
             Mean action noise std: 2.93
          Mean value_function loss: 15.9012
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.1589
                       Mean reward: 10.96
               Mean episode length: 167.70
    Episode_Reward/reaching_object: 0.6078
     Episode_Reward/lifting_object: 2.0596
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.13s
                      Time elapsed: 00:22:20
                               ETA: 05:32:16

################################################################################
                     [1m Learning iteration 630/10000 [0m                     

                       Computation: 46558 steps/s (collection: 1.998s, learning 0.114s)
             Mean action noise std: 2.93
          Mean value_function loss: 24.0707
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.1725
                       Mean reward: 10.84
               Mean episode length: 177.28
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 2.3935
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.11s
                      Time elapsed: 00:22:22
                               ETA: 05:32:14

################################################################################
                     [1m Learning iteration 631/10000 [0m                     

                       Computation: 46723 steps/s (collection: 1.983s, learning 0.121s)
             Mean action noise std: 2.93
          Mean value_function loss: 16.3438
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 77.1947
                       Mean reward: 18.78
               Mean episode length: 177.64
    Episode_Reward/reaching_object: 0.5922
     Episode_Reward/lifting_object: 1.8586
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.10s
                      Time elapsed: 00:22:24
                               ETA: 05:32:11

################################################################################
                     [1m Learning iteration 632/10000 [0m                     

                       Computation: 46996 steps/s (collection: 1.991s, learning 0.100s)
             Mean action noise std: 2.93
          Mean value_function loss: 20.2115
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.2149
                       Mean reward: 7.80
               Mean episode length: 174.53
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 2.1739
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.09s
                      Time elapsed: 00:22:26
                               ETA: 05:32:09

################################################################################
                     [1m Learning iteration 633/10000 [0m                     

                       Computation: 47377 steps/s (collection: 1.980s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 24.3026
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.2313
                       Mean reward: 7.51
               Mean episode length: 153.58
    Episode_Reward/reaching_object: 0.5785
     Episode_Reward/lifting_object: 1.7800
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.07s
                      Time elapsed: 00:22:28
                               ETA: 05:32:06

################################################################################
                     [1m Learning iteration 634/10000 [0m                     

                       Computation: 47046 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 38.1650
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.2480
                       Mean reward: 9.97
               Mean episode length: 150.96
    Episode_Reward/reaching_object: 0.5896
     Episode_Reward/lifting_object: 2.0931
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.09s
                      Time elapsed: 00:22:30
                               ETA: 05:32:03

################################################################################
                     [1m Learning iteration 635/10000 [0m                     

                       Computation: 46815 steps/s (collection: 1.997s, learning 0.103s)
             Mean action noise std: 2.94
          Mean value_function loss: 16.0082
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.2705
                       Mean reward: 12.51
               Mean episode length: 151.05
    Episode_Reward/reaching_object: 0.5760
     Episode_Reward/lifting_object: 1.8455
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.10s
                      Time elapsed: 00:22:32
                               ETA: 05:32:00

################################################################################
                     [1m Learning iteration 636/10000 [0m                     

                       Computation: 46833 steps/s (collection: 2.003s, learning 0.096s)
             Mean action noise std: 2.94
          Mean value_function loss: 22.0703
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 77.2873
                       Mean reward: 2.09
               Mean episode length: 155.93
    Episode_Reward/reaching_object: 0.5734
     Episode_Reward/lifting_object: 1.8836
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.10s
                      Time elapsed: 00:22:34
                               ETA: 05:31:58

################################################################################
                     [1m Learning iteration 637/10000 [0m                     

                       Computation: 46377 steps/s (collection: 2.011s, learning 0.109s)
             Mean action noise std: 2.94
          Mean value_function loss: 19.6577
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.3121
                       Mean reward: 10.68
               Mean episode length: 159.61
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 1.9357
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.12s
                      Time elapsed: 00:22:37
                               ETA: 05:31:56

################################################################################
                     [1m Learning iteration 638/10000 [0m                     

                       Computation: 46392 steps/s (collection: 2.003s, learning 0.116s)
             Mean action noise std: 2.95
          Mean value_function loss: 22.7162
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.3345
                       Mean reward: 14.74
               Mean episode length: 137.58
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 2.5028
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.12s
                      Time elapsed: 00:22:39
                               ETA: 05:31:53

################################################################################
                     [1m Learning iteration 639/10000 [0m                     

                       Computation: 45291 steps/s (collection: 2.010s, learning 0.161s)
             Mean action noise std: 2.95
          Mean value_function loss: 35.9071
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.3513
                       Mean reward: 6.32
               Mean episode length: 134.65
    Episode_Reward/reaching_object: 0.5600
     Episode_Reward/lifting_object: 2.2411
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.17s
                      Time elapsed: 00:22:41
                               ETA: 05:31:52

################################################################################
                     [1m Learning iteration 640/10000 [0m                     

                       Computation: 45996 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 2.95
          Mean value_function loss: 25.9448
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.3743
                       Mean reward: 10.03
               Mean episode length: 151.62
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: 2.0065
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.14s
                      Time elapsed: 00:22:43
                               ETA: 05:31:50

################################################################################
                     [1m Learning iteration 641/10000 [0m                     

                       Computation: 45671 steps/s (collection: 2.039s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 28.8756
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.3870
                       Mean reward: 15.20
               Mean episode length: 145.29
    Episode_Reward/reaching_object: 0.5563
     Episode_Reward/lifting_object: 2.2895
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.15s
                      Time elapsed: 00:22:45
                               ETA: 05:31:48

################################################################################
                     [1m Learning iteration 642/10000 [0m                     

                       Computation: 45982 steps/s (collection: 2.046s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 36.6020
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.4005
                       Mean reward: 13.44
               Mean episode length: 137.86
    Episode_Reward/reaching_object: 0.5433
     Episode_Reward/lifting_object: 1.9502
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.14s
                      Time elapsed: 00:22:47
                               ETA: 05:31:46

################################################################################
                     [1m Learning iteration 643/10000 [0m                     

                       Computation: 46015 steps/s (collection: 2.047s, learning 0.090s)
             Mean action noise std: 2.96
          Mean value_function loss: 32.5045
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.4278
                       Mean reward: 12.27
               Mean episode length: 155.68
    Episode_Reward/reaching_object: 0.5485
     Episode_Reward/lifting_object: 1.9056
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.14s
                      Time elapsed: 00:22:49
                               ETA: 05:31:44

################################################################################
                     [1m Learning iteration 644/10000 [0m                     

                       Computation: 47038 steps/s (collection: 1.992s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 35.0389
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 77.4529
                       Mean reward: 16.70
               Mean episode length: 151.54
    Episode_Reward/reaching_object: 0.5491
     Episode_Reward/lifting_object: 2.3066
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.09s
                      Time elapsed: 00:22:52
                               ETA: 05:31:42

################################################################################
                     [1m Learning iteration 645/10000 [0m                     

                       Computation: 45722 steps/s (collection: 2.005s, learning 0.145s)
             Mean action noise std: 2.96
          Mean value_function loss: 22.4645
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 77.4636
                       Mean reward: 19.66
               Mean episode length: 153.65
    Episode_Reward/reaching_object: 0.5575
     Episode_Reward/lifting_object: 2.5905
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.15s
                      Time elapsed: 00:22:54
                               ETA: 05:31:40

################################################################################
                     [1m Learning iteration 646/10000 [0m                     

                       Computation: 45271 steps/s (collection: 2.017s, learning 0.155s)
             Mean action noise std: 2.96
          Mean value_function loss: 33.3812
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.4765
                       Mean reward: 15.30
               Mean episode length: 145.99
    Episode_Reward/reaching_object: 0.5754
     Episode_Reward/lifting_object: 2.8117
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.17s
                      Time elapsed: 00:22:56
                               ETA: 05:31:38

################################################################################
                     [1m Learning iteration 647/10000 [0m                     

                       Computation: 45520 steps/s (collection: 2.049s, learning 0.111s)
             Mean action noise std: 2.96
          Mean value_function loss: 32.3167
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.4998
                       Mean reward: 10.55
               Mean episode length: 145.48
    Episode_Reward/reaching_object: 0.5398
     Episode_Reward/lifting_object: 2.3447
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.16s
                      Time elapsed: 00:22:58
                               ETA: 05:31:37

################################################################################
                     [1m Learning iteration 648/10000 [0m                     

                       Computation: 45250 steps/s (collection: 2.058s, learning 0.115s)
             Mean action noise std: 2.97
          Mean value_function loss: 22.9672
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 77.5282
                       Mean reward: 15.18
               Mean episode length: 143.23
    Episode_Reward/reaching_object: 0.5394
     Episode_Reward/lifting_object: 2.5728
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.17s
                      Time elapsed: 00:23:00
                               ETA: 05:31:35

################################################################################
                     [1m Learning iteration 649/10000 [0m                     

                       Computation: 46922 steps/s (collection: 2.007s, learning 0.088s)
             Mean action noise std: 2.97
          Mean value_function loss: 35.1532
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.5451
                       Mean reward: 12.87
               Mean episode length: 137.61
    Episode_Reward/reaching_object: 0.5870
     Episode_Reward/lifting_object: 2.5936
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.10s
                      Time elapsed: 00:23:02
                               ETA: 05:31:33

################################################################################
                     [1m Learning iteration 650/10000 [0m                     

                       Computation: 47347 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 2.97
          Mean value_function loss: 31.4401
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.5624
                       Mean reward: 15.19
               Mean episode length: 154.19
    Episode_Reward/reaching_object: 0.5503
     Episode_Reward/lifting_object: 2.2389
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.08s
                      Time elapsed: 00:23:04
                               ETA: 05:31:30

################################################################################
                     [1m Learning iteration 651/10000 [0m                     

                       Computation: 46909 steps/s (collection: 2.004s, learning 0.092s)
             Mean action noise std: 2.97
          Mean value_function loss: 31.4436
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.5813
                       Mean reward: 12.91
               Mean episode length: 146.76
    Episode_Reward/reaching_object: 0.5577
     Episode_Reward/lifting_object: 2.5622
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.10s
                      Time elapsed: 00:23:06
                               ETA: 05:31:27

################################################################################
                     [1m Learning iteration 652/10000 [0m                     

                       Computation: 46846 steps/s (collection: 1.998s, learning 0.100s)
             Mean action noise std: 2.97
          Mean value_function loss: 26.2313
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.6018
                       Mean reward: 16.51
               Mean episode length: 150.67
    Episode_Reward/reaching_object: 0.5499
     Episode_Reward/lifting_object: 2.2598
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.10s
                      Time elapsed: 00:23:09
                               ETA: 05:31:25

################################################################################
                     [1m Learning iteration 653/10000 [0m                     

                       Computation: 46726 steps/s (collection: 2.014s, learning 0.090s)
             Mean action noise std: 2.97
          Mean value_function loss: 17.7536
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.6203
                       Mean reward: 14.37
               Mean episode length: 138.10
    Episode_Reward/reaching_object: 0.5451
     Episode_Reward/lifting_object: 2.6325
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.10s
                      Time elapsed: 00:23:11
                               ETA: 05:31:22

################################################################################
                     [1m Learning iteration 654/10000 [0m                     

                       Computation: 45537 steps/s (collection: 2.022s, learning 0.137s)
             Mean action noise std: 2.98
          Mean value_function loss: 15.4954
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.6380
                       Mean reward: 16.13
               Mean episode length: 157.61
    Episode_Reward/reaching_object: 0.5537
     Episode_Reward/lifting_object: 2.8389
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.16s
                      Time elapsed: 00:23:13
                               ETA: 05:31:20

################################################################################
                     [1m Learning iteration 655/10000 [0m                     

                       Computation: 46459 steps/s (collection: 1.982s, learning 0.134s)
             Mean action noise std: 2.98
          Mean value_function loss: 16.3300
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 77.6464
                       Mean reward: 19.38
               Mean episode length: 148.10
    Episode_Reward/reaching_object: 0.5793
     Episode_Reward/lifting_object: 3.0563
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.12s
                      Time elapsed: 00:23:15
                               ETA: 05:31:18

################################################################################
                     [1m Learning iteration 656/10000 [0m                     

                       Computation: 47225 steps/s (collection: 1.992s, learning 0.090s)
             Mean action noise std: 2.98
          Mean value_function loss: 28.7537
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.6602
                       Mean reward: 15.63
               Mean episode length: 160.96
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 2.6378
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.08s
                      Time elapsed: 00:23:17
                               ETA: 05:31:15

################################################################################
                     [1m Learning iteration 657/10000 [0m                     

                       Computation: 46874 steps/s (collection: 2.010s, learning 0.088s)
             Mean action noise std: 2.98
          Mean value_function loss: 29.9505
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.6761
                       Mean reward: 13.94
               Mean episode length: 149.84
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 2.6361
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.10s
                      Time elapsed: 00:23:19
                               ETA: 05:31:13

################################################################################
                     [1m Learning iteration 658/10000 [0m                     

                       Computation: 46444 steps/s (collection: 2.012s, learning 0.105s)
             Mean action noise std: 2.98
          Mean value_function loss: 26.7204
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.6986
                       Mean reward: 13.92
               Mean episode length: 150.20
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 2.6987
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.12s
                      Time elapsed: 00:23:21
                               ETA: 05:31:11

################################################################################
                     [1m Learning iteration 659/10000 [0m                     

                       Computation: 46676 steps/s (collection: 2.009s, learning 0.097s)
             Mean action noise std: 2.98
          Mean value_function loss: 25.9262
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 77.7213
                       Mean reward: 19.02
               Mean episode length: 170.31
    Episode_Reward/reaching_object: 0.5802
     Episode_Reward/lifting_object: 2.8412
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.11s
                      Time elapsed: 00:23:23
                               ETA: 05:31:08

################################################################################
                     [1m Learning iteration 660/10000 [0m                     

                       Computation: 46986 steps/s (collection: 1.992s, learning 0.100s)
             Mean action noise std: 2.98
          Mean value_function loss: 28.2085
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.7273
                       Mean reward: 18.14
               Mean episode length: 151.12
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 2.9708
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.09s
                      Time elapsed: 00:23:25
                               ETA: 05:31:05

################################################################################
                     [1m Learning iteration 661/10000 [0m                     

                       Computation: 46956 steps/s (collection: 1.975s, learning 0.118s)
             Mean action noise std: 2.99
          Mean value_function loss: 41.3819
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.7403
                       Mean reward: 19.86
               Mean episode length: 154.04
    Episode_Reward/reaching_object: 0.6005
     Episode_Reward/lifting_object: 3.0004
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.09s
                      Time elapsed: 00:23:28
                               ETA: 05:31:03

################################################################################
                     [1m Learning iteration 662/10000 [0m                     

                       Computation: 46410 steps/s (collection: 2.018s, learning 0.101s)
             Mean action noise std: 2.99
          Mean value_function loss: 35.8851
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.7634
                       Mean reward: 20.17
               Mean episode length: 150.75
    Episode_Reward/reaching_object: 0.5908
     Episode_Reward/lifting_object: 2.8292
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.12s
                      Time elapsed: 00:23:30
                               ETA: 05:31:01

################################################################################
                     [1m Learning iteration 663/10000 [0m                     

                       Computation: 45867 steps/s (collection: 2.021s, learning 0.122s)
             Mean action noise std: 2.99
          Mean value_function loss: 29.0920
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.7882
                       Mean reward: 20.03
               Mean episode length: 149.00
    Episode_Reward/reaching_object: 0.5941
     Episode_Reward/lifting_object: 2.7835
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.14s
                      Time elapsed: 00:23:32
                               ETA: 05:30:59

################################################################################
                     [1m Learning iteration 664/10000 [0m                     

                       Computation: 46707 steps/s (collection: 2.012s, learning 0.093s)
             Mean action noise std: 2.99
          Mean value_function loss: 22.7114
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 77.8108
                       Mean reward: 20.04
               Mean episode length: 156.86
    Episode_Reward/reaching_object: 0.5902
     Episode_Reward/lifting_object: 3.1832
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.10s
                      Time elapsed: 00:23:34
                               ETA: 05:30:56

################################################################################
                     [1m Learning iteration 665/10000 [0m                     

                       Computation: 46780 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 2.99
          Mean value_function loss: 16.7242
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 77.8255
                       Mean reward: 20.90
               Mean episode length: 154.70
    Episode_Reward/reaching_object: 0.6002
     Episode_Reward/lifting_object: 3.1974
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.10s
                      Time elapsed: 00:23:36
                               ETA: 05:30:54

################################################################################
                     [1m Learning iteration 666/10000 [0m                     

                       Computation: 27763 steps/s (collection: 3.434s, learning 0.107s)
             Mean action noise std: 3.00
          Mean value_function loss: 18.0353
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 77.8336
                       Mean reward: 25.37
               Mean episode length: 147.61
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 3.4562
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.54s
                      Time elapsed: 00:23:40
                               ETA: 05:31:11

################################################################################
                     [1m Learning iteration 667/10000 [0m                     

                       Computation: 14468 steps/s (collection: 6.674s, learning 0.120s)
             Mean action noise std: 3.00
          Mean value_function loss: 18.9609
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 77.8486
                       Mean reward: 22.35
               Mean episode length: 152.55
    Episode_Reward/reaching_object: 0.5921
     Episode_Reward/lifting_object: 3.6793
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.79s
                      Time elapsed: 00:23:46
                               ETA: 05:32:15

################################################################################
                     [1m Learning iteration 668/10000 [0m                     

                       Computation: 14598 steps/s (collection: 6.602s, learning 0.132s)
             Mean action noise std: 3.00
          Mean value_function loss: 37.6386
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.8652
                       Mean reward: 17.20
               Mean episode length: 153.77
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 3.5813
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.73s
                      Time elapsed: 00:23:53
                               ETA: 05:33:17

################################################################################
                     [1m Learning iteration 669/10000 [0m                     

                       Computation: 14388 steps/s (collection: 6.713s, learning 0.119s)
             Mean action noise std: 3.00
          Mean value_function loss: 35.0732
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 77.8775
                       Mean reward: 14.50
               Mean episode length: 155.01
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 3.0005
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.83s
                      Time elapsed: 00:24:00
                               ETA: 05:34:20

################################################################################
                     [1m Learning iteration 670/10000 [0m                     

                       Computation: 14349 steps/s (collection: 6.729s, learning 0.121s)
             Mean action noise std: 3.00
          Mean value_function loss: 17.2087
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 77.8875
                       Mean reward: 16.39
               Mean episode length: 159.05
    Episode_Reward/reaching_object: 0.5398
     Episode_Reward/lifting_object: 2.5975
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.85s
                      Time elapsed: 00:24:07
                               ETA: 05:35:23

################################################################################
                     [1m Learning iteration 671/10000 [0m                     

                       Computation: 14158 steps/s (collection: 6.826s, learning 0.118s)
             Mean action noise std: 3.00
          Mean value_function loss: 39.5521
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 77.9032
                       Mean reward: 15.58
               Mean episode length: 148.81
    Episode_Reward/reaching_object: 0.5511
     Episode_Reward/lifting_object: 3.3249
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.94s
                      Time elapsed: 00:24:14
                               ETA: 05:36:27

################################################################################
                     [1m Learning iteration 672/10000 [0m                     

                       Computation: 14291 steps/s (collection: 6.747s, learning 0.132s)
             Mean action noise std: 3.00
          Mean value_function loss: 31.6839
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 77.9191
                       Mean reward: 17.71
               Mean episode length: 149.64
    Episode_Reward/reaching_object: 0.5653
     Episode_Reward/lifting_object: 3.1232
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.88s
                      Time elapsed: 00:24:21
                               ETA: 05:37:30

################################################################################
                     [1m Learning iteration 673/10000 [0m                     

                       Computation: 13975 steps/s (collection: 6.918s, learning 0.116s)
             Mean action noise std: 3.01
          Mean value_function loss: 39.8802
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.9328
                       Mean reward: 25.56
               Mean episode length: 157.31
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 3.3888
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.03s
                      Time elapsed: 00:24:28
                               ETA: 05:38:36

################################################################################
                     [1m Learning iteration 674/10000 [0m                     

                       Computation: 14606 steps/s (collection: 6.593s, learning 0.137s)
             Mean action noise std: 3.01
          Mean value_function loss: 56.9935
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 77.9475
                       Mean reward: 12.51
               Mean episode length: 141.41
    Episode_Reward/reaching_object: 0.5519
     Episode_Reward/lifting_object: 3.0427
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.73s
                      Time elapsed: 00:24:34
                               ETA: 05:39:36

################################################################################
                     [1m Learning iteration 675/10000 [0m                     

                       Computation: 23342 steps/s (collection: 4.100s, learning 0.112s)
             Mean action noise std: 3.01
          Mean value_function loss: 33.7064
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.9563
                       Mean reward: 19.43
               Mean episode length: 149.14
    Episode_Reward/reaching_object: 0.5756
     Episode_Reward/lifting_object: 2.8658
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.21s
                      Time elapsed: 00:24:39
                               ETA: 05:40:02

################################################################################
                     [1m Learning iteration 676/10000 [0m                     

                       Computation: 47801 steps/s (collection: 1.965s, learning 0.091s)
             Mean action noise std: 3.01
          Mean value_function loss: 29.7283
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 77.9729
                       Mean reward: 18.60
               Mean episode length: 143.24
    Episode_Reward/reaching_object: 0.5759
     Episode_Reward/lifting_object: 3.2795
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.06s
                      Time elapsed: 00:24:41
                               ETA: 05:39:58

################################################################################
                     [1m Learning iteration 677/10000 [0m                     

                       Computation: 48442 steps/s (collection: 1.932s, learning 0.097s)
             Mean action noise std: 3.01
          Mean value_function loss: 41.0676
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.9827
                       Mean reward: 19.16
               Mean episode length: 151.20
    Episode_Reward/reaching_object: 0.5738
     Episode_Reward/lifting_object: 3.4035
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.03s
                      Time elapsed: 00:24:43
                               ETA: 05:39:54

################################################################################
                     [1m Learning iteration 678/10000 [0m                     

                       Computation: 47718 steps/s (collection: 1.964s, learning 0.096s)
             Mean action noise std: 3.01
          Mean value_function loss: 29.3416
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 77.9936
                       Mean reward: 23.77
               Mean episode length: 150.26
    Episode_Reward/reaching_object: 0.5727
     Episode_Reward/lifting_object: 3.7080
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.06s
                      Time elapsed: 00:24:45
                               ETA: 05:39:50

################################################################################
                     [1m Learning iteration 679/10000 [0m                     

                       Computation: 47281 steps/s (collection: 1.986s, learning 0.094s)
             Mean action noise std: 3.01
          Mean value_function loss: 34.9785
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.0072
                       Mean reward: 17.82
               Mean episode length: 147.68
    Episode_Reward/reaching_object: 0.6112
     Episode_Reward/lifting_object: 3.8151
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.08s
                      Time elapsed: 00:24:47
                               ETA: 05:39:46

################################################################################
                     [1m Learning iteration 680/10000 [0m                     

                       Computation: 47406 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 3.01
          Mean value_function loss: 36.6955
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.0218
                       Mean reward: 15.79
               Mean episode length: 158.44
    Episode_Reward/reaching_object: 0.5703
     Episode_Reward/lifting_object: 3.3848
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.07s
                      Time elapsed: 00:24:49
                               ETA: 05:39:42

################################################################################
                     [1m Learning iteration 681/10000 [0m                     

                       Computation: 47538 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 35.3743
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.0346
                       Mean reward: 5.39
               Mean episode length: 150.90
    Episode_Reward/reaching_object: 0.5834
     Episode_Reward/lifting_object: 3.3571
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.07s
                      Time elapsed: 00:24:51
                               ETA: 05:39:38

################################################################################
                     [1m Learning iteration 682/10000 [0m                     

                       Computation: 47441 steps/s (collection: 1.987s, learning 0.086s)
             Mean action noise std: 3.02
          Mean value_function loss: 32.6320
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 78.0489
                       Mean reward: 19.35
               Mean episode length: 139.72
    Episode_Reward/reaching_object: 0.5547
     Episode_Reward/lifting_object: 3.4407
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.07s
                      Time elapsed: 00:24:53
                               ETA: 05:39:35

################################################################################
                     [1m Learning iteration 683/10000 [0m                     

                       Computation: 47215 steps/s (collection: 1.973s, learning 0.109s)
             Mean action noise std: 3.02
          Mean value_function loss: 42.3136
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.0605
                       Mean reward: 16.38
               Mean episode length: 139.04
    Episode_Reward/reaching_object: 0.5627
     Episode_Reward/lifting_object: 3.4861
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.08s
                      Time elapsed: 00:24:55
                               ETA: 05:39:31

################################################################################
                     [1m Learning iteration 684/10000 [0m                     

                       Computation: 47883 steps/s (collection: 1.954s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 47.4156
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.0768
                       Mean reward: 24.23
               Mean episode length: 151.92
    Episode_Reward/reaching_object: 0.5774
     Episode_Reward/lifting_object: 3.7535
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.05s
                      Time elapsed: 00:24:57
                               ETA: 05:39:27

################################################################################
                     [1m Learning iteration 685/10000 [0m                     

                       Computation: 47285 steps/s (collection: 1.979s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 43.6151
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.0957
                       Mean reward: 14.83
               Mean episode length: 146.79
    Episode_Reward/reaching_object: 0.5546
     Episode_Reward/lifting_object: 3.0045
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.08s
                      Time elapsed: 00:24:59
                               ETA: 05:39:23

################################################################################
                     [1m Learning iteration 686/10000 [0m                     

                       Computation: 47501 steps/s (collection: 1.966s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 40.4568
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.1083
                       Mean reward: 17.44
               Mean episode length: 160.45
    Episode_Reward/reaching_object: 0.5554
     Episode_Reward/lifting_object: 3.5815
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.07s
                      Time elapsed: 00:25:01
                               ETA: 05:39:20

################################################################################
                     [1m Learning iteration 687/10000 [0m                     

                       Computation: 47559 steps/s (collection: 1.967s, learning 0.100s)
             Mean action noise std: 3.03
          Mean value_function loss: 30.4253
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.1326
                       Mean reward: 22.14
               Mean episode length: 146.22
    Episode_Reward/reaching_object: 0.5651
     Episode_Reward/lifting_object: 3.5130
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.07s
                      Time elapsed: 00:25:03
                               ETA: 05:39:16

################################################################################
                     [1m Learning iteration 688/10000 [0m                     

                       Computation: 47750 steps/s (collection: 1.953s, learning 0.106s)
             Mean action noise std: 3.03
          Mean value_function loss: 33.3340
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 78.1467
                       Mean reward: 19.10
               Mean episode length: 141.91
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 3.8436
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.06s
                      Time elapsed: 00:25:05
                               ETA: 05:39:12

################################################################################
                     [1m Learning iteration 689/10000 [0m                     

                       Computation: 47316 steps/s (collection: 1.963s, learning 0.115s)
             Mean action noise std: 3.03
          Mean value_function loss: 30.4563
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.1585
                       Mean reward: 22.99
               Mean episode length: 145.41
    Episode_Reward/reaching_object: 0.5659
     Episode_Reward/lifting_object: 3.7696
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.08s
                      Time elapsed: 00:25:07
                               ETA: 05:39:08

################################################################################
                     [1m Learning iteration 690/10000 [0m                     

                       Computation: 47627 steps/s (collection: 1.963s, learning 0.101s)
             Mean action noise std: 3.03
          Mean value_function loss: 46.9600
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 78.1703
                       Mean reward: 20.27
               Mean episode length: 149.28
    Episode_Reward/reaching_object: 0.5677
     Episode_Reward/lifting_object: 3.7653
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.06s
                      Time elapsed: 00:25:10
                               ETA: 05:39:05

################################################################################
                     [1m Learning iteration 691/10000 [0m                     

                       Computation: 47601 steps/s (collection: 1.965s, learning 0.101s)
             Mean action noise std: 3.03
          Mean value_function loss: 33.8951
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.1796
                       Mean reward: 23.57
               Mean episode length: 135.36
    Episode_Reward/reaching_object: 0.5438
     Episode_Reward/lifting_object: 2.8987
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.07s
                      Time elapsed: 00:25:12
                               ETA: 05:39:01

################################################################################
                     [1m Learning iteration 692/10000 [0m                     

                       Computation: 47214 steps/s (collection: 1.982s, learning 0.101s)
             Mean action noise std: 3.03
          Mean value_function loss: 32.2672
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 78.1951
                       Mean reward: 19.05
               Mean episode length: 146.22
    Episode_Reward/reaching_object: 0.5536
     Episode_Reward/lifting_object: 3.6368
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.08s
                      Time elapsed: 00:25:14
                               ETA: 05:38:57

################################################################################
                     [1m Learning iteration 693/10000 [0m                     

                       Computation: 47387 steps/s (collection: 1.964s, learning 0.111s)
             Mean action noise std: 3.03
          Mean value_function loss: 34.2459
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.2088
                       Mean reward: 24.59
               Mean episode length: 146.02
    Episode_Reward/reaching_object: 0.5418
     Episode_Reward/lifting_object: 3.5561
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.07s
                      Time elapsed: 00:25:16
                               ETA: 05:38:53

################################################################################
                     [1m Learning iteration 694/10000 [0m                     

                       Computation: 47304 steps/s (collection: 1.983s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 24.3363
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.2186
                       Mean reward: 20.27
               Mean episode length: 133.91
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 3.3790
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.08s
                      Time elapsed: 00:25:18
                               ETA: 05:38:50

################################################################################
                     [1m Learning iteration 695/10000 [0m                     

                       Computation: 47564 steps/s (collection: 1.952s, learning 0.115s)
             Mean action noise std: 3.03
          Mean value_function loss: 31.4543
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.2286
                       Mean reward: 22.97
               Mean episode length: 144.02
    Episode_Reward/reaching_object: 0.5428
     Episode_Reward/lifting_object: 3.5905
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.07s
                      Time elapsed: 00:25:20
                               ETA: 05:38:46

################################################################################
                     [1m Learning iteration 696/10000 [0m                     

                       Computation: 47073 steps/s (collection: 1.978s, learning 0.111s)
             Mean action noise std: 3.04
          Mean value_function loss: 38.4694
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 78.2437
                       Mean reward: 17.11
               Mean episode length: 130.86
    Episode_Reward/reaching_object: 0.5178
     Episode_Reward/lifting_object: 3.5633
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.09s
                      Time elapsed: 00:25:22
                               ETA: 05:38:43

################################################################################
                     [1m Learning iteration 697/10000 [0m                     

                       Computation: 46985 steps/s (collection: 1.977s, learning 0.115s)
             Mean action noise std: 3.04
          Mean value_function loss: 57.1910
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.2584
                       Mean reward: 23.63
               Mean episode length: 128.31
    Episode_Reward/reaching_object: 0.5393
     Episode_Reward/lifting_object: 3.6019
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.09s
                      Time elapsed: 00:25:24
                               ETA: 05:38:39

################################################################################
                     [1m Learning iteration 698/10000 [0m                     

                       Computation: 46404 steps/s (collection: 2.008s, learning 0.110s)
             Mean action noise std: 3.04
          Mean value_function loss: 32.0167
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.2751
                       Mean reward: 22.07
               Mean episode length: 147.73
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 3.3592
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.12s
                      Time elapsed: 00:25:26
                               ETA: 05:38:36

################################################################################
                     [1m Learning iteration 699/10000 [0m                     

                       Computation: 47125 steps/s (collection: 1.976s, learning 0.110s)
             Mean action noise std: 3.04
          Mean value_function loss: 38.5109
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.2846
                       Mean reward: 19.80
               Mean episode length: 135.60
    Episode_Reward/reaching_object: 0.5406
     Episode_Reward/lifting_object: 3.6893
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.09s
                      Time elapsed: 00:25:28
                               ETA: 05:38:33

################################################################################
                     [1m Learning iteration 700/10000 [0m                     

                       Computation: 47607 steps/s (collection: 1.975s, learning 0.090s)
             Mean action noise std: 3.04
          Mean value_function loss: 39.0335
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 78.2951
                       Mean reward: 14.80
               Mean episode length: 125.17
    Episode_Reward/reaching_object: 0.5385
     Episode_Reward/lifting_object: 3.3621
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.06s
                      Time elapsed: 00:25:30
                               ETA: 05:38:29

################################################################################
                     [1m Learning iteration 701/10000 [0m                     

                       Computation: 47107 steps/s (collection: 1.986s, learning 0.101s)
             Mean action noise std: 3.04
          Mean value_function loss: 55.5359
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.3075
                       Mean reward: 20.11
               Mean episode length: 133.85
    Episode_Reward/reaching_object: 0.5465
     Episode_Reward/lifting_object: 3.5602
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.09s
                      Time elapsed: 00:25:32
                               ETA: 05:38:25

################################################################################
                     [1m Learning iteration 702/10000 [0m                     

                       Computation: 46649 steps/s (collection: 1.996s, learning 0.111s)
             Mean action noise std: 3.04
          Mean value_function loss: 95.7976
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.3191
                       Mean reward: 16.09
               Mean episode length: 133.57
    Episode_Reward/reaching_object: 0.5684
     Episode_Reward/lifting_object: 3.5946
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.11s
                      Time elapsed: 00:25:35
                               ETA: 05:38:22

################################################################################
                     [1m Learning iteration 703/10000 [0m                     

                       Computation: 47110 steps/s (collection: 1.978s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 117.5727
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.3392
                       Mean reward: 22.13
               Mean episode length: 141.00
    Episode_Reward/reaching_object: 0.5827
     Episode_Reward/lifting_object: 4.1375
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.09s
                      Time elapsed: 00:25:37
                               ETA: 05:38:19

################################################################################
                     [1m Learning iteration 704/10000 [0m                     

                       Computation: 46855 steps/s (collection: 1.991s, learning 0.107s)
             Mean action noise std: 3.05
          Mean value_function loss: 51.2898
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.3659
                       Mean reward: 23.99
               Mean episode length: 144.09
    Episode_Reward/reaching_object: 0.5826
     Episode_Reward/lifting_object: 4.3585
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.10s
                      Time elapsed: 00:25:39
                               ETA: 05:38:15

################################################################################
                     [1m Learning iteration 705/10000 [0m                     

                       Computation: 47987 steps/s (collection: 1.956s, learning 0.093s)
             Mean action noise std: 3.05
          Mean value_function loss: 49.0092
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 78.3757
                       Mean reward: 14.74
               Mean episode length: 138.05
    Episode_Reward/reaching_object: 0.5613
     Episode_Reward/lifting_object: 3.7120
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.05s
                      Time elapsed: 00:25:41
                               ETA: 05:38:12

################################################################################
                     [1m Learning iteration 706/10000 [0m                     

                       Computation: 48011 steps/s (collection: 1.952s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 47.0324
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.3796
                       Mean reward: 16.64
               Mean episode length: 144.88
    Episode_Reward/reaching_object: 0.5622
     Episode_Reward/lifting_object: 3.3278
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.05s
                      Time elapsed: 00:25:43
                               ETA: 05:38:08

################################################################################
                     [1m Learning iteration 707/10000 [0m                     

                       Computation: 47729 steps/s (collection: 1.963s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 38.9977
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.3896
                       Mean reward: 22.95
               Mean episode length: 152.83
    Episode_Reward/reaching_object: 0.5709
     Episode_Reward/lifting_object: 4.4244
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.06s
                      Time elapsed: 00:25:45
                               ETA: 05:38:04

################################################################################
                     [1m Learning iteration 708/10000 [0m                     

                       Computation: 47489 steps/s (collection: 1.982s, learning 0.088s)
             Mean action noise std: 3.05
          Mean value_function loss: 39.8635
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.4019
                       Mean reward: 20.63
               Mean episode length: 134.79
    Episode_Reward/reaching_object: 0.5722
     Episode_Reward/lifting_object: 4.4258
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.07s
                      Time elapsed: 00:25:47
                               ETA: 05:38:00

################################################################################
                     [1m Learning iteration 709/10000 [0m                     

                       Computation: 47430 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 27.1172
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.4104
                       Mean reward: 20.60
               Mean episode length: 143.22
    Episode_Reward/reaching_object: 0.5442
     Episode_Reward/lifting_object: 4.1191
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.07s
                      Time elapsed: 00:25:49
                               ETA: 05:37:56

################################################################################
                     [1m Learning iteration 710/10000 [0m                     

                       Computation: 47664 steps/s (collection: 1.964s, learning 0.099s)
             Mean action noise std: 3.05
          Mean value_function loss: 39.3546
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 78.4170
                       Mean reward: 24.47
               Mean episode length: 156.51
    Episode_Reward/reaching_object: 0.5617
     Episode_Reward/lifting_object: 4.2000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.06s
                      Time elapsed: 00:25:51
                               ETA: 05:37:53

################################################################################
                     [1m Learning iteration 711/10000 [0m                     

                       Computation: 47480 steps/s (collection: 1.982s, learning 0.089s)
             Mean action noise std: 3.05
          Mean value_function loss: 41.7171
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.4247
                       Mean reward: 25.96
               Mean episode length: 153.08
    Episode_Reward/reaching_object: 0.5856
     Episode_Reward/lifting_object: 4.8271
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.07s
                      Time elapsed: 00:25:53
                               ETA: 05:37:49

################################################################################
                     [1m Learning iteration 712/10000 [0m                     

                       Computation: 47840 steps/s (collection: 1.964s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 30.7071
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 78.4375
                       Mean reward: 23.71
               Mean episode length: 148.12
    Episode_Reward/reaching_object: 0.5624
     Episode_Reward/lifting_object: 4.4831
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.05s
                      Time elapsed: 00:25:55
                               ETA: 05:37:45

################################################################################
                     [1m Learning iteration 713/10000 [0m                     

                       Computation: 47306 steps/s (collection: 1.979s, learning 0.099s)
             Mean action noise std: 3.06
          Mean value_function loss: 72.7115
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.4490
                       Mean reward: 29.77
               Mean episode length: 148.68
    Episode_Reward/reaching_object: 0.5889
     Episode_Reward/lifting_object: 5.0963
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.08s
                      Time elapsed: 00:25:57
                               ETA: 05:37:42

################################################################################
                     [1m Learning iteration 714/10000 [0m                     

                       Computation: 47587 steps/s (collection: 1.973s, learning 0.093s)
             Mean action noise std: 3.06
          Mean value_function loss: 30.6319
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.4568
                       Mean reward: 24.73
               Mean episode length: 141.91
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 4.2831
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.07s
                      Time elapsed: 00:25:59
                               ETA: 05:37:38

################################################################################
                     [1m Learning iteration 715/10000 [0m                     

                       Computation: 47508 steps/s (collection: 1.980s, learning 0.090s)
             Mean action noise std: 3.06
          Mean value_function loss: 36.8995
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.4702
                       Mean reward: 23.00
               Mean episode length: 150.97
    Episode_Reward/reaching_object: 0.5695
     Episode_Reward/lifting_object: 4.5995
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.07s
                      Time elapsed: 00:26:01
                               ETA: 05:37:34

################################################################################
                     [1m Learning iteration 716/10000 [0m                     

                       Computation: 47242 steps/s (collection: 1.979s, learning 0.102s)
             Mean action noise std: 3.06
          Mean value_function loss: 29.9358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.4845
                       Mean reward: 31.09
               Mean episode length: 134.23
    Episode_Reward/reaching_object: 0.5879
     Episode_Reward/lifting_object: 4.9417
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.08s
                      Time elapsed: 00:26:04
                               ETA: 05:37:31

################################################################################
                     [1m Learning iteration 717/10000 [0m                     

                       Computation: 47068 steps/s (collection: 1.980s, learning 0.109s)
             Mean action noise std: 3.06
          Mean value_function loss: 29.6723
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.4939
                       Mean reward: 21.85
               Mean episode length: 156.40
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 4.7152
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.09s
                      Time elapsed: 00:26:06
                               ETA: 05:37:28

################################################################################
                     [1m Learning iteration 718/10000 [0m                     

                       Computation: 46702 steps/s (collection: 2.000s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 35.5894
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.5054
                       Mean reward: 24.05
               Mean episode length: 148.05
    Episode_Reward/reaching_object: 0.5941
     Episode_Reward/lifting_object: 5.0981
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.10s
                      Time elapsed: 00:26:08
                               ETA: 05:37:24

################################################################################
                     [1m Learning iteration 719/10000 [0m                     

                       Computation: 47273 steps/s (collection: 1.969s, learning 0.110s)
             Mean action noise std: 3.06
          Mean value_function loss: 29.3351
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.5131
                       Mean reward: 25.46
               Mean episode length: 141.26
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 5.0891
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.08s
                      Time elapsed: 00:26:10
                               ETA: 05:37:21

################################################################################
                     [1m Learning iteration 720/10000 [0m                     

                       Computation: 46502 steps/s (collection: 2.017s, learning 0.097s)
             Mean action noise std: 3.06
          Mean value_function loss: 36.4411
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.5201
                       Mean reward: 31.66
               Mean episode length: 145.47
    Episode_Reward/reaching_object: 0.6147
     Episode_Reward/lifting_object: 5.4106
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.11s
                      Time elapsed: 00:26:12
                               ETA: 05:37:18

################################################################################
                     [1m Learning iteration 721/10000 [0m                     

                       Computation: 47049 steps/s (collection: 1.980s, learning 0.110s)
             Mean action noise std: 3.07
          Mean value_function loss: 36.6978
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 78.5305
                       Mean reward: 33.82
               Mean episode length: 135.62
    Episode_Reward/reaching_object: 0.5987
     Episode_Reward/lifting_object: 4.7384
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.09s
                      Time elapsed: 00:26:14
                               ETA: 05:37:14

################################################################################
                     [1m Learning iteration 722/10000 [0m                     

                       Computation: 46692 steps/s (collection: 2.005s, learning 0.101s)
             Mean action noise std: 3.07
          Mean value_function loss: 44.2087
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.5426
                       Mean reward: 32.04
               Mean episode length: 160.20
    Episode_Reward/reaching_object: 0.5928
     Episode_Reward/lifting_object: 5.1229
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.11s
                      Time elapsed: 00:26:16
                               ETA: 05:37:11

################################################################################
                     [1m Learning iteration 723/10000 [0m                     

                       Computation: 47224 steps/s (collection: 1.981s, learning 0.100s)
             Mean action noise std: 3.07
          Mean value_function loss: 58.5805
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.5522
                       Mean reward: 22.50
               Mean episode length: 147.68
    Episode_Reward/reaching_object: 0.6062
     Episode_Reward/lifting_object: 4.8877
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.08s
                      Time elapsed: 00:26:18
                               ETA: 05:37:08

################################################################################
                     [1m Learning iteration 724/10000 [0m                     

                       Computation: 47254 steps/s (collection: 1.983s, learning 0.097s)
             Mean action noise std: 3.07
          Mean value_function loss: 41.6141
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.5633
                       Mean reward: 25.94
               Mean episode length: 147.68
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: 4.8729
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.08s
                      Time elapsed: 00:26:20
                               ETA: 05:37:04

################################################################################
                     [1m Learning iteration 725/10000 [0m                     

                       Computation: 47073 steps/s (collection: 1.981s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 55.2323
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.5740
                       Mean reward: 25.60
               Mean episode length: 147.71
    Episode_Reward/reaching_object: 0.5796
     Episode_Reward/lifting_object: 5.1540
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.09s
                      Time elapsed: 00:26:22
                               ETA: 05:37:01

################################################################################
                     [1m Learning iteration 726/10000 [0m                     

                       Computation: 47323 steps/s (collection: 1.986s, learning 0.091s)
             Mean action noise std: 3.07
          Mean value_function loss: 37.1373
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.5851
                       Mean reward: 29.45
               Mean episode length: 145.00
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 4.6865
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.08s
                      Time elapsed: 00:26:24
                               ETA: 05:36:58

################################################################################
                     [1m Learning iteration 727/10000 [0m                     

                       Computation: 46781 steps/s (collection: 1.995s, learning 0.107s)
             Mean action noise std: 3.07
          Mean value_function loss: 32.7897
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.5924
                       Mean reward: 27.61
               Mean episode length: 142.29
    Episode_Reward/reaching_object: 0.5597
     Episode_Reward/lifting_object: 5.0493
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.10s
                      Time elapsed: 00:26:27
                               ETA: 05:36:54

################################################################################
                     [1m Learning iteration 728/10000 [0m                     

                       Computation: 47186 steps/s (collection: 1.977s, learning 0.106s)
             Mean action noise std: 3.07
          Mean value_function loss: 47.9193
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 78.5994
                       Mean reward: 26.74
               Mean episode length: 136.60
    Episode_Reward/reaching_object: 0.5633
     Episode_Reward/lifting_object: 5.0715
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.08s
                      Time elapsed: 00:26:29
                               ETA: 05:36:51

################################################################################
                     [1m Learning iteration 729/10000 [0m                     

                       Computation: 47049 steps/s (collection: 1.990s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 45.1590
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.6089
                       Mean reward: 33.31
               Mean episode length: 134.15
    Episode_Reward/reaching_object: 0.5612
     Episode_Reward/lifting_object: 5.4202
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.09s
                      Time elapsed: 00:26:31
                               ETA: 05:36:48

################################################################################
                     [1m Learning iteration 730/10000 [0m                     

                       Computation: 46911 steps/s (collection: 1.998s, learning 0.098s)
             Mean action noise std: 3.08
          Mean value_function loss: 46.1786
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.6184
                       Mean reward: 21.68
               Mean episode length: 134.70
    Episode_Reward/reaching_object: 0.5394
     Episode_Reward/lifting_object: 4.9620
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.10s
                      Time elapsed: 00:26:33
                               ETA: 05:36:44

################################################################################
                     [1m Learning iteration 731/10000 [0m                     

                       Computation: 46814 steps/s (collection: 1.983s, learning 0.117s)
             Mean action noise std: 3.08
          Mean value_function loss: 48.9999
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.6271
                       Mean reward: 27.72
               Mean episode length: 128.19
    Episode_Reward/reaching_object: 0.5168
     Episode_Reward/lifting_object: 4.7631
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.10s
                      Time elapsed: 00:26:35
                               ETA: 05:36:41

################################################################################
                     [1m Learning iteration 732/10000 [0m                     

                       Computation: 46884 steps/s (collection: 1.977s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 54.9817
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 78.6380
                       Mean reward: 22.24
               Mean episode length: 127.05
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 4.3089
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.10s
                      Time elapsed: 00:26:37
                               ETA: 05:36:38

################################################################################
                     [1m Learning iteration 733/10000 [0m                     

                       Computation: 46601 steps/s (collection: 1.989s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 53.4409
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.6434
                       Mean reward: 23.69
               Mean episode length: 130.22
    Episode_Reward/reaching_object: 0.5449
     Episode_Reward/lifting_object: 5.3791
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.11s
                      Time elapsed: 00:26:39
                               ETA: 05:36:35

################################################################################
                     [1m Learning iteration 734/10000 [0m                     

                       Computation: 47047 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 3.08
          Mean value_function loss: 39.4083
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.6503
                       Mean reward: 30.25
               Mean episode length: 124.84
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 5.4833
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.09s
                      Time elapsed: 00:26:41
                               ETA: 05:36:32

################################################################################
                     [1m Learning iteration 735/10000 [0m                     

                       Computation: 46675 steps/s (collection: 2.013s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 65.0158
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.6563
                       Mean reward: 27.82
               Mean episode length: 135.08
    Episode_Reward/reaching_object: 0.5467
     Episode_Reward/lifting_object: 5.5234
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.11s
                      Time elapsed: 00:26:43
                               ETA: 05:36:29

################################################################################
                     [1m Learning iteration 736/10000 [0m                     

                       Computation: 47071 steps/s (collection: 1.999s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 94.3538
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 78.6601
                       Mean reward: 24.38
               Mean episode length: 138.63
    Episode_Reward/reaching_object: 0.5611
     Episode_Reward/lifting_object: 5.3864
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.09s
                      Time elapsed: 00:26:45
                               ETA: 05:36:25

################################################################################
                     [1m Learning iteration 737/10000 [0m                     

                       Computation: 46869 steps/s (collection: 1.993s, learning 0.105s)
             Mean action noise std: 3.08
          Mean value_function loss: 46.4288
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 78.6610
                       Mean reward: 28.26
               Mean episode length: 144.22
    Episode_Reward/reaching_object: 0.5770
     Episode_Reward/lifting_object: 5.8366
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.10s
                      Time elapsed: 00:26:47
                               ETA: 05:36:22

################################################################################
                     [1m Learning iteration 738/10000 [0m                     

                       Computation: 46753 steps/s (collection: 2.007s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 37.2751
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.6620
                       Mean reward: 35.31
               Mean episode length: 141.87
    Episode_Reward/reaching_object: 0.5472
     Episode_Reward/lifting_object: 5.8065
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.10s
                      Time elapsed: 00:26:50
                               ETA: 05:36:19

################################################################################
                     [1m Learning iteration 739/10000 [0m                     

                       Computation: 46848 steps/s (collection: 2.000s, learning 0.099s)
             Mean action noise std: 3.08
          Mean value_function loss: 45.6081
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 78.6649
                       Mean reward: 32.57
               Mean episode length: 133.90
    Episode_Reward/reaching_object: 0.5755
     Episode_Reward/lifting_object: 6.2604
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.10s
                      Time elapsed: 00:26:52
                               ETA: 05:36:16

################################################################################
                     [1m Learning iteration 740/10000 [0m                     

                       Computation: 47063 steps/s (collection: 2.003s, learning 0.086s)
             Mean action noise std: 3.08
          Mean value_function loss: 36.0395
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 78.6711
                       Mean reward: 32.85
               Mean episode length: 139.29
    Episode_Reward/reaching_object: 0.5622
     Episode_Reward/lifting_object: 6.0507
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.09s
                      Time elapsed: 00:26:54
                               ETA: 05:36:12

################################################################################
                     [1m Learning iteration 741/10000 [0m                     

                       Computation: 47005 steps/s (collection: 1.996s, learning 0.095s)
             Mean action noise std: 3.08
          Mean value_function loss: 37.4218
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.6794
                       Mean reward: 25.50
               Mean episode length: 129.59
    Episode_Reward/reaching_object: 0.5584
     Episode_Reward/lifting_object: 5.9091
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.09s
                      Time elapsed: 00:26:56
                               ETA: 05:36:09

################################################################################
                     [1m Learning iteration 742/10000 [0m                     

                       Computation: 47358 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 38.4621
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 78.6864
                       Mean reward: 29.12
               Mean episode length: 124.35
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 6.1506
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.08s
                      Time elapsed: 00:26:58
                               ETA: 05:36:06

################################################################################
                     [1m Learning iteration 743/10000 [0m                     

                       Computation: 47066 steps/s (collection: 1.992s, learning 0.097s)
             Mean action noise std: 3.08
          Mean value_function loss: 43.0968
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.6893
                       Mean reward: 34.67
               Mean episode length: 138.37
    Episode_Reward/reaching_object: 0.5686
     Episode_Reward/lifting_object: 6.3467
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.09s
                      Time elapsed: 00:27:00
                               ETA: 05:36:02

################################################################################
                     [1m Learning iteration 744/10000 [0m                     

                       Computation: 46320 steps/s (collection: 2.027s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 57.3691
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.6952
                       Mean reward: 31.57
               Mean episode length: 149.35
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 5.9753
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.12s
                      Time elapsed: 00:27:02
                               ETA: 05:35:59

################################################################################
                     [1m Learning iteration 745/10000 [0m                     

                       Computation: 46386 steps/s (collection: 2.021s, learning 0.098s)
             Mean action noise std: 3.08
          Mean value_function loss: 57.7058
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 78.7059
                       Mean reward: 30.53
               Mean episode length: 130.28
    Episode_Reward/reaching_object: 0.5524
     Episode_Reward/lifting_object: 5.8136
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.12s
                      Time elapsed: 00:27:04
                               ETA: 05:35:57

################################################################################
                     [1m Learning iteration 746/10000 [0m                     

                       Computation: 46337 steps/s (collection: 1.991s, learning 0.131s)
             Mean action noise std: 3.09
          Mean value_function loss: 40.5139
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.7178
                       Mean reward: 36.48
               Mean episode length: 136.14
    Episode_Reward/reaching_object: 0.5491
     Episode_Reward/lifting_object: 5.6491
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.12s
                      Time elapsed: 00:27:06
                               ETA: 05:35:54

################################################################################
                     [1m Learning iteration 747/10000 [0m                     

                       Computation: 46258 steps/s (collection: 2.008s, learning 0.117s)
             Mean action noise std: 3.09
          Mean value_function loss: 48.3988
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 78.7269
                       Mean reward: 30.76
               Mean episode length: 137.64
    Episode_Reward/reaching_object: 0.5515
     Episode_Reward/lifting_object: 6.0988
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.13s
                      Time elapsed: 00:27:09
                               ETA: 05:35:51

################################################################################
                     [1m Learning iteration 748/10000 [0m                     

                       Computation: 46053 steps/s (collection: 2.022s, learning 0.113s)
             Mean action noise std: 3.09
          Mean value_function loss: 44.8616
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.7321
                       Mean reward: 31.43
               Mean episode length: 136.19
    Episode_Reward/reaching_object: 0.5549
     Episode_Reward/lifting_object: 6.0514
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.13s
                      Time elapsed: 00:27:11
                               ETA: 05:35:48

################################################################################
                     [1m Learning iteration 749/10000 [0m                     

                       Computation: 46381 steps/s (collection: 2.016s, learning 0.103s)
             Mean action noise std: 3.09
          Mean value_function loss: 51.6332
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.7404
                       Mean reward: 34.31
               Mean episode length: 130.37
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 6.4694
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.12s
                      Time elapsed: 00:27:13
                               ETA: 05:35:45

################################################################################
                     [1m Learning iteration 750/10000 [0m                     

                       Computation: 46879 steps/s (collection: 1.994s, learning 0.103s)
             Mean action noise std: 3.09
          Mean value_function loss: 50.2678
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.7480
                       Mean reward: 31.70
               Mean episode length: 127.10
    Episode_Reward/reaching_object: 0.5838
     Episode_Reward/lifting_object: 6.5787
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.10s
                      Time elapsed: 00:27:15
                               ETA: 05:35:42

################################################################################
                     [1m Learning iteration 751/10000 [0m                     

                       Computation: 46744 steps/s (collection: 2.016s, learning 0.087s)
             Mean action noise std: 3.09
          Mean value_function loss: 59.7836
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.7553
                       Mean reward: 35.12
               Mean episode length: 120.32
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 6.0352
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.10s
                      Time elapsed: 00:27:17
                               ETA: 05:35:39

################################################################################
                     [1m Learning iteration 752/10000 [0m                     

                       Computation: 45972 steps/s (collection: 2.020s, learning 0.118s)
             Mean action noise std: 3.09
          Mean value_function loss: 57.9095
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.7635
                       Mean reward: 32.93
               Mean episode length: 126.58
    Episode_Reward/reaching_object: 0.5392
     Episode_Reward/lifting_object: 5.8977
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.14s
                      Time elapsed: 00:27:19
                               ETA: 05:35:36

################################################################################
                     [1m Learning iteration 753/10000 [0m                     

                       Computation: 45604 steps/s (collection: 2.047s, learning 0.109s)
             Mean action noise std: 3.09
          Mean value_function loss: 46.3241
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 78.7715
                       Mean reward: 37.57
               Mean episode length: 121.95
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 6.0924
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.16s
                      Time elapsed: 00:27:21
                               ETA: 05:35:34

################################################################################
                     [1m Learning iteration 754/10000 [0m                     

                       Computation: 45228 steps/s (collection: 2.052s, learning 0.121s)
             Mean action noise std: 3.09
          Mean value_function loss: 64.2376
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.7773
                       Mean reward: 38.48
               Mean episode length: 127.02
    Episode_Reward/reaching_object: 0.5410
     Episode_Reward/lifting_object: 6.2342
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.17s
                      Time elapsed: 00:27:23
                               ETA: 05:35:32

################################################################################
                     [1m Learning iteration 755/10000 [0m                     

                       Computation: 44944 steps/s (collection: 2.075s, learning 0.112s)
             Mean action noise std: 3.09
          Mean value_function loss: 38.6268
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.7842
                       Mean reward: 39.43
               Mean episode length: 123.51
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 6.9231
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.19s
                      Time elapsed: 00:27:26
                               ETA: 05:35:30

################################################################################
                     [1m Learning iteration 756/10000 [0m                     

                       Computation: 45905 steps/s (collection: 2.034s, learning 0.107s)
             Mean action noise std: 3.09
          Mean value_function loss: 54.3171
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.7882
                       Mean reward: 35.76
               Mean episode length: 130.35
    Episode_Reward/reaching_object: 0.5425
     Episode_Reward/lifting_object: 6.3964
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.14s
                      Time elapsed: 00:27:28
                               ETA: 05:35:27

################################################################################
                     [1m Learning iteration 757/10000 [0m                     

                       Computation: 45483 steps/s (collection: 2.057s, learning 0.105s)
             Mean action noise std: 3.09
          Mean value_function loss: 66.9648
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.7935
                       Mean reward: 35.18
               Mean episode length: 125.59
    Episode_Reward/reaching_object: 0.5369
     Episode_Reward/lifting_object: 6.3845
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 31.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.16s
                      Time elapsed: 00:27:30
                               ETA: 05:35:25

################################################################################
                     [1m Learning iteration 758/10000 [0m                     

                       Computation: 45920 steps/s (collection: 2.044s, learning 0.097s)
             Mean action noise std: 3.09
          Mean value_function loss: 48.0202
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 78.8025
                       Mean reward: 34.71
               Mean episode length: 123.16
    Episode_Reward/reaching_object: 0.5349
     Episode_Reward/lifting_object: 6.1410
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.14s
                      Time elapsed: 00:27:32
                               ETA: 05:35:22

################################################################################
                     [1m Learning iteration 759/10000 [0m                     

                       Computation: 45681 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 3.09
          Mean value_function loss: 55.9476
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.8074
                       Mean reward: 29.69
               Mean episode length: 120.37
    Episode_Reward/reaching_object: 0.5423
     Episode_Reward/lifting_object: 6.3833
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.15s
                      Time elapsed: 00:27:34
                               ETA: 05:35:19

################################################################################
                     [1m Learning iteration 760/10000 [0m                     

                       Computation: 45669 steps/s (collection: 2.037s, learning 0.115s)
             Mean action noise std: 3.10
          Mean value_function loss: 55.4071
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 78.8139
                       Mean reward: 34.68
               Mean episode length: 112.79
    Episode_Reward/reaching_object: 0.5367
     Episode_Reward/lifting_object: 6.3755
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.15s
                      Time elapsed: 00:27:36
                               ETA: 05:35:17

################################################################################
                     [1m Learning iteration 761/10000 [0m                     

                       Computation: 40914 steps/s (collection: 2.314s, learning 0.089s)
             Mean action noise std: 3.10
          Mean value_function loss: 52.1095
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.8183
                       Mean reward: 31.22
               Mean episode length: 122.47
    Episode_Reward/reaching_object: 0.5307
     Episode_Reward/lifting_object: 6.3055
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.40s
                      Time elapsed: 00:27:39
                               ETA: 05:35:18

################################################################################
                     [1m Learning iteration 762/10000 [0m                     

                       Computation: 43852 steps/s (collection: 2.150s, learning 0.091s)
             Mean action noise std: 3.10
          Mean value_function loss: 60.2791
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 78.8233
                       Mean reward: 27.49
               Mean episode length: 126.09
    Episode_Reward/reaching_object: 0.5299
     Episode_Reward/lifting_object: 5.9489
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.24s
                      Time elapsed: 00:27:41
                               ETA: 05:35:16

################################################################################
                     [1m Learning iteration 763/10000 [0m                     

                       Computation: 45225 steps/s (collection: 2.071s, learning 0.103s)
             Mean action noise std: 3.10
          Mean value_function loss: 46.7593
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 78.8281
                       Mean reward: 42.59
               Mean episode length: 112.90
    Episode_Reward/reaching_object: 0.5394
     Episode_Reward/lifting_object: 6.8764
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.17s
                      Time elapsed: 00:27:43
                               ETA: 05:35:14

################################################################################
                     [1m Learning iteration 764/10000 [0m                     

                       Computation: 44335 steps/s (collection: 2.117s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 59.3759
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.8371
                       Mean reward: 30.99
               Mean episode length: 114.22
    Episode_Reward/reaching_object: 0.5336
     Episode_Reward/lifting_object: 6.5454
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.22s
                      Time elapsed: 00:27:45
                               ETA: 05:35:12

################################################################################
                     [1m Learning iteration 765/10000 [0m                     

                       Computation: 44853 steps/s (collection: 2.085s, learning 0.107s)
             Mean action noise std: 3.10
          Mean value_function loss: 48.6419
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 78.8430
                       Mean reward: 31.22
               Mean episode length: 120.47
    Episode_Reward/reaching_object: 0.5235
     Episode_Reward/lifting_object: 6.1772
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.19s
                      Time elapsed: 00:27:48
                               ETA: 05:35:10

################################################################################
                     [1m Learning iteration 766/10000 [0m                     

                       Computation: 45732 steps/s (collection: 2.037s, learning 0.112s)
             Mean action noise std: 3.10
          Mean value_function loss: 55.8682
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.8466
                       Mean reward: 33.11
               Mean episode length: 122.98
    Episode_Reward/reaching_object: 0.5317
     Episode_Reward/lifting_object: 6.6397
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.15s
                      Time elapsed: 00:27:50
                               ETA: 05:35:08

################################################################################
                     [1m Learning iteration 767/10000 [0m                     

                       Computation: 42805 steps/s (collection: 2.179s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 62.4456
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.8516
                       Mean reward: 31.72
               Mean episode length: 119.56
    Episode_Reward/reaching_object: 0.5428
     Episode_Reward/lifting_object: 7.0783
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.30s
                      Time elapsed: 00:27:52
                               ETA: 05:35:07

################################################################################
                     [1m Learning iteration 768/10000 [0m                     

                       Computation: 45436 steps/s (collection: 2.072s, learning 0.092s)
             Mean action noise std: 3.10
          Mean value_function loss: 42.2529
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.8590
                       Mean reward: 36.97
               Mean episode length: 112.30
    Episode_Reward/reaching_object: 0.5359
     Episode_Reward/lifting_object: 6.9674
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.16s
                      Time elapsed: 00:27:54
                               ETA: 05:35:05

################################################################################
                     [1m Learning iteration 769/10000 [0m                     

                       Computation: 46135 steps/s (collection: 2.023s, learning 0.108s)
             Mean action noise std: 3.10
          Mean value_function loss: 71.2185
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.8658
                       Mean reward: 36.06
               Mean episode length: 111.86
    Episode_Reward/reaching_object: 0.5258
     Episode_Reward/lifting_object: 6.5751
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.13s
                      Time elapsed: 00:27:56
                               ETA: 05:35:02

################################################################################
                     [1m Learning iteration 770/10000 [0m                     

                       Computation: 46234 steps/s (collection: 2.030s, learning 0.097s)
             Mean action noise std: 3.10
          Mean value_function loss: 67.8942
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.8719
                       Mean reward: 30.36
               Mean episode length: 121.40
    Episode_Reward/reaching_object: 0.5342
     Episode_Reward/lifting_object: 6.8304
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.13s
                      Time elapsed: 00:27:58
                               ETA: 05:34:59

################################################################################
                     [1m Learning iteration 771/10000 [0m                     

                       Computation: 46197 steps/s (collection: 2.022s, learning 0.106s)
             Mean action noise std: 3.10
          Mean value_function loss: 80.9917
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.8772
                       Mean reward: 40.74
               Mean episode length: 120.57
    Episode_Reward/reaching_object: 0.5140
     Episode_Reward/lifting_object: 6.8386
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.13s
                      Time elapsed: 00:28:01
                               ETA: 05:34:56

################################################################################
                     [1m Learning iteration 772/10000 [0m                     

                       Computation: 45809 steps/s (collection: 2.055s, learning 0.091s)
             Mean action noise std: 3.10
          Mean value_function loss: 58.4672
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 78.8832
                       Mean reward: 34.41
               Mean episode length: 111.90
    Episode_Reward/reaching_object: 0.5197
     Episode_Reward/lifting_object: 7.0845
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.15s
                      Time elapsed: 00:28:03
                               ETA: 05:34:54

################################################################################
                     [1m Learning iteration 773/10000 [0m                     

                       Computation: 43524 steps/s (collection: 2.144s, learning 0.115s)
             Mean action noise std: 3.10
          Mean value_function loss: 60.7621
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 78.8863
                       Mean reward: 38.02
               Mean episode length: 119.26
    Episode_Reward/reaching_object: 0.5335
     Episode_Reward/lifting_object: 7.4038
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.26s
                      Time elapsed: 00:28:05
                               ETA: 05:34:53

################################################################################
                     [1m Learning iteration 774/10000 [0m                     

                       Computation: 45213 steps/s (collection: 2.084s, learning 0.090s)
             Mean action noise std: 3.10
          Mean value_function loss: 62.6133
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 78.8901
                       Mean reward: 38.62
               Mean episode length: 103.59
    Episode_Reward/reaching_object: 0.5203
     Episode_Reward/lifting_object: 7.3479
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.17s
                      Time elapsed: 00:28:07
                               ETA: 05:34:50

################################################################################
                     [1m Learning iteration 775/10000 [0m                     

                       Computation: 45755 steps/s (collection: 2.063s, learning 0.086s)
             Mean action noise std: 3.10
          Mean value_function loss: 74.5222
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 78.8921
                       Mean reward: 40.93
               Mean episode length: 116.07
    Episode_Reward/reaching_object: 0.5281
     Episode_Reward/lifting_object: 7.3274
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.15s
                      Time elapsed: 00:28:09
                               ETA: 05:34:48

################################################################################
                     [1m Learning iteration 776/10000 [0m                     

                       Computation: 44917 steps/s (collection: 2.094s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 62.7627
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 78.8926
                       Mean reward: 41.48
               Mean episode length: 107.35
    Episode_Reward/reaching_object: 0.5349
     Episode_Reward/lifting_object: 7.7262
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.19s
                      Time elapsed: 00:28:12
                               ETA: 05:34:46

################################################################################
                     [1m Learning iteration 777/10000 [0m                     

                       Computation: 45536 steps/s (collection: 2.065s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 54.9897
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 78.8928
                       Mean reward: 39.47
               Mean episode length: 110.19
    Episode_Reward/reaching_object: 0.5178
     Episode_Reward/lifting_object: 7.2562
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.16s
                      Time elapsed: 00:28:14
                               ETA: 05:34:43

################################################################################
                     [1m Learning iteration 778/10000 [0m                     

                       Computation: 43711 steps/s (collection: 2.114s, learning 0.135s)
             Mean action noise std: 3.10
          Mean value_function loss: 58.2062
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 78.8931
                       Mean reward: 39.75
               Mean episode length: 113.65
    Episode_Reward/reaching_object: 0.5049
     Episode_Reward/lifting_object: 7.0818
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.25s
                      Time elapsed: 00:28:16
                               ETA: 05:34:42

################################################################################
                     [1m Learning iteration 779/10000 [0m                     

                       Computation: 44870 steps/s (collection: 2.079s, learning 0.112s)
             Mean action noise std: 3.10
          Mean value_function loss: 48.9665
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 78.8933
                       Mean reward: 48.14
               Mean episode length: 109.60
    Episode_Reward/reaching_object: 0.5188
     Episode_Reward/lifting_object: 7.7114
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.19s
                      Time elapsed: 00:28:18
                               ETA: 05:34:40

################################################################################
                     [1m Learning iteration 780/10000 [0m                     

                       Computation: 46086 steps/s (collection: 2.043s, learning 0.090s)
             Mean action noise std: 3.10
          Mean value_function loss: 50.5987
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 78.8938
                       Mean reward: 39.02
               Mean episode length: 112.09
    Episode_Reward/reaching_object: 0.4890
     Episode_Reward/lifting_object: 7.2469
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.13s
                      Time elapsed: 00:28:20
                               ETA: 05:34:37

################################################################################
                     [1m Learning iteration 781/10000 [0m                     

                       Computation: 46607 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 60.6510
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 78.8941
                       Mean reward: 38.10
               Mean episode length: 108.78
    Episode_Reward/reaching_object: 0.5079
     Episode_Reward/lifting_object: 7.6476
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.11s
                      Time elapsed: 00:28:22
                               ETA: 05:34:34

################################################################################
                     [1m Learning iteration 782/10000 [0m                     

                       Computation: 44876 steps/s (collection: 2.090s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 57.4947
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 78.8943
                       Mean reward: 33.64
               Mean episode length: 100.09
    Episode_Reward/reaching_object: 0.4982
     Episode_Reward/lifting_object: 7.2011
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.19s
                      Time elapsed: 00:28:25
                               ETA: 05:34:32

################################################################################
                     [1m Learning iteration 783/10000 [0m                     

                       Computation: 45727 steps/s (collection: 2.057s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 54.9732
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.8953
                       Mean reward: 37.00
               Mean episode length: 102.19
    Episode_Reward/reaching_object: 0.4824
     Episode_Reward/lifting_object: 6.6321
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.15s
                      Time elapsed: 00:28:27
                               ETA: 05:34:30

################################################################################
                     [1m Learning iteration 784/10000 [0m                     

                       Computation: 45012 steps/s (collection: 2.050s, learning 0.134s)
             Mean action noise std: 3.10
          Mean value_function loss: 81.9419
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 78.8982
                       Mean reward: 39.05
               Mean episode length: 108.92
    Episode_Reward/reaching_object: 0.4841
     Episode_Reward/lifting_object: 7.2581
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.18s
                      Time elapsed: 00:28:29
                               ETA: 05:34:28

################################################################################
                     [1m Learning iteration 785/10000 [0m                     

                       Computation: 44403 steps/s (collection: 2.073s, learning 0.141s)
             Mean action noise std: 3.11
          Mean value_function loss: 68.5016
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.9040
                       Mean reward: 41.12
               Mean episode length: 109.99
    Episode_Reward/reaching_object: 0.4860
     Episode_Reward/lifting_object: 6.9813
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.21s
                      Time elapsed: 00:28:31
                               ETA: 05:34:26

################################################################################
                     [1m Learning iteration 786/10000 [0m                     

                       Computation: 45716 steps/s (collection: 2.042s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 83.9291
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 78.9107
                       Mean reward: 34.95
               Mean episode length: 103.52
    Episode_Reward/reaching_object: 0.4753
     Episode_Reward/lifting_object: 6.5620
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.15s
                      Time elapsed: 00:28:33
                               ETA: 05:34:23

################################################################################
                     [1m Learning iteration 787/10000 [0m                     

                       Computation: 45074 steps/s (collection: 2.088s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 81.8601
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 78.9171
                       Mean reward: 38.17
               Mean episode length: 101.76
    Episode_Reward/reaching_object: 0.4785
     Episode_Reward/lifting_object: 6.5365
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.18s
                      Time elapsed: 00:28:35
                               ETA: 05:34:21

################################################################################
                     [1m Learning iteration 788/10000 [0m                     

                       Computation: 45098 steps/s (collection: 2.078s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 89.8704
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 78.9240
                       Mean reward: 36.51
               Mean episode length: 102.36
    Episode_Reward/reaching_object: 0.5154
     Episode_Reward/lifting_object: 7.5975
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.18s
                      Time elapsed: 00:28:38
                               ETA: 05:34:19

################################################################################
                     [1m Learning iteration 789/10000 [0m                     

                       Computation: 44911 steps/s (collection: 2.064s, learning 0.125s)
             Mean action noise std: 3.11
          Mean value_function loss: 81.4955
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.9290
                       Mean reward: 29.49
               Mean episode length: 101.27
    Episode_Reward/reaching_object: 0.4782
     Episode_Reward/lifting_object: 6.6424
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.19s
                      Time elapsed: 00:28:40
                               ETA: 05:34:17

################################################################################
                     [1m Learning iteration 790/10000 [0m                     

                       Computation: 43507 steps/s (collection: 2.136s, learning 0.124s)
             Mean action noise std: 3.11
          Mean value_function loss: 66.8636
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 78.9353
                       Mean reward: 37.01
               Mean episode length: 107.47
    Episode_Reward/reaching_object: 0.4844
     Episode_Reward/lifting_object: 6.6607
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 41.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.26s
                      Time elapsed: 00:28:42
                               ETA: 05:34:16

################################################################################
                     [1m Learning iteration 791/10000 [0m                     

                       Computation: 43462 steps/s (collection: 2.160s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 65.4317
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.9389
                       Mean reward: 42.07
               Mean episode length: 105.79
    Episode_Reward/reaching_object: 0.4972
     Episode_Reward/lifting_object: 7.4319
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.26s
                      Time elapsed: 00:28:44
                               ETA: 05:34:15

################################################################################
                     [1m Learning iteration 792/10000 [0m                     

                       Computation: 45593 steps/s (collection: 2.063s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 78.7128
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 78.9419
                       Mean reward: 44.88
               Mean episode length: 100.52
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: 7.6787
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.16s
                      Time elapsed: 00:28:46
                               ETA: 05:34:12

################################################################################
                     [1m Learning iteration 793/10000 [0m                     

                       Computation: 46234 steps/s (collection: 2.028s, learning 0.099s)
             Mean action noise std: 3.11
          Mean value_function loss: 87.8945
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 78.9442
                       Mean reward: 36.04
               Mean episode length: 104.94
    Episode_Reward/reaching_object: 0.5010
     Episode_Reward/lifting_object: 7.7438
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.9583
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.13s
                      Time elapsed: 00:28:49
                               ETA: 05:34:09

################################################################################
                     [1m Learning iteration 794/10000 [0m                     

                       Computation: 45930 steps/s (collection: 2.048s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 86.0235
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 78.9461
                       Mean reward: 29.17
               Mean episode length: 106.42
    Episode_Reward/reaching_object: 0.5075
     Episode_Reward/lifting_object: 7.3824
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.14s
                      Time elapsed: 00:28:51
                               ETA: 05:34:07

################################################################################
                     [1m Learning iteration 795/10000 [0m                     

                       Computation: 45804 steps/s (collection: 2.044s, learning 0.103s)
             Mean action noise std: 3.11
          Mean value_function loss: 81.2981
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 78.9474
                       Mean reward: 43.66
               Mean episode length: 102.46
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: 7.7661
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.15s
                      Time elapsed: 00:28:53
                               ETA: 05:34:04

################################################################################
                     [1m Learning iteration 796/10000 [0m                     

                       Computation: 45237 steps/s (collection: 2.061s, learning 0.112s)
             Mean action noise std: 3.11
          Mean value_function loss: 80.6966
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 78.9497
                       Mean reward: 36.85
               Mean episode length: 100.66
    Episode_Reward/reaching_object: 0.4991
     Episode_Reward/lifting_object: 7.4284
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.17s
                      Time elapsed: 00:28:55
                               ETA: 05:34:02

################################################################################
                     [1m Learning iteration 797/10000 [0m                     

                       Computation: 45725 steps/s (collection: 2.050s, learning 0.100s)
             Mean action noise std: 3.11
          Mean value_function loss: 79.6082
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.9520
                       Mean reward: 37.15
               Mean episode length: 113.63
    Episode_Reward/reaching_object: 0.4822
     Episode_Reward/lifting_object: 6.8826
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.15s
                      Time elapsed: 00:28:57
                               ETA: 05:34:00

################################################################################
                     [1m Learning iteration 798/10000 [0m                     

                       Computation: 45731 steps/s (collection: 2.056s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 65.4274
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.9560
                       Mean reward: 36.56
               Mean episode length: 94.05
    Episode_Reward/reaching_object: 0.4872
     Episode_Reward/lifting_object: 6.9290
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.15s
                      Time elapsed: 00:28:59
                               ETA: 05:33:57

################################################################################
                     [1m Learning iteration 799/10000 [0m                     

                       Computation: 45692 steps/s (collection: 2.046s, learning 0.105s)
             Mean action noise std: 3.11
          Mean value_function loss: 85.1445
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 78.9618
                       Mean reward: 35.86
               Mean episode length: 102.86
    Episode_Reward/reaching_object: 0.4783
     Episode_Reward/lifting_object: 6.6919
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.15s
                      Time elapsed: 00:29:01
                               ETA: 05:33:55

################################################################################
                     [1m Learning iteration 800/10000 [0m                     

                       Computation: 44845 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 71.7054
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 78.9677
                       Mean reward: 38.36
               Mean episode length: 109.90
    Episode_Reward/reaching_object: 0.4940
     Episode_Reward/lifting_object: 6.8999
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.19s
                      Time elapsed: 00:29:04
                               ETA: 05:33:53

################################################################################
                     [1m Learning iteration 801/10000 [0m                     

                       Computation: 43985 steps/s (collection: 2.123s, learning 0.112s)
             Mean action noise std: 3.11
          Mean value_function loss: 87.6237
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.9721
                       Mean reward: 46.31
               Mean episode length: 101.00
    Episode_Reward/reaching_object: 0.5087
     Episode_Reward/lifting_object: 7.4154
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.23s
                      Time elapsed: 00:29:06
                               ETA: 05:33:51

################################################################################
                     [1m Learning iteration 802/10000 [0m                     

                       Computation: 45531 steps/s (collection: 2.036s, learning 0.123s)
             Mean action noise std: 3.11
          Mean value_function loss: 85.2148
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.9804
                       Mean reward: 42.41
               Mean episode length: 103.92
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 7.0100
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.16s
                      Time elapsed: 00:29:08
                               ETA: 05:33:49

################################################################################
                     [1m Learning iteration 803/10000 [0m                     

                       Computation: 45112 steps/s (collection: 2.049s, learning 0.131s)
             Mean action noise std: 3.11
          Mean value_function loss: 71.3608
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.9868
                       Mean reward: 31.83
               Mean episode length: 112.43
    Episode_Reward/reaching_object: 0.5131
     Episode_Reward/lifting_object: 6.6700
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.18s
                      Time elapsed: 00:29:10
                               ETA: 05:33:47

################################################################################
                     [1m Learning iteration 804/10000 [0m                     

                       Computation: 45196 steps/s (collection: 2.072s, learning 0.103s)
             Mean action noise std: 3.11
          Mean value_function loss: 77.4627
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 78.9909
                       Mean reward: 43.74
               Mean episode length: 103.94
    Episode_Reward/reaching_object: 0.5209
     Episode_Reward/lifting_object: 7.2526
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.18s
                      Time elapsed: 00:29:12
                               ETA: 05:33:44

################################################################################
                     [1m Learning iteration 805/10000 [0m                     

                       Computation: 45543 steps/s (collection: 2.043s, learning 0.116s)
             Mean action noise std: 3.12
          Mean value_function loss: 88.0823
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.9950
                       Mean reward: 39.75
               Mean episode length: 112.13
    Episode_Reward/reaching_object: 0.5404
     Episode_Reward/lifting_object: 7.2693
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.16s
                      Time elapsed: 00:29:15
                               ETA: 05:33:42

################################################################################
                     [1m Learning iteration 806/10000 [0m                     

                       Computation: 44302 steps/s (collection: 2.104s, learning 0.115s)
             Mean action noise std: 3.12
          Mean value_function loss: 80.5294
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.0011
                       Mean reward: 43.74
               Mean episode length: 121.94
    Episode_Reward/reaching_object: 0.5490
     Episode_Reward/lifting_object: 7.3736
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.22s
                      Time elapsed: 00:29:17
                               ETA: 05:33:40

################################################################################
                     [1m Learning iteration 807/10000 [0m                     

                       Computation: 44182 steps/s (collection: 2.090s, learning 0.135s)
             Mean action noise std: 3.12
          Mean value_function loss: 60.1842
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.0061
                       Mean reward: 48.10
               Mean episode length: 104.25
    Episode_Reward/reaching_object: 0.5355
     Episode_Reward/lifting_object: 7.6536
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.22s
                      Time elapsed: 00:29:19
                               ETA: 05:33:39

################################################################################
                     [1m Learning iteration 808/10000 [0m                     

                       Computation: 44560 steps/s (collection: 2.072s, learning 0.134s)
             Mean action noise std: 3.12
          Mean value_function loss: 67.5926
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.0110
                       Mean reward: 48.78
               Mean episode length: 102.34
    Episode_Reward/reaching_object: 0.5316
     Episode_Reward/lifting_object: 8.3251
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.21s
                      Time elapsed: 00:29:21
                               ETA: 05:33:37

################################################################################
                     [1m Learning iteration 809/10000 [0m                     

                       Computation: 44686 steps/s (collection: 2.099s, learning 0.101s)
             Mean action noise std: 3.12
          Mean value_function loss: 82.3193
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.0168
                       Mean reward: 41.94
               Mean episode length: 104.28
    Episode_Reward/reaching_object: 0.5528
     Episode_Reward/lifting_object: 8.3186
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.20s
                      Time elapsed: 00:29:23
                               ETA: 05:33:35

################################################################################
                     [1m Learning iteration 810/10000 [0m                     

                       Computation: 44920 steps/s (collection: 2.089s, learning 0.100s)
             Mean action noise std: 3.12
          Mean value_function loss: 93.5196
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.0220
                       Mean reward: 49.06
               Mean episode length: 116.77
    Episode_Reward/reaching_object: 0.5452
     Episode_Reward/lifting_object: 8.1299
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.19s
                      Time elapsed: 00:29:26
                               ETA: 05:33:33

################################################################################
                     [1m Learning iteration 811/10000 [0m                     

                       Computation: 44236 steps/s (collection: 2.113s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 65.3797
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 79.0250
                       Mean reward: 41.79
               Mean episode length: 111.76
    Episode_Reward/reaching_object: 0.5457
     Episode_Reward/lifting_object: 8.0159
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.22s
                      Time elapsed: 00:29:28
                               ETA: 05:33:31

################################################################################
                     [1m Learning iteration 812/10000 [0m                     

                       Computation: 44303 steps/s (collection: 2.102s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 83.9715
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 79.0281
                       Mean reward: 37.76
               Mean episode length: 106.41
    Episode_Reward/reaching_object: 0.5492
     Episode_Reward/lifting_object: 8.0356
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.22s
                      Time elapsed: 00:29:30
                               ETA: 05:33:29

################################################################################
                     [1m Learning iteration 813/10000 [0m                     

                       Computation: 44166 steps/s (collection: 2.125s, learning 0.101s)
             Mean action noise std: 3.12
          Mean value_function loss: 91.9786
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 79.0318
                       Mean reward: 39.67
               Mean episode length: 108.07
    Episode_Reward/reaching_object: 0.5446
     Episode_Reward/lifting_object: 8.2597
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.23s
                      Time elapsed: 00:29:32
                               ETA: 05:33:28

################################################################################
                     [1m Learning iteration 814/10000 [0m                     

                       Computation: 43884 steps/s (collection: 2.143s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 69.3853
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.0369
                       Mean reward: 52.20
               Mean episode length: 112.04
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 8.3004
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.24s
                      Time elapsed: 00:29:35
                               ETA: 05:33:26

################################################################################
                     [1m Learning iteration 815/10000 [0m                     

                       Computation: 43958 steps/s (collection: 2.119s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 73.8297
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.0413
                       Mean reward: 48.96
               Mean episode length: 108.62
    Episode_Reward/reaching_object: 0.5377
     Episode_Reward/lifting_object: 8.5980
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.24s
                      Time elapsed: 00:29:37
                               ETA: 05:33:25

################################################################################
                     [1m Learning iteration 816/10000 [0m                     

                       Computation: 44151 steps/s (collection: 2.129s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 103.0505
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 79.0457
                       Mean reward: 38.54
               Mean episode length: 102.67
    Episode_Reward/reaching_object: 0.5357
     Episode_Reward/lifting_object: 8.4736
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.23s
                      Time elapsed: 00:29:39
                               ETA: 05:33:23

################################################################################
                     [1m Learning iteration 817/10000 [0m                     

                       Computation: 44000 steps/s (collection: 2.110s, learning 0.125s)
             Mean action noise std: 3.12
          Mean value_function loss: 97.4838
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.0529
                       Mean reward: 45.42
               Mean episode length: 101.93
    Episode_Reward/reaching_object: 0.5254
     Episode_Reward/lifting_object: 8.3261
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.23s
                      Time elapsed: 00:29:41
                               ETA: 05:33:22

################################################################################
                     [1m Learning iteration 818/10000 [0m                     

                       Computation: 44854 steps/s (collection: 2.098s, learning 0.094s)
             Mean action noise std: 3.12
          Mean value_function loss: 154.6904
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 79.0602
                       Mean reward: 36.24
               Mean episode length: 101.55
    Episode_Reward/reaching_object: 0.5216
     Episode_Reward/lifting_object: 8.3156
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.19s
                      Time elapsed: 00:29:43
                               ETA: 05:33:20

################################################################################
                     [1m Learning iteration 819/10000 [0m                     

                       Computation: 44584 steps/s (collection: 2.097s, learning 0.108s)
             Mean action noise std: 3.12
          Mean value_function loss: 109.4519
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.0677
                       Mean reward: 34.29
               Mean episode length: 98.15
    Episode_Reward/reaching_object: 0.5373
     Episode_Reward/lifting_object: 8.4152
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.20s
                      Time elapsed: 00:29:46
                               ETA: 05:33:18

################################################################################
                     [1m Learning iteration 820/10000 [0m                     

                       Computation: 44624 steps/s (collection: 2.107s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 98.8690
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.0761
                       Mean reward: 44.26
               Mean episode length: 100.81
    Episode_Reward/reaching_object: 0.5186
     Episode_Reward/lifting_object: 8.3631
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.20s
                      Time elapsed: 00:29:48
                               ETA: 05:33:16

################################################################################
                     [1m Learning iteration 821/10000 [0m                     

                       Computation: 44850 steps/s (collection: 2.090s, learning 0.102s)
             Mean action noise std: 3.13
          Mean value_function loss: 87.1880
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.0825
                       Mean reward: 46.96
               Mean episode length: 99.25
    Episode_Reward/reaching_object: 0.5202
     Episode_Reward/lifting_object: 8.6253
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.19s
                      Time elapsed: 00:29:50
                               ETA: 05:33:14

################################################################################
                     [1m Learning iteration 822/10000 [0m                     

                       Computation: 42864 steps/s (collection: 2.177s, learning 0.117s)
             Mean action noise std: 3.13
          Mean value_function loss: 90.3711
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.0888
                       Mean reward: 50.46
               Mean episode length: 110.59
    Episode_Reward/reaching_object: 0.5344
     Episode_Reward/lifting_object: 9.0528
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.29s
                      Time elapsed: 00:29:52
                               ETA: 05:33:13

################################################################################
                     [1m Learning iteration 823/10000 [0m                     

                       Computation: 44256 steps/s (collection: 2.113s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 145.3249
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.0969
                       Mean reward: 46.17
               Mean episode length: 105.86
    Episode_Reward/reaching_object: 0.5336
     Episode_Reward/lifting_object: 9.1726
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.22s
                      Time elapsed: 00:29:55
                               ETA: 05:33:11

################################################################################
                     [1m Learning iteration 824/10000 [0m                     

                       Computation: 44342 steps/s (collection: 2.118s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 104.6552
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.1072
                       Mean reward: 42.33
               Mean episode length: 98.14
    Episode_Reward/reaching_object: 0.5094
     Episode_Reward/lifting_object: 7.9673
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.22s
                      Time elapsed: 00:29:57
                               ETA: 05:33:09

################################################################################
                     [1m Learning iteration 825/10000 [0m                     

                       Computation: 43875 steps/s (collection: 2.141s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 92.6460
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.1128
                       Mean reward: 48.06
               Mean episode length: 99.53
    Episode_Reward/reaching_object: 0.5193
     Episode_Reward/lifting_object: 9.2482
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.24s
                      Time elapsed: 00:29:59
                               ETA: 05:33:08

################################################################################
                     [1m Learning iteration 826/10000 [0m                     

                       Computation: 42735 steps/s (collection: 2.151s, learning 0.150s)
             Mean action noise std: 3.13
          Mean value_function loss: 72.2249
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.1156
                       Mean reward: 42.53
               Mean episode length: 101.18
    Episode_Reward/reaching_object: 0.5073
     Episode_Reward/lifting_object: 8.9764
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.30s
                      Time elapsed: 00:30:01
                               ETA: 05:33:07

################################################################################
                     [1m Learning iteration 827/10000 [0m                     

                       Computation: 43979 steps/s (collection: 2.138s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 78.3654
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.1183
                       Mean reward: 39.71
               Mean episode length: 100.53
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 8.5430
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 41.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.24s
                      Time elapsed: 00:30:04
                               ETA: 05:33:05

################################################################################
                     [1m Learning iteration 828/10000 [0m                     

                       Computation: 44166 steps/s (collection: 2.107s, learning 0.119s)
             Mean action noise std: 3.13
          Mean value_function loss: 96.7000
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.1238
                       Mean reward: 48.97
               Mean episode length: 106.40
    Episode_Reward/reaching_object: 0.5278
     Episode_Reward/lifting_object: 9.6318
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.23s
                      Time elapsed: 00:30:06
                               ETA: 05:33:04

################################################################################
                     [1m Learning iteration 829/10000 [0m                     

                       Computation: 44666 steps/s (collection: 2.103s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 80.3126
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.1345
                       Mean reward: 52.35
               Mean episode length: 100.48
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 9.2496
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.20s
                      Time elapsed: 00:30:08
                               ETA: 05:33:02

################################################################################
                     [1m Learning iteration 830/10000 [0m                     

                       Computation: 44024 steps/s (collection: 2.131s, learning 0.102s)
             Mean action noise std: 3.13
          Mean value_function loss: 106.4869
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.1400
                       Mean reward: 50.48
               Mean episode length: 106.68
    Episode_Reward/reaching_object: 0.5238
     Episode_Reward/lifting_object: 9.7120
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.23s
                      Time elapsed: 00:30:10
                               ETA: 05:33:00

################################################################################
                     [1m Learning iteration 831/10000 [0m                     

                       Computation: 44949 steps/s (collection: 2.090s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 98.0950
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.1459
                       Mean reward: 53.53
               Mean episode length: 101.65
    Episode_Reward/reaching_object: 0.5176
     Episode_Reward/lifting_object: 9.3622
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 41.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.19s
                      Time elapsed: 00:30:12
                               ETA: 05:32:58

################################################################################
                     [1m Learning iteration 832/10000 [0m                     

                       Computation: 43831 steps/s (collection: 2.135s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 91.1126
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 79.1506
                       Mean reward: 57.31
               Mean episode length: 106.54
    Episode_Reward/reaching_object: 0.5264
     Episode_Reward/lifting_object: 10.0880
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.24s
                      Time elapsed: 00:30:15
                               ETA: 05:32:57

################################################################################
                     [1m Learning iteration 833/10000 [0m                     

                       Computation: 44070 steps/s (collection: 2.138s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 86.9907
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.1554
                       Mean reward: 53.98
               Mean episode length: 95.89
    Episode_Reward/reaching_object: 0.5287
     Episode_Reward/lifting_object: 10.4099
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.23s
                      Time elapsed: 00:30:17
                               ETA: 05:32:55

################################################################################
                     [1m Learning iteration 834/10000 [0m                     

                       Computation: 43914 steps/s (collection: 2.134s, learning 0.104s)
             Mean action noise std: 3.13
          Mean value_function loss: 95.0653
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 79.1591
                       Mean reward: 41.50
               Mean episode length: 99.99
    Episode_Reward/reaching_object: 0.4939
     Episode_Reward/lifting_object: 9.1771
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.24s
                      Time elapsed: 00:30:19
                               ETA: 05:32:54

################################################################################
                     [1m Learning iteration 835/10000 [0m                     

                       Computation: 43480 steps/s (collection: 2.127s, learning 0.134s)
             Mean action noise std: 3.13
          Mean value_function loss: 96.0955
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.1631
                       Mean reward: 59.68
               Mean episode length: 96.93
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 10.0983
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 40.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.26s
                      Time elapsed: 00:30:21
                               ETA: 05:32:52

################################################################################
                     [1m Learning iteration 836/10000 [0m                     

                       Computation: 44232 steps/s (collection: 2.123s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 89.7406
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.1666
                       Mean reward: 42.87
               Mean episode length: 97.47
    Episode_Reward/reaching_object: 0.5164
     Episode_Reward/lifting_object: 9.7901
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 41.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.22s
                      Time elapsed: 00:30:24
                               ETA: 05:32:51

################################################################################
                     [1m Learning iteration 837/10000 [0m                     

                       Computation: 43772 steps/s (collection: 2.138s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 91.0720
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.1676
                       Mean reward: 56.19
               Mean episode length: 105.08
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: 9.8110
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 42.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.25s
                      Time elapsed: 00:30:26
                               ETA: 05:32:49

################################################################################
                     [1m Learning iteration 838/10000 [0m                     

                       Computation: 43719 steps/s (collection: 2.128s, learning 0.121s)
             Mean action noise std: 3.13
          Mean value_function loss: 102.2307
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.1685
                       Mean reward: 42.48
               Mean episode length: 95.93
    Episode_Reward/reaching_object: 0.5086
     Episode_Reward/lifting_object: 9.8244
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.25s
                      Time elapsed: 00:30:28
                               ETA: 05:32:48

################################################################################
                     [1m Learning iteration 839/10000 [0m                     

                       Computation: 43850 steps/s (collection: 2.105s, learning 0.137s)
             Mean action noise std: 3.13
          Mean value_function loss: 89.4109
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 79.1698
                       Mean reward: 57.64
               Mean episode length: 91.51
    Episode_Reward/reaching_object: 0.5104
     Episode_Reward/lifting_object: 9.7106
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 42.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.24s
                      Time elapsed: 00:30:30
                               ETA: 05:32:46

################################################################################
                     [1m Learning iteration 840/10000 [0m                     

                       Computation: 44267 steps/s (collection: 2.106s, learning 0.115s)
             Mean action noise std: 3.13
          Mean value_function loss: 100.1612
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 79.1702
                       Mean reward: 56.71
               Mean episode length: 93.81
    Episode_Reward/reaching_object: 0.5192
     Episode_Reward/lifting_object: 10.1876
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 42.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.22s
                      Time elapsed: 00:30:33
                               ETA: 05:32:45

################################################################################
                     [1m Learning iteration 841/10000 [0m                     

                       Computation: 43333 steps/s (collection: 2.167s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 101.7181
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.1710
                       Mean reward: 52.49
               Mean episode length: 98.89
    Episode_Reward/reaching_object: 0.4856
     Episode_Reward/lifting_object: 9.5637
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 46.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.27s
                      Time elapsed: 00:30:35
                               ETA: 05:32:43

################################################################################
                     [1m Learning iteration 842/10000 [0m                     

                       Computation: 44066 steps/s (collection: 2.121s, learning 0.110s)
             Mean action noise std: 3.14
          Mean value_function loss: 117.0465
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.1732
                       Mean reward: 50.13
               Mean episode length: 97.61
    Episode_Reward/reaching_object: 0.4830
     Episode_Reward/lifting_object: 9.3059
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 45.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.23s
                      Time elapsed: 00:30:37
                               ETA: 05:32:42

################################################################################
                     [1m Learning iteration 843/10000 [0m                     

                       Computation: 43350 steps/s (collection: 2.154s, learning 0.114s)
             Mean action noise std: 3.14
          Mean value_function loss: 98.5123
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.1787
                       Mean reward: 51.07
               Mean episode length: 89.11
    Episode_Reward/reaching_object: 0.4899
     Episode_Reward/lifting_object: 9.5426
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 44.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.27s
                      Time elapsed: 00:30:39
                               ETA: 05:32:40

################################################################################
                     [1m Learning iteration 844/10000 [0m                     

                       Computation: 44501 steps/s (collection: 2.111s, learning 0.098s)
             Mean action noise std: 3.14
          Mean value_function loss: 106.1143
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 79.1844
                       Mean reward: 54.94
               Mean episode length: 90.21
    Episode_Reward/reaching_object: 0.4830
     Episode_Reward/lifting_object: 9.9482
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.21s
                      Time elapsed: 00:30:42
                               ETA: 05:32:39

################################################################################
                     [1m Learning iteration 845/10000 [0m                     

                       Computation: 44053 steps/s (collection: 2.140s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 110.5962
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.1890
                       Mean reward: 53.14
               Mean episode length: 86.30
    Episode_Reward/reaching_object: 0.4920
     Episode_Reward/lifting_object: 10.1135
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.23s
                      Time elapsed: 00:30:44
                               ETA: 05:32:37

################################################################################
                     [1m Learning iteration 846/10000 [0m                     

                       Computation: 44278 steps/s (collection: 2.112s, learning 0.109s)
             Mean action noise std: 3.14
          Mean value_function loss: 122.0722
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.1931
                       Mean reward: 54.57
               Mean episode length: 92.73
    Episode_Reward/reaching_object: 0.4768
     Episode_Reward/lifting_object: 9.2356
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.22s
                      Time elapsed: 00:30:46
                               ETA: 05:32:35

################################################################################
                     [1m Learning iteration 847/10000 [0m                     

                       Computation: 43856 steps/s (collection: 2.150s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 95.9637
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.1993
                       Mean reward: 53.83
               Mean episode length: 88.92
    Episode_Reward/reaching_object: 0.4771
     Episode_Reward/lifting_object: 9.7822
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.24s
                      Time elapsed: 00:30:48
                               ETA: 05:32:34

################################################################################
                     [1m Learning iteration 848/10000 [0m                     

                       Computation: 43376 steps/s (collection: 2.140s, learning 0.127s)
             Mean action noise std: 3.14
          Mean value_function loss: 123.0942
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 79.2039
                       Mean reward: 45.03
               Mean episode length: 87.96
    Episode_Reward/reaching_object: 0.4843
     Episode_Reward/lifting_object: 10.1911
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 46.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.27s
                      Time elapsed: 00:30:50
                               ETA: 05:32:32

################################################################################
                     [1m Learning iteration 849/10000 [0m                     

                       Computation: 43774 steps/s (collection: 2.152s, learning 0.094s)
             Mean action noise std: 3.14
          Mean value_function loss: 122.3027
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.2075
                       Mean reward: 54.07
               Mean episode length: 89.81
    Episode_Reward/reaching_object: 0.4811
     Episode_Reward/lifting_object: 10.2715
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.25s
                      Time elapsed: 00:30:53
                               ETA: 05:32:31

################################################################################
                     [1m Learning iteration 850/10000 [0m                     

                       Computation: 44020 steps/s (collection: 2.142s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 117.0350
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.2125
                       Mean reward: 63.01
               Mean episode length: 91.52
    Episode_Reward/reaching_object: 0.4802
     Episode_Reward/lifting_object: 10.4130
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.23s
                      Time elapsed: 00:30:55
                               ETA: 05:32:29

################################################################################
                     [1m Learning iteration 851/10000 [0m                     

                       Computation: 43688 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 121.1943
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.2168
                       Mean reward: 54.51
               Mean episode length: 87.25
    Episode_Reward/reaching_object: 0.4753
     Episode_Reward/lifting_object: 10.6106
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.25s
                      Time elapsed: 00:30:57
                               ETA: 05:32:28

################################################################################
                     [1m Learning iteration 852/10000 [0m                     

                       Computation: 43755 steps/s (collection: 2.113s, learning 0.133s)
             Mean action noise std: 3.14
          Mean value_function loss: 97.6347
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.2217
                       Mean reward: 60.80
               Mean episode length: 85.09
    Episode_Reward/reaching_object: 0.4717
     Episode_Reward/lifting_object: 9.9910
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.25s
                      Time elapsed: 00:30:59
                               ETA: 05:32:26

################################################################################
                     [1m Learning iteration 853/10000 [0m                     

                       Computation: 43678 steps/s (collection: 2.136s, learning 0.114s)
             Mean action noise std: 3.14
          Mean value_function loss: 104.6659
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.2243
                       Mean reward: 66.68
               Mean episode length: 88.55
    Episode_Reward/reaching_object: 0.4819
     Episode_Reward/lifting_object: 10.8161
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.25s
                      Time elapsed: 00:31:02
                               ETA: 05:32:25

################################################################################
                     [1m Learning iteration 854/10000 [0m                     

                       Computation: 43411 steps/s (collection: 2.136s, learning 0.128s)
             Mean action noise std: 3.14
          Mean value_function loss: 107.9730
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.2267
                       Mean reward: 52.49
               Mean episode length: 88.77
    Episode_Reward/reaching_object: 0.4676
     Episode_Reward/lifting_object: 9.8739
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.26s
                      Time elapsed: 00:31:04
                               ETA: 05:32:24

################################################################################
                     [1m Learning iteration 855/10000 [0m                     

                       Computation: 43786 steps/s (collection: 2.125s, learning 0.120s)
             Mean action noise std: 3.14
          Mean value_function loss: 96.3267
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.2295
                       Mean reward: 56.59
               Mean episode length: 80.82
    Episode_Reward/reaching_object: 0.4893
     Episode_Reward/lifting_object: 11.2770
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.25s
                      Time elapsed: 00:31:06
                               ETA: 05:32:22

################################################################################
                     [1m Learning iteration 856/10000 [0m                     

                       Computation: 41607 steps/s (collection: 2.198s, learning 0.165s)
             Mean action noise std: 3.14
          Mean value_function loss: 112.6075
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.2325
                       Mean reward: 39.39
               Mean episode length: 87.62
    Episode_Reward/reaching_object: 0.4769
     Episode_Reward/lifting_object: 10.5005
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 47.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.36s
                      Time elapsed: 00:31:09
                               ETA: 05:32:22

################################################################################
                     [1m Learning iteration 857/10000 [0m                     

                       Computation: 44156 steps/s (collection: 2.126s, learning 0.100s)
             Mean action noise std: 3.14
          Mean value_function loss: 127.3920
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.2359
                       Mean reward: 51.00
               Mean episode length: 86.02
    Episode_Reward/reaching_object: 0.4816
     Episode_Reward/lifting_object: 10.6677
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.23s
                      Time elapsed: 00:31:11
                               ETA: 05:32:20

################################################################################
                     [1m Learning iteration 858/10000 [0m                     

                       Computation: 43670 steps/s (collection: 2.156s, learning 0.095s)
             Mean action noise std: 3.14
          Mean value_function loss: 124.9692
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 79.2390
                       Mean reward: 51.32
               Mean episode length: 86.17
    Episode_Reward/reaching_object: 0.4731
     Episode_Reward/lifting_object: 10.5365
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.25s
                      Time elapsed: 00:31:13
                               ETA: 05:32:19

################################################################################
                     [1m Learning iteration 859/10000 [0m                     

                       Computation: 44567 steps/s (collection: 2.108s, learning 0.098s)
             Mean action noise std: 3.14
          Mean value_function loss: 129.2418
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 79.2445
                       Mean reward: 64.74
               Mean episode length: 86.97
    Episode_Reward/reaching_object: 0.4737
     Episode_Reward/lifting_object: 10.8598
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.21s
                      Time elapsed: 00:31:15
                               ETA: 05:32:17

################################################################################
                     [1m Learning iteration 860/10000 [0m                     

                       Computation: 44273 steps/s (collection: 2.116s, learning 0.104s)
             Mean action noise std: 3.14
          Mean value_function loss: 148.1625
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 79.2515
                       Mean reward: 60.16
               Mean episode length: 90.51
    Episode_Reward/reaching_object: 0.4905
     Episode_Reward/lifting_object: 11.6928
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.22s
                      Time elapsed: 00:31:17
                               ETA: 05:32:15

################################################################################
                     [1m Learning iteration 861/10000 [0m                     

                       Computation: 44264 steps/s (collection: 2.126s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 129.6209
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.2598
                       Mean reward: 58.96
               Mean episode length: 85.51
    Episode_Reward/reaching_object: 0.4896
     Episode_Reward/lifting_object: 11.5855
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.22s
                      Time elapsed: 00:31:20
                               ETA: 05:32:13

################################################################################
                     [1m Learning iteration 862/10000 [0m                     

                       Computation: 43189 steps/s (collection: 2.159s, learning 0.117s)
             Mean action noise std: 3.15
          Mean value_function loss: 130.3023
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 79.2650
                       Mean reward: 64.99
               Mean episode length: 89.42
    Episode_Reward/reaching_object: 0.4811
     Episode_Reward/lifting_object: 11.8817
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 48.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.28s
                      Time elapsed: 00:31:22
                               ETA: 05:32:12

################################################################################
                     [1m Learning iteration 863/10000 [0m                     

                       Computation: 43489 steps/s (collection: 2.138s, learning 0.122s)
             Mean action noise std: 3.15
          Mean value_function loss: 126.0481
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.2680
                       Mean reward: 57.20
               Mean episode length: 89.48
    Episode_Reward/reaching_object: 0.4856
     Episode_Reward/lifting_object: 11.8170
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 47.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.26s
                      Time elapsed: 00:31:24
                               ETA: 05:32:11

################################################################################
                     [1m Learning iteration 864/10000 [0m                     

                       Computation: 43531 steps/s (collection: 2.166s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 181.1530
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.2718
                       Mean reward: 57.70
               Mean episode length: 92.31
    Episode_Reward/reaching_object: 0.4874
     Episode_Reward/lifting_object: 11.9026
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.26s
                      Time elapsed: 00:31:26
                               ETA: 05:32:10

################################################################################
                     [1m Learning iteration 865/10000 [0m                     

                       Computation: 42583 steps/s (collection: 2.183s, learning 0.126s)
             Mean action noise std: 3.15
          Mean value_function loss: 149.4542
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 79.2765
                       Mean reward: 66.55
               Mean episode length: 84.03
    Episode_Reward/reaching_object: 0.4712
     Episode_Reward/lifting_object: 11.3931
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.31s
                      Time elapsed: 00:31:29
                               ETA: 05:32:09

################################################################################
                     [1m Learning iteration 866/10000 [0m                     

                       Computation: 43655 steps/s (collection: 2.151s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 137.5115
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 79.2785
                       Mean reward: 58.63
               Mean episode length: 80.39
    Episode_Reward/reaching_object: 0.4796
     Episode_Reward/lifting_object: 12.0465
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.25s
                      Time elapsed: 00:31:31
                               ETA: 05:32:07

################################################################################
                     [1m Learning iteration 867/10000 [0m                     

                       Computation: 43576 steps/s (collection: 2.143s, learning 0.113s)
             Mean action noise std: 3.15
          Mean value_function loss: 143.4204
               Mean surrogate loss: 0.0233
                 Mean entropy loss: 79.2795
                       Mean reward: 57.54
               Mean episode length: 88.69
    Episode_Reward/reaching_object: 0.4892
     Episode_Reward/lifting_object: 12.0520
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.26s
                      Time elapsed: 00:31:33
                               ETA: 05:32:06

################################################################################
                     [1m Learning iteration 868/10000 [0m                     

                       Computation: 43859 steps/s (collection: 2.134s, learning 0.107s)
             Mean action noise std: 3.15
          Mean value_function loss: 127.4566
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.2798
                       Mean reward: 48.03
               Mean episode length: 84.31
    Episode_Reward/reaching_object: 0.4798
     Episode_Reward/lifting_object: 11.8594
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.24s
                      Time elapsed: 00:31:36
                               ETA: 05:32:04

################################################################################
                     [1m Learning iteration 869/10000 [0m                     

                       Computation: 44391 steps/s (collection: 2.120s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 142.4535
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.2806
                       Mean reward: 61.74
               Mean episode length: 89.20
    Episode_Reward/reaching_object: 0.4945
     Episode_Reward/lifting_object: 12.4153
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.21s
                      Time elapsed: 00:31:38
                               ETA: 05:32:02

################################################################################
                     [1m Learning iteration 870/10000 [0m                     

                       Computation: 44352 steps/s (collection: 2.124s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 155.8853
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.2835
                       Mean reward: 69.58
               Mean episode length: 88.15
    Episode_Reward/reaching_object: 0.5039
     Episode_Reward/lifting_object: 12.6952
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.22s
                      Time elapsed: 00:31:40
                               ETA: 05:32:01

################################################################################
                     [1m Learning iteration 871/10000 [0m                     

                       Computation: 43889 steps/s (collection: 2.120s, learning 0.120s)
             Mean action noise std: 3.15
          Mean value_function loss: 122.4725
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.2882
                       Mean reward: 55.74
               Mean episode length: 92.45
    Episode_Reward/reaching_object: 0.4936
     Episode_Reward/lifting_object: 11.6779
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.24s
                      Time elapsed: 00:31:42
                               ETA: 05:31:59

################################################################################
                     [1m Learning iteration 872/10000 [0m                     

                       Computation: 44006 steps/s (collection: 2.119s, learning 0.115s)
             Mean action noise std: 3.15
          Mean value_function loss: 119.1883
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.2916
                       Mean reward: 69.40
               Mean episode length: 83.77
    Episode_Reward/reaching_object: 0.5123
     Episode_Reward/lifting_object: 12.8719
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 45.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.23s
                      Time elapsed: 00:31:44
                               ETA: 05:31:57

################################################################################
                     [1m Learning iteration 873/10000 [0m                     

                       Computation: 44273 steps/s (collection: 2.100s, learning 0.120s)
             Mean action noise std: 3.15
          Mean value_function loss: 132.1530
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.2945
                       Mean reward: 71.02
               Mean episode length: 89.28
    Episode_Reward/reaching_object: 0.5105
     Episode_Reward/lifting_object: 13.4633
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.22s
                      Time elapsed: 00:31:47
                               ETA: 05:31:56

################################################################################
                     [1m Learning iteration 874/10000 [0m                     

                       Computation: 43471 steps/s (collection: 2.161s, learning 0.100s)
             Mean action noise std: 3.15
          Mean value_function loss: 146.7988
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.2958
                       Mean reward: 67.37
               Mean episode length: 93.73
    Episode_Reward/reaching_object: 0.5092
     Episode_Reward/lifting_object: 13.0627
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 43.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.26s
                      Time elapsed: 00:31:49
                               ETA: 05:31:54

################################################################################
                     [1m Learning iteration 875/10000 [0m                     

                       Computation: 45278 steps/s (collection: 2.076s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 137.2152
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.2975
                       Mean reward: 62.69
               Mean episode length: 86.19
    Episode_Reward/reaching_object: 0.5047
     Episode_Reward/lifting_object: 13.0330
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.17s
                      Time elapsed: 00:31:51
                               ETA: 05:31:52

################################################################################
                     [1m Learning iteration 876/10000 [0m                     

                       Computation: 44478 steps/s (collection: 2.107s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 131.6828
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 79.3003
                       Mean reward: 74.68
               Mean episode length: 93.29
    Episode_Reward/reaching_object: 0.5110
     Episode_Reward/lifting_object: 13.2240
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.21s
                      Time elapsed: 00:31:53
                               ETA: 05:31:50

################################################################################
                     [1m Learning iteration 877/10000 [0m                     

                       Computation: 43358 steps/s (collection: 2.160s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 156.8570
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.3016
                       Mean reward: 59.65
               Mean episode length: 85.09
    Episode_Reward/reaching_object: 0.5072
     Episode_Reward/lifting_object: 13.3237
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.27s
                      Time elapsed: 00:31:56
                               ETA: 05:31:49

################################################################################
                     [1m Learning iteration 878/10000 [0m                     

                       Computation: 43867 steps/s (collection: 2.136s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 154.2828
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 79.3025
                       Mean reward: 74.57
               Mean episode length: 89.65
    Episode_Reward/reaching_object: 0.4923
     Episode_Reward/lifting_object: 13.4675
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.24s
                      Time elapsed: 00:31:58
                               ETA: 05:31:47

################################################################################
                     [1m Learning iteration 879/10000 [0m                     

                       Computation: 44025 steps/s (collection: 2.114s, learning 0.119s)
             Mean action noise std: 3.15
          Mean value_function loss: 143.6706
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.3040
                       Mean reward: 70.56
               Mean episode length: 81.06
    Episode_Reward/reaching_object: 0.4904
     Episode_Reward/lifting_object: 13.2697
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.23s
                      Time elapsed: 00:32:00
                               ETA: 05:31:46

################################################################################
                     [1m Learning iteration 880/10000 [0m                     

                       Computation: 44115 steps/s (collection: 2.120s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 147.0960
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.3072
                       Mean reward: 72.93
               Mean episode length: 90.70
    Episode_Reward/reaching_object: 0.4844
     Episode_Reward/lifting_object: 13.6219
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.23s
                      Time elapsed: 00:32:02
                               ETA: 05:31:44

################################################################################
                     [1m Learning iteration 881/10000 [0m                     

                       Computation: 44023 steps/s (collection: 2.129s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 157.5651
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 79.3097
                       Mean reward: 83.54
               Mean episode length: 85.66
    Episode_Reward/reaching_object: 0.4861
     Episode_Reward/lifting_object: 14.0013
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.23s
                      Time elapsed: 00:32:05
                               ETA: 05:31:42

################################################################################
                     [1m Learning iteration 882/10000 [0m                     

                       Computation: 44395 steps/s (collection: 2.117s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 174.3743
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 79.3120
                       Mean reward: 65.99
               Mean episode length: 84.55
    Episode_Reward/reaching_object: 0.4825
     Episode_Reward/lifting_object: 13.5225
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.21s
                      Time elapsed: 00:32:07
                               ETA: 05:31:40

################################################################################
                     [1m Learning iteration 883/10000 [0m                     

                       Computation: 43582 steps/s (collection: 2.149s, learning 0.107s)
             Mean action noise std: 3.15
          Mean value_function loss: 179.8904
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.3144
                       Mean reward: 67.88
               Mean episode length: 87.74
    Episode_Reward/reaching_object: 0.4945
     Episode_Reward/lifting_object: 14.3047
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.26s
                      Time elapsed: 00:32:09
                               ETA: 05:31:39

################################################################################
                     [1m Learning iteration 884/10000 [0m                     

                       Computation: 43624 steps/s (collection: 2.136s, learning 0.118s)
             Mean action noise std: 3.15
          Mean value_function loss: 183.4313
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 79.3158
                       Mean reward: 71.64
               Mean episode length: 80.41
    Episode_Reward/reaching_object: 0.4890
     Episode_Reward/lifting_object: 14.4335
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.25s
                      Time elapsed: 00:32:11
                               ETA: 05:31:37

################################################################################
                     [1m Learning iteration 885/10000 [0m                     

                       Computation: 43934 steps/s (collection: 2.122s, learning 0.115s)
             Mean action noise std: 3.15
          Mean value_function loss: 163.4235
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 79.3168
                       Mean reward: 69.25
               Mean episode length: 82.74
    Episode_Reward/reaching_object: 0.4902
     Episode_Reward/lifting_object: 14.7746
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.24s
                      Time elapsed: 00:32:13
                               ETA: 05:31:36

################################################################################
                     [1m Learning iteration 886/10000 [0m                     

                       Computation: 44270 steps/s (collection: 2.114s, learning 0.106s)
             Mean action noise std: 3.15
          Mean value_function loss: 184.7019
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 79.3178
                       Mean reward: 70.52
               Mean episode length: 83.53
    Episode_Reward/reaching_object: 0.5025
     Episode_Reward/lifting_object: 15.1237
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 48.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.22s
                      Time elapsed: 00:32:16
                               ETA: 05:31:34

################################################################################
                     [1m Learning iteration 887/10000 [0m                     

                       Computation: 44401 steps/s (collection: 2.110s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 178.4845
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 79.3183
                       Mean reward: 79.53
               Mean episode length: 85.57
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: 14.9037
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.21s
                      Time elapsed: 00:32:18
                               ETA: 05:31:32

################################################################################
                     [1m Learning iteration 888/10000 [0m                     

                       Computation: 43868 steps/s (collection: 2.133s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 165.7946
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.3202
                       Mean reward: 74.81
               Mean episode length: 91.01
    Episode_Reward/reaching_object: 0.5012
     Episode_Reward/lifting_object: 14.7402
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 44.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.24s
                      Time elapsed: 00:32:20
                               ETA: 05:31:31

################################################################################
                     [1m Learning iteration 889/10000 [0m                     

                       Computation: 44336 steps/s (collection: 2.113s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 159.9165
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.3228
                       Mean reward: 95.60
               Mean episode length: 91.84
    Episode_Reward/reaching_object: 0.5265
     Episode_Reward/lifting_object: 16.4177
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 44.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.22s
                      Time elapsed: 00:32:22
                               ETA: 05:31:29

################################################################################
                     [1m Learning iteration 890/10000 [0m                     

                       Computation: 43445 steps/s (collection: 2.143s, learning 0.120s)
             Mean action noise std: 3.15
          Mean value_function loss: 177.4893
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.3241
                       Mean reward: 85.92
               Mean episode length: 83.35
    Episode_Reward/reaching_object: 0.5113
     Episode_Reward/lifting_object: 15.6803
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 48.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.26s
                      Time elapsed: 00:32:25
                               ETA: 05:31:27

################################################################################
                     [1m Learning iteration 891/10000 [0m                     

                       Computation: 44239 steps/s (collection: 2.118s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 168.7975
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.3253
                       Mean reward: 85.85
               Mean episode length: 94.16
    Episode_Reward/reaching_object: 0.5263
     Episode_Reward/lifting_object: 16.2928
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.22s
                      Time elapsed: 00:32:27
                               ETA: 05:31:26

################################################################################
                     [1m Learning iteration 892/10000 [0m                     

                       Computation: 44274 steps/s (collection: 2.123s, learning 0.097s)
             Mean action noise std: 3.15
          Mean value_function loss: 194.7525
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 79.3262
                       Mean reward: 76.91
               Mean episode length: 80.64
    Episode_Reward/reaching_object: 0.5100
     Episode_Reward/lifting_object: 15.5836
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.22s
                      Time elapsed: 00:32:29
                               ETA: 05:31:24

################################################################################
                     [1m Learning iteration 893/10000 [0m                     

                       Computation: 44433 steps/s (collection: 2.117s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 185.1667
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.3285
                       Mean reward: 69.32
               Mean episode length: 84.72
    Episode_Reward/reaching_object: 0.5021
     Episode_Reward/lifting_object: 16.1908
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 47.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.21s
                      Time elapsed: 00:32:31
                               ETA: 05:31:22

################################################################################
                     [1m Learning iteration 894/10000 [0m                     

                       Computation: 44299 steps/s (collection: 2.124s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 196.9914
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.3300
                       Mean reward: 76.30
               Mean episode length: 86.01
    Episode_Reward/reaching_object: 0.5018
     Episode_Reward/lifting_object: 15.8134
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 48.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.22s
                      Time elapsed: 00:32:34
                               ETA: 05:31:20

################################################################################
                     [1m Learning iteration 895/10000 [0m                     

                       Computation: 44827 steps/s (collection: 2.100s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 189.4915
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 79.3320
                       Mean reward: 86.83
               Mean episode length: 81.94
    Episode_Reward/reaching_object: 0.5032
     Episode_Reward/lifting_object: 16.4582
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 44.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.19s
                      Time elapsed: 00:32:36
                               ETA: 05:31:18

################################################################################
                     [1m Learning iteration 896/10000 [0m                     

                       Computation: 43869 steps/s (collection: 2.114s, learning 0.127s)
             Mean action noise std: 3.16
          Mean value_function loss: 171.6629
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.3334
                       Mean reward: 82.19
               Mean episode length: 84.92
    Episode_Reward/reaching_object: 0.5082
     Episode_Reward/lifting_object: 17.0925
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.24s
                      Time elapsed: 00:32:38
                               ETA: 05:31:16

################################################################################
                     [1m Learning iteration 897/10000 [0m                     

                       Computation: 44180 steps/s (collection: 2.132s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 190.0111
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.3346
                       Mean reward: 89.20
               Mean episode length: 88.06
    Episode_Reward/reaching_object: 0.5157
     Episode_Reward/lifting_object: 16.8857
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.23s
                      Time elapsed: 00:32:40
                               ETA: 05:31:15

################################################################################
                     [1m Learning iteration 898/10000 [0m                     

                       Computation: 44040 steps/s (collection: 2.116s, learning 0.116s)
             Mean action noise std: 3.16
          Mean value_function loss: 186.5474
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.3356
                       Mean reward: 97.61
               Mean episode length: 87.40
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 17.2367
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 45.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.23s
                      Time elapsed: 00:32:42
                               ETA: 05:31:13

################################################################################
                     [1m Learning iteration 899/10000 [0m                     

                       Computation: 44697 steps/s (collection: 2.088s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 173.2122
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.3375
                       Mean reward: 88.02
               Mean episode length: 84.79
    Episode_Reward/reaching_object: 0.5117
     Episode_Reward/lifting_object: 17.7007
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.20s
                      Time elapsed: 00:32:45
                               ETA: 05:31:11

################################################################################
                     [1m Learning iteration 900/10000 [0m                     

                       Computation: 43578 steps/s (collection: 2.138s, learning 0.118s)
             Mean action noise std: 3.16
          Mean value_function loss: 179.4470
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 79.3385
                       Mean reward: 76.41
               Mean episode length: 84.16
    Episode_Reward/reaching_object: 0.4994
     Episode_Reward/lifting_object: 17.1559
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.26s
                      Time elapsed: 00:32:47
                               ETA: 05:31:10

################################################################################
                     [1m Learning iteration 901/10000 [0m                     

                       Computation: 44199 steps/s (collection: 2.119s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 181.0763
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.3391
                       Mean reward: 96.36
               Mean episode length: 93.28
    Episode_Reward/reaching_object: 0.5030
     Episode_Reward/lifting_object: 17.3100
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.22s
                      Time elapsed: 00:32:49
                               ETA: 05:31:08

################################################################################
                     [1m Learning iteration 902/10000 [0m                     

                       Computation: 44168 steps/s (collection: 2.110s, learning 0.116s)
             Mean action noise std: 3.16
          Mean value_function loss: 189.8946
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.3396
                       Mean reward: 85.27
               Mean episode length: 92.07
    Episode_Reward/reaching_object: 0.4987
     Episode_Reward/lifting_object: 16.7367
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.23s
                      Time elapsed: 00:32:51
                               ETA: 05:31:06

################################################################################
                     [1m Learning iteration 903/10000 [0m                     

                       Computation: 44970 steps/s (collection: 2.093s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 213.0136
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.3394
                       Mean reward: 75.21
               Mean episode length: 86.66
    Episode_Reward/reaching_object: 0.4817
     Episode_Reward/lifting_object: 16.3652
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.19s
                      Time elapsed: 00:32:53
                               ETA: 05:31:04

################################################################################
                     [1m Learning iteration 904/10000 [0m                     

                       Computation: 43805 steps/s (collection: 2.150s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 215.3283
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.3392
                       Mean reward: 95.65
               Mean episode length: 82.59
    Episode_Reward/reaching_object: 0.4887
     Episode_Reward/lifting_object: 18.0395
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.24s
                      Time elapsed: 00:32:56
                               ETA: 05:31:02

################################################################################
                     [1m Learning iteration 905/10000 [0m                     

                       Computation: 44184 steps/s (collection: 2.119s, learning 0.106s)
             Mean action noise std: 3.16
          Mean value_function loss: 226.8130
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 79.3397
                       Mean reward: 79.78
               Mean episode length: 84.11
    Episode_Reward/reaching_object: 0.4726
     Episode_Reward/lifting_object: 17.5686
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.22s
                      Time elapsed: 00:32:58
                               ETA: 05:31:00

################################################################################
                     [1m Learning iteration 906/10000 [0m                     

                       Computation: 44363 steps/s (collection: 2.123s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 226.6306
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.3395
                       Mean reward: 98.17
               Mean episode length: 86.14
    Episode_Reward/reaching_object: 0.4916
     Episode_Reward/lifting_object: 18.2264
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.22s
                      Time elapsed: 00:33:00
                               ETA: 05:30:59

################################################################################
                     [1m Learning iteration 907/10000 [0m                     

                       Computation: 44573 steps/s (collection: 2.108s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 245.2397
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.3387
                       Mean reward: 90.39
               Mean episode length: 79.77
    Episode_Reward/reaching_object: 0.4837
     Episode_Reward/lifting_object: 18.1908
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.21s
                      Time elapsed: 00:33:02
                               ETA: 05:30:57

################################################################################
                     [1m Learning iteration 908/10000 [0m                     

                       Computation: 43933 steps/s (collection: 2.122s, learning 0.115s)
             Mean action noise std: 3.16
          Mean value_function loss: 264.9836
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 79.3399
                       Mean reward: 93.60
               Mean episode length: 82.57
    Episode_Reward/reaching_object: 0.4931
     Episode_Reward/lifting_object: 19.4217
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.24s
                      Time elapsed: 00:33:05
                               ETA: 05:30:55

################################################################################
                     [1m Learning iteration 909/10000 [0m                     

                       Computation: 44352 steps/s (collection: 2.118s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 238.4083
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.3415
                       Mean reward: 102.02
               Mean episode length: 87.51
    Episode_Reward/reaching_object: 0.4970
     Episode_Reward/lifting_object: 19.4468
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.22s
                      Time elapsed: 00:33:07
                               ETA: 05:30:53

################################################################################
                     [1m Learning iteration 910/10000 [0m                     

                       Computation: 43814 steps/s (collection: 2.142s, learning 0.102s)
             Mean action noise std: 3.16
          Mean value_function loss: 244.7139
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.3419
                       Mean reward: 104.61
               Mean episode length: 87.15
    Episode_Reward/reaching_object: 0.5097
     Episode_Reward/lifting_object: 20.2256
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.24s
                      Time elapsed: 00:33:09
                               ETA: 05:30:52

################################################################################
                     [1m Learning iteration 911/10000 [0m                     

                       Computation: 44699 steps/s (collection: 2.100s, learning 0.100s)
             Mean action noise std: 3.16
          Mean value_function loss: 248.6105
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.3425
                       Mean reward: 107.92
               Mean episode length: 84.45
    Episode_Reward/reaching_object: 0.5149
     Episode_Reward/lifting_object: 20.3137
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 49.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.20s
                      Time elapsed: 00:33:11
                               ETA: 05:30:50

################################################################################
                     [1m Learning iteration 912/10000 [0m                     

                       Computation: 43743 steps/s (collection: 2.129s, learning 0.119s)
             Mean action noise std: 3.16
          Mean value_function loss: 289.1833
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 79.3440
                       Mean reward: 99.48
               Mean episode length: 81.28
    Episode_Reward/reaching_object: 0.4841
     Episode_Reward/lifting_object: 18.8353
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.25s
                      Time elapsed: 00:33:14
                               ETA: 05:30:48

################################################################################
                     [1m Learning iteration 913/10000 [0m                     

                       Computation: 43175 steps/s (collection: 2.168s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 265.6169
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.3457
                       Mean reward: 100.77
               Mean episode length: 82.67
    Episode_Reward/reaching_object: 0.4934
     Episode_Reward/lifting_object: 18.7327
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.28s
                      Time elapsed: 00:33:16
                               ETA: 05:30:47

################################################################################
                     [1m Learning iteration 914/10000 [0m                     

                       Computation: 43739 steps/s (collection: 2.151s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 253.6378
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 79.3467
                       Mean reward: 96.84
               Mean episode length: 84.20
    Episode_Reward/reaching_object: 0.5170
     Episode_Reward/lifting_object: 19.4883
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.25s
                      Time elapsed: 00:33:18
                               ETA: 05:30:45

################################################################################
                     [1m Learning iteration 915/10000 [0m                     

                       Computation: 42724 steps/s (collection: 2.176s, learning 0.125s)
             Mean action noise std: 3.16
          Mean value_function loss: 272.3976
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 79.3467
                       Mean reward: 128.20
               Mean episode length: 91.91
    Episode_Reward/reaching_object: 0.5396
     Episode_Reward/lifting_object: 22.0927
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 44.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.30s
                      Time elapsed: 00:33:20
                               ETA: 05:30:44

################################################################################
                     [1m Learning iteration 916/10000 [0m                     

                       Computation: 42762 steps/s (collection: 2.175s, learning 0.124s)
             Mean action noise std: 3.16
          Mean value_function loss: 280.7471
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 79.3467
                       Mean reward: 118.09
               Mean episode length: 89.40
    Episode_Reward/reaching_object: 0.5572
     Episode_Reward/lifting_object: 22.8851
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.30s
                      Time elapsed: 00:33:23
                               ETA: 05:30:43

################################################################################
                     [1m Learning iteration 917/10000 [0m                     

                       Computation: 43001 steps/s (collection: 2.164s, learning 0.122s)
             Mean action noise std: 3.16
          Mean value_function loss: 278.2352
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 79.3468
                       Mean reward: 101.71
               Mean episode length: 88.61
    Episode_Reward/reaching_object: 0.5619
     Episode_Reward/lifting_object: 22.5987
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.29s
                      Time elapsed: 00:33:25
                               ETA: 05:30:42

################################################################################
                     [1m Learning iteration 918/10000 [0m                     

                       Computation: 44089 steps/s (collection: 2.135s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 262.3882
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 79.3469
                       Mean reward: 114.23
               Mean episode length: 86.30
    Episode_Reward/reaching_object: 0.5588
     Episode_Reward/lifting_object: 24.0947
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.23s
                      Time elapsed: 00:33:27
                               ETA: 05:30:40

################################################################################
                     [1m Learning iteration 919/10000 [0m                     

                       Computation: 43374 steps/s (collection: 2.147s, learning 0.120s)
             Mean action noise std: 3.16
          Mean value_function loss: 250.4524
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.3473
                       Mean reward: 116.79
               Mean episode length: 85.22
    Episode_Reward/reaching_object: 0.5681
     Episode_Reward/lifting_object: 24.0739
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.27s
                      Time elapsed: 00:33:29
                               ETA: 05:30:39

################################################################################
                     [1m Learning iteration 920/10000 [0m                     

                       Computation: 43580 steps/s (collection: 2.132s, learning 0.124s)
             Mean action noise std: 3.16
          Mean value_function loss: 266.7522
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.3481
                       Mean reward: 123.11
               Mean episode length: 84.21
    Episode_Reward/reaching_object: 0.5562
     Episode_Reward/lifting_object: 23.5457
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.26s
                      Time elapsed: 00:33:32
                               ETA: 05:30:37

################################################################################
                     [1m Learning iteration 921/10000 [0m                     

                       Computation: 43024 steps/s (collection: 2.167s, learning 0.118s)
             Mean action noise std: 3.16
          Mean value_function loss: 258.1535
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 79.3473
                       Mean reward: 118.12
               Mean episode length: 83.58
    Episode_Reward/reaching_object: 0.5584
     Episode_Reward/lifting_object: 22.7817
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.28s
                      Time elapsed: 00:33:34
                               ETA: 05:30:36

################################################################################
                     [1m Learning iteration 922/10000 [0m                     

                       Computation: 43350 steps/s (collection: 2.157s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 250.5825
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.3471
                       Mean reward: 108.89
               Mean episode length: 86.53
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 22.7627
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.27s
                      Time elapsed: 00:33:36
                               ETA: 05:30:35

################################################################################
                     [1m Learning iteration 923/10000 [0m                     

                       Computation: 43333 steps/s (collection: 2.164s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 253.0589
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.3487
                       Mean reward: 107.69
               Mean episode length: 82.38
    Episode_Reward/reaching_object: 0.5266
     Episode_Reward/lifting_object: 21.0653
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 46.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.27s
                      Time elapsed: 00:33:39
                               ETA: 05:30:33

################################################################################
                     [1m Learning iteration 924/10000 [0m                     

                       Computation: 43784 steps/s (collection: 2.124s, learning 0.122s)
             Mean action noise std: 3.16
          Mean value_function loss: 251.0927
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 79.3526
                       Mean reward: 98.66
               Mean episode length: 84.05
    Episode_Reward/reaching_object: 0.5337
     Episode_Reward/lifting_object: 22.1076
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.25s
                      Time elapsed: 00:33:41
                               ETA: 05:30:32

################################################################################
                     [1m Learning iteration 925/10000 [0m                     

                       Computation: 45088 steps/s (collection: 2.085s, learning 0.096s)
             Mean action noise std: 3.16
          Mean value_function loss: 253.7964
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.3537
                       Mean reward: 108.99
               Mean episode length: 85.06
    Episode_Reward/reaching_object: 0.5190
     Episode_Reward/lifting_object: 21.4325
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 50.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.18s
                      Time elapsed: 00:33:43
                               ETA: 05:30:30

################################################################################
                     [1m Learning iteration 926/10000 [0m                     

                       Computation: 44542 steps/s (collection: 2.103s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 255.8019
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.3544
                       Mean reward: 105.85
               Mean episode length: 83.09
    Episode_Reward/reaching_object: 0.5212
     Episode_Reward/lifting_object: 21.0970
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.21s
                      Time elapsed: 00:33:45
                               ETA: 05:30:28

################################################################################
                     [1m Learning iteration 927/10000 [0m                     

                       Computation: 44147 steps/s (collection: 2.107s, learning 0.120s)
             Mean action noise std: 3.16
          Mean value_function loss: 256.3544
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 79.3543
                       Mean reward: 94.64
               Mean episode length: 83.94
    Episode_Reward/reaching_object: 0.5052
     Episode_Reward/lifting_object: 20.0998
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.23s
                      Time elapsed: 00:33:47
                               ETA: 05:30:26

################################################################################
                     [1m Learning iteration 928/10000 [0m                     

                       Computation: 44618 steps/s (collection: 2.112s, learning 0.091s)
             Mean action noise std: 3.16
          Mean value_function loss: 256.1904
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 79.3547
                       Mean reward: 96.02
               Mean episode length: 85.21
    Episode_Reward/reaching_object: 0.5101
     Episode_Reward/lifting_object: 20.7839
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 47.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.20s
                      Time elapsed: 00:33:50
                               ETA: 05:30:24

################################################################################
                     [1m Learning iteration 929/10000 [0m                     

                       Computation: 45187 steps/s (collection: 2.078s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 296.0366
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 79.3549
                       Mean reward: 116.65
               Mean episode length: 87.90
    Episode_Reward/reaching_object: 0.5088
     Episode_Reward/lifting_object: 21.4992
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.18s
                      Time elapsed: 00:33:52
                               ETA: 05:30:21

################################################################################
                     [1m Learning iteration 930/10000 [0m                     

                       Computation: 44995 steps/s (collection: 2.084s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 261.4307
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.3550
                       Mean reward: 99.13
               Mean episode length: 83.21
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 21.8992
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.18s
                      Time elapsed: 00:33:54
                               ETA: 05:30:19

################################################################################
                     [1m Learning iteration 931/10000 [0m                     

                       Computation: 45241 steps/s (collection: 2.072s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 277.1453
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.3557
                       Mean reward: 112.79
               Mean episode length: 85.08
    Episode_Reward/reaching_object: 0.5082
     Episode_Reward/lifting_object: 22.4079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 47.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.17s
                      Time elapsed: 00:33:56
                               ETA: 05:30:17

################################################################################
                     [1m Learning iteration 932/10000 [0m                     

                       Computation: 45468 steps/s (collection: 2.067s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 293.1029
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.3561
                       Mean reward: 116.46
               Mean episode length: 80.75
    Episode_Reward/reaching_object: 0.5173
     Episode_Reward/lifting_object: 22.6727
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.16s
                      Time elapsed: 00:33:58
                               ETA: 05:30:15

################################################################################
                     [1m Learning iteration 933/10000 [0m                     

                       Computation: 45526 steps/s (collection: 2.062s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 295.8586
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 79.3566
                       Mean reward: 121.01
               Mean episode length: 85.64
    Episode_Reward/reaching_object: 0.5065
     Episode_Reward/lifting_object: 22.1888
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.16s
                      Time elapsed: 00:34:00
                               ETA: 05:30:12

################################################################################
                     [1m Learning iteration 934/10000 [0m                     

                       Computation: 45674 steps/s (collection: 2.062s, learning 0.091s)
             Mean action noise std: 3.16
          Mean value_function loss: 291.0201
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 79.3571
                       Mean reward: 125.79
               Mean episode length: 90.55
    Episode_Reward/reaching_object: 0.5183
     Episode_Reward/lifting_object: 23.5352
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.15s
                      Time elapsed: 00:34:03
                               ETA: 05:30:10

################################################################################
                     [1m Learning iteration 935/10000 [0m                     

                       Computation: 46005 steps/s (collection: 2.048s, learning 0.089s)
             Mean action noise std: 3.16
          Mean value_function loss: 279.6922
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 79.3572
                       Mean reward: 124.06
               Mean episode length: 87.80
    Episode_Reward/reaching_object: 0.5240
     Episode_Reward/lifting_object: 24.4616
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.14s
                      Time elapsed: 00:34:05
                               ETA: 05:30:07

################################################################################
                     [1m Learning iteration 936/10000 [0m                     

                       Computation: 45918 steps/s (collection: 2.051s, learning 0.090s)
             Mean action noise std: 3.16
          Mean value_function loss: 299.4517
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 79.3573
                       Mean reward: 129.07
               Mean episode length: 87.61
    Episode_Reward/reaching_object: 0.5304
     Episode_Reward/lifting_object: 25.4434
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 47.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.14s
                      Time elapsed: 00:34:07
                               ETA: 05:30:04

################################################################################
                     [1m Learning iteration 937/10000 [0m                     

                       Computation: 45111 steps/s (collection: 2.071s, learning 0.108s)
             Mean action noise std: 3.16
          Mean value_function loss: 305.5260
               Mean surrogate loss: 0.0166
                 Mean entropy loss: 79.3574
                       Mean reward: 136.08
               Mean episode length: 82.88
    Episode_Reward/reaching_object: 0.5340
     Episode_Reward/lifting_object: 25.9248
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.18s
                      Time elapsed: 00:34:09
                               ETA: 05:30:02

################################################################################
                     [1m Learning iteration 938/10000 [0m                     

                       Computation: 45637 steps/s (collection: 2.061s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 297.3946
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 79.3574
                       Mean reward: 132.64
               Mean episode length: 85.41
    Episode_Reward/reaching_object: 0.5264
     Episode_Reward/lifting_object: 25.1501
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.15s
                      Time elapsed: 00:34:11
                               ETA: 05:30:00

################################################################################
                     [1m Learning iteration 939/10000 [0m                     

                       Computation: 46065 steps/s (collection: 2.034s, learning 0.100s)
             Mean action noise std: 3.16
          Mean value_function loss: 302.6986
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 79.3577
                       Mean reward: 118.09
               Mean episode length: 84.12
    Episode_Reward/reaching_object: 0.5230
     Episode_Reward/lifting_object: 24.8725
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.13s
                      Time elapsed: 00:34:13
                               ETA: 05:29:57

################################################################################
                     [1m Learning iteration 940/10000 [0m                     

                       Computation: 45963 steps/s (collection: 2.025s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 322.8698
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.3585
                       Mean reward: 125.14
               Mean episode length: 84.67
    Episode_Reward/reaching_object: 0.5220
     Episode_Reward/lifting_object: 25.8757
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.14s
                      Time elapsed: 00:34:15
                               ETA: 05:29:54

################################################################################
                     [1m Learning iteration 941/10000 [0m                     

                       Computation: 45810 steps/s (collection: 2.038s, learning 0.108s)
             Mean action noise std: 3.16
          Mean value_function loss: 350.1142
               Mean surrogate loss: 0.0222
                 Mean entropy loss: 79.3600
                       Mean reward: 150.63
               Mean episode length: 90.47
    Episode_Reward/reaching_object: 0.5205
     Episode_Reward/lifting_object: 25.2829
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.15s
                      Time elapsed: 00:34:18
                               ETA: 05:29:52

################################################################################
                     [1m Learning iteration 942/10000 [0m                     

                       Computation: 46384 steps/s (collection: 2.022s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 330.1225
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 79.3601
                       Mean reward: 127.25
               Mean episode length: 87.04
    Episode_Reward/reaching_object: 0.5258
     Episode_Reward/lifting_object: 25.7591
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.12s
                      Time elapsed: 00:34:20
                               ETA: 05:29:49

################################################################################
                     [1m Learning iteration 943/10000 [0m                     

                       Computation: 46259 steps/s (collection: 2.035s, learning 0.091s)
             Mean action noise std: 3.16
          Mean value_function loss: 327.7487
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.3601
                       Mean reward: 144.05
               Mean episode length: 85.73
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 27.8462
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 46.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.13s
                      Time elapsed: 00:34:22
                               ETA: 05:29:46

################################################################################
                     [1m Learning iteration 944/10000 [0m                     

                       Computation: 45509 steps/s (collection: 2.055s, learning 0.106s)
             Mean action noise std: 3.16
          Mean value_function loss: 334.4492
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.3602
                       Mean reward: 135.73
               Mean episode length: 84.35
    Episode_Reward/reaching_object: 0.5539
     Episode_Reward/lifting_object: 28.2117
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.16s
                      Time elapsed: 00:34:24
                               ETA: 05:29:44

################################################################################
                     [1m Learning iteration 945/10000 [0m                     

                       Computation: 45517 steps/s (collection: 2.062s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 359.0589
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.3610
                       Mean reward: 153.52
               Mean episode length: 89.28
    Episode_Reward/reaching_object: 0.5556
     Episode_Reward/lifting_object: 28.9490
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 47.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.16s
                      Time elapsed: 00:34:26
                               ETA: 05:29:41

################################################################################
                     [1m Learning iteration 946/10000 [0m                     

                       Computation: 46291 steps/s (collection: 2.034s, learning 0.090s)
             Mean action noise std: 3.16
          Mean value_function loss: 348.0953
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.3610
                       Mean reward: 136.58
               Mean episode length: 84.83
    Episode_Reward/reaching_object: 0.5540
     Episode_Reward/lifting_object: 28.3846
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.12s
                      Time elapsed: 00:34:28
                               ETA: 05:29:39

################################################################################
                     [1m Learning iteration 947/10000 [0m                     

                       Computation: 46176 steps/s (collection: 2.035s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 371.2488
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.3614
                       Mean reward: 141.12
               Mean episode length: 83.77
    Episode_Reward/reaching_object: 0.5602
     Episode_Reward/lifting_object: 29.2514
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.13s
                      Time elapsed: 00:34:30
                               ETA: 05:29:36

################################################################################
                     [1m Learning iteration 948/10000 [0m                     

                       Computation: 45667 steps/s (collection: 2.058s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 389.8767
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 79.3628
                       Mean reward: 134.80
               Mean episode length: 88.57
    Episode_Reward/reaching_object: 0.5556
     Episode_Reward/lifting_object: 29.9411
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.15s
                      Time elapsed: 00:34:33
                               ETA: 05:29:33

################################################################################
                     [1m Learning iteration 949/10000 [0m                     

                       Computation: 46405 steps/s (collection: 2.031s, learning 0.088s)
             Mean action noise std: 3.16
          Mean value_function loss: 342.7595
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.3637
                       Mean reward: 187.55
               Mean episode length: 94.41
    Episode_Reward/reaching_object: 0.5646
     Episode_Reward/lifting_object: 30.3521
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.12s
                      Time elapsed: 00:34:35
                               ETA: 05:29:31

################################################################################
                     [1m Learning iteration 950/10000 [0m                     

                       Computation: 46008 steps/s (collection: 2.032s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 343.6716
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 79.3643
                       Mean reward: 150.70
               Mean episode length: 89.20
    Episode_Reward/reaching_object: 0.5733
     Episode_Reward/lifting_object: 31.1959
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.14s
                      Time elapsed: 00:34:37
                               ETA: 05:29:28

################################################################################
                     [1m Learning iteration 951/10000 [0m                     

                       Computation: 45772 steps/s (collection: 2.055s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 348.9647
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 79.3645
                       Mean reward: 181.38
               Mean episode length: 90.94
    Episode_Reward/reaching_object: 0.5614
     Episode_Reward/lifting_object: 30.5968
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.15s
                      Time elapsed: 00:34:39
                               ETA: 05:29:25

################################################################################
                     [1m Learning iteration 952/10000 [0m                     

                       Computation: 46016 steps/s (collection: 2.045s, learning 0.092s)
             Mean action noise std: 3.16
          Mean value_function loss: 370.3393
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.3647
                       Mean reward: 179.26
               Mean episode length: 91.82
    Episode_Reward/reaching_object: 0.5860
     Episode_Reward/lifting_object: 32.3697
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.14s
                      Time elapsed: 00:34:41
                               ETA: 05:29:23

################################################################################
                     [1m Learning iteration 953/10000 [0m                     

                       Computation: 45633 steps/s (collection: 2.048s, learning 0.106s)
             Mean action noise std: 3.16
          Mean value_function loss: 368.0095
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.3651
                       Mean reward: 165.22
               Mean episode length: 93.78
    Episode_Reward/reaching_object: 0.5708
     Episode_Reward/lifting_object: 30.8375
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.15s
                      Time elapsed: 00:34:43
                               ETA: 05:29:20

################################################################################
                     [1m Learning iteration 954/10000 [0m                     

                       Computation: 45263 steps/s (collection: 2.060s, learning 0.112s)
             Mean action noise std: 3.16
          Mean value_function loss: 371.4138
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 79.3662
                       Mean reward: 150.48
               Mean episode length: 85.79
    Episode_Reward/reaching_object: 0.5756
     Episode_Reward/lifting_object: 32.4955
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.17s
                      Time elapsed: 00:34:45
                               ETA: 05:29:18

################################################################################
                     [1m Learning iteration 955/10000 [0m                     

                       Computation: 45422 steps/s (collection: 2.050s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 363.9117
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 79.3674
                       Mean reward: 141.90
               Mean episode length: 88.68
    Episode_Reward/reaching_object: 0.5663
     Episode_Reward/lifting_object: 30.9218
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.16s
                      Time elapsed: 00:34:48
                               ETA: 05:29:16

################################################################################
                     [1m Learning iteration 956/10000 [0m                     

                       Computation: 45549 steps/s (collection: 2.061s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 381.8972
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.3681
                       Mean reward: 156.90
               Mean episode length: 86.94
    Episode_Reward/reaching_object: 0.5846
     Episode_Reward/lifting_object: 32.6456
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.16s
                      Time elapsed: 00:34:50
                               ETA: 05:29:13

################################################################################
                     [1m Learning iteration 957/10000 [0m                     

                       Computation: 45855 steps/s (collection: 2.058s, learning 0.086s)
             Mean action noise std: 3.16
          Mean value_function loss: 397.9678
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.3694
                       Mean reward: 168.71
               Mean episode length: 87.24
    Episode_Reward/reaching_object: 0.5654
     Episode_Reward/lifting_object: 31.7825
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.14s
                      Time elapsed: 00:34:52
                               ETA: 05:29:11

################################################################################
                     [1m Learning iteration 958/10000 [0m                     

                       Computation: 45808 steps/s (collection: 2.053s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 425.8721
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 79.3708
                       Mean reward: 150.45
               Mean episode length: 87.15
    Episode_Reward/reaching_object: 0.5614
     Episode_Reward/lifting_object: 31.3099
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.15s
                      Time elapsed: 00:34:54
                               ETA: 05:29:08

################################################################################
                     [1m Learning iteration 959/10000 [0m                     

                       Computation: 45025 steps/s (collection: 2.074s, learning 0.110s)
             Mean action noise std: 3.16
          Mean value_function loss: 415.8178
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.3720
                       Mean reward: 173.37
               Mean episode length: 90.19
    Episode_Reward/reaching_object: 0.5685
     Episode_Reward/lifting_object: 32.2627
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.18s
                      Time elapsed: 00:34:56
                               ETA: 05:29:06

################################################################################
                     [1m Learning iteration 960/10000 [0m                     

                       Computation: 45279 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 403.3566
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.3725
                       Mean reward: 163.01
               Mean episode length: 92.20
    Episode_Reward/reaching_object: 0.5914
     Episode_Reward/lifting_object: 34.1302
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 44.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.17s
                      Time elapsed: 00:34:58
                               ETA: 05:29:04

################################################################################
                     [1m Learning iteration 961/10000 [0m                     

                       Computation: 44237 steps/s (collection: 2.132s, learning 0.090s)
             Mean action noise std: 3.16
          Mean value_function loss: 406.3680
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.3737
                       Mean reward: 173.43
               Mean episode length: 90.01
    Episode_Reward/reaching_object: 0.5877
     Episode_Reward/lifting_object: 35.4935
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.7917
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.22s
                      Time elapsed: 00:35:01
                               ETA: 05:29:02

################################################################################
                     [1m Learning iteration 962/10000 [0m                     

                       Computation: 45196 steps/s (collection: 2.085s, learning 0.090s)
             Mean action noise std: 3.16
          Mean value_function loss: 391.7647
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 79.3740
                       Mean reward: 169.20
               Mean episode length: 86.52
    Episode_Reward/reaching_object: 0.5854
     Episode_Reward/lifting_object: 35.3393
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 45.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.18s
                      Time elapsed: 00:35:03
                               ETA: 05:28:59

################################################################################
                     [1m Learning iteration 963/10000 [0m                     

                       Computation: 45762 steps/s (collection: 2.061s, learning 0.087s)
             Mean action noise std: 3.16
          Mean value_function loss: 406.5734
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 79.3741
                       Mean reward: 177.31
               Mean episode length: 89.33
    Episode_Reward/reaching_object: 0.5895
     Episode_Reward/lifting_object: 35.6621
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 43.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.15s
                      Time elapsed: 00:35:05
                               ETA: 05:28:57

################################################################################
                     [1m Learning iteration 964/10000 [0m                     

                       Computation: 45397 steps/s (collection: 2.061s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 396.9170
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.3750
                       Mean reward: 183.30
               Mean episode length: 90.86
    Episode_Reward/reaching_object: 0.5915
     Episode_Reward/lifting_object: 34.7902
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.17s
                      Time elapsed: 00:35:07
                               ETA: 05:28:55

################################################################################
                     [1m Learning iteration 965/10000 [0m                     

                       Computation: 44695 steps/s (collection: 2.095s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 433.0662
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.3779
                       Mean reward: 157.59
               Mean episode length: 91.44
    Episode_Reward/reaching_object: 0.5966
     Episode_Reward/lifting_object: 36.3777
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 44.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.20s
                      Time elapsed: 00:35:09
                               ETA: 05:28:53

################################################################################
                     [1m Learning iteration 966/10000 [0m                     

                       Computation: 44770 steps/s (collection: 2.087s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 425.6820
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.3795
                       Mean reward: 198.15
               Mean episode length: 98.39
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 33.8553
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 43.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.20s
                      Time elapsed: 00:35:12
                               ETA: 05:28:50

################################################################################
                     [1m Learning iteration 967/10000 [0m                     

                       Computation: 44965 steps/s (collection: 2.079s, learning 0.107s)
             Mean action noise std: 3.16
          Mean value_function loss: 412.7898
               Mean surrogate loss: 0.0249
                 Mean entropy loss: 79.3803
                       Mean reward: 163.15
               Mean episode length: 85.15
    Episode_Reward/reaching_object: 0.5818
     Episode_Reward/lifting_object: 34.2974
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.19s
                      Time elapsed: 00:35:14
                               ETA: 05:28:48

################################################################################
                     [1m Learning iteration 968/10000 [0m                     

                       Computation: 44038 steps/s (collection: 2.114s, learning 0.118s)
             Mean action noise std: 3.16
          Mean value_function loss: 402.1051
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 79.3805
                       Mean reward: 196.19
               Mean episode length: 88.80
    Episode_Reward/reaching_object: 0.6164
     Episode_Reward/lifting_object: 38.3886
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 40.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.23s
                      Time elapsed: 00:35:16
                               ETA: 05:28:47

################################################################################
                     [1m Learning iteration 969/10000 [0m                     

                       Computation: 44737 steps/s (collection: 2.086s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 400.2213
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 79.3806
                       Mean reward: 220.56
               Mean episode length: 93.05
    Episode_Reward/reaching_object: 0.6137
     Episode_Reward/lifting_object: 38.3952
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 41.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.20s
                      Time elapsed: 00:35:18
                               ETA: 05:28:45

################################################################################
                     [1m Learning iteration 970/10000 [0m                     

                       Computation: 44816 steps/s (collection: 2.105s, learning 0.089s)
             Mean action noise std: 3.16
          Mean value_function loss: 388.1891
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 79.3807
                       Mean reward: 200.22
               Mean episode length: 92.30
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 38.3754
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 43.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.19s
                      Time elapsed: 00:35:20
                               ETA: 05:28:42

################################################################################
                     [1m Learning iteration 971/10000 [0m                     

                       Computation: 45254 steps/s (collection: 2.078s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 410.2178
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 79.3807
                       Mean reward: 211.10
               Mean episode length: 101.10
    Episode_Reward/reaching_object: 0.6232
     Episode_Reward/lifting_object: 38.6516
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 47.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.17s
                      Time elapsed: 00:35:22
                               ETA: 05:28:40

################################################################################
                     [1m Learning iteration 972/10000 [0m                     

                       Computation: 45177 steps/s (collection: 2.084s, learning 0.092s)
             Mean action noise std: 3.16
          Mean value_function loss: 418.1019
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 79.3808
                       Mean reward: 175.25
               Mean episode length: 91.60
    Episode_Reward/reaching_object: 0.6205
     Episode_Reward/lifting_object: 36.5982
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.18s
                      Time elapsed: 00:35:25
                               ETA: 05:28:38

################################################################################
                     [1m Learning iteration 973/10000 [0m                     

                       Computation: 44240 steps/s (collection: 2.121s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 385.7450
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.3808
                       Mean reward: 187.28
               Mean episode length: 94.29
    Episode_Reward/reaching_object: 0.5971
     Episode_Reward/lifting_object: 34.2905
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.22s
                      Time elapsed: 00:35:27
                               ETA: 05:28:36

################################################################################
                     [1m Learning iteration 974/10000 [0m                     

                       Computation: 45632 steps/s (collection: 2.062s, learning 0.092s)
             Mean action noise std: 3.16
          Mean value_function loss: 402.6035
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 79.3806
                       Mean reward: 192.89
               Mean episode length: 99.76
    Episode_Reward/reaching_object: 0.5919
     Episode_Reward/lifting_object: 34.1016
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 43.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.15s
                      Time elapsed: 00:35:29
                               ETA: 05:28:34

################################################################################
                     [1m Learning iteration 975/10000 [0m                     

                       Computation: 45096 steps/s (collection: 2.072s, learning 0.108s)
             Mean action noise std: 3.17
          Mean value_function loss: 397.4201
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 79.3803
                       Mean reward: 162.59
               Mean episode length: 87.75
    Episode_Reward/reaching_object: 0.5978
     Episode_Reward/lifting_object: 35.0355
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 42.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.18s
                      Time elapsed: 00:35:31
                               ETA: 05:28:31

################################################################################
                     [1m Learning iteration 976/10000 [0m                     

                       Computation: 45821 steps/s (collection: 2.046s, learning 0.100s)
             Mean action noise std: 3.17
          Mean value_function loss: 409.0678
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 79.3804
                       Mean reward: 178.90
               Mean episode length: 90.17
    Episode_Reward/reaching_object: 0.6207
     Episode_Reward/lifting_object: 37.1663
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 42.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.15s
                      Time elapsed: 00:35:33
                               ETA: 05:28:29

################################################################################
                     [1m Learning iteration 977/10000 [0m                     

                       Computation: 41842 steps/s (collection: 2.163s, learning 0.186s)
             Mean action noise std: 3.17
          Mean value_function loss: 411.1193
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 79.3805
                       Mean reward: 207.36
               Mean episode length: 99.43
    Episode_Reward/reaching_object: 0.6072
     Episode_Reward/lifting_object: 37.1255
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.35s
                      Time elapsed: 00:35:36
                               ETA: 05:28:28

################################################################################
                     [1m Learning iteration 978/10000 [0m                     

                       Computation: 42293 steps/s (collection: 2.231s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 399.5987
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 79.3806
                       Mean reward: 220.97
               Mean episode length: 103.31
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 38.0465
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 43.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.32s
                      Time elapsed: 00:35:38
                               ETA: 05:28:27

################################################################################
                     [1m Learning iteration 979/10000 [0m                     

                       Computation: 44498 steps/s (collection: 2.117s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 434.8915
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 79.3806
                       Mean reward: 197.71
               Mean episode length: 94.00
    Episode_Reward/reaching_object: 0.6036
     Episode_Reward/lifting_object: 37.5991
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.21s
                      Time elapsed: 00:35:40
                               ETA: 05:28:25

################################################################################
                     [1m Learning iteration 980/10000 [0m                     

                       Computation: 44124 steps/s (collection: 2.120s, learning 0.108s)
             Mean action noise std: 3.17
          Mean value_function loss: 424.7916
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 79.3808
                       Mean reward: 161.11
               Mean episode length: 89.24
    Episode_Reward/reaching_object: 0.5820
     Episode_Reward/lifting_object: 34.8090
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.23s
                      Time elapsed: 00:35:42
                               ETA: 05:28:24

################################################################################
                     [1m Learning iteration 981/10000 [0m                     

                       Computation: 44599 steps/s (collection: 2.113s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 423.5126
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 79.3811
                       Mean reward: 158.08
               Mean episode length: 87.97
    Episode_Reward/reaching_object: 0.5594
     Episode_Reward/lifting_object: 32.6825
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.20s
                      Time elapsed: 00:35:45
                               ETA: 05:28:22

################################################################################
                     [1m Learning iteration 982/10000 [0m                     

                       Computation: 42150 steps/s (collection: 2.193s, learning 0.140s)
             Mean action noise std: 3.17
          Mean value_function loss: 453.5699
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.3821
                       Mean reward: 197.04
               Mean episode length: 94.51
    Episode_Reward/reaching_object: 0.6053
     Episode_Reward/lifting_object: 36.9888
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.33s
                      Time elapsed: 00:35:47
                               ETA: 05:28:21

################################################################################
                     [1m Learning iteration 983/10000 [0m                     

                       Computation: 41476 steps/s (collection: 2.245s, learning 0.125s)
             Mean action noise std: 3.17
          Mean value_function loss: 465.2877
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 79.3826
                       Mean reward: 150.80
               Mean episode length: 85.91
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 34.2156
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.37s
                      Time elapsed: 00:35:49
                               ETA: 05:28:20

################################################################################
                     [1m Learning iteration 984/10000 [0m                     

                       Computation: 43118 steps/s (collection: 2.179s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 446.1467
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 79.3825
                       Mean reward: 168.51
               Mean episode length: 83.91
    Episode_Reward/reaching_object: 0.5907
     Episode_Reward/lifting_object: 36.5717
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.28s
                      Time elapsed: 00:35:52
                               ETA: 05:28:19

################################################################################
                     [1m Learning iteration 985/10000 [0m                     

                       Computation: 42632 steps/s (collection: 2.186s, learning 0.120s)
             Mean action noise std: 3.17
          Mean value_function loss: 452.8857
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 79.3827
                       Mean reward: 199.98
               Mean episode length: 93.92
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 37.0614
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.31s
                      Time elapsed: 00:35:54
                               ETA: 05:28:18

################################################################################
                     [1m Learning iteration 986/10000 [0m                     

                       Computation: 42543 steps/s (collection: 2.207s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 458.7908
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.3829
                       Mean reward: 198.01
               Mean episode length: 92.22
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 37.9096
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 46.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.31s
                      Time elapsed: 00:35:56
                               ETA: 05:28:17

################################################################################
                     [1m Learning iteration 987/10000 [0m                     

                       Computation: 43703 steps/s (collection: 2.156s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 438.6275
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.3832
                       Mean reward: 207.37
               Mean episode length: 95.07
    Episode_Reward/reaching_object: 0.6262
     Episode_Reward/lifting_object: 40.8400
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.25s
                      Time elapsed: 00:35:59
                               ETA: 05:28:15

################################################################################
                     [1m Learning iteration 988/10000 [0m                     

                       Computation: 44528 steps/s (collection: 2.115s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 435.3747
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 79.3836
                       Mean reward: 259.49
               Mean episode length: 106.67
    Episode_Reward/reaching_object: 0.6293
     Episode_Reward/lifting_object: 41.5964
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.21s
                      Time elapsed: 00:36:01
                               ETA: 05:28:13

################################################################################
                     [1m Learning iteration 989/10000 [0m                     

                       Computation: 44329 steps/s (collection: 2.120s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 437.3504
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 79.3836
                       Mean reward: 211.88
               Mean episode length: 96.85
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: 40.6518
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 41.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.22s
                      Time elapsed: 00:36:03
                               ETA: 05:28:11

################################################################################
                     [1m Learning iteration 990/10000 [0m                     

                       Computation: 43031 steps/s (collection: 2.186s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 439.5547
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 79.3835
                       Mean reward: 210.78
               Mean episode length: 95.98
    Episode_Reward/reaching_object: 0.6384
     Episode_Reward/lifting_object: 41.8623
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 42.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.28s
                      Time elapsed: 00:36:05
                               ETA: 05:28:10

################################################################################
                     [1m Learning iteration 991/10000 [0m                     

                       Computation: 44567 steps/s (collection: 2.116s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 461.4805
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 79.3836
                       Mean reward: 207.75
               Mean episode length: 102.17
    Episode_Reward/reaching_object: 0.6584
     Episode_Reward/lifting_object: 44.3617
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.21s
                      Time elapsed: 00:36:07
                               ETA: 05:28:08

################################################################################
                     [1m Learning iteration 992/10000 [0m                     

                       Computation: 45108 steps/s (collection: 2.089s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 457.9920
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 79.3842
                       Mean reward: 233.74
               Mean episode length: 101.96
    Episode_Reward/reaching_object: 0.6320
     Episode_Reward/lifting_object: 40.6015
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.18s
                      Time elapsed: 00:36:10
                               ETA: 05:28:06

################################################################################
                     [1m Learning iteration 993/10000 [0m                     

                       Computation: 44295 steps/s (collection: 2.126s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 490.9990
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 79.3854
                       Mean reward: 215.72
               Mean episode length: 94.61
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 42.8471
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.22s
                      Time elapsed: 00:36:12
                               ETA: 05:28:04

################################################################################
                     [1m Learning iteration 994/10000 [0m                     

                       Computation: 44707 steps/s (collection: 2.111s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 487.9591
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.3871
                       Mean reward: 219.80
               Mean episode length: 97.84
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 41.6374
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 42.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.20s
                      Time elapsed: 00:36:14
                               ETA: 05:28:02

################################################################################
                     [1m Learning iteration 995/10000 [0m                     

                       Computation: 44656 steps/s (collection: 2.098s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 462.1259
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 79.3873
                       Mean reward: 186.58
               Mean episode length: 93.73
    Episode_Reward/reaching_object: 0.6370
     Episode_Reward/lifting_object: 41.0305
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.20s
                      Time elapsed: 00:36:16
                               ETA: 05:28:00

################################################################################
                     [1m Learning iteration 996/10000 [0m                     

                       Computation: 43946 steps/s (collection: 2.141s, learning 0.096s)
             Mean action noise std: 3.17
          Mean value_function loss: 468.0857
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 79.3879
                       Mean reward: 206.98
               Mean episode length: 99.61
    Episode_Reward/reaching_object: 0.6377
     Episode_Reward/lifting_object: 42.7176
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.24s
                      Time elapsed: 00:36:18
                               ETA: 05:27:58

################################################################################
                     [1m Learning iteration 997/10000 [0m                     

                       Computation: 44136 steps/s (collection: 2.125s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 490.0561
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 79.3878
                       Mean reward: 219.76
               Mean episode length: 98.65
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 45.5474
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.23s
                      Time elapsed: 00:36:21
                               ETA: 05:27:56

################################################################################
                     [1m Learning iteration 998/10000 [0m                     

                       Computation: 43651 steps/s (collection: 2.162s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 507.5577
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 79.3882
                       Mean reward: 273.10
               Mean episode length: 110.46
    Episode_Reward/reaching_object: 0.6575
     Episode_Reward/lifting_object: 44.7820
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.25s
                      Time elapsed: 00:36:23
                               ETA: 05:27:55

################################################################################
                     [1m Learning iteration 999/10000 [0m                     

                       Computation: 44890 steps/s (collection: 2.087s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 456.4433
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 79.3885
                       Mean reward: 258.21
               Mean episode length: 106.17
    Episode_Reward/reaching_object: 0.6892
     Episode_Reward/lifting_object: 46.8032
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.19s
                      Time elapsed: 00:36:25
                               ETA: 05:27:53

################################################################################
                    [1m Learning iteration 1000/10000 [0m                     

                       Computation: 13973 steps/s (collection: 6.913s, learning 0.122s)
             Mean action noise std: 3.17
          Mean value_function loss: 458.6934
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 79.3887
                       Mean reward: 232.00
               Mean episode length: 100.90
    Episode_Reward/reaching_object: 0.6850
     Episode_Reward/lifting_object: 47.5628
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.04s
                      Time elapsed: 00:36:32
                               ETA: 05:28:34

################################################################################
                    [1m Learning iteration 1001/10000 [0m                     

                       Computation: 14035 steps/s (collection: 6.887s, learning 0.117s)
             Mean action noise std: 3.17
          Mean value_function loss: 447.3347
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.3889
                       Mean reward: 227.43
               Mean episode length: 101.13
    Episode_Reward/reaching_object: 0.7129
     Episode_Reward/lifting_object: 49.2984
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.00s
                      Time elapsed: 00:36:39
                               ETA: 05:29:15

################################################################################
                    [1m Learning iteration 1002/10000 [0m                     

                       Computation: 13679 steps/s (collection: 7.075s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 464.0101
               Mean surrogate loss: 0.0184
                 Mean entropy loss: 79.3890
                       Mean reward: 241.27
               Mean episode length: 109.05
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 49.0229
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.19s
                      Time elapsed: 00:36:46
                               ETA: 05:29:58

################################################################################
                    [1m Learning iteration 1003/10000 [0m                     

                       Computation: 13828 steps/s (collection: 6.983s, learning 0.126s)
             Mean action noise std: 3.17
          Mean value_function loss: 467.4900
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 79.3890
                       Mean reward: 248.89
               Mean episode length: 104.90
    Episode_Reward/reaching_object: 0.7036
     Episode_Reward/lifting_object: 48.5626
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.11s
                      Time elapsed: 00:36:53
                               ETA: 05:30:39

################################################################################
                    [1m Learning iteration 1004/10000 [0m                     

                       Computation: 13905 steps/s (collection: 6.949s, learning 0.121s)
             Mean action noise std: 3.17
          Mean value_function loss: 486.2670
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 79.3893
                       Mean reward: 242.42
               Mean episode length: 102.78
    Episode_Reward/reaching_object: 0.7138
     Episode_Reward/lifting_object: 48.6245
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.07s
                      Time elapsed: 00:37:01
                               ETA: 05:31:21

################################################################################
                    [1m Learning iteration 1005/10000 [0m                     

                       Computation: 13970 steps/s (collection: 6.918s, learning 0.118s)
             Mean action noise std: 3.17
          Mean value_function loss: 466.7241
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 79.3907
                       Mean reward: 223.41
               Mean episode length: 103.47
    Episode_Reward/reaching_object: 0.6651
     Episode_Reward/lifting_object: 44.2107
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.04s
                      Time elapsed: 00:37:08
                               ETA: 05:32:02

################################################################################
                    [1m Learning iteration 1006/10000 [0m                     

                       Computation: 13716 steps/s (collection: 7.029s, learning 0.138s)
             Mean action noise std: 3.17
          Mean value_function loss: 483.8598
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 79.3914
                       Mean reward: 215.32
               Mean episode length: 100.92
    Episode_Reward/reaching_object: 0.6792
     Episode_Reward/lifting_object: 46.2313
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.17s
                      Time elapsed: 00:37:15
                               ETA: 05:32:44

################################################################################
                    [1m Learning iteration 1007/10000 [0m                     

                       Computation: 13877 steps/s (collection: 6.965s, learning 0.119s)
             Mean action noise std: 3.17
          Mean value_function loss: 447.4382
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.3918
                       Mean reward: 235.52
               Mean episode length: 105.80
    Episode_Reward/reaching_object: 0.6984
     Episode_Reward/lifting_object: 47.3367
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.08s
                      Time elapsed: 00:37:22
                               ETA: 05:33:25

################################################################################
                    [1m Learning iteration 1008/10000 [0m                     

                       Computation: 15605 steps/s (collection: 6.197s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 469.0919
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 79.3924
                       Mean reward: 276.96
               Mean episode length: 108.90
    Episode_Reward/reaching_object: 0.7285
     Episode_Reward/lifting_object: 52.6016
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.30s
                      Time elapsed: 00:37:28
                               ETA: 05:33:59

################################################################################
                    [1m Learning iteration 1009/10000 [0m                     

                       Computation: 45649 steps/s (collection: 2.064s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 475.5086
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 79.3925
                       Mean reward: 231.72
               Mean episode length: 101.74
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 54.8961
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.15s
                      Time elapsed: 00:37:30
                               ETA: 05:33:56

################################################################################
                    [1m Learning iteration 1010/10000 [0m                     

                       Computation: 45283 steps/s (collection: 2.069s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 493.8750
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 79.3930
                       Mean reward: 226.12
               Mean episode length: 103.33
    Episode_Reward/reaching_object: 0.7304
     Episode_Reward/lifting_object: 51.8966
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.17s
                      Time elapsed: 00:37:32
                               ETA: 05:33:53

################################################################################
                    [1m Learning iteration 1011/10000 [0m                     

                       Computation: 45310 steps/s (collection: 2.063s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 468.4587
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 79.3938
                       Mean reward: 285.59
               Mean episode length: 120.69
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 52.4209
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.17s
                      Time elapsed: 00:37:35
                               ETA: 05:33:51

################################################################################
                    [1m Learning iteration 1012/10000 [0m                     

                       Computation: 46082 steps/s (collection: 2.039s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 493.4913
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 79.3940
                       Mean reward: 297.11
               Mean episode length: 115.72
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 55.9754
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.13s
                      Time elapsed: 00:37:37
                               ETA: 05:33:47

################################################################################
                    [1m Learning iteration 1013/10000 [0m                     

                       Computation: 45654 steps/s (collection: 2.038s, learning 0.115s)
             Mean action noise std: 3.17
          Mean value_function loss: 448.8649
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 79.3942
                       Mean reward: 282.75
               Mean episode length: 113.29
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 54.0899
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.15s
                      Time elapsed: 00:37:39
                               ETA: 05:33:45

################################################################################
                    [1m Learning iteration 1014/10000 [0m                     

                       Computation: 44826 steps/s (collection: 2.086s, learning 0.107s)
             Mean action noise std: 3.17
          Mean value_function loss: 473.4867
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.3943
                       Mean reward: 234.27
               Mean episode length: 107.50
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 52.6881
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.19s
                      Time elapsed: 00:37:41
                               ETA: 05:33:42

################################################################################
                    [1m Learning iteration 1015/10000 [0m                     

                       Computation: 44995 steps/s (collection: 2.076s, learning 0.108s)
             Mean action noise std: 3.17
          Mean value_function loss: 467.7630
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 79.3945
                       Mean reward: 328.10
               Mean episode length: 120.44
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 56.8569
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.18s
                      Time elapsed: 00:37:43
                               ETA: 05:33:39

################################################################################
                    [1m Learning iteration 1016/10000 [0m                     

                       Computation: 45541 steps/s (collection: 2.064s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 484.7506
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 79.3955
                       Mean reward: 257.40
               Mean episode length: 117.96
    Episode_Reward/reaching_object: 0.7304
     Episode_Reward/lifting_object: 51.4386
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.16s
                      Time elapsed: 00:37:45
                               ETA: 05:33:37

################################################################################
                    [1m Learning iteration 1017/10000 [0m                     

                       Computation: 45863 steps/s (collection: 2.045s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 471.0432
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 79.3957
                       Mean reward: 253.97
               Mean episode length: 111.78
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 52.9190
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.14s
                      Time elapsed: 00:37:48
                               ETA: 05:33:34

################################################################################
                    [1m Learning iteration 1018/10000 [0m                     

                       Computation: 44322 steps/s (collection: 2.106s, learning 0.112s)
             Mean action noise std: 3.17
          Mean value_function loss: 465.1937
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 79.3959
                       Mean reward: 271.57
               Mean episode length: 116.43
    Episode_Reward/reaching_object: 0.7292
     Episode_Reward/lifting_object: 51.2472
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.22s
                      Time elapsed: 00:37:50
                               ETA: 05:33:31

################################################################################
                    [1m Learning iteration 1019/10000 [0m                     

                       Computation: 42723 steps/s (collection: 2.149s, learning 0.152s)
             Mean action noise std: 3.17
          Mean value_function loss: 487.4983
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.3962
                       Mean reward: 264.76
               Mean episode length: 120.06
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 56.5417
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.30s
                      Time elapsed: 00:37:52
                               ETA: 05:33:30

################################################################################
                    [1m Learning iteration 1020/10000 [0m                     

                       Computation: 45448 steps/s (collection: 2.065s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 484.6319
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.3977
                       Mean reward: 289.11
               Mean episode length: 121.17
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 56.5472
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.16s
                      Time elapsed: 00:37:54
                               ETA: 05:33:27

################################################################################
                    [1m Learning iteration 1021/10000 [0m                     

                       Computation: 45531 steps/s (collection: 2.054s, learning 0.105s)
             Mean action noise std: 3.17
          Mean value_function loss: 474.9068
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 79.3989
                       Mean reward: 274.48
               Mean episode length: 116.95
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 56.7858
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.16s
                      Time elapsed: 00:37:56
                               ETA: 05:33:24

################################################################################
                    [1m Learning iteration 1022/10000 [0m                     

                       Computation: 45595 steps/s (collection: 2.048s, learning 0.108s)
             Mean action noise std: 3.17
          Mean value_function loss: 472.4810
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 79.3991
                       Mean reward: 343.61
               Mean episode length: 127.75
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 55.4473
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.16s
                      Time elapsed: 00:37:59
                               ETA: 05:33:21

################################################################################
                    [1m Learning iteration 1023/10000 [0m                     

                       Computation: 44160 steps/s (collection: 2.132s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 485.3555
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 79.3993
                       Mean reward: 309.52
               Mean episode length: 126.69
    Episode_Reward/reaching_object: 0.8015
     Episode_Reward/lifting_object: 58.4877
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.23s
                      Time elapsed: 00:38:01
                               ETA: 05:33:19

################################################################################
                    [1m Learning iteration 1024/10000 [0m                     

                       Computation: 44906 steps/s (collection: 2.082s, learning 0.107s)
             Mean action noise std: 3.17
          Mean value_function loss: 475.1346
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 79.3994
                       Mean reward: 339.00
               Mean episode length: 129.61
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 62.9544
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.19s
                      Time elapsed: 00:38:03
                               ETA: 05:33:16

################################################################################
                    [1m Learning iteration 1025/10000 [0m                     

                       Computation: 45586 steps/s (collection: 2.062s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 482.3557
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 79.3995
                       Mean reward: 335.69
               Mean episode length: 130.17
    Episode_Reward/reaching_object: 0.8354
     Episode_Reward/lifting_object: 62.6019
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.16s
                      Time elapsed: 00:38:05
                               ETA: 05:33:14

################################################################################
                    [1m Learning iteration 1026/10000 [0m                     

                       Computation: 45426 steps/s (collection: 2.060s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 489.9690
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.3997
                       Mean reward: 293.55
               Mean episode length: 120.71
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 63.0245
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.16s
                      Time elapsed: 00:38:07
                               ETA: 05:33:11

################################################################################
                    [1m Learning iteration 1027/10000 [0m                     

                       Computation: 43374 steps/s (collection: 2.173s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 455.3193
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 79.4002
                       Mean reward: 326.82
               Mean episode length: 125.97
    Episode_Reward/reaching_object: 0.8255
     Episode_Reward/lifting_object: 63.3150
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.27s
                      Time elapsed: 00:38:10
                               ETA: 05:33:09

################################################################################
                    [1m Learning iteration 1028/10000 [0m                     

                       Computation: 45112 steps/s (collection: 2.079s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 469.7290
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.4010
                       Mean reward: 287.64
               Mean episode length: 123.17
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 57.3007
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.18s
                      Time elapsed: 00:38:12
                               ETA: 05:33:06

################################################################################
                    [1m Learning iteration 1029/10000 [0m                     

                       Computation: 45367 steps/s (collection: 2.071s, learning 0.096s)
             Mean action noise std: 3.17
          Mean value_function loss: 486.3861
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.4034
                       Mean reward: 264.23
               Mean episode length: 108.12
    Episode_Reward/reaching_object: 0.8082
     Episode_Reward/lifting_object: 60.7352
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.17s
                      Time elapsed: 00:38:14
                               ETA: 05:33:03

################################################################################
                    [1m Learning iteration 1030/10000 [0m                     

                       Computation: 45219 steps/s (collection: 2.049s, learning 0.125s)
             Mean action noise std: 3.17
          Mean value_function loss: 489.6622
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.4036
                       Mean reward: 301.37
               Mean episode length: 123.63
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 58.7598
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.17s
                      Time elapsed: 00:38:16
                               ETA: 05:33:01

################################################################################
                    [1m Learning iteration 1031/10000 [0m                     

                       Computation: 45471 steps/s (collection: 2.065s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 457.9556
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 79.4035
                       Mean reward: 302.02
               Mean episode length: 123.22
    Episode_Reward/reaching_object: 0.8072
     Episode_Reward/lifting_object: 59.0981
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.16s
                      Time elapsed: 00:38:18
                               ETA: 05:32:58

################################################################################
                    [1m Learning iteration 1032/10000 [0m                     

                       Computation: 45230 steps/s (collection: 2.080s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 467.8331
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 79.4039
                       Mean reward: 316.25
               Mean episode length: 127.76
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 62.3188
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.17s
                      Time elapsed: 00:38:20
                               ETA: 05:32:55

################################################################################
                    [1m Learning iteration 1033/10000 [0m                     

                       Computation: 45501 steps/s (collection: 2.049s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 475.8211
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 79.4039
                       Mean reward: 301.23
               Mean episode length: 119.70
    Episode_Reward/reaching_object: 0.8226
     Episode_Reward/lifting_object: 61.2237
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.16s
                      Time elapsed: 00:38:23
                               ETA: 05:32:52

################################################################################
                    [1m Learning iteration 1034/10000 [0m                     

                       Computation: 45126 steps/s (collection: 2.067s, learning 0.112s)
             Mean action noise std: 3.17
          Mean value_function loss: 455.7505
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 79.4040
                       Mean reward: 284.15
               Mean episode length: 117.38
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 59.5094
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.18s
                      Time elapsed: 00:38:25
                               ETA: 05:32:50

################################################################################
                    [1m Learning iteration 1035/10000 [0m                     

                       Computation: 44930 steps/s (collection: 2.084s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 464.6115
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.4044
                       Mean reward: 271.54
               Mean episode length: 118.39
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 57.8559
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.19s
                      Time elapsed: 00:38:27
                               ETA: 05:32:47

################################################################################
                    [1m Learning iteration 1036/10000 [0m                     

                       Computation: 45586 steps/s (collection: 2.064s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 489.3464
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.4044
                       Mean reward: 257.54
               Mean episode length: 116.31
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 55.5634
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.16s
                      Time elapsed: 00:38:29
                               ETA: 05:32:44

################################################################################
                    [1m Learning iteration 1037/10000 [0m                     

                       Computation: 45916 steps/s (collection: 2.053s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 504.4579
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 79.4046
                       Mean reward: 247.91
               Mean episode length: 113.06
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 54.4501
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.14s
                      Time elapsed: 00:38:31
                               ETA: 05:32:41

################################################################################
                    [1m Learning iteration 1038/10000 [0m                     

                       Computation: 46053 steps/s (collection: 2.045s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 506.4079
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.4044
                       Mean reward: 271.28
               Mean episode length: 121.73
    Episode_Reward/reaching_object: 0.7365
     Episode_Reward/lifting_object: 51.1743
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.13s
                      Time elapsed: 00:38:33
                               ETA: 05:32:38

################################################################################
                    [1m Learning iteration 1039/10000 [0m                     

                       Computation: 45239 steps/s (collection: 2.083s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 478.2835
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 79.4037
                       Mean reward: 248.45
               Mean episode length: 109.89
    Episode_Reward/reaching_object: 0.7094
     Episode_Reward/lifting_object: 48.1953
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.17s
                      Time elapsed: 00:38:36
                               ETA: 05:32:36

################################################################################
                    [1m Learning iteration 1040/10000 [0m                     

                       Computation: 45774 steps/s (collection: 2.048s, learning 0.100s)
             Mean action noise std: 3.17
          Mean value_function loss: 497.6396
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.4054
                       Mean reward: 261.41
               Mean episode length: 115.10
    Episode_Reward/reaching_object: 0.7262
     Episode_Reward/lifting_object: 48.8582
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.15s
                      Time elapsed: 00:38:38
                               ETA: 05:32:33

################################################################################
                    [1m Learning iteration 1041/10000 [0m                     

                       Computation: 45938 steps/s (collection: 2.043s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 473.1454
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.4074
                       Mean reward: 253.70
               Mean episode length: 119.67
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 51.8181
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.14s
                      Time elapsed: 00:38:40
                               ETA: 05:32:30

################################################################################
                    [1m Learning iteration 1042/10000 [0m                     

                       Computation: 45059 steps/s (collection: 2.079s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 458.5956
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 79.4079
                       Mean reward: 308.58
               Mean episode length: 122.48
    Episode_Reward/reaching_object: 0.8025
     Episode_Reward/lifting_object: 58.2958
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.18s
                      Time elapsed: 00:38:42
                               ETA: 05:32:27

################################################################################
                    [1m Learning iteration 1043/10000 [0m                     

                       Computation: 45598 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 461.3846
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 79.4079
                       Mean reward: 307.92
               Mean episode length: 124.19
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 66.2928
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.16s
                      Time elapsed: 00:38:44
                               ETA: 05:32:24

################################################################################
                    [1m Learning iteration 1044/10000 [0m                     

                       Computation: 45601 steps/s (collection: 2.067s, learning 0.089s)
             Mean action noise std: 3.17
          Mean value_function loss: 460.8311
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 79.4080
                       Mean reward: 293.14
               Mean episode length: 122.57
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 65.1383
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.16s
                      Time elapsed: 00:38:46
                               ETA: 05:32:22

################################################################################
                    [1m Learning iteration 1045/10000 [0m                     

                       Computation: 45229 steps/s (collection: 2.087s, learning 0.086s)
             Mean action noise std: 3.17
          Mean value_function loss: 453.0607
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 79.4082
                       Mean reward: 352.92
               Mean episode length: 136.18
    Episode_Reward/reaching_object: 0.8795
     Episode_Reward/lifting_object: 67.8593
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.17s
                      Time elapsed: 00:38:49
                               ETA: 05:32:19

################################################################################
                    [1m Learning iteration 1046/10000 [0m                     

                       Computation: 45832 steps/s (collection: 2.056s, learning 0.089s)
             Mean action noise std: 3.17
          Mean value_function loss: 458.3493
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 79.4083
                       Mean reward: 405.66
               Mean episode length: 144.57
    Episode_Reward/reaching_object: 0.9712
     Episode_Reward/lifting_object: 78.4335
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.14s
                      Time elapsed: 00:38:51
                               ETA: 05:32:16

################################################################################
                    [1m Learning iteration 1047/10000 [0m                     

                       Computation: 45652 steps/s (collection: 2.064s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 459.1533
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 79.4085
                       Mean reward: 369.12
               Mean episode length: 137.39
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 81.0403
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.15s
                      Time elapsed: 00:38:53
                               ETA: 05:32:13

################################################################################
                    [1m Learning iteration 1048/10000 [0m                     

                       Computation: 45114 steps/s (collection: 2.091s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 456.2841
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 79.4087
                       Mean reward: 377.07
               Mean episode length: 140.80
    Episode_Reward/reaching_object: 0.9217
     Episode_Reward/lifting_object: 74.0212
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.18s
                      Time elapsed: 00:38:55
                               ETA: 05:32:10

################################################################################
                    [1m Learning iteration 1049/10000 [0m                     

                       Computation: 46193 steps/s (collection: 2.035s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 467.9453
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 79.4090
                       Mean reward: 381.24
               Mean episode length: 138.55
    Episode_Reward/reaching_object: 0.9468
     Episode_Reward/lifting_object: 76.5916
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.13s
                      Time elapsed: 00:38:57
                               ETA: 05:32:07

################################################################################
                    [1m Learning iteration 1050/10000 [0m                     

                       Computation: 44949 steps/s (collection: 2.066s, learning 0.121s)
             Mean action noise std: 3.17
          Mean value_function loss: 474.5854
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.4096
                       Mean reward: 387.94
               Mean episode length: 142.87
    Episode_Reward/reaching_object: 0.9399
     Episode_Reward/lifting_object: 74.4756
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.19s
                      Time elapsed: 00:38:59
                               ETA: 05:32:05

################################################################################
                    [1m Learning iteration 1051/10000 [0m                     

                       Computation: 45155 steps/s (collection: 2.074s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 464.6067
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 79.4132
                       Mean reward: 386.83
               Mean episode length: 140.59
    Episode_Reward/reaching_object: 0.9256
     Episode_Reward/lifting_object: 74.7687
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.18s
                      Time elapsed: 00:39:02
                               ETA: 05:32:02

################################################################################
                    [1m Learning iteration 1052/10000 [0m                     

                       Computation: 44542 steps/s (collection: 2.083s, learning 0.124s)
             Mean action noise std: 3.17
          Mean value_function loss: 479.9525
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.4150
                       Mean reward: 344.43
               Mean episode length: 128.62
    Episode_Reward/reaching_object: 0.8989
     Episode_Reward/lifting_object: 71.3540
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.21s
                      Time elapsed: 00:39:04
                               ETA: 05:32:00

################################################################################
                    [1m Learning iteration 1053/10000 [0m                     

                       Computation: 44568 steps/s (collection: 2.103s, learning 0.103s)
             Mean action noise std: 3.17
          Mean value_function loss: 472.6958
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 79.4157
                       Mean reward: 360.54
               Mean episode length: 136.87
    Episode_Reward/reaching_object: 0.9134
     Episode_Reward/lifting_object: 74.0902
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.21s
                      Time elapsed: 00:39:06
                               ETA: 05:31:57

################################################################################
                    [1m Learning iteration 1054/10000 [0m                     

                       Computation: 45528 steps/s (collection: 2.066s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 459.5899
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 79.4167
                       Mean reward: 364.45
               Mean episode length: 144.01
    Episode_Reward/reaching_object: 0.9085
     Episode_Reward/lifting_object: 73.2338
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.16s
                      Time elapsed: 00:39:08
                               ETA: 05:31:55

################################################################################
                    [1m Learning iteration 1055/10000 [0m                     

                       Computation: 45894 steps/s (collection: 2.053s, learning 0.089s)
             Mean action noise std: 3.17
          Mean value_function loss: 459.9410
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 79.4173
                       Mean reward: 410.48
               Mean episode length: 148.38
    Episode_Reward/reaching_object: 0.9330
     Episode_Reward/lifting_object: 75.8231
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.14s
                      Time elapsed: 00:39:10
                               ETA: 05:31:52

################################################################################
                    [1m Learning iteration 1056/10000 [0m                     

                       Computation: 44973 steps/s (collection: 2.094s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 483.0873
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.4177
                       Mean reward: 357.75
               Mean episode length: 136.06
    Episode_Reward/reaching_object: 0.8959
     Episode_Reward/lifting_object: 72.1310
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.19s
                      Time elapsed: 00:39:12
                               ETA: 05:31:49

################################################################################
                    [1m Learning iteration 1057/10000 [0m                     

                       Computation: 44913 steps/s (collection: 2.084s, learning 0.105s)
             Mean action noise std: 3.17
          Mean value_function loss: 486.0222
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 79.4190
                       Mean reward: 394.45
               Mean episode length: 140.31
    Episode_Reward/reaching_object: 0.9467
     Episode_Reward/lifting_object: 78.7076
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.19s
                      Time elapsed: 00:39:15
                               ETA: 05:31:47

################################################################################
                    [1m Learning iteration 1058/10000 [0m                     

                       Computation: 45497 steps/s (collection: 2.066s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 485.4538
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 79.4205
                       Mean reward: 339.06
               Mean episode length: 125.97
    Episode_Reward/reaching_object: 0.9043
     Episode_Reward/lifting_object: 74.7977
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.16s
                      Time elapsed: 00:39:17
                               ETA: 05:31:44

################################################################################
                    [1m Learning iteration 1059/10000 [0m                     

                       Computation: 43831 steps/s (collection: 2.130s, learning 0.113s)
             Mean action noise std: 3.17
          Mean value_function loss: 467.4323
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 79.4213
                       Mean reward: 393.42
               Mean episode length: 138.58
    Episode_Reward/reaching_object: 0.8860
     Episode_Reward/lifting_object: 73.3038
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.24s
                      Time elapsed: 00:39:19
                               ETA: 05:31:42

################################################################################
                    [1m Learning iteration 1060/10000 [0m                     

                       Computation: 45079 steps/s (collection: 2.067s, learning 0.114s)
             Mean action noise std: 3.17
          Mean value_function loss: 458.9347
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.4214
                       Mean reward: 358.90
               Mean episode length: 134.61
    Episode_Reward/reaching_object: 0.9410
     Episode_Reward/lifting_object: 77.8851
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.18s
                      Time elapsed: 00:39:21
                               ETA: 05:31:39

################################################################################
                    [1m Learning iteration 1061/10000 [0m                     

                       Computation: 44678 steps/s (collection: 2.092s, learning 0.109s)
             Mean action noise std: 3.17
          Mean value_function loss: 455.3949
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 79.4215
                       Mean reward: 425.50
               Mean episode length: 147.20
    Episode_Reward/reaching_object: 0.9459
     Episode_Reward/lifting_object: 79.5892
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.20s
                      Time elapsed: 00:39:23
                               ETA: 05:31:37

################################################################################
                    [1m Learning iteration 1062/10000 [0m                     

                       Computation: 44351 steps/s (collection: 2.109s, learning 0.107s)
             Mean action noise std: 3.17
          Mean value_function loss: 469.2213
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.4219
                       Mean reward: 424.39
               Mean episode length: 154.17
    Episode_Reward/reaching_object: 0.9672
     Episode_Reward/lifting_object: 81.3111
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.22s
                      Time elapsed: 00:39:26
                               ETA: 05:31:34

################################################################################
                    [1m Learning iteration 1063/10000 [0m                     

                       Computation: 43963 steps/s (collection: 2.123s, learning 0.114s)
             Mean action noise std: 3.17
          Mean value_function loss: 479.6138
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.4242
                       Mean reward: 419.71
               Mean episode length: 143.20
    Episode_Reward/reaching_object: 0.9684
     Episode_Reward/lifting_object: 82.8462
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.24s
                      Time elapsed: 00:39:28
                               ETA: 05:31:32

################################################################################
                    [1m Learning iteration 1064/10000 [0m                     

                       Computation: 44206 steps/s (collection: 2.118s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 478.5107
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.4251
                       Mean reward: 399.66
               Mean episode length: 144.71
    Episode_Reward/reaching_object: 0.9547
     Episode_Reward/lifting_object: 81.0680
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.22s
                      Time elapsed: 00:39:30
                               ETA: 05:31:30

################################################################################
                    [1m Learning iteration 1065/10000 [0m                     

                       Computation: 44767 steps/s (collection: 2.093s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 473.1890
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 79.4270
                       Mean reward: 384.89
               Mean episode length: 142.85
    Episode_Reward/reaching_object: 0.9434
     Episode_Reward/lifting_object: 79.3393
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.20s
                      Time elapsed: 00:39:32
                               ETA: 05:31:27

################################################################################
                    [1m Learning iteration 1066/10000 [0m                     

                       Computation: 44798 steps/s (collection: 2.084s, learning 0.110s)
             Mean action noise std: 3.18
          Mean value_function loss: 452.4225
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.4296
                       Mean reward: 391.12
               Mean episode length: 137.15
    Episode_Reward/reaching_object: 0.9751
     Episode_Reward/lifting_object: 83.3136
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.19s
                      Time elapsed: 00:39:34
                               ETA: 05:31:25

################################################################################
                    [1m Learning iteration 1067/10000 [0m                     

                       Computation: 45072 steps/s (collection: 2.071s, learning 0.110s)
             Mean action noise std: 3.18
          Mean value_function loss: 454.9282
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 79.4303
                       Mean reward: 440.78
               Mean episode length: 154.53
    Episode_Reward/reaching_object: 0.9640
     Episode_Reward/lifting_object: 81.3993
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.18s
                      Time elapsed: 00:39:37
                               ETA: 05:31:22

################################################################################
                    [1m Learning iteration 1068/10000 [0m                     

                       Computation: 45123 steps/s (collection: 2.065s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 455.3006
               Mean surrogate loss: 0.0159
                 Mean entropy loss: 79.4302
                       Mean reward: 455.19
               Mean episode length: 153.84
    Episode_Reward/reaching_object: 0.9526
     Episode_Reward/lifting_object: 80.0104
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.18s
                      Time elapsed: 00:39:39
                               ETA: 05:31:20

################################################################################
                    [1m Learning iteration 1069/10000 [0m                     

                       Computation: 45957 steps/s (collection: 2.048s, learning 0.091s)
             Mean action noise std: 3.18
          Mean value_function loss: 501.1075
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 79.4302
                       Mean reward: 391.18
               Mean episode length: 145.52
    Episode_Reward/reaching_object: 0.9508
     Episode_Reward/lifting_object: 80.6119
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.14s
                      Time elapsed: 00:39:41
                               ETA: 05:31:17

################################################################################
                    [1m Learning iteration 1070/10000 [0m                     

                       Computation: 45601 steps/s (collection: 2.067s, learning 0.089s)
             Mean action noise std: 3.18
          Mean value_function loss: 463.8929
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 79.4302
                       Mean reward: 402.16
               Mean episode length: 145.37
    Episode_Reward/reaching_object: 0.9484
     Episode_Reward/lifting_object: 80.2285
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.16s
                      Time elapsed: 00:39:43
                               ETA: 05:31:14

################################################################################
                    [1m Learning iteration 1071/10000 [0m                     

                       Computation: 45993 steps/s (collection: 2.050s, learning 0.088s)
             Mean action noise std: 3.18
          Mean value_function loss: 469.5580
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 79.4298
                       Mean reward: 410.01
               Mean episode length: 146.39
    Episode_Reward/reaching_object: 0.9805
     Episode_Reward/lifting_object: 83.7246
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.14s
                      Time elapsed: 00:39:45
                               ETA: 05:31:11

################################################################################
                    [1m Learning iteration 1072/10000 [0m                     

                       Computation: 45520 steps/s (collection: 2.062s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 502.4786
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 79.4294
                       Mean reward: 379.09
               Mean episode length: 142.00
    Episode_Reward/reaching_object: 0.9443
     Episode_Reward/lifting_object: 81.0217
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.16s
                      Time elapsed: 00:39:47
                               ETA: 05:31:08

################################################################################
                    [1m Learning iteration 1073/10000 [0m                     

                       Computation: 45425 steps/s (collection: 2.050s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 509.7756
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 79.4300
                       Mean reward: 382.69
               Mean episode length: 137.59
    Episode_Reward/reaching_object: 0.9427
     Episode_Reward/lifting_object: 79.5334
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.16s
                      Time elapsed: 00:39:50
                               ETA: 05:31:05

################################################################################
                    [1m Learning iteration 1074/10000 [0m                     

                       Computation: 45900 steps/s (collection: 2.028s, learning 0.113s)
             Mean action noise std: 3.18
          Mean value_function loss: 522.4346
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.4302
                       Mean reward: 452.88
               Mean episode length: 154.08
    Episode_Reward/reaching_object: 0.9959
     Episode_Reward/lifting_object: 85.2297
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.14s
                      Time elapsed: 00:39:52
                               ETA: 05:31:03

################################################################################
                    [1m Learning iteration 1075/10000 [0m                     

                       Computation: 45695 steps/s (collection: 2.058s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 543.2008
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.4279
                       Mean reward: 369.19
               Mean episode length: 132.65
    Episode_Reward/reaching_object: 0.9054
     Episode_Reward/lifting_object: 74.2091
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.15s
                      Time elapsed: 00:39:54
                               ETA: 05:31:00

################################################################################
                    [1m Learning iteration 1076/10000 [0m                     

                       Computation: 45659 steps/s (collection: 2.050s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 493.5301
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 79.4284
                       Mean reward: 397.78
               Mean episode length: 147.44
    Episode_Reward/reaching_object: 0.9629
     Episode_Reward/lifting_object: 81.0904
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.15s
                      Time elapsed: 00:39:56
                               ETA: 05:30:57

################################################################################
                    [1m Learning iteration 1077/10000 [0m                     

                       Computation: 45981 steps/s (collection: 2.049s, learning 0.089s)
             Mean action noise std: 3.18
          Mean value_function loss: 488.6616
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 79.4297
                       Mean reward: 356.93
               Mean episode length: 132.26
    Episode_Reward/reaching_object: 0.9186
     Episode_Reward/lifting_object: 77.1806
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.14s
                      Time elapsed: 00:39:58
                               ETA: 05:30:54

################################################################################
                    [1m Learning iteration 1078/10000 [0m                     

                       Computation: 46020 steps/s (collection: 2.045s, learning 0.091s)
             Mean action noise std: 3.18
          Mean value_function loss: 499.9534
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.4300
                       Mean reward: 396.91
               Mean episode length: 144.05
    Episode_Reward/reaching_object: 0.8947
     Episode_Reward/lifting_object: 75.1407
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.14s
                      Time elapsed: 00:40:00
                               ETA: 05:30:51

################################################################################
                    [1m Learning iteration 1079/10000 [0m                     

                       Computation: 45135 steps/s (collection: 2.070s, learning 0.108s)
             Mean action noise std: 3.18
          Mean value_function loss: 487.3640
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.4323
                       Mean reward: 399.13
               Mean episode length: 140.96
    Episode_Reward/reaching_object: 0.9152
     Episode_Reward/lifting_object: 77.4140
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.18s
                      Time elapsed: 00:40:02
                               ETA: 05:30:48

################################################################################
                    [1m Learning iteration 1080/10000 [0m                     

                       Computation: 45612 steps/s (collection: 2.048s, learning 0.108s)
             Mean action noise std: 3.18
          Mean value_function loss: 493.8937
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 79.4342
                       Mean reward: 445.33
               Mean episode length: 151.06
    Episode_Reward/reaching_object: 0.9721
     Episode_Reward/lifting_object: 83.1486
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.16s
                      Time elapsed: 00:40:05
                               ETA: 05:30:46

################################################################################
                    [1m Learning iteration 1081/10000 [0m                     

                       Computation: 45131 steps/s (collection: 2.070s, learning 0.108s)
             Mean action noise std: 3.18
          Mean value_function loss: 476.0186
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.4347
                       Mean reward: 460.74
               Mean episode length: 156.90
    Episode_Reward/reaching_object: 1.0014
     Episode_Reward/lifting_object: 85.5639
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.18s
                      Time elapsed: 00:40:07
                               ETA: 05:30:43

################################################################################
                    [1m Learning iteration 1082/10000 [0m                     

                       Computation: 46024 steps/s (collection: 2.043s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 450.0645
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 79.4349
                       Mean reward: 445.74
               Mean episode length: 154.30
    Episode_Reward/reaching_object: 0.9875
     Episode_Reward/lifting_object: 85.6211
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.14s
                      Time elapsed: 00:40:09
                               ETA: 05:30:40

################################################################################
                    [1m Learning iteration 1083/10000 [0m                     

                       Computation: 46256 steps/s (collection: 2.024s, learning 0.101s)
             Mean action noise std: 3.18
          Mean value_function loss: 464.4520
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 79.4351
                       Mean reward: 424.37
               Mean episode length: 148.29
    Episode_Reward/reaching_object: 1.0174
     Episode_Reward/lifting_object: 90.2618
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.13s
                      Time elapsed: 00:40:11
                               ETA: 05:30:37

################################################################################
                    [1m Learning iteration 1084/10000 [0m                     

                       Computation: 45349 steps/s (collection: 2.068s, learning 0.100s)
             Mean action noise std: 3.18
          Mean value_function loss: 464.4345
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.4357
                       Mean reward: 395.28
               Mean episode length: 140.39
    Episode_Reward/reaching_object: 0.9721
     Episode_Reward/lifting_object: 84.0631
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.17s
                      Time elapsed: 00:40:13
                               ETA: 05:30:34

################################################################################
                    [1m Learning iteration 1085/10000 [0m                     

                       Computation: 44701 steps/s (collection: 2.100s, learning 0.100s)
             Mean action noise std: 3.18
          Mean value_function loss: 471.2788
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.4370
                       Mean reward: 390.32
               Mean episode length: 140.08
    Episode_Reward/reaching_object: 1.0311
     Episode_Reward/lifting_object: 90.6054
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.20s
                      Time elapsed: 00:40:15
                               ETA: 05:30:32

################################################################################
                    [1m Learning iteration 1086/10000 [0m                     

                       Computation: 44534 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 472.0603
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.4379
                       Mean reward: 520.73
               Mean episode length: 168.78
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 93.5024
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.21s
                      Time elapsed: 00:40:18
                               ETA: 05:30:29

################################################################################
                    [1m Learning iteration 1087/10000 [0m                     

                       Computation: 43933 steps/s (collection: 2.131s, learning 0.107s)
             Mean action noise std: 3.18
          Mean value_function loss: 484.5034
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.4406
                       Mean reward: 440.19
               Mean episode length: 149.08
    Episode_Reward/reaching_object: 1.0191
     Episode_Reward/lifting_object: 89.0143
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.24s
                      Time elapsed: 00:40:20
                               ETA: 05:30:27

################################################################################
                    [1m Learning iteration 1088/10000 [0m                     

                       Computation: 44894 steps/s (collection: 2.097s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 467.0460
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.4437
                       Mean reward: 412.84
               Mean episode length: 145.41
    Episode_Reward/reaching_object: 0.9896
     Episode_Reward/lifting_object: 86.0561
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.19s
                      Time elapsed: 00:40:22
                               ETA: 05:30:25

################################################################################
                    [1m Learning iteration 1089/10000 [0m                     

                       Computation: 45792 steps/s (collection: 2.057s, learning 0.090s)
             Mean action noise std: 3.18
          Mean value_function loss: 505.2195
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.4467
                       Mean reward: 376.64
               Mean episode length: 139.00
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 92.9236
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.15s
                      Time elapsed: 00:40:24
                               ETA: 05:30:22

################################################################################
                    [1m Learning iteration 1090/10000 [0m                     

                       Computation: 45442 steps/s (collection: 2.069s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 499.9804
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 79.4493
                       Mean reward: 475.52
               Mean episode length: 158.38
    Episode_Reward/reaching_object: 1.0069
     Episode_Reward/lifting_object: 86.6013
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.16s
                      Time elapsed: 00:40:26
                               ETA: 05:30:19

################################################################################
                    [1m Learning iteration 1091/10000 [0m                     

                       Computation: 46120 steps/s (collection: 2.038s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 481.0134
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 79.4501
                       Mean reward: 453.18
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 0.9885
     Episode_Reward/lifting_object: 84.8299
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.13s
                      Time elapsed: 00:40:28
                               ETA: 05:30:16

################################################################################
                    [1m Learning iteration 1092/10000 [0m                     

                       Computation: 45782 steps/s (collection: 2.057s, learning 0.091s)
             Mean action noise std: 3.18
          Mean value_function loss: 495.9399
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.4501
                       Mean reward: 394.64
               Mean episode length: 142.61
    Episode_Reward/reaching_object: 1.0438
     Episode_Reward/lifting_object: 92.8622
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.15s
                      Time elapsed: 00:40:31
                               ETA: 05:30:13

################################################################################
                    [1m Learning iteration 1093/10000 [0m                     

                       Computation: 45514 steps/s (collection: 2.068s, learning 0.092s)
             Mean action noise std: 3.18
          Mean value_function loss: 532.5926
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.4504
                       Mean reward: 490.85
               Mean episode length: 160.42
    Episode_Reward/reaching_object: 1.0319
     Episode_Reward/lifting_object: 90.5640
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.16s
                      Time elapsed: 00:40:33
                               ETA: 05:30:11

################################################################################
                    [1m Learning iteration 1094/10000 [0m                     

                       Computation: 46121 steps/s (collection: 2.028s, learning 0.104s)
             Mean action noise std: 3.18
          Mean value_function loss: 504.7981
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.4522
                       Mean reward: 440.47
               Mean episode length: 148.64
    Episode_Reward/reaching_object: 1.0250
     Episode_Reward/lifting_object: 89.9529
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.13s
                      Time elapsed: 00:40:35
                               ETA: 05:30:08

################################################################################
                    [1m Learning iteration 1095/10000 [0m                     

                       Computation: 45364 steps/s (collection: 2.055s, learning 0.112s)
             Mean action noise std: 3.18
          Mean value_function loss: 512.1762
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.4544
                       Mean reward: 365.73
               Mean episode length: 134.78
    Episode_Reward/reaching_object: 0.9448
     Episode_Reward/lifting_object: 81.6001
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.17s
                      Time elapsed: 00:40:37
                               ETA: 05:30:05

################################################################################
                    [1m Learning iteration 1096/10000 [0m                     

                       Computation: 45425 steps/s (collection: 2.071s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 476.7322
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 79.4577
                       Mean reward: 525.97
               Mean episode length: 162.11
    Episode_Reward/reaching_object: 1.0021
     Episode_Reward/lifting_object: 88.4182
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.16s
                      Time elapsed: 00:40:39
                               ETA: 05:30:02

################################################################################
                    [1m Learning iteration 1097/10000 [0m                     

                       Computation: 45913 steps/s (collection: 2.043s, learning 0.099s)
             Mean action noise std: 3.18
          Mean value_function loss: 472.5784
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.4589
                       Mean reward: 414.42
               Mean episode length: 141.70
    Episode_Reward/reaching_object: 0.9496
     Episode_Reward/lifting_object: 81.7780
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.14s
                      Time elapsed: 00:40:41
                               ETA: 05:29:59

################################################################################
                    [1m Learning iteration 1098/10000 [0m                     

                       Computation: 46288 steps/s (collection: 2.030s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 462.7495
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 79.4595
                       Mean reward: 393.93
               Mean episode length: 140.85
    Episode_Reward/reaching_object: 1.0097
     Episode_Reward/lifting_object: 89.0597
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.12s
                      Time elapsed: 00:40:44
                               ETA: 05:29:56

################################################################################
                    [1m Learning iteration 1099/10000 [0m                     

                       Computation: 46076 steps/s (collection: 2.047s, learning 0.086s)
             Mean action noise std: 3.18
          Mean value_function loss: 460.8695
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 79.4602
                       Mean reward: 462.76
               Mean episode length: 155.80
    Episode_Reward/reaching_object: 1.0298
     Episode_Reward/lifting_object: 91.3532
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.13s
                      Time elapsed: 00:40:46
                               ETA: 05:29:53

################################################################################
                    [1m Learning iteration 1100/10000 [0m                     

                       Computation: 44880 steps/s (collection: 2.096s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 485.5651
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.4608
                       Mean reward: 480.35
               Mean episode length: 159.17
    Episode_Reward/reaching_object: 1.0804
     Episode_Reward/lifting_object: 97.6964
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.19s
                      Time elapsed: 00:40:48
                               ETA: 05:29:51

################################################################################
                    [1m Learning iteration 1101/10000 [0m                     

                       Computation: 45406 steps/s (collection: 2.060s, learning 0.105s)
             Mean action noise std: 3.18
          Mean value_function loss: 461.1091
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.4639
                       Mean reward: 486.73
               Mean episode length: 162.29
    Episode_Reward/reaching_object: 1.0649
     Episode_Reward/lifting_object: 94.6787
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.16s
                      Time elapsed: 00:40:50
                               ETA: 05:29:48

################################################################################
                    [1m Learning iteration 1102/10000 [0m                     

                       Computation: 45254 steps/s (collection: 2.055s, learning 0.118s)
             Mean action noise std: 3.18
          Mean value_function loss: 502.7511
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.4704
                       Mean reward: 485.26
               Mean episode length: 160.40
    Episode_Reward/reaching_object: 1.0603
     Episode_Reward/lifting_object: 94.2544
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.17s
                      Time elapsed: 00:40:52
                               ETA: 05:29:46

################################################################################
                    [1m Learning iteration 1103/10000 [0m                     

                       Computation: 45970 steps/s (collection: 2.047s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 479.7186
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.4735
                       Mean reward: 493.24
               Mean episode length: 161.78
    Episode_Reward/reaching_object: 1.0699
     Episode_Reward/lifting_object: 96.2838
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.14s
                      Time elapsed: 00:40:54
                               ETA: 05:29:43

################################################################################
                    [1m Learning iteration 1104/10000 [0m                     

                       Computation: 45605 steps/s (collection: 2.065s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 513.5786
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 79.4749
                       Mean reward: 393.67
               Mean episode length: 139.49
    Episode_Reward/reaching_object: 1.0453
     Episode_Reward/lifting_object: 93.2518
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.16s
                      Time elapsed: 00:40:56
                               ETA: 05:29:40

################################################################################
                    [1m Learning iteration 1105/10000 [0m                     

                       Computation: 45685 steps/s (collection: 2.061s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 489.6576
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.4770
                       Mean reward: 422.68
               Mean episode length: 142.27
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 95.2784
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.15s
                      Time elapsed: 00:40:59
                               ETA: 05:29:37

################################################################################
                    [1m Learning iteration 1106/10000 [0m                     

                       Computation: 45479 steps/s (collection: 2.058s, learning 0.104s)
             Mean action noise std: 3.19
          Mean value_function loss: 495.6149
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.4794
                       Mean reward: 471.13
               Mean episode length: 157.12
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 93.1523
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.16s
                      Time elapsed: 00:41:01
                               ETA: 05:29:34

################################################################################
                    [1m Learning iteration 1107/10000 [0m                     

                       Computation: 45839 steps/s (collection: 2.055s, learning 0.089s)
             Mean action noise std: 3.19
          Mean value_function loss: 543.1729
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.4841
                       Mean reward: 485.26
               Mean episode length: 157.68
    Episode_Reward/reaching_object: 1.0247
     Episode_Reward/lifting_object: 91.4104
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.14s
                      Time elapsed: 00:41:03
                               ETA: 05:29:32

################################################################################
                    [1m Learning iteration 1108/10000 [0m                     

                       Computation: 45011 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 3.19
          Mean value_function loss: 499.3904
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.4904
                       Mean reward: 460.79
               Mean episode length: 152.69
    Episode_Reward/reaching_object: 0.9758
     Episode_Reward/lifting_object: 85.3074
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.18s
                      Time elapsed: 00:41:05
                               ETA: 05:29:29

################################################################################
                    [1m Learning iteration 1109/10000 [0m                     

                       Computation: 44794 steps/s (collection: 2.078s, learning 0.116s)
             Mean action noise std: 3.19
          Mean value_function loss: 501.7163
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.4946
                       Mean reward: 493.99
               Mean episode length: 157.97
    Episode_Reward/reaching_object: 1.0392
     Episode_Reward/lifting_object: 92.8274
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.19s
                      Time elapsed: 00:41:07
                               ETA: 05:29:27

################################################################################
                    [1m Learning iteration 1110/10000 [0m                     

                       Computation: 45384 steps/s (collection: 2.063s, learning 0.103s)
             Mean action noise std: 3.19
          Mean value_function loss: 525.0749
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.4971
                       Mean reward: 460.06
               Mean episode length: 147.78
    Episode_Reward/reaching_object: 1.0088
     Episode_Reward/lifting_object: 89.5428
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.17s
                      Time elapsed: 00:41:09
                               ETA: 05:29:24

################################################################################
                    [1m Learning iteration 1111/10000 [0m                     

                       Computation: 45852 steps/s (collection: 2.051s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 495.0586
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.5012
                       Mean reward: 488.31
               Mean episode length: 158.29
    Episode_Reward/reaching_object: 1.0099
     Episode_Reward/lifting_object: 89.6913
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.14s
                      Time elapsed: 00:41:12
                               ETA: 05:29:21

################################################################################
                    [1m Learning iteration 1112/10000 [0m                     

                       Computation: 45650 steps/s (collection: 2.056s, learning 0.098s)
             Mean action noise std: 3.19
          Mean value_function loss: 514.7982
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 79.5040
                       Mean reward: 492.59
               Mean episode length: 157.62
    Episode_Reward/reaching_object: 1.0223
     Episode_Reward/lifting_object: 90.7018
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.15s
                      Time elapsed: 00:41:14
                               ETA: 05:29:18

################################################################################
                    [1m Learning iteration 1113/10000 [0m                     

                       Computation: 45846 steps/s (collection: 2.057s, learning 0.087s)
             Mean action noise std: 3.19
          Mean value_function loss: 494.9753
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 79.5045
                       Mean reward: 434.47
               Mean episode length: 146.40
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 96.3330
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.14s
                      Time elapsed: 00:41:16
                               ETA: 05:29:15

################################################################################
                    [1m Learning iteration 1114/10000 [0m                     

                       Computation: 45540 steps/s (collection: 2.063s, learning 0.095s)
             Mean action noise std: 3.19
          Mean value_function loss: 507.5886
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.5054
                       Mean reward: 511.48
               Mean episode length: 165.72
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 95.2632
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.16s
                      Time elapsed: 00:41:18
                               ETA: 05:29:13

################################################################################
                    [1m Learning iteration 1115/10000 [0m                     

                       Computation: 45962 steps/s (collection: 2.049s, learning 0.090s)
             Mean action noise std: 3.19
          Mean value_function loss: 516.7656
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.5065
                       Mean reward: 434.49
               Mean episode length: 144.37
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 95.2090
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.14s
                      Time elapsed: 00:41:20
                               ETA: 05:29:10

################################################################################
                    [1m Learning iteration 1116/10000 [0m                     

                       Computation: 45977 steps/s (collection: 2.047s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 500.5549
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.5094
                       Mean reward: 499.36
               Mean episode length: 159.02
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 95.1885
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.14s
                      Time elapsed: 00:41:22
                               ETA: 05:29:07

################################################################################
                    [1m Learning iteration 1117/10000 [0m                     

                       Computation: 45960 steps/s (collection: 2.045s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 506.1211
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 79.5138
                       Mean reward: 474.88
               Mean episode length: 153.66
    Episode_Reward/reaching_object: 1.0158
     Episode_Reward/lifting_object: 91.5851
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.14s
                      Time elapsed: 00:41:25
                               ETA: 05:29:04

################################################################################
                    [1m Learning iteration 1118/10000 [0m                     

                       Computation: 45987 steps/s (collection: 2.045s, learning 0.092s)
             Mean action noise std: 3.19
          Mean value_function loss: 519.5535
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.5150
                       Mean reward: 446.82
               Mean episode length: 150.15
    Episode_Reward/reaching_object: 0.9976
     Episode_Reward/lifting_object: 89.1203
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.14s
                      Time elapsed: 00:41:27
                               ETA: 05:29:01

################################################################################
                    [1m Learning iteration 1119/10000 [0m                     

                       Computation: 45828 steps/s (collection: 2.046s, learning 0.099s)
             Mean action noise std: 3.19
          Mean value_function loss: 515.9666
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 79.5122
                       Mean reward: 428.77
               Mean episode length: 148.03
    Episode_Reward/reaching_object: 1.0383
     Episode_Reward/lifting_object: 93.4614
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.15s
                      Time elapsed: 00:41:29
                               ETA: 05:28:58

################################################################################
                    [1m Learning iteration 1120/10000 [0m                     

                       Computation: 46253 steps/s (collection: 2.035s, learning 0.090s)
             Mean action noise std: 3.19
          Mean value_function loss: 512.1437
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 79.5129
                       Mean reward: 491.63
               Mean episode length: 159.78
    Episode_Reward/reaching_object: 1.0993
     Episode_Reward/lifting_object: 100.9920
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.13s
                      Time elapsed: 00:41:31
                               ETA: 05:28:55

################################################################################
                    [1m Learning iteration 1121/10000 [0m                     

                       Computation: 45330 steps/s (collection: 2.068s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 495.4025
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 79.5130
                       Mean reward: 444.49
               Mean episode length: 145.08
    Episode_Reward/reaching_object: 0.9995
     Episode_Reward/lifting_object: 90.1905
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.17s
                      Time elapsed: 00:41:33
                               ETA: 05:28:53

################################################################################
                    [1m Learning iteration 1122/10000 [0m                     

                       Computation: 45918 steps/s (collection: 2.046s, learning 0.095s)
             Mean action noise std: 3.19
          Mean value_function loss: 510.9809
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 79.5131
                       Mean reward: 405.94
               Mean episode length: 142.87
    Episode_Reward/reaching_object: 0.9969
     Episode_Reward/lifting_object: 89.8533
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.14s
                      Time elapsed: 00:41:35
                               ETA: 05:28:50

################################################################################
                    [1m Learning iteration 1123/10000 [0m                     

                       Computation: 44789 steps/s (collection: 2.101s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 488.7634
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.5139
                       Mean reward: 449.10
               Mean episode length: 156.35
    Episode_Reward/reaching_object: 1.0197
     Episode_Reward/lifting_object: 92.4081
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.19s
                      Time elapsed: 00:41:37
                               ETA: 05:28:47

################################################################################
                    [1m Learning iteration 1124/10000 [0m                     

                       Computation: 46061 steps/s (collection: 2.040s, learning 0.095s)
             Mean action noise std: 3.19
          Mean value_function loss: 487.0207
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.5171
                       Mean reward: 446.83
               Mean episode length: 145.41
    Episode_Reward/reaching_object: 1.1151
     Episode_Reward/lifting_object: 102.9713
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.13s
                      Time elapsed: 00:41:40
                               ETA: 05:28:44

################################################################################
                    [1m Learning iteration 1125/10000 [0m                     

                       Computation: 45433 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 501.1211
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.5200
                       Mean reward: 477.11
               Mean episode length: 155.25
    Episode_Reward/reaching_object: 1.0874
     Episode_Reward/lifting_object: 99.5613
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.16s
                      Time elapsed: 00:41:42
                               ETA: 05:28:42

################################################################################
                    [1m Learning iteration 1126/10000 [0m                     

                       Computation: 45378 steps/s (collection: 2.054s, learning 0.112s)
             Mean action noise std: 3.19
          Mean value_function loss: 497.7421
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 79.5223
                       Mean reward: 448.84
               Mean episode length: 151.94
    Episode_Reward/reaching_object: 1.0186
     Episode_Reward/lifting_object: 91.7324
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.17s
                      Time elapsed: 00:41:44
                               ETA: 05:28:39

################################################################################
                    [1m Learning iteration 1127/10000 [0m                     

                       Computation: 45236 steps/s (collection: 2.079s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 493.4486
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.5239
                       Mean reward: 496.12
               Mean episode length: 158.69
    Episode_Reward/reaching_object: 1.0853
     Episode_Reward/lifting_object: 99.7634
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.17s
                      Time elapsed: 00:41:46
                               ETA: 05:28:36

################################################################################
                    [1m Learning iteration 1128/10000 [0m                     

                       Computation: 45589 steps/s (collection: 2.065s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 482.1779
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 79.5260
                       Mean reward: 532.27
               Mean episode length: 169.77
    Episode_Reward/reaching_object: 1.1201
     Episode_Reward/lifting_object: 103.7483
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.16s
                      Time elapsed: 00:41:48
                               ETA: 05:28:34

################################################################################
                    [1m Learning iteration 1129/10000 [0m                     

                       Computation: 45687 steps/s (collection: 2.055s, learning 0.097s)
             Mean action noise std: 3.20
          Mean value_function loss: 501.4538
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 79.5266
                       Mean reward: 520.89
               Mean episode length: 164.65
    Episode_Reward/reaching_object: 1.0863
     Episode_Reward/lifting_object: 98.8696
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.15s
                      Time elapsed: 00:41:50
                               ETA: 05:28:31

################################################################################
                    [1m Learning iteration 1130/10000 [0m                     

                       Computation: 44823 steps/s (collection: 2.087s, learning 0.106s)
             Mean action noise std: 3.20
          Mean value_function loss: 533.3224
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 79.5267
                       Mean reward: 532.09
               Mean episode length: 166.50
    Episode_Reward/reaching_object: 1.1077
     Episode_Reward/lifting_object: 102.0975
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.19s
                      Time elapsed: 00:41:53
                               ETA: 05:28:28

################################################################################
                    [1m Learning iteration 1131/10000 [0m                     

                       Computation: 44740 steps/s (collection: 2.076s, learning 0.121s)
             Mean action noise std: 3.20
          Mean value_function loss: 496.9328
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 79.5268
                       Mean reward: 516.54
               Mean episode length: 160.21
    Episode_Reward/reaching_object: 1.0835
     Episode_Reward/lifting_object: 98.9371
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.20s
                      Time elapsed: 00:41:55
                               ETA: 05:28:26

################################################################################
                    [1m Learning iteration 1132/10000 [0m                     

                       Computation: 45641 steps/s (collection: 2.043s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 480.7476
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.5271
                       Mean reward: 471.28
               Mean episode length: 155.72
    Episode_Reward/reaching_object: 1.1270
     Episode_Reward/lifting_object: 104.1537
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.15s
                      Time elapsed: 00:41:57
                               ETA: 05:28:23

################################################################################
                    [1m Learning iteration 1133/10000 [0m                     

                       Computation: 45388 steps/s (collection: 2.073s, learning 0.093s)
             Mean action noise std: 3.20
          Mean value_function loss: 492.4385
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 79.5277
                       Mean reward: 490.97
               Mean episode length: 161.08
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 95.7143
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.17s
                      Time elapsed: 00:41:59
                               ETA: 05:28:21

################################################################################
                    [1m Learning iteration 1134/10000 [0m                     

                       Computation: 45509 steps/s (collection: 2.066s, learning 0.094s)
             Mean action noise std: 3.20
          Mean value_function loss: 598.2303
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.5296
                       Mean reward: 479.68
               Mean episode length: 152.01
    Episode_Reward/reaching_object: 1.0734
     Episode_Reward/lifting_object: 98.1651
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.16s
                      Time elapsed: 00:42:01
                               ETA: 05:28:18

################################################################################
                    [1m Learning iteration 1135/10000 [0m                     

                       Computation: 45287 steps/s (collection: 2.079s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 598.5243
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.5341
                       Mean reward: 439.17
               Mean episode length: 146.50
    Episode_Reward/reaching_object: 1.0306
     Episode_Reward/lifting_object: 92.6349
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.17s
                      Time elapsed: 00:42:03
                               ETA: 05:28:15

################################################################################
                    [1m Learning iteration 1136/10000 [0m                     

                       Computation: 44833 steps/s (collection: 2.081s, learning 0.112s)
             Mean action noise std: 3.20
          Mean value_function loss: 591.1744
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.5352
                       Mean reward: 467.30
               Mean episode length: 151.59
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 92.4970
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.19s
                      Time elapsed: 00:42:06
                               ETA: 05:28:13

################################################################################
                    [1m Learning iteration 1137/10000 [0m                     

                       Computation: 44417 steps/s (collection: 2.102s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 529.1120
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 79.5386
                       Mean reward: 484.46
               Mean episode length: 155.63
    Episode_Reward/reaching_object: 1.0649
     Episode_Reward/lifting_object: 96.8847
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.21s
                      Time elapsed: 00:42:08
                               ETA: 05:28:11

################################################################################
                    [1m Learning iteration 1138/10000 [0m                     

                       Computation: 45533 steps/s (collection: 2.057s, learning 0.102s)
             Mean action noise std: 3.20
          Mean value_function loss: 521.3510
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 79.5410
                       Mean reward: 471.63
               Mean episode length: 157.28
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 99.1049
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.16s
                      Time elapsed: 00:42:10
                               ETA: 05:28:08

################################################################################
                    [1m Learning iteration 1139/10000 [0m                     

                       Computation: 45136 steps/s (collection: 2.060s, learning 0.118s)
             Mean action noise std: 3.20
          Mean value_function loss: 490.6834
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.5412
                       Mean reward: 451.54
               Mean episode length: 147.85
    Episode_Reward/reaching_object: 1.0306
     Episode_Reward/lifting_object: 93.0395
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.18s
                      Time elapsed: 00:42:12
                               ETA: 05:28:05

################################################################################
                    [1m Learning iteration 1140/10000 [0m                     

                       Computation: 45205 steps/s (collection: 2.079s, learning 0.096s)
             Mean action noise std: 3.20
          Mean value_function loss: 533.9207
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.5434
                       Mean reward: 535.18
               Mean episode length: 166.02
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 96.1947
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.17s
                      Time elapsed: 00:42:14
                               ETA: 05:28:03

################################################################################
                    [1m Learning iteration 1141/10000 [0m                     

                       Computation: 45462 steps/s (collection: 2.066s, learning 0.097s)
             Mean action noise std: 3.20
          Mean value_function loss: 509.2806
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.5473
                       Mean reward: 477.88
               Mean episode length: 152.08
    Episode_Reward/reaching_object: 1.1033
     Episode_Reward/lifting_object: 101.6866
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.16s
                      Time elapsed: 00:42:16
                               ETA: 05:28:00

################################################################################
                    [1m Learning iteration 1142/10000 [0m                     

                       Computation: 45108 steps/s (collection: 2.079s, learning 0.101s)
             Mean action noise std: 3.20
          Mean value_function loss: 564.2495
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.5495
                       Mean reward: 458.25
               Mean episode length: 153.69
    Episode_Reward/reaching_object: 1.1069
     Episode_Reward/lifting_object: 101.6774
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.18s
                      Time elapsed: 00:42:19
                               ETA: 05:27:57

################################################################################
                    [1m Learning iteration 1143/10000 [0m                     

                       Computation: 45376 steps/s (collection: 2.074s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 501.2994
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.5513
                       Mean reward: 438.15
               Mean episode length: 146.51
    Episode_Reward/reaching_object: 1.0645
     Episode_Reward/lifting_object: 96.0669
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.17s
                      Time elapsed: 00:42:21
                               ETA: 05:27:55

################################################################################
                    [1m Learning iteration 1144/10000 [0m                     

                       Computation: 44922 steps/s (collection: 2.078s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 523.6162
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.5536
                       Mean reward: 549.01
               Mean episode length: 168.56
    Episode_Reward/reaching_object: 1.1482
     Episode_Reward/lifting_object: 106.8413
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.19s
                      Time elapsed: 00:42:23
                               ETA: 05:27:52

################################################################################
                    [1m Learning iteration 1145/10000 [0m                     

                       Computation: 45286 steps/s (collection: 2.076s, learning 0.095s)
             Mean action noise std: 3.20
          Mean value_function loss: 551.8130
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.5588
                       Mean reward: 532.70
               Mean episode length: 164.67
    Episode_Reward/reaching_object: 1.0910
     Episode_Reward/lifting_object: 100.9102
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.17s
                      Time elapsed: 00:42:25
                               ETA: 05:27:50

################################################################################
                    [1m Learning iteration 1146/10000 [0m                     

                       Computation: 44236 steps/s (collection: 2.108s, learning 0.115s)
             Mean action noise std: 3.20
          Mean value_function loss: 491.5488
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.5610
                       Mean reward: 554.84
               Mean episode length: 172.34
    Episode_Reward/reaching_object: 1.1068
     Episode_Reward/lifting_object: 102.7767
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.22s
                      Time elapsed: 00:42:27
                               ETA: 05:27:47

################################################################################
                    [1m Learning iteration 1147/10000 [0m                     

                       Computation: 45243 steps/s (collection: 2.080s, learning 0.093s)
             Mean action noise std: 3.20
          Mean value_function loss: 531.6449
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 79.5637
                       Mean reward: 525.39
               Mean episode length: 165.79
    Episode_Reward/reaching_object: 1.1381
     Episode_Reward/lifting_object: 106.0993
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.17s
                      Time elapsed: 00:42:30
                               ETA: 05:27:45

################################################################################
                    [1m Learning iteration 1148/10000 [0m                     

                       Computation: 45143 steps/s (collection: 2.086s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 514.2275
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.5685
                       Mean reward: 491.80
               Mean episode length: 153.95
    Episode_Reward/reaching_object: 1.1277
     Episode_Reward/lifting_object: 104.2683
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.18s
                      Time elapsed: 00:42:32
                               ETA: 05:27:42

################################################################################
                    [1m Learning iteration 1149/10000 [0m                     

                       Computation: 45268 steps/s (collection: 2.081s, learning 0.091s)
             Mean action noise std: 3.20
          Mean value_function loss: 518.8429
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.5710
                       Mean reward: 613.41
               Mean episode length: 181.35
    Episode_Reward/reaching_object: 1.1067
     Episode_Reward/lifting_object: 102.7179
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.17s
                      Time elapsed: 00:42:34
                               ETA: 05:27:40

################################################################################
                    [1m Learning iteration 1150/10000 [0m                     

                       Computation: 44020 steps/s (collection: 2.115s, learning 0.118s)
             Mean action noise std: 3.20
          Mean value_function loss: 547.0679
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.5744
                       Mean reward: 542.43
               Mean episode length: 164.92
    Episode_Reward/reaching_object: 1.1442
     Episode_Reward/lifting_object: 106.2001
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.23s
                      Time elapsed: 00:42:36
                               ETA: 05:27:38

################################################################################
                    [1m Learning iteration 1151/10000 [0m                     

                       Computation: 44249 steps/s (collection: 2.121s, learning 0.101s)
             Mean action noise std: 3.20
          Mean value_function loss: 555.0584
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 79.5772
                       Mean reward: 447.20
               Mean episode length: 147.10
    Episode_Reward/reaching_object: 1.1007
     Episode_Reward/lifting_object: 101.2041
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.22s
                      Time elapsed: 00:42:38
                               ETA: 05:27:35

################################################################################
                    [1m Learning iteration 1152/10000 [0m                     

                       Computation: 45055 steps/s (collection: 2.082s, learning 0.100s)
             Mean action noise std: 3.21
          Mean value_function loss: 553.0559
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.5810
                       Mean reward: 477.13
               Mean episode length: 151.19
    Episode_Reward/reaching_object: 1.1374
     Episode_Reward/lifting_object: 105.2601
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.18s
                      Time elapsed: 00:42:41
                               ETA: 05:27:33

################################################################################
                    [1m Learning iteration 1153/10000 [0m                     

                       Computation: 44739 steps/s (collection: 2.087s, learning 0.111s)
             Mean action noise std: 3.21
          Mean value_function loss: 597.9470
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.5900
                       Mean reward: 516.98
               Mean episode length: 160.87
    Episode_Reward/reaching_object: 1.0747
     Episode_Reward/lifting_object: 98.3661
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.20s
                      Time elapsed: 00:42:43
                               ETA: 05:27:30

################################################################################
                    [1m Learning iteration 1154/10000 [0m                     

                       Computation: 45299 steps/s (collection: 2.076s, learning 0.094s)
             Mean action noise std: 3.21
          Mean value_function loss: 578.1139
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 79.5953
                       Mean reward: 546.42
               Mean episode length: 169.12
    Episode_Reward/reaching_object: 1.0979
     Episode_Reward/lifting_object: 101.5519
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.17s
                      Time elapsed: 00:42:45
                               ETA: 05:27:28

################################################################################
                    [1m Learning iteration 1155/10000 [0m                     

                       Computation: 45367 steps/s (collection: 2.075s, learning 0.092s)
             Mean action noise std: 3.21
          Mean value_function loss: 543.5345
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.5963
                       Mean reward: 527.75
               Mean episode length: 164.32
    Episode_Reward/reaching_object: 1.0853
     Episode_Reward/lifting_object: 99.2717
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.17s
                      Time elapsed: 00:42:47
                               ETA: 05:27:25

################################################################################
                    [1m Learning iteration 1156/10000 [0m                     

                       Computation: 45236 steps/s (collection: 2.082s, learning 0.092s)
             Mean action noise std: 3.21
          Mean value_function loss: 558.8764
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.5968
                       Mean reward: 471.37
               Mean episode length: 150.15
    Episode_Reward/reaching_object: 1.0887
     Episode_Reward/lifting_object: 99.9892
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.17s
                      Time elapsed: 00:42:49
                               ETA: 05:27:23

################################################################################
                    [1m Learning iteration 1157/10000 [0m                     

                       Computation: 45282 steps/s (collection: 2.074s, learning 0.097s)
             Mean action noise std: 3.21
          Mean value_function loss: 547.8618
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 79.5971
                       Mean reward: 489.23
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 1.0599
     Episode_Reward/lifting_object: 96.2506
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.17s
                      Time elapsed: 00:42:51
                               ETA: 05:27:20

################################################################################
                    [1m Learning iteration 1158/10000 [0m                     

                       Computation: 45695 steps/s (collection: 2.062s, learning 0.089s)
             Mean action noise std: 3.21
          Mean value_function loss: 601.4974
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 79.5984
                       Mean reward: 481.34
               Mean episode length: 155.12
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 98.2932
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.15s
                      Time elapsed: 00:42:54
                               ETA: 05:27:17

################################################################################
                    [1m Learning iteration 1159/10000 [0m                     

                       Computation: 45234 steps/s (collection: 2.073s, learning 0.100s)
             Mean action noise std: 3.21
          Mean value_function loss: 591.7009
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 79.5997
                       Mean reward: 525.74
               Mean episode length: 161.41
    Episode_Reward/reaching_object: 1.0339
     Episode_Reward/lifting_object: 93.6112
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.17s
                      Time elapsed: 00:42:56
                               ETA: 05:27:15

################################################################################
                    [1m Learning iteration 1160/10000 [0m                     

                       Computation: 45128 steps/s (collection: 2.081s, learning 0.097s)
             Mean action noise std: 3.21
          Mean value_function loss: 573.7427
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 79.6015
                       Mean reward: 487.57
               Mean episode length: 158.70
    Episode_Reward/reaching_object: 1.0228
     Episode_Reward/lifting_object: 92.7861
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.18s
                      Time elapsed: 00:42:58
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1161/10000 [0m                     

                       Computation: 42102 steps/s (collection: 2.206s, learning 0.129s)
             Mean action noise std: 3.21
          Mean value_function loss: 589.9593
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 79.6044
                       Mean reward: 454.92
               Mean episode length: 146.14
    Episode_Reward/reaching_object: 1.0724
     Episode_Reward/lifting_object: 98.1800
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.33s
                      Time elapsed: 00:43:00
                               ETA: 05:27:11

################################################################################
                    [1m Learning iteration 1162/10000 [0m                     

                       Computation: 41042 steps/s (collection: 2.303s, learning 0.093s)
             Mean action noise std: 3.21
          Mean value_function loss: 566.2511
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.6091
                       Mean reward: 488.49
               Mean episode length: 155.43
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 95.1205
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.40s
                      Time elapsed: 00:43:03
                               ETA: 05:27:10

################################################################################
                    [1m Learning iteration 1163/10000 [0m                     

                       Computation: 35920 steps/s (collection: 2.437s, learning 0.300s)
             Mean action noise std: 3.21
          Mean value_function loss: 562.1363
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.6170
                       Mean reward: 511.51
               Mean episode length: 158.38
    Episode_Reward/reaching_object: 1.0679
     Episode_Reward/lifting_object: 98.2821
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.74s
                      Time elapsed: 00:43:05
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1164/10000 [0m                     

                       Computation: 33350 steps/s (collection: 2.830s, learning 0.118s)
             Mean action noise std: 3.21
          Mean value_function loss: 619.0924
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.6259
                       Mean reward: 483.43
               Mean episode length: 153.52
    Episode_Reward/reaching_object: 1.1463
     Episode_Reward/lifting_object: 105.4887
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.95s
                      Time elapsed: 00:43:08
                               ETA: 05:27:15

################################################################################
                    [1m Learning iteration 1165/10000 [0m                     

                       Computation: 41762 steps/s (collection: 2.250s, learning 0.104s)
             Mean action noise std: 3.21
          Mean value_function loss: 541.1132
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.6318
                       Mean reward: 483.06
               Mean episode length: 156.86
    Episode_Reward/reaching_object: 1.0948
     Episode_Reward/lifting_object: 100.0516
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.35s
                      Time elapsed: 00:43:11
                               ETA: 05:27:14

################################################################################
                    [1m Learning iteration 1166/10000 [0m                     

                       Computation: 41392 steps/s (collection: 2.238s, learning 0.137s)
             Mean action noise std: 3.21
          Mean value_function loss: 593.2089
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.6360
                       Mean reward: 519.93
               Mean episode length: 161.89
    Episode_Reward/reaching_object: 1.0860
     Episode_Reward/lifting_object: 99.5067
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.37s
                      Time elapsed: 00:43:13
                               ETA: 05:27:13

################################################################################
                    [1m Learning iteration 1167/10000 [0m                     

                       Computation: 39943 steps/s (collection: 2.335s, learning 0.127s)
             Mean action noise std: 3.21
          Mean value_function loss: 590.1397
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 79.6384
                       Mean reward: 515.39
               Mean episode length: 159.49
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 96.5839
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.46s
                      Time elapsed: 00:43:16
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1168/10000 [0m                     

                       Computation: 38369 steps/s (collection: 2.465s, learning 0.097s)
             Mean action noise std: 3.22
          Mean value_function loss: 613.7287
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.6445
                       Mean reward: 503.48
               Mean episode length: 155.59
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 96.8102
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.56s
                      Time elapsed: 00:43:18
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1169/10000 [0m                     

                       Computation: 38872 steps/s (collection: 2.381s, learning 0.148s)
             Mean action noise std: 3.22
          Mean value_function loss: 568.1922
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.6506
                       Mean reward: 506.41
               Mean episode length: 155.38
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 98.0790
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.53s
                      Time elapsed: 00:43:21
                               ETA: 05:27:13

################################################################################
                    [1m Learning iteration 1170/10000 [0m                     

                       Computation: 40369 steps/s (collection: 2.306s, learning 0.129s)
             Mean action noise std: 3.22
          Mean value_function loss: 550.5513
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.6553
                       Mean reward: 532.52
               Mean episode length: 159.94
    Episode_Reward/reaching_object: 1.0957
     Episode_Reward/lifting_object: 101.3569
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.44s
                      Time elapsed: 00:43:23
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1171/10000 [0m                     

                       Computation: 41236 steps/s (collection: 2.247s, learning 0.137s)
             Mean action noise std: 3.22
          Mean value_function loss: 517.2580
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.6586
                       Mean reward: 525.30
               Mean episode length: 162.50
    Episode_Reward/reaching_object: 1.0906
     Episode_Reward/lifting_object: 101.0963
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.38s
                      Time elapsed: 00:43:25
                               ETA: 05:27:11

################################################################################
                    [1m Learning iteration 1172/10000 [0m                     

                       Computation: 39855 steps/s (collection: 2.362s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 494.8527
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 79.6595
                       Mean reward: 574.19
               Mean episode length: 172.57
    Episode_Reward/reaching_object: 1.1321
     Episode_Reward/lifting_object: 106.3136
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.47s
                      Time elapsed: 00:43:28
                               ETA: 05:27:11

################################################################################
                    [1m Learning iteration 1173/10000 [0m                     

                       Computation: 39436 steps/s (collection: 2.387s, learning 0.106s)
             Mean action noise std: 3.22
          Mean value_function loss: 563.9641
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 79.6604
                       Mean reward: 582.54
               Mean episode length: 174.07
    Episode_Reward/reaching_object: 1.1702
     Episode_Reward/lifting_object: 109.7002
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.49s
                      Time elapsed: 00:43:30
                               ETA: 05:27:10

################################################################################
                    [1m Learning iteration 1174/10000 [0m                     

                       Computation: 38651 steps/s (collection: 2.371s, learning 0.172s)
             Mean action noise std: 3.22
          Mean value_function loss: 532.8715
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.6625
                       Mean reward: 589.41
               Mean episode length: 174.64
    Episode_Reward/reaching_object: 1.1914
     Episode_Reward/lifting_object: 112.3474
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.54s
                      Time elapsed: 00:43:33
                               ETA: 05:27:11

################################################################################
                    [1m Learning iteration 1175/10000 [0m                     

                       Computation: 39093 steps/s (collection: 2.314s, learning 0.200s)
             Mean action noise std: 3.22
          Mean value_function loss: 504.1372
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 79.6657
                       Mean reward: 552.23
               Mean episode length: 168.85
    Episode_Reward/reaching_object: 1.2009
     Episode_Reward/lifting_object: 114.0942
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.51s
                      Time elapsed: 00:43:35
                               ETA: 05:27:10

################################################################################
                    [1m Learning iteration 1176/10000 [0m                     

                       Computation: 42379 steps/s (collection: 2.200s, learning 0.120s)
             Mean action noise std: 3.22
          Mean value_function loss: 526.6870
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 79.6682
                       Mean reward: 523.46
               Mean episode length: 165.61
    Episode_Reward/reaching_object: 1.1869
     Episode_Reward/lifting_object: 111.9371
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.32s
                      Time elapsed: 00:43:38
                               ETA: 05:27:09

################################################################################
                    [1m Learning iteration 1177/10000 [0m                     

                       Computation: 42519 steps/s (collection: 2.203s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 576.1624
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.6715
                       Mean reward: 564.66
               Mean episode length: 169.99
    Episode_Reward/reaching_object: 1.1887
     Episode_Reward/lifting_object: 112.0247
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.31s
                      Time elapsed: 00:43:40
                               ETA: 05:27:07

################################################################################
                    [1m Learning iteration 1178/10000 [0m                     

                       Computation: 43879 steps/s (collection: 2.147s, learning 0.093s)
             Mean action noise std: 3.22
          Mean value_function loss: 541.4895
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.6750
                       Mean reward: 597.46
               Mean episode length: 177.09
    Episode_Reward/reaching_object: 1.1809
     Episode_Reward/lifting_object: 110.9284
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.24s
                      Time elapsed: 00:43:42
                               ETA: 05:27:05

################################################################################
                    [1m Learning iteration 1179/10000 [0m                     

                       Computation: 44154 steps/s (collection: 2.120s, learning 0.106s)
             Mean action noise std: 3.22
          Mean value_function loss: 515.9829
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.6817
                       Mean reward: 541.77
               Mean episode length: 169.29
    Episode_Reward/reaching_object: 1.1545
     Episode_Reward/lifting_object: 109.3171
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.23s
                      Time elapsed: 00:43:45
                               ETA: 05:27:03

################################################################################
                    [1m Learning iteration 1180/10000 [0m                     

                       Computation: 43313 steps/s (collection: 2.176s, learning 0.093s)
             Mean action noise std: 3.22
          Mean value_function loss: 540.5830
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 79.6852
                       Mean reward: 504.77
               Mean episode length: 155.69
    Episode_Reward/reaching_object: 1.2068
     Episode_Reward/lifting_object: 114.7989
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.27s
                      Time elapsed: 00:43:47
                               ETA: 05:27:01

################################################################################
                    [1m Learning iteration 1181/10000 [0m                     

                       Computation: 41965 steps/s (collection: 2.229s, learning 0.114s)
             Mean action noise std: 3.22
          Mean value_function loss: 556.2437
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.6904
                       Mean reward: 538.05
               Mean episode length: 162.72
    Episode_Reward/reaching_object: 1.1609
     Episode_Reward/lifting_object: 109.7163
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.34s
                      Time elapsed: 00:43:49
                               ETA: 05:27:00

################################################################################
                    [1m Learning iteration 1182/10000 [0m                     

                       Computation: 42654 steps/s (collection: 2.185s, learning 0.120s)
             Mean action noise std: 3.22
          Mean value_function loss: 537.1608
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.6973
                       Mean reward: 476.86
               Mean episode length: 155.94
    Episode_Reward/reaching_object: 1.1442
     Episode_Reward/lifting_object: 107.7885
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.30s
                      Time elapsed: 00:43:51
                               ETA: 05:26:58

################################################################################
                    [1m Learning iteration 1183/10000 [0m                     

                       Computation: 43063 steps/s (collection: 2.163s, learning 0.120s)
             Mean action noise std: 3.22
          Mean value_function loss: 556.9047
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 79.7038
                       Mean reward: 585.43
               Mean episode length: 172.73
    Episode_Reward/reaching_object: 1.1446
     Episode_Reward/lifting_object: 107.9084
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.28s
                      Time elapsed: 00:43:54
                               ETA: 05:26:56

################################################################################
                    [1m Learning iteration 1184/10000 [0m                     

                       Computation: 40305 steps/s (collection: 2.331s, learning 0.108s)
             Mean action noise std: 3.22
          Mean value_function loss: 570.7048
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 79.7075
                       Mean reward: 521.49
               Mean episode length: 158.13
    Episode_Reward/reaching_object: 1.1140
     Episode_Reward/lifting_object: 104.3254
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.44s
                      Time elapsed: 00:43:56
                               ETA: 05:26:56

################################################################################
                    [1m Learning iteration 1185/10000 [0m                     

                       Computation: 40601 steps/s (collection: 2.302s, learning 0.119s)
             Mean action noise std: 3.22
          Mean value_function loss: 542.2010
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.7088
                       Mean reward: 548.50
               Mean episode length: 163.79
    Episode_Reward/reaching_object: 1.1351
     Episode_Reward/lifting_object: 107.8287
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.42s
                      Time elapsed: 00:43:59
                               ETA: 05:26:55

################################################################################
                    [1m Learning iteration 1186/10000 [0m                     

                       Computation: 41509 steps/s (collection: 2.259s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 573.9111
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.7104
                       Mean reward: 522.80
               Mean episode length: 159.70
    Episode_Reward/reaching_object: 1.1091
     Episode_Reward/lifting_object: 104.1197
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.37s
                      Time elapsed: 00:44:01
                               ETA: 05:26:54

################################################################################
                    [1m Learning iteration 1187/10000 [0m                     

                       Computation: 41726 steps/s (collection: 2.232s, learning 0.124s)
             Mean action noise std: 3.23
          Mean value_function loss: 575.5808
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.7113
                       Mean reward: 549.84
               Mean episode length: 162.62
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 102.3328
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.36s
                      Time elapsed: 00:44:03
                               ETA: 05:26:53

################################################################################
                    [1m Learning iteration 1188/10000 [0m                     

                       Computation: 42759 steps/s (collection: 2.178s, learning 0.121s)
             Mean action noise std: 3.23
          Mean value_function loss: 574.5416
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 79.7138
                       Mean reward: 591.60
               Mean episode length: 171.75
    Episode_Reward/reaching_object: 1.1703
     Episode_Reward/lifting_object: 111.5361
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.30s
                      Time elapsed: 00:44:06
                               ETA: 05:26:51

################################################################################
                    [1m Learning iteration 1189/10000 [0m                     

                       Computation: 42747 steps/s (collection: 2.204s, learning 0.095s)
             Mean action noise std: 3.23
          Mean value_function loss: 616.1623
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.7186
                       Mean reward: 546.41
               Mean episode length: 163.51
    Episode_Reward/reaching_object: 1.1133
     Episode_Reward/lifting_object: 105.7140
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.30s
                      Time elapsed: 00:44:08
                               ETA: 05:26:49

################################################################################
                    [1m Learning iteration 1190/10000 [0m                     

                       Computation: 43042 steps/s (collection: 2.185s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 605.5578
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.7256
                       Mean reward: 530.94
               Mean episode length: 159.59
    Episode_Reward/reaching_object: 1.1602
     Episode_Reward/lifting_object: 110.8975
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.28s
                      Time elapsed: 00:44:10
                               ETA: 05:26:47

################################################################################
                    [1m Learning iteration 1191/10000 [0m                     

                       Computation: 43355 steps/s (collection: 2.168s, learning 0.100s)
             Mean action noise std: 3.23
          Mean value_function loss: 540.1801
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.7309
                       Mean reward: 555.94
               Mean episode length: 168.48
    Episode_Reward/reaching_object: 1.1314
     Episode_Reward/lifting_object: 107.9882
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.27s
                      Time elapsed: 00:44:13
                               ETA: 05:26:46

################################################################################
                    [1m Learning iteration 1192/10000 [0m                     

                       Computation: 42934 steps/s (collection: 2.145s, learning 0.145s)
             Mean action noise std: 3.23
          Mean value_function loss: 584.7705
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.7349
                       Mean reward: 534.25
               Mean episode length: 161.53
    Episode_Reward/reaching_object: 1.1601
     Episode_Reward/lifting_object: 110.7781
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.29s
                      Time elapsed: 00:44:15
                               ETA: 05:26:44

################################################################################
                    [1m Learning iteration 1193/10000 [0m                     

                       Computation: 43491 steps/s (collection: 2.166s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 520.4339
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.7399
                       Mean reward: 561.91
               Mean episode length: 166.84
    Episode_Reward/reaching_object: 1.1012
     Episode_Reward/lifting_object: 103.8498
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.26s
                      Time elapsed: 00:44:17
                               ETA: 05:26:42

################################################################################
                    [1m Learning iteration 1194/10000 [0m                     

                       Computation: 43821 steps/s (collection: 2.145s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 508.1723
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.7464
                       Mean reward: 577.17
               Mean episode length: 174.42
    Episode_Reward/reaching_object: 1.2152
     Episode_Reward/lifting_object: 116.4872
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.24s
                      Time elapsed: 00:44:19
                               ETA: 05:26:40

################################################################################
                    [1m Learning iteration 1195/10000 [0m                     

                       Computation: 43069 steps/s (collection: 2.182s, learning 0.100s)
             Mean action noise std: 3.23
          Mean value_function loss: 549.1968
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.7594
                       Mean reward: 554.17
               Mean episode length: 167.67
    Episode_Reward/reaching_object: 1.1824
     Episode_Reward/lifting_object: 112.4639
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.28s
                      Time elapsed: 00:44:22
                               ETA: 05:26:38

################################################################################
                    [1m Learning iteration 1196/10000 [0m                     

                       Computation: 42545 steps/s (collection: 2.166s, learning 0.145s)
             Mean action noise std: 3.23
          Mean value_function loss: 514.0157
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 79.7763
                       Mean reward: 587.37
               Mean episode length: 172.23
    Episode_Reward/reaching_object: 1.1897
     Episode_Reward/lifting_object: 113.3713
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.31s
                      Time elapsed: 00:44:24
                               ETA: 05:26:36

################################################################################
                    [1m Learning iteration 1197/10000 [0m                     

                       Computation: 42472 steps/s (collection: 2.204s, learning 0.110s)
             Mean action noise std: 3.23
          Mean value_function loss: 495.1747
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.7778
                       Mean reward: 582.18
               Mean episode length: 173.93
    Episode_Reward/reaching_object: 1.1850
     Episode_Reward/lifting_object: 113.1590
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.31s
                      Time elapsed: 00:44:26
                               ETA: 05:26:35

################################################################################
                    [1m Learning iteration 1198/10000 [0m                     

                       Computation: 43108 steps/s (collection: 2.179s, learning 0.101s)
             Mean action noise std: 3.24
          Mean value_function loss: 533.7128
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.7787
                       Mean reward: 555.24
               Mean episode length: 166.01
    Episode_Reward/reaching_object: 1.1684
     Episode_Reward/lifting_object: 111.4552
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.28s
                      Time elapsed: 00:44:28
                               ETA: 05:26:33

################################################################################
                    [1m Learning iteration 1199/10000 [0m                     

                       Computation: 42624 steps/s (collection: 2.162s, learning 0.145s)
             Mean action noise std: 3.24
          Mean value_function loss: 528.9304
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.7818
                       Mean reward: 573.48
               Mean episode length: 173.63
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 118.1505
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.31s
                      Time elapsed: 00:44:31
                               ETA: 05:26:31

################################################################################
                    [1m Learning iteration 1200/10000 [0m                     

                       Computation: 43151 steps/s (collection: 2.182s, learning 0.097s)
             Mean action noise std: 3.24
          Mean value_function loss: 488.3804
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 79.7868
                       Mean reward: 592.97
               Mean episode length: 173.33
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 114.9658
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.28s
                      Time elapsed: 00:44:33
                               ETA: 05:26:29

################################################################################
                    [1m Learning iteration 1201/10000 [0m                     

                       Computation: 43089 steps/s (collection: 2.169s, learning 0.112s)
             Mean action noise std: 3.24
          Mean value_function loss: 521.1401
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.7902
                       Mean reward: 547.94
               Mean episode length: 167.75
    Episode_Reward/reaching_object: 1.1723
     Episode_Reward/lifting_object: 111.7474
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.28s
                      Time elapsed: 00:44:35
                               ETA: 05:26:28

################################################################################
                    [1m Learning iteration 1202/10000 [0m                     

                       Computation: 43492 steps/s (collection: 2.169s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 552.2901
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.7958
                       Mean reward: 640.18
               Mean episode length: 185.97
    Episode_Reward/reaching_object: 1.2054
     Episode_Reward/lifting_object: 115.9734
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.26s
                      Time elapsed: 00:44:38
                               ETA: 05:26:26

################################################################################
                    [1m Learning iteration 1203/10000 [0m                     

                       Computation: 43376 steps/s (collection: 2.160s, learning 0.106s)
             Mean action noise std: 3.24
          Mean value_function loss: 544.3306
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 79.8039
                       Mean reward: 565.43
               Mean episode length: 170.09
    Episode_Reward/reaching_object: 1.1695
     Episode_Reward/lifting_object: 112.0441
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.27s
                      Time elapsed: 00:44:40
                               ETA: 05:26:24

################################################################################
                    [1m Learning iteration 1204/10000 [0m                     

                       Computation: 41413 steps/s (collection: 2.282s, learning 0.092s)
             Mean action noise std: 3.24
          Mean value_function loss: 494.0414
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.8104
                       Mean reward: 540.94
               Mean episode length: 165.76
    Episode_Reward/reaching_object: 1.1556
     Episode_Reward/lifting_object: 110.4732
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.37s
                      Time elapsed: 00:44:42
                               ETA: 05:26:23

################################################################################
                    [1m Learning iteration 1205/10000 [0m                     

                       Computation: 41943 steps/s (collection: 2.246s, learning 0.098s)
             Mean action noise std: 3.24
          Mean value_function loss: 522.4602
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.8181
                       Mean reward: 588.20
               Mean episode length: 175.13
    Episode_Reward/reaching_object: 1.1703
     Episode_Reward/lifting_object: 112.4730
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.34s
                      Time elapsed: 00:44:45
                               ETA: 05:26:21

################################################################################
                    [1m Learning iteration 1206/10000 [0m                     

                       Computation: 41160 steps/s (collection: 2.274s, learning 0.114s)
             Mean action noise std: 3.24
          Mean value_function loss: 553.5489
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.8232
                       Mean reward: 536.63
               Mean episode length: 162.57
    Episode_Reward/reaching_object: 1.1331
     Episode_Reward/lifting_object: 108.0519
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.39s
                      Time elapsed: 00:44:47
                               ETA: 05:26:20

################################################################################
                    [1m Learning iteration 1207/10000 [0m                     

                       Computation: 41678 steps/s (collection: 2.234s, learning 0.125s)
             Mean action noise std: 3.24
          Mean value_function loss: 499.9940
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.8304
                       Mean reward: 583.89
               Mean episode length: 175.64
    Episode_Reward/reaching_object: 1.1684
     Episode_Reward/lifting_object: 112.2691
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.36s
                      Time elapsed: 00:44:49
                               ETA: 05:26:19

################################################################################
                    [1m Learning iteration 1208/10000 [0m                     

                       Computation: 40690 steps/s (collection: 2.267s, learning 0.149s)
             Mean action noise std: 3.24
          Mean value_function loss: 509.7462
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 79.8405
                       Mean reward: 525.36
               Mean episode length: 162.99
    Episode_Reward/reaching_object: 1.1523
     Episode_Reward/lifting_object: 110.7912
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.42s
                      Time elapsed: 00:44:52
                               ETA: 05:26:18

################################################################################
                    [1m Learning iteration 1209/10000 [0m                     

                       Computation: 42364 steps/s (collection: 2.213s, learning 0.108s)
             Mean action noise std: 3.25
          Mean value_function loss: 508.9085
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.8461
                       Mean reward: 609.49
               Mean episode length: 180.80
    Episode_Reward/reaching_object: 1.2126
     Episode_Reward/lifting_object: 117.1375
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.32s
                      Time elapsed: 00:44:54
                               ETA: 05:26:16

################################################################################
                    [1m Learning iteration 1210/10000 [0m                     

                       Computation: 39526 steps/s (collection: 2.371s, learning 0.116s)
             Mean action noise std: 3.25
          Mean value_function loss: 525.7486
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.8534
                       Mean reward: 540.19
               Mean episode length: 165.67
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: 111.3571
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.49s
                      Time elapsed: 00:44:57
                               ETA: 05:26:16

################################################################################
                    [1m Learning iteration 1211/10000 [0m                     

                       Computation: 39661 steps/s (collection: 2.306s, learning 0.173s)
             Mean action noise std: 3.25
          Mean value_function loss: 476.8461
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.8594
                       Mean reward: 555.97
               Mean episode length: 167.56
    Episode_Reward/reaching_object: 1.2130
     Episode_Reward/lifting_object: 117.6915
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.48s
                      Time elapsed: 00:44:59
                               ETA: 05:26:16

################################################################################
                    [1m Learning iteration 1212/10000 [0m                     

                       Computation: 40160 steps/s (collection: 2.328s, learning 0.120s)
             Mean action noise std: 3.25
          Mean value_function loss: 521.1854
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 79.8610
                       Mean reward: 588.72
               Mean episode length: 175.25
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 119.3432
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.45s
                      Time elapsed: 00:45:01
                               ETA: 05:26:15

################################################################################
                    [1m Learning iteration 1213/10000 [0m                     

                       Computation: 40287 steps/s (collection: 2.275s, learning 0.165s)
             Mean action noise std: 3.25
          Mean value_function loss: 541.7830
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 79.8622
                       Mean reward: 589.12
               Mean episode length: 173.18
    Episode_Reward/reaching_object: 1.1939
     Episode_Reward/lifting_object: 115.1886
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.44s
                      Time elapsed: 00:45:04
                               ETA: 05:26:14

################################################################################
                    [1m Learning iteration 1214/10000 [0m                     

                       Computation: 40726 steps/s (collection: 2.280s, learning 0.134s)
             Mean action noise std: 3.25
          Mean value_function loss: 508.0465
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 79.8637
                       Mean reward: 555.25
               Mean episode length: 167.15
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 116.1473
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.41s
                      Time elapsed: 00:45:06
                               ETA: 05:26:14

################################################################################
                    [1m Learning iteration 1215/10000 [0m                     

                       Computation: 40612 steps/s (collection: 2.285s, learning 0.135s)
             Mean action noise std: 3.25
          Mean value_function loss: 495.1245
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 79.8665
                       Mean reward: 516.35
               Mean episode length: 159.28
    Episode_Reward/reaching_object: 1.1608
     Episode_Reward/lifting_object: 111.5944
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.42s
                      Time elapsed: 00:45:09
                               ETA: 05:26:13

################################################################################
                    [1m Learning iteration 1216/10000 [0m                     

                       Computation: 41495 steps/s (collection: 2.245s, learning 0.124s)
             Mean action noise std: 3.25
          Mean value_function loss: 500.1021
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.8706
                       Mean reward: 562.22
               Mean episode length: 167.80
    Episode_Reward/reaching_object: 1.2213
     Episode_Reward/lifting_object: 117.5512
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.37s
                      Time elapsed: 00:45:11
                               ETA: 05:26:11

################################################################################
                    [1m Learning iteration 1217/10000 [0m                     

                       Computation: 41327 steps/s (collection: 2.256s, learning 0.123s)
             Mean action noise std: 3.25
          Mean value_function loss: 499.1407
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.8778
                       Mean reward: 551.00
               Mean episode length: 164.01
    Episode_Reward/reaching_object: 1.2154
     Episode_Reward/lifting_object: 117.5580
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.38s
                      Time elapsed: 00:45:14
                               ETA: 05:26:10

################################################################################
                    [1m Learning iteration 1218/10000 [0m                     

                       Computation: 40379 steps/s (collection: 2.241s, learning 0.193s)
             Mean action noise std: 3.25
          Mean value_function loss: 495.5235
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 79.8796
                       Mean reward: 586.86
               Mean episode length: 175.82
    Episode_Reward/reaching_object: 1.2169
     Episode_Reward/lifting_object: 117.8541
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.43s
                      Time elapsed: 00:45:16
                               ETA: 05:26:10

################################################################################
                    [1m Learning iteration 1219/10000 [0m                     

                       Computation: 39213 steps/s (collection: 2.408s, learning 0.099s)
             Mean action noise std: 3.25
          Mean value_function loss: 509.1173
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.8807
                       Mean reward: 548.25
               Mean episode length: 166.09
    Episode_Reward/reaching_object: 1.1885
     Episode_Reward/lifting_object: 114.3068
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.51s
                      Time elapsed: 00:45:18
                               ETA: 05:26:09

################################################################################
                    [1m Learning iteration 1220/10000 [0m                     

                       Computation: 38699 steps/s (collection: 2.270s, learning 0.270s)
             Mean action noise std: 3.25
          Mean value_function loss: 458.8636
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.8824
                       Mean reward: 603.27
               Mean episode length: 177.99
    Episode_Reward/reaching_object: 1.2128
     Episode_Reward/lifting_object: 116.6077
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.54s
                      Time elapsed: 00:45:21
                               ETA: 05:26:09

################################################################################
                    [1m Learning iteration 1221/10000 [0m                     

                       Computation: 35859 steps/s (collection: 2.631s, learning 0.110s)
             Mean action noise std: 3.25
          Mean value_function loss: 473.6873
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.8839
                       Mean reward: 570.46
               Mean episode length: 172.78
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 118.1842
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.74s
                      Time elapsed: 00:45:24
                               ETA: 05:26:11

################################################################################
                    [1m Learning iteration 1222/10000 [0m                     

                       Computation: 42552 steps/s (collection: 2.200s, learning 0.111s)
             Mean action noise std: 3.25
          Mean value_function loss: 463.5106
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 79.8868
                       Mean reward: 605.53
               Mean episode length: 179.71
    Episode_Reward/reaching_object: 1.2542
     Episode_Reward/lifting_object: 121.0367
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.31s
                      Time elapsed: 00:45:26
                               ETA: 05:26:09

################################################################################
                    [1m Learning iteration 1223/10000 [0m                     

                       Computation: 41495 steps/s (collection: 2.249s, learning 0.120s)
             Mean action noise std: 3.25
          Mean value_function loss: 465.9517
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.8885
                       Mean reward: 596.51
               Mean episode length: 176.68
    Episode_Reward/reaching_object: 1.2751
     Episode_Reward/lifting_object: 123.8126
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.37s
                      Time elapsed: 00:45:28
                               ETA: 05:26:08

################################################################################
                    [1m Learning iteration 1224/10000 [0m                     

                       Computation: 42230 steps/s (collection: 2.228s, learning 0.100s)
             Mean action noise std: 3.25
          Mean value_function loss: 477.6812
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.8916
                       Mean reward: 622.71
               Mean episode length: 182.48
    Episode_Reward/reaching_object: 1.3056
     Episode_Reward/lifting_object: 126.5281
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.33s
                      Time elapsed: 00:45:31
                               ETA: 05:26:06

################################################################################
                    [1m Learning iteration 1225/10000 [0m                     

                       Computation: 40585 steps/s (collection: 2.295s, learning 0.127s)
             Mean action noise std: 3.25
          Mean value_function loss: 535.7802
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.8980
                       Mean reward: 635.36
               Mean episode length: 185.88
    Episode_Reward/reaching_object: 1.3166
     Episode_Reward/lifting_object: 128.0624
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.42s
                      Time elapsed: 00:45:33
                               ETA: 05:26:06

################################################################################
                    [1m Learning iteration 1226/10000 [0m                     

                       Computation: 39239 steps/s (collection: 2.386s, learning 0.119s)
             Mean action noise std: 3.25
          Mean value_function loss: 452.0621
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 79.9032
                       Mean reward: 664.10
               Mean episode length: 193.99
    Episode_Reward/reaching_object: 1.2855
     Episode_Reward/lifting_object: 124.0123
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.51s
                      Time elapsed: 00:45:36
                               ETA: 05:26:05

################################################################################
                    [1m Learning iteration 1227/10000 [0m                     

                       Computation: 41923 steps/s (collection: 2.203s, learning 0.142s)
             Mean action noise std: 3.25
          Mean value_function loss: 454.6308
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.9044
                       Mean reward: 645.92
               Mean episode length: 187.60
    Episode_Reward/reaching_object: 1.2649
     Episode_Reward/lifting_object: 122.5143
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.34s
                      Time elapsed: 00:45:38
                               ETA: 05:26:04

################################################################################
                    [1m Learning iteration 1228/10000 [0m                     

                       Computation: 43236 steps/s (collection: 2.157s, learning 0.117s)
             Mean action noise std: 3.26
          Mean value_function loss: 533.0390
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.9056
                       Mean reward: 644.69
               Mean episode length: 186.46
    Episode_Reward/reaching_object: 1.2730
     Episode_Reward/lifting_object: 122.6664
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.27s
                      Time elapsed: 00:45:40
                               ETA: 05:26:02

################################################################################
                    [1m Learning iteration 1229/10000 [0m                     

                       Computation: 44450 steps/s (collection: 2.103s, learning 0.108s)
             Mean action noise std: 3.26
          Mean value_function loss: 526.0578
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.9074
                       Mean reward: 619.87
               Mean episode length: 182.25
    Episode_Reward/reaching_object: 1.2296
     Episode_Reward/lifting_object: 117.3361
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.21s
                      Time elapsed: 00:45:43
                               ETA: 05:26:00

################################################################################
                    [1m Learning iteration 1230/10000 [0m                     

                       Computation: 41587 steps/s (collection: 2.217s, learning 0.147s)
             Mean action noise std: 3.26
          Mean value_function loss: 518.0158
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.9084
                       Mean reward: 618.86
               Mean episode length: 182.81
    Episode_Reward/reaching_object: 1.2846
     Episode_Reward/lifting_object: 124.2188
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.36s
                      Time elapsed: 00:45:45
                               ETA: 05:25:58

################################################################################
                    [1m Learning iteration 1231/10000 [0m                     

                       Computation: 40179 steps/s (collection: 2.346s, learning 0.101s)
             Mean action noise std: 3.26
          Mean value_function loss: 457.3872
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.9108
                       Mean reward: 562.82
               Mean episode length: 170.58
    Episode_Reward/reaching_object: 1.2376
     Episode_Reward/lifting_object: 118.9192
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.45s
                      Time elapsed: 00:45:47
                               ETA: 05:25:58

################################################################################
                    [1m Learning iteration 1232/10000 [0m                     

                       Computation: 42219 steps/s (collection: 2.143s, learning 0.186s)
             Mean action noise std: 3.26
          Mean value_function loss: 525.6099
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.9147
                       Mean reward: 563.34
               Mean episode length: 170.11
    Episode_Reward/reaching_object: 1.2225
     Episode_Reward/lifting_object: 117.4787
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.33s
                      Time elapsed: 00:45:50
                               ETA: 05:25:56

################################################################################
                    [1m Learning iteration 1233/10000 [0m                     

                       Computation: 37308 steps/s (collection: 2.496s, learning 0.139s)
             Mean action noise std: 3.26
          Mean value_function loss: 526.3194
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 79.9226
                       Mean reward: 615.07
               Mean episode length: 179.74
    Episode_Reward/reaching_object: 1.2438
     Episode_Reward/lifting_object: 119.6220
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.63s
                      Time elapsed: 00:45:52
                               ETA: 05:25:57

################################################################################
                    [1m Learning iteration 1234/10000 [0m                     

                       Computation: 41004 steps/s (collection: 2.290s, learning 0.108s)
             Mean action noise std: 3.26
          Mean value_function loss: 499.8524
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.9278
                       Mean reward: 603.16
               Mean episode length: 177.19
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 121.4456
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.40s
                      Time elapsed: 00:45:55
                               ETA: 05:25:56

################################################################################
                    [1m Learning iteration 1235/10000 [0m                     

                       Computation: 40918 steps/s (collection: 2.263s, learning 0.139s)
             Mean action noise std: 3.26
          Mean value_function loss: 533.0458
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.9319
                       Mean reward: 579.47
               Mean episode length: 174.34
    Episode_Reward/reaching_object: 1.2385
     Episode_Reward/lifting_object: 119.4687
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.40s
                      Time elapsed: 00:45:57
                               ETA: 05:25:55

################################################################################
                    [1m Learning iteration 1236/10000 [0m                     

                       Computation: 42361 steps/s (collection: 2.210s, learning 0.111s)
             Mean action noise std: 3.26
          Mean value_function loss: 518.9741
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.9359
                       Mean reward: 607.97
               Mean episode length: 183.38
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 119.2590
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.32s
                      Time elapsed: 00:45:59
                               ETA: 05:25:53

################################################################################
                    [1m Learning iteration 1237/10000 [0m                     

                       Computation: 40139 steps/s (collection: 2.352s, learning 0.097s)
             Mean action noise std: 3.26
          Mean value_function loss: 525.3485
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.9445
                       Mean reward: 524.25
               Mean episode length: 160.42
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 118.2526
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.45s
                      Time elapsed: 00:46:02
                               ETA: 05:25:52

################################################################################
                    [1m Learning iteration 1238/10000 [0m                     

                       Computation: 40483 steps/s (collection: 2.247s, learning 0.181s)
             Mean action noise std: 3.26
          Mean value_function loss: 502.6719
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.9501
                       Mean reward: 611.43
               Mean episode length: 179.43
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 116.6790
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.43s
                      Time elapsed: 00:46:04
                               ETA: 05:25:52

################################################################################
                    [1m Learning iteration 1239/10000 [0m                     

                       Computation: 41425 steps/s (collection: 2.267s, learning 0.106s)
             Mean action noise std: 3.26
          Mean value_function loss: 565.1729
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.9565
                       Mean reward: 627.81
               Mean episode length: 183.06
    Episode_Reward/reaching_object: 1.2935
     Episode_Reward/lifting_object: 125.6292
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.37s
                      Time elapsed: 00:46:07
                               ETA: 05:25:50

################################################################################
                    [1m Learning iteration 1240/10000 [0m                     

                       Computation: 44714 steps/s (collection: 2.109s, learning 0.090s)
             Mean action noise std: 3.26
          Mean value_function loss: 487.4053
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.9623
                       Mean reward: 606.03
               Mean episode length: 180.16
    Episode_Reward/reaching_object: 1.2910
     Episode_Reward/lifting_object: 125.7401
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.20s
                      Time elapsed: 00:46:09
                               ETA: 05:25:48

################################################################################
                    [1m Learning iteration 1241/10000 [0m                     

                       Computation: 43281 steps/s (collection: 2.143s, learning 0.128s)
             Mean action noise std: 3.27
          Mean value_function loss: 529.6099
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.9684
                       Mean reward: 647.56
               Mean episode length: 186.79
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 121.2513
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.27s
                      Time elapsed: 00:46:11
                               ETA: 05:25:46

################################################################################
                    [1m Learning iteration 1242/10000 [0m                     

                       Computation: 43023 steps/s (collection: 2.194s, learning 0.091s)
             Mean action noise std: 3.27
          Mean value_function loss: 433.0974
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 79.9749
                       Mean reward: 649.82
               Mean episode length: 190.26
    Episode_Reward/reaching_object: 1.2826
     Episode_Reward/lifting_object: 124.4032
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.28s
                      Time elapsed: 00:46:13
                               ETA: 05:25:44

################################################################################
                    [1m Learning iteration 1243/10000 [0m                     

                       Computation: 44034 steps/s (collection: 2.121s, learning 0.112s)
             Mean action noise std: 3.27
          Mean value_function loss: 470.2083
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.9813
                       Mean reward: 715.55
               Mean episode length: 202.30
    Episode_Reward/reaching_object: 1.3338
     Episode_Reward/lifting_object: 130.5218
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.23s
                      Time elapsed: 00:46:16
                               ETA: 05:25:42

################################################################################
                    [1m Learning iteration 1244/10000 [0m                     

                       Computation: 42874 steps/s (collection: 2.168s, learning 0.125s)
             Mean action noise std: 3.27
          Mean value_function loss: 418.0123
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.9854
                       Mean reward: 654.38
               Mean episode length: 189.44
    Episode_Reward/reaching_object: 1.3166
     Episode_Reward/lifting_object: 128.0934
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.29s
                      Time elapsed: 00:46:18
                               ETA: 05:25:40

################################################################################
                    [1m Learning iteration 1245/10000 [0m                     

                       Computation: 44551 steps/s (collection: 2.080s, learning 0.126s)
             Mean action noise std: 3.27
          Mean value_function loss: 494.4563
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 79.9937
                       Mean reward: 626.95
               Mean episode length: 184.66
    Episode_Reward/reaching_object: 1.3320
     Episode_Reward/lifting_object: 130.7124
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.21s
                      Time elapsed: 00:46:20
                               ETA: 05:25:38

################################################################################
                    [1m Learning iteration 1246/10000 [0m                     

                       Computation: 43538 steps/s (collection: 2.116s, learning 0.142s)
             Mean action noise std: 3.27
          Mean value_function loss: 522.8718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.0104
                       Mean reward: 616.92
               Mean episode length: 182.59
    Episode_Reward/reaching_object: 1.3168
     Episode_Reward/lifting_object: 127.4472
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.26s
                      Time elapsed: 00:46:22
                               ETA: 05:25:36

################################################################################
                    [1m Learning iteration 1247/10000 [0m                     

                       Computation: 43117 steps/s (collection: 2.166s, learning 0.114s)
             Mean action noise std: 3.27
          Mean value_function loss: 456.8016
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.0210
                       Mean reward: 641.66
               Mean episode length: 183.74
    Episode_Reward/reaching_object: 1.3266
     Episode_Reward/lifting_object: 129.7267
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.28s
                      Time elapsed: 00:46:25
                               ETA: 05:25:34

################################################################################
                    [1m Learning iteration 1248/10000 [0m                     

                       Computation: 44690 steps/s (collection: 2.090s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 422.5780
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.0324
                       Mean reward: 594.29
               Mean episode length: 176.36
    Episode_Reward/reaching_object: 1.3135
     Episode_Reward/lifting_object: 128.4769
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.20s
                      Time elapsed: 00:46:27
                               ETA: 05:25:31

################################################################################
                    [1m Learning iteration 1249/10000 [0m                     

                       Computation: 44660 steps/s (collection: 2.106s, learning 0.095s)
             Mean action noise std: 3.28
          Mean value_function loss: 457.3453
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 80.0397
                       Mean reward: 644.65
               Mean episode length: 189.70
    Episode_Reward/reaching_object: 1.3252
     Episode_Reward/lifting_object: 129.1651
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.20s
                      Time elapsed: 00:46:29
                               ETA: 05:25:29

################################################################################
                    [1m Learning iteration 1250/10000 [0m                     

                       Computation: 44853 steps/s (collection: 2.083s, learning 0.109s)
             Mean action noise std: 3.28
          Mean value_function loss: 530.3738
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.0439
                       Mean reward: 654.77
               Mean episode length: 187.62
    Episode_Reward/reaching_object: 1.3232
     Episode_Reward/lifting_object: 128.7340
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.19s
                      Time elapsed: 00:46:31
                               ETA: 05:25:26

################################################################################
                    [1m Learning iteration 1251/10000 [0m                     

                       Computation: 43232 steps/s (collection: 2.133s, learning 0.141s)
             Mean action noise std: 3.28
          Mean value_function loss: 502.9454
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 80.0493
                       Mean reward: 633.56
               Mean episode length: 184.58
    Episode_Reward/reaching_object: 1.2461
     Episode_Reward/lifting_object: 119.9825
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.27s
                      Time elapsed: 00:46:34
                               ETA: 05:25:24

################################################################################
                    [1m Learning iteration 1252/10000 [0m                     

                       Computation: 42192 steps/s (collection: 2.216s, learning 0.114s)
             Mean action noise std: 3.28
          Mean value_function loss: 532.8815
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.0559
                       Mean reward: 603.98
               Mean episode length: 177.08
    Episode_Reward/reaching_object: 1.2722
     Episode_Reward/lifting_object: 122.7644
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.33s
                      Time elapsed: 00:46:36
                               ETA: 05:25:23

################################################################################
                    [1m Learning iteration 1253/10000 [0m                     

                       Computation: 43713 steps/s (collection: 2.156s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 465.6365
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 80.0628
                       Mean reward: 663.80
               Mean episode length: 191.88
    Episode_Reward/reaching_object: 1.2773
     Episode_Reward/lifting_object: 123.5468
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.25s
                      Time elapsed: 00:46:38
                               ETA: 05:25:21

################################################################################
                    [1m Learning iteration 1254/10000 [0m                     

                       Computation: 42999 steps/s (collection: 2.192s, learning 0.095s)
             Mean action noise std: 3.28
          Mean value_function loss: 456.0626
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.0695
                       Mean reward: 617.29
               Mean episode length: 180.67
    Episode_Reward/reaching_object: 1.3100
     Episode_Reward/lifting_object: 127.5017
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.29s
                      Time elapsed: 00:46:40
                               ETA: 05:25:19

################################################################################
                    [1m Learning iteration 1255/10000 [0m                     

                       Computation: 42626 steps/s (collection: 2.210s, learning 0.096s)
             Mean action noise std: 3.28
          Mean value_function loss: 442.3689
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.0795
                       Mean reward: 625.91
               Mean episode length: 183.50
    Episode_Reward/reaching_object: 1.2695
     Episode_Reward/lifting_object: 122.8594
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.31s
                      Time elapsed: 00:46:43
                               ETA: 05:25:17

################################################################################
                    [1m Learning iteration 1256/10000 [0m                     

                       Computation: 40170 steps/s (collection: 2.288s, learning 0.159s)
             Mean action noise std: 3.28
          Mean value_function loss: 446.4685
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 80.0887
                       Mean reward: 654.36
               Mean episode length: 192.11
    Episode_Reward/reaching_object: 1.3361
     Episode_Reward/lifting_object: 130.1885
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.45s
                      Time elapsed: 00:46:45
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1257/10000 [0m                     

                       Computation: 40244 steps/s (collection: 2.273s, learning 0.170s)
             Mean action noise std: 3.28
          Mean value_function loss: 495.4936
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.0941
                       Mean reward: 620.07
               Mean episode length: 182.01
    Episode_Reward/reaching_object: 1.3771
     Episode_Reward/lifting_object: 135.1658
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.44s
                      Time elapsed: 00:46:48
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1258/10000 [0m                     

                       Computation: 40490 steps/s (collection: 2.261s, learning 0.167s)
             Mean action noise std: 3.28
          Mean value_function loss: 502.9077
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 80.1042
                       Mean reward: 627.57
               Mean episode length: 183.81
    Episode_Reward/reaching_object: 1.2723
     Episode_Reward/lifting_object: 122.7879
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.43s
                      Time elapsed: 00:46:50
                               ETA: 05:25:15

################################################################################
                    [1m Learning iteration 1259/10000 [0m                     

                       Computation: 40896 steps/s (collection: 2.236s, learning 0.168s)
             Mean action noise std: 3.29
          Mean value_function loss: 429.2765
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 80.1109
                       Mean reward: 672.58
               Mean episode length: 194.03
    Episode_Reward/reaching_object: 1.3581
     Episode_Reward/lifting_object: 132.6913
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.40s
                      Time elapsed: 00:46:52
                               ETA: 05:25:14

################################################################################
                    [1m Learning iteration 1260/10000 [0m                     

                       Computation: 40996 steps/s (collection: 2.261s, learning 0.137s)
             Mean action noise std: 3.29
          Mean value_function loss: 477.6221
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.1157
                       Mean reward: 619.49
               Mean episode length: 184.14
    Episode_Reward/reaching_object: 1.3324
     Episode_Reward/lifting_object: 128.9638
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.40s
                      Time elapsed: 00:46:55
                               ETA: 05:25:13

################################################################################
                    [1m Learning iteration 1261/10000 [0m                     

                       Computation: 36170 steps/s (collection: 2.558s, learning 0.160s)
             Mean action noise std: 3.29
          Mean value_function loss: 476.8915
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.1188
                       Mean reward: 642.39
               Mean episode length: 189.44
    Episode_Reward/reaching_object: 1.3501
     Episode_Reward/lifting_object: 132.0648
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.72s
                      Time elapsed: 00:46:58
                               ETA: 05:25:14

################################################################################
                    [1m Learning iteration 1262/10000 [0m                     

                       Computation: 41469 steps/s (collection: 2.264s, learning 0.107s)
             Mean action noise std: 3.29
          Mean value_function loss: 433.3599
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.1256
                       Mean reward: 663.51
               Mean episode length: 190.84
    Episode_Reward/reaching_object: 1.3225
     Episode_Reward/lifting_object: 128.9845
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.37s
                      Time elapsed: 00:47:00
                               ETA: 05:25:12

################################################################################
                    [1m Learning iteration 1263/10000 [0m                     

                       Computation: 36187 steps/s (collection: 2.610s, learning 0.107s)
             Mean action noise std: 3.29
          Mean value_function loss: 477.9898
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 80.1343
                       Mean reward: 661.13
               Mean episode length: 192.68
    Episode_Reward/reaching_object: 1.3286
     Episode_Reward/lifting_object: 129.5376
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.72s
                      Time elapsed: 00:47:03
                               ETA: 05:25:14

################################################################################
                    [1m Learning iteration 1264/10000 [0m                     

                       Computation: 33566 steps/s (collection: 2.759s, learning 0.170s)
             Mean action noise std: 3.29
          Mean value_function loss: 475.2150
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.1384
                       Mean reward: 624.50
               Mean episode length: 183.38
    Episode_Reward/reaching_object: 1.3063
     Episode_Reward/lifting_object: 126.5221
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.93s
                      Time elapsed: 00:47:06
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1265/10000 [0m                     

                       Computation: 37898 steps/s (collection: 2.431s, learning 0.163s)
             Mean action noise std: 3.29
          Mean value_function loss: 458.2552
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.1441
                       Mean reward: 677.78
               Mean episode length: 197.17
    Episode_Reward/reaching_object: 1.3100
     Episode_Reward/lifting_object: 127.6012
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.59s
                      Time elapsed: 00:47:08
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1266/10000 [0m                     

                       Computation: 39590 steps/s (collection: 2.373s, learning 0.110s)
             Mean action noise std: 3.29
          Mean value_function loss: 410.4549
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.1510
                       Mean reward: 680.23
               Mean episode length: 195.17
    Episode_Reward/reaching_object: 1.3841
     Episode_Reward/lifting_object: 135.4730
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.48s
                      Time elapsed: 00:47:11
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1267/10000 [0m                     

                       Computation: 35148 steps/s (collection: 2.568s, learning 0.229s)
             Mean action noise std: 3.29
          Mean value_function loss: 455.2559
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.1599
                       Mean reward: 638.57
               Mean episode length: 187.87
    Episode_Reward/reaching_object: 1.3479
     Episode_Reward/lifting_object: 131.9136
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.80s
                      Time elapsed: 00:47:13
                               ETA: 05:25:17

################################################################################
                    [1m Learning iteration 1268/10000 [0m                     

                       Computation: 31874 steps/s (collection: 2.832s, learning 0.252s)
             Mean action noise std: 3.29
          Mean value_function loss: 425.5194
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.1642
                       Mean reward: 619.70
               Mean episode length: 182.39
    Episode_Reward/reaching_object: 1.3538
     Episode_Reward/lifting_object: 132.6940
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 3.08s
                      Time elapsed: 00:47:17
                               ETA: 05:25:21

################################################################################
                    [1m Learning iteration 1269/10000 [0m                     

                       Computation: 34208 steps/s (collection: 2.704s, learning 0.170s)
             Mean action noise std: 3.29
          Mean value_function loss: 486.7267
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 80.1673
                       Mean reward: 641.39
               Mean episode length: 185.25
    Episode_Reward/reaching_object: 1.2886
     Episode_Reward/lifting_object: 124.5864
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.87s
                      Time elapsed: 00:47:19
                               ETA: 05:25:23

################################################################################
                    [1m Learning iteration 1270/10000 [0m                     

                       Computation: 43699 steps/s (collection: 2.148s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 433.8332
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.1745
                       Mean reward: 694.18
               Mean episode length: 199.02
    Episode_Reward/reaching_object: 1.3490
     Episode_Reward/lifting_object: 132.4975
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.25s
                      Time elapsed: 00:47:22
                               ETA: 05:25:21

################################################################################
                    [1m Learning iteration 1271/10000 [0m                     

                       Computation: 44465 steps/s (collection: 2.113s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 451.1529
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.1826
                       Mean reward: 640.06
               Mean episode length: 186.33
    Episode_Reward/reaching_object: 1.3325
     Episode_Reward/lifting_object: 130.2361
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.21s
                      Time elapsed: 00:47:24
                               ETA: 05:25:19

################################################################################
                    [1m Learning iteration 1272/10000 [0m                     

                       Computation: 44654 steps/s (collection: 2.092s, learning 0.109s)
             Mean action noise std: 3.30
          Mean value_function loss: 423.7169
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 80.1910
                       Mean reward: 728.99
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 1.3607
     Episode_Reward/lifting_object: 133.4767
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.20s
                      Time elapsed: 00:47:26
                               ETA: 05:25:16

################################################################################
                    [1m Learning iteration 1273/10000 [0m                     

                       Computation: 44848 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 443.2829
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.1989
                       Mean reward: 719.18
               Mean episode length: 204.51
    Episode_Reward/reaching_object: 1.4126
     Episode_Reward/lifting_object: 139.1931
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.19s
                      Time elapsed: 00:47:28
                               ETA: 05:25:14

################################################################################
                    [1m Learning iteration 1274/10000 [0m                     

                       Computation: 45074 steps/s (collection: 2.072s, learning 0.109s)
             Mean action noise std: 3.30
          Mean value_function loss: 388.1675
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 80.2112
                       Mean reward: 707.76
               Mean episode length: 203.58
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 135.6688
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.18s
                      Time elapsed: 00:47:30
                               ETA: 05:25:11

################################################################################
                    [1m Learning iteration 1275/10000 [0m                     

                       Computation: 44999 steps/s (collection: 2.087s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 458.5032
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.2191
                       Mean reward: 622.78
               Mean episode length: 186.66
    Episode_Reward/reaching_object: 1.3706
     Episode_Reward/lifting_object: 134.0169
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.18s
                      Time elapsed: 00:47:33
                               ETA: 05:25:08

################################################################################
                    [1m Learning iteration 1276/10000 [0m                     

                       Computation: 45598 steps/s (collection: 2.060s, learning 0.096s)
             Mean action noise std: 3.30
          Mean value_function loss: 488.1363
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.2243
                       Mean reward: 654.21
               Mean episode length: 187.46
    Episode_Reward/reaching_object: 1.3625
     Episode_Reward/lifting_object: 132.6982
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.16s
                      Time elapsed: 00:47:35
                               ETA: 05:25:06

################################################################################
                    [1m Learning iteration 1277/10000 [0m                     

                       Computation: 45053 steps/s (collection: 2.077s, learning 0.105s)
             Mean action noise std: 3.30
          Mean value_function loss: 444.1113
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.2343
                       Mean reward: 738.14
               Mean episode length: 206.47
    Episode_Reward/reaching_object: 1.3596
     Episode_Reward/lifting_object: 133.3790
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.18s
                      Time elapsed: 00:47:37
                               ETA: 05:25:03

################################################################################
                    [1m Learning iteration 1278/10000 [0m                     

                       Computation: 44400 steps/s (collection: 2.101s, learning 0.113s)
             Mean action noise std: 3.31
          Mean value_function loss: 445.2200
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.2471
                       Mean reward: 686.79
               Mean episode length: 194.98
    Episode_Reward/reaching_object: 1.3745
     Episode_Reward/lifting_object: 135.2020
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.21s
                      Time elapsed: 00:47:39
                               ETA: 05:25:01

################################################################################
                    [1m Learning iteration 1279/10000 [0m                     

                       Computation: 45145 steps/s (collection: 2.079s, learning 0.099s)
             Mean action noise std: 3.31
          Mean value_function loss: 442.0453
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.2641
                       Mean reward: 664.08
               Mean episode length: 193.96
    Episode_Reward/reaching_object: 1.3957
     Episode_Reward/lifting_object: 137.2941
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.18s
                      Time elapsed: 00:47:41
                               ETA: 05:24:58

################################################################################
                    [1m Learning iteration 1280/10000 [0m                     

                       Computation: 45949 steps/s (collection: 2.024s, learning 0.115s)
             Mean action noise std: 3.31
          Mean value_function loss: 419.8689
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.2854
                       Mean reward: 650.62
               Mean episode length: 191.09
    Episode_Reward/reaching_object: 1.3805
     Episode_Reward/lifting_object: 135.7322
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.14s
                      Time elapsed: 00:47:43
                               ETA: 05:24:55

################################################################################
                    [1m Learning iteration 1281/10000 [0m                     

                       Computation: 43951 steps/s (collection: 2.117s, learning 0.120s)
             Mean action noise std: 3.31
          Mean value_function loss: 424.0719
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 80.2985
                       Mean reward: 697.10
               Mean episode length: 201.67
    Episode_Reward/reaching_object: 1.3846
     Episode_Reward/lifting_object: 135.4911
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.24s
                      Time elapsed: 00:47:46
                               ETA: 05:24:53

################################################################################
                    [1m Learning iteration 1282/10000 [0m                     

                       Computation: 44383 steps/s (collection: 2.090s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 448.8012
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.3066
                       Mean reward: 669.21
               Mean episode length: 193.51
    Episode_Reward/reaching_object: 1.4033
     Episode_Reward/lifting_object: 138.4916
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.21s
                      Time elapsed: 00:47:48
                               ETA: 05:24:51

################################################################################
                    [1m Learning iteration 1283/10000 [0m                     

                       Computation: 39002 steps/s (collection: 2.308s, learning 0.213s)
             Mean action noise std: 3.31
          Mean value_function loss: 400.6314
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 80.3103
                       Mean reward: 734.84
               Mean episode length: 205.43
    Episode_Reward/reaching_object: 1.4352
     Episode_Reward/lifting_object: 142.5703
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.52s
                      Time elapsed: 00:47:50
                               ETA: 05:24:50

################################################################################
                    [1m Learning iteration 1284/10000 [0m                     

                       Computation: 41422 steps/s (collection: 2.275s, learning 0.098s)
             Mean action noise std: 3.31
          Mean value_function loss: 401.8689
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 80.3120
                       Mean reward: 669.06
               Mean episode length: 192.80
    Episode_Reward/reaching_object: 1.3696
     Episode_Reward/lifting_object: 134.8875
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.37s
                      Time elapsed: 00:47:53
                               ETA: 05:24:49

################################################################################
                    [1m Learning iteration 1285/10000 [0m                     

                       Computation: 44349 steps/s (collection: 2.122s, learning 0.095s)
             Mean action noise std: 3.31
          Mean value_function loss: 392.1294
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 80.3128
                       Mean reward: 691.44
               Mean episode length: 198.55
    Episode_Reward/reaching_object: 1.4011
     Episode_Reward/lifting_object: 138.4296
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.22s
                      Time elapsed: 00:47:55
                               ETA: 05:24:47

################################################################################
                    [1m Learning iteration 1286/10000 [0m                     

                       Computation: 44720 steps/s (collection: 2.081s, learning 0.118s)
             Mean action noise std: 3.31
          Mean value_function loss: 456.2561
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.3147
                       Mean reward: 724.36
               Mean episode length: 205.15
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 140.1358
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.20s
                      Time elapsed: 00:47:57
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1287/10000 [0m                     

                       Computation: 38308 steps/s (collection: 2.418s, learning 0.148s)
             Mean action noise std: 3.31
          Mean value_function loss: 400.8294
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.3210
                       Mean reward: 716.87
               Mean episode length: 205.88
    Episode_Reward/reaching_object: 1.3886
     Episode_Reward/lifting_object: 136.4714
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.57s
                      Time elapsed: 00:48:00
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1288/10000 [0m                     

                       Computation: 37917 steps/s (collection: 2.454s, learning 0.139s)
             Mean action noise std: 3.32
          Mean value_function loss: 419.6818
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.3313
                       Mean reward: 690.57
               Mean episode length: 197.18
    Episode_Reward/reaching_object: 1.3434
     Episode_Reward/lifting_object: 131.4194
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.59s
                      Time elapsed: 00:48:02
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1289/10000 [0m                     

                       Computation: 37972 steps/s (collection: 2.438s, learning 0.151s)
             Mean action noise std: 3.32
          Mean value_function loss: 401.8596
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.3408
                       Mean reward: 734.16
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 1.4426
     Episode_Reward/lifting_object: 142.9623
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.59s
                      Time elapsed: 00:48:05
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1290/10000 [0m                     

                       Computation: 37881 steps/s (collection: 2.419s, learning 0.177s)
             Mean action noise std: 3.32
          Mean value_function loss: 426.0228
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 80.3438
                       Mean reward: 697.59
               Mean episode length: 198.46
    Episode_Reward/reaching_object: 1.3509
     Episode_Reward/lifting_object: 132.8705
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.60s
                      Time elapsed: 00:48:08
                               ETA: 05:24:45

################################################################################
                    [1m Learning iteration 1291/10000 [0m                     

                       Computation: 39384 steps/s (collection: 2.364s, learning 0.132s)
             Mean action noise std: 3.32
          Mean value_function loss: 412.5353
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.3449
                       Mean reward: 671.58
               Mean episode length: 194.67
    Episode_Reward/reaching_object: 1.3955
     Episode_Reward/lifting_object: 138.0290
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.50s
                      Time elapsed: 00:48:10
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1292/10000 [0m                     

                       Computation: 38085 steps/s (collection: 2.394s, learning 0.188s)
             Mean action noise std: 3.32
          Mean value_function loss: 397.7252
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.3453
                       Mean reward: 664.87
               Mean episode length: 192.23
    Episode_Reward/reaching_object: 1.3514
     Episode_Reward/lifting_object: 132.4033
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.58s
                      Time elapsed: 00:48:13
                               ETA: 05:24:44

################################################################################
                    [1m Learning iteration 1293/10000 [0m                     

                       Computation: 40754 steps/s (collection: 2.292s, learning 0.121s)
             Mean action noise std: 3.32
          Mean value_function loss: 402.9763
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 80.3474
                       Mean reward: 734.98
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 1.3932
     Episode_Reward/lifting_object: 137.5281
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.41s
                      Time elapsed: 00:48:15
                               ETA: 05:24:43

################################################################################
                    [1m Learning iteration 1294/10000 [0m                     

                       Computation: 41769 steps/s (collection: 2.253s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 375.9875
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 80.3500
                       Mean reward: 706.91
               Mean episode length: 199.90
    Episode_Reward/reaching_object: 1.3989
     Episode_Reward/lifting_object: 138.6013
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.35s
                      Time elapsed: 00:48:17
                               ETA: 05:24:42

################################################################################
                    [1m Learning iteration 1295/10000 [0m                     

                       Computation: 44536 steps/s (collection: 2.111s, learning 0.096s)
             Mean action noise std: 3.32
          Mean value_function loss: 370.0432
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.3540
                       Mean reward: 708.71
               Mean episode length: 202.67
    Episode_Reward/reaching_object: 1.4084
     Episode_Reward/lifting_object: 139.3257
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.21s
                      Time elapsed: 00:48:20
                               ETA: 05:24:39

################################################################################
                    [1m Learning iteration 1296/10000 [0m                     

                       Computation: 46806 steps/s (collection: 2.005s, learning 0.096s)
             Mean action noise std: 3.32
          Mean value_function loss: 397.8619
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.3661
                       Mean reward: 687.72
               Mean episode length: 196.84
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 137.5866
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.10s
                      Time elapsed: 00:48:22
                               ETA: 05:24:36

################################################################################
                    [1m Learning iteration 1297/10000 [0m                     

                       Computation: 45665 steps/s (collection: 2.054s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 378.4252
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.3806
                       Mean reward: 714.52
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 1.4072
     Episode_Reward/lifting_object: 139.9497
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.15s
                      Time elapsed: 00:48:24
                               ETA: 05:24:33

################################################################################
                    [1m Learning iteration 1298/10000 [0m                     

                       Computation: 46145 steps/s (collection: 2.028s, learning 0.102s)
             Mean action noise std: 3.32
          Mean value_function loss: 373.5218
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.3898
                       Mean reward: 738.83
               Mean episode length: 207.97
    Episode_Reward/reaching_object: 1.4368
     Episode_Reward/lifting_object: 143.0246
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.13s
                      Time elapsed: 00:48:26
                               ETA: 05:24:30

################################################################################
                    [1m Learning iteration 1299/10000 [0m                     

                       Computation: 45353 steps/s (collection: 2.050s, learning 0.118s)
             Mean action noise std: 3.33
          Mean value_function loss: 393.7096
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 80.3988
                       Mean reward: 760.11
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.4234
     Episode_Reward/lifting_object: 140.8140
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.17s
                      Time elapsed: 00:48:28
                               ETA: 05:24:28

################################################################################
                    [1m Learning iteration 1300/10000 [0m                     

                       Computation: 45311 steps/s (collection: 2.079s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 358.2311
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.4103
                       Mean reward: 663.08
               Mean episode length: 191.71
    Episode_Reward/reaching_object: 1.3999
     Episode_Reward/lifting_object: 138.0272
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.17s
                      Time elapsed: 00:48:30
                               ETA: 05:24:25

################################################################################
                    [1m Learning iteration 1301/10000 [0m                     

                       Computation: 43158 steps/s (collection: 2.154s, learning 0.124s)
             Mean action noise std: 3.33
          Mean value_function loss: 464.8464
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 80.4169
                       Mean reward: 656.19
               Mean episode length: 190.95
    Episode_Reward/reaching_object: 1.3658
     Episode_Reward/lifting_object: 133.7980
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.28s
                      Time elapsed: 00:48:33
                               ETA: 05:24:23

################################################################################
                    [1m Learning iteration 1302/10000 [0m                     

                       Computation: 40587 steps/s (collection: 2.331s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 388.3932
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.4180
                       Mean reward: 687.10
               Mean episode length: 198.70
    Episode_Reward/reaching_object: 1.3742
     Episode_Reward/lifting_object: 134.9218
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.42s
                      Time elapsed: 00:48:35
                               ETA: 05:24:22

################################################################################
                    [1m Learning iteration 1303/10000 [0m                     

                       Computation: 44276 steps/s (collection: 2.120s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 435.7755
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.4235
                       Mean reward: 658.92
               Mean episode length: 192.64
    Episode_Reward/reaching_object: 1.3795
     Episode_Reward/lifting_object: 134.5251
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.22s
                      Time elapsed: 00:48:37
                               ETA: 05:24:20

################################################################################
                    [1m Learning iteration 1304/10000 [0m                     

                       Computation: 44807 steps/s (collection: 2.094s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 393.4956
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 80.4434
                       Mean reward: 688.62
               Mean episode length: 195.34
    Episode_Reward/reaching_object: 1.3585
     Episode_Reward/lifting_object: 132.5456
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.19s
                      Time elapsed: 00:48:39
                               ETA: 05:24:17

################################################################################
                    [1m Learning iteration 1305/10000 [0m                     

                       Computation: 45109 steps/s (collection: 2.047s, learning 0.133s)
             Mean action noise std: 3.33
          Mean value_function loss: 476.2125
               Mean surrogate loss: 0.0265
                 Mean entropy loss: 80.4608
                       Mean reward: 657.34
               Mean episode length: 189.97
    Episode_Reward/reaching_object: 1.3128
     Episode_Reward/lifting_object: 127.2119
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.18s
                      Time elapsed: 00:48:42
                               ETA: 05:24:14

################################################################################
                    [1m Learning iteration 1306/10000 [0m                     

                       Computation: 44808 steps/s (collection: 2.053s, learning 0.141s)
             Mean action noise std: 3.33
          Mean value_function loss: 405.9584
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 80.4633
                       Mean reward: 682.90
               Mean episode length: 196.55
    Episode_Reward/reaching_object: 1.3883
     Episode_Reward/lifting_object: 136.5344
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.19s
                      Time elapsed: 00:48:44
                               ETA: 05:24:12

################################################################################
                    [1m Learning iteration 1307/10000 [0m                     

                       Computation: 45827 steps/s (collection: 2.052s, learning 0.094s)
             Mean action noise std: 3.33
          Mean value_function loss: 434.7933
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 80.4648
                       Mean reward: 647.66
               Mean episode length: 187.43
    Episode_Reward/reaching_object: 1.4011
     Episode_Reward/lifting_object: 137.3978
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.15s
                      Time elapsed: 00:48:46
                               ETA: 05:24:09

################################################################################
                    [1m Learning iteration 1308/10000 [0m                     

                       Computation: 45049 steps/s (collection: 2.088s, learning 0.094s)
             Mean action noise std: 3.33
          Mean value_function loss: 384.2365
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.4686
                       Mean reward: 709.61
               Mean episode length: 202.25
    Episode_Reward/reaching_object: 1.4055
     Episode_Reward/lifting_object: 138.2674
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.18s
                      Time elapsed: 00:48:48
                               ETA: 05:24:06

################################################################################
                    [1m Learning iteration 1309/10000 [0m                     

                       Computation: 42960 steps/s (collection: 2.173s, learning 0.115s)
             Mean action noise std: 3.34
          Mean value_function loss: 428.0972
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.4753
                       Mean reward: 684.41
               Mean episode length: 196.95
    Episode_Reward/reaching_object: 1.4170
     Episode_Reward/lifting_object: 139.7234
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.29s
                      Time elapsed: 00:48:50
                               ETA: 05:24:05

################################################################################
                    [1m Learning iteration 1310/10000 [0m                     

                       Computation: 45727 steps/s (collection: 2.048s, learning 0.102s)
             Mean action noise std: 3.34
          Mean value_function loss: 414.3288
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 80.4831
                       Mean reward: 714.65
               Mean episode length: 201.62
    Episode_Reward/reaching_object: 1.4189
     Episode_Reward/lifting_object: 140.3166
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.15s
                      Time elapsed: 00:48:53
                               ETA: 05:24:02

################################################################################
                    [1m Learning iteration 1311/10000 [0m                     

                       Computation: 44026 steps/s (collection: 2.090s, learning 0.143s)
             Mean action noise std: 3.34
          Mean value_function loss: 382.8658
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.4910
                       Mean reward: 720.45
               Mean episode length: 203.92
    Episode_Reward/reaching_object: 1.3751
     Episode_Reward/lifting_object: 135.6745
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.23s
                      Time elapsed: 00:48:55
                               ETA: 05:23:59

################################################################################
                    [1m Learning iteration 1312/10000 [0m                     

                       Computation: 44035 steps/s (collection: 2.130s, learning 0.103s)
             Mean action noise std: 3.34
          Mean value_function loss: 401.6854
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.5025
                       Mean reward: 673.81
               Mean episode length: 194.66
    Episode_Reward/reaching_object: 1.4311
     Episode_Reward/lifting_object: 141.9880
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.23s
                      Time elapsed: 00:48:57
                               ETA: 05:23:57

################################################################################
                    [1m Learning iteration 1313/10000 [0m                     

                       Computation: 45222 steps/s (collection: 2.079s, learning 0.095s)
             Mean action noise std: 3.34
          Mean value_function loss: 475.0035
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.5127
                       Mean reward: 727.76
               Mean episode length: 206.59
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 141.3791
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.17s
                      Time elapsed: 00:48:59
                               ETA: 05:23:55

################################################################################
                    [1m Learning iteration 1314/10000 [0m                     

                       Computation: 44219 steps/s (collection: 2.135s, learning 0.089s)
             Mean action noise std: 3.34
          Mean value_function loss: 397.7143
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.5250
                       Mean reward: 714.24
               Mean episode length: 203.47
    Episode_Reward/reaching_object: 1.3571
     Episode_Reward/lifting_object: 133.6527
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.22s
                      Time elapsed: 00:49:01
                               ETA: 05:23:52

################################################################################
                    [1m Learning iteration 1315/10000 [0m                     

                       Computation: 44461 steps/s (collection: 2.104s, learning 0.107s)
             Mean action noise std: 3.34
          Mean value_function loss: 389.4406
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 80.5357
                       Mean reward: 715.02
               Mean episode length: 202.56
    Episode_Reward/reaching_object: 1.4153
     Episode_Reward/lifting_object: 140.1332
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.21s
                      Time elapsed: 00:49:04
                               ETA: 05:23:50

################################################################################
                    [1m Learning iteration 1316/10000 [0m                     

                       Computation: 45949 steps/s (collection: 2.037s, learning 0.102s)
             Mean action noise std: 3.34
          Mean value_function loss: 368.2057
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.5443
                       Mean reward: 715.56
               Mean episode length: 202.62
    Episode_Reward/reaching_object: 1.4399
     Episode_Reward/lifting_object: 142.9578
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.14s
                      Time elapsed: 00:49:06
                               ETA: 05:23:47

################################################################################
                    [1m Learning iteration 1317/10000 [0m                     

                       Computation: 44715 steps/s (collection: 2.086s, learning 0.112s)
             Mean action noise std: 3.35
          Mean value_function loss: 381.2817
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.5536
                       Mean reward: 745.44
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 1.4519
     Episode_Reward/lifting_object: 144.8202
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.20s
                      Time elapsed: 00:49:08
                               ETA: 05:23:44

################################################################################
                    [1m Learning iteration 1318/10000 [0m                     

                       Computation: 45803 steps/s (collection: 2.046s, learning 0.100s)
             Mean action noise std: 3.35
          Mean value_function loss: 444.1572
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 80.5610
                       Mean reward: 710.56
               Mean episode length: 201.31
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 135.2876
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.15s
                      Time elapsed: 00:49:10
                               ETA: 05:23:42

################################################################################
                    [1m Learning iteration 1319/10000 [0m                     

                       Computation: 43325 steps/s (collection: 2.140s, learning 0.129s)
             Mean action noise std: 3.35
          Mean value_function loss: 353.1801
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.5707
                       Mean reward: 770.97
               Mean episode length: 216.20
    Episode_Reward/reaching_object: 1.4757
     Episode_Reward/lifting_object: 148.3670
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.27s
                      Time elapsed: 00:49:12
                               ETA: 05:23:40

################################################################################
                    [1m Learning iteration 1320/10000 [0m                     

                       Computation: 44864 steps/s (collection: 2.065s, learning 0.126s)
             Mean action noise std: 3.35
          Mean value_function loss: 387.7935
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.5820
                       Mean reward: 662.28
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 1.3781
     Episode_Reward/lifting_object: 135.9426
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.19s
                      Time elapsed: 00:49:15
                               ETA: 05:23:37

################################################################################
                    [1m Learning iteration 1321/10000 [0m                     

                       Computation: 43930 steps/s (collection: 2.095s, learning 0.143s)
             Mean action noise std: 3.35
          Mean value_function loss: 406.5712
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.5907
                       Mean reward: 782.91
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.4461
     Episode_Reward/lifting_object: 143.6583
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.24s
                      Time elapsed: 00:49:17
                               ETA: 05:23:35

################################################################################
                    [1m Learning iteration 1322/10000 [0m                     

                       Computation: 44258 steps/s (collection: 2.128s, learning 0.094s)
             Mean action noise std: 3.35
          Mean value_function loss: 382.0558
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.5976
                       Mean reward: 700.07
               Mean episode length: 200.59
    Episode_Reward/reaching_object: 1.3884
     Episode_Reward/lifting_object: 137.6952
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.22s
                      Time elapsed: 00:49:19
                               ETA: 05:23:32

################################################################################
                    [1m Learning iteration 1323/10000 [0m                     

                       Computation: 44242 steps/s (collection: 2.116s, learning 0.106s)
             Mean action noise std: 3.35
          Mean value_function loss: 340.1234
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 80.6075
                       Mean reward: 711.25
               Mean episode length: 200.54
    Episode_Reward/reaching_object: 1.4050
     Episode_Reward/lifting_object: 140.1373
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.22s
                      Time elapsed: 00:49:21
                               ETA: 05:23:30

################################################################################
                    [1m Learning iteration 1324/10000 [0m                     

                       Computation: 45126 steps/s (collection: 2.079s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 316.5737
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.6155
                       Mean reward: 746.72
               Mean episode length: 211.81
    Episode_Reward/reaching_object: 1.4575
     Episode_Reward/lifting_object: 146.3268
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.18s
                      Time elapsed: 00:49:23
                               ETA: 05:23:27

################################################################################
                    [1m Learning iteration 1325/10000 [0m                     

                       Computation: 44240 steps/s (collection: 2.128s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 331.5834
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.6318
                       Mean reward: 766.23
               Mean episode length: 216.28
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 142.2955
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.22s
                      Time elapsed: 00:49:26
                               ETA: 05:23:25

################################################################################
                    [1m Learning iteration 1326/10000 [0m                     

                       Computation: 43988 steps/s (collection: 2.058s, learning 0.177s)
             Mean action noise std: 3.36
          Mean value_function loss: 347.4862
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.6479
                       Mean reward: 741.82
               Mean episode length: 208.55
    Episode_Reward/reaching_object: 1.4498
     Episode_Reward/lifting_object: 144.8610
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.23s
                      Time elapsed: 00:49:28
                               ETA: 05:23:23

################################################################################
                    [1m Learning iteration 1327/10000 [0m                     

                       Computation: 45249 steps/s (collection: 2.069s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 347.2832
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 80.6609
                       Mean reward: 664.02
               Mean episode length: 193.32
    Episode_Reward/reaching_object: 1.3929
     Episode_Reward/lifting_object: 138.3032
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.17s
                      Time elapsed: 00:49:30
                               ETA: 05:23:20

################################################################################
                    [1m Learning iteration 1328/10000 [0m                     

                       Computation: 44450 steps/s (collection: 2.099s, learning 0.112s)
             Mean action noise std: 3.36
          Mean value_function loss: 395.7899
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.6695
                       Mean reward: 737.56
               Mean episode length: 206.55
    Episode_Reward/reaching_object: 1.4583
     Episode_Reward/lifting_object: 145.6892
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.21s
                      Time elapsed: 00:49:32
                               ETA: 05:23:18

################################################################################
                    [1m Learning iteration 1329/10000 [0m                     

                       Computation: 46290 steps/s (collection: 2.028s, learning 0.095s)
             Mean action noise std: 3.36
          Mean value_function loss: 376.1744
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.6764
                       Mean reward: 645.91
               Mean episode length: 188.75
    Episode_Reward/reaching_object: 1.3923
     Episode_Reward/lifting_object: 138.0302
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.12s
                      Time elapsed: 00:49:34
                               ETA: 05:23:15

################################################################################
                    [1m Learning iteration 1330/10000 [0m                     

                       Computation: 45452 steps/s (collection: 2.056s, learning 0.107s)
             Mean action noise std: 3.36
          Mean value_function loss: 374.7495
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.6871
                       Mean reward: 653.34
               Mean episode length: 186.76
    Episode_Reward/reaching_object: 1.4210
     Episode_Reward/lifting_object: 141.6012
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.16s
                      Time elapsed: 00:49:37
                               ETA: 05:23:12

################################################################################
                    [1m Learning iteration 1331/10000 [0m                     

                       Computation: 46380 steps/s (collection: 2.027s, learning 0.093s)
             Mean action noise std: 3.36
          Mean value_function loss: 334.8912
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.6979
                       Mean reward: 725.75
               Mean episode length: 204.79
    Episode_Reward/reaching_object: 1.4083
     Episode_Reward/lifting_object: 141.1919
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.12s
                      Time elapsed: 00:49:39
                               ETA: 05:23:09

################################################################################
                    [1m Learning iteration 1332/10000 [0m                     

                       Computation: 43696 steps/s (collection: 2.108s, learning 0.142s)
             Mean action noise std: 3.37
          Mean value_function loss: 354.5495
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.7094
                       Mean reward: 719.95
               Mean episode length: 205.48
    Episode_Reward/reaching_object: 1.4386
     Episode_Reward/lifting_object: 144.2929
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.25s
                      Time elapsed: 00:49:41
                               ETA: 05:23:07

################################################################################
                    [1m Learning iteration 1333/10000 [0m                     

                       Computation: 18168 steps/s (collection: 5.287s, learning 0.124s)
             Mean action noise std: 3.37
          Mean value_function loss: 344.5398
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 80.7229
                       Mean reward: 733.62
               Mean episode length: 207.68
    Episode_Reward/reaching_object: 1.4786
     Episode_Reward/lifting_object: 148.5076
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.41s
                      Time elapsed: 00:49:46
                               ETA: 05:23:25

################################################################################
                    [1m Learning iteration 1334/10000 [0m                     

                       Computation: 13646 steps/s (collection: 7.068s, learning 0.136s)
             Mean action noise std: 3.37
          Mean value_function loss: 360.5542
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 80.7266
                       Mean reward: 713.26
               Mean episode length: 201.65
    Episode_Reward/reaching_object: 1.3871
     Episode_Reward/lifting_object: 138.3732
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.20s
                      Time elapsed: 00:49:54
                               ETA: 05:23:55

################################################################################
                    [1m Learning iteration 1335/10000 [0m                     

                       Computation: 13765 steps/s (collection: 7.022s, learning 0.119s)
             Mean action noise std: 3.37
          Mean value_function loss: 361.4790
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 80.7292
                       Mean reward: 679.04
               Mean episode length: 194.73
    Episode_Reward/reaching_object: 1.3727
     Episode_Reward/lifting_object: 136.8128
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.14s
                      Time elapsed: 00:50:01
                               ETA: 05:24:25

################################################################################
                    [1m Learning iteration 1336/10000 [0m                     

                       Computation: 14134 steps/s (collection: 6.824s, learning 0.132s)
             Mean action noise std: 3.37
          Mean value_function loss: 368.7874
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 80.7305
                       Mean reward: 696.17
               Mean episode length: 197.83
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 138.1602
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.96s
                      Time elapsed: 00:50:08
                               ETA: 05:24:53

################################################################################
                    [1m Learning iteration 1337/10000 [0m                     

                       Computation: 13951 steps/s (collection: 6.897s, learning 0.149s)
             Mean action noise std: 3.37
          Mean value_function loss: 386.5008
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.7323
                       Mean reward: 725.74
               Mean episode length: 206.34
    Episode_Reward/reaching_object: 1.3809
     Episode_Reward/lifting_object: 137.5509
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.05s
                      Time elapsed: 00:50:15
                               ETA: 05:25:22

################################################################################
                    [1m Learning iteration 1338/10000 [0m                     

                       Computation: 13841 steps/s (collection: 6.967s, learning 0.135s)
             Mean action noise std: 3.37
          Mean value_function loss: 408.8654
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.7383
                       Mean reward: 656.94
               Mean episode length: 190.26
    Episode_Reward/reaching_object: 1.4132
     Episode_Reward/lifting_object: 140.8501
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.10s
                      Time elapsed: 00:50:22
                               ETA: 05:25:51

################################################################################
                    [1m Learning iteration 1339/10000 [0m                     

                       Computation: 14020 steps/s (collection: 6.905s, learning 0.107s)
             Mean action noise std: 3.37
          Mean value_function loss: 424.2133
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.7490
                       Mean reward: 653.17
               Mean episode length: 192.38
    Episode_Reward/reaching_object: 1.3681
     Episode_Reward/lifting_object: 135.3769
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.01s
                      Time elapsed: 00:50:29
                               ETA: 05:26:20

################################################################################
                    [1m Learning iteration 1340/10000 [0m                     

                       Computation: 14074 steps/s (collection: 6.855s, learning 0.130s)
             Mean action noise std: 3.37
          Mean value_function loss: 431.8397
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.7606
                       Mean reward: 684.14
               Mean episode length: 198.05
    Episode_Reward/reaching_object: 1.3841
     Episode_Reward/lifting_object: 137.3655
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.98s
                      Time elapsed: 00:50:36
                               ETA: 05:26:48

################################################################################
                    [1m Learning iteration 1341/10000 [0m                     

                       Computation: 12788 steps/s (collection: 7.524s, learning 0.163s)
             Mean action noise std: 3.37
          Mean value_function loss: 461.0032
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.7677
                       Mean reward: 637.24
               Mean episode length: 188.01
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 132.7873
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.69s
                      Time elapsed: 00:50:44
                               ETA: 05:27:21

################################################################################
                    [1m Learning iteration 1342/10000 [0m                     

                       Computation: 46181 steps/s (collection: 2.016s, learning 0.113s)
             Mean action noise std: 3.38
          Mean value_function loss: 422.5268
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.7762
                       Mean reward: 702.78
               Mean episode length: 201.87
    Episode_Reward/reaching_object: 1.3502
     Episode_Reward/lifting_object: 133.7411
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.13s
                      Time elapsed: 00:50:46
                               ETA: 05:27:17

################################################################################
                    [1m Learning iteration 1343/10000 [0m                     

                       Computation: 45766 steps/s (collection: 2.039s, learning 0.109s)
             Mean action noise std: 3.38
          Mean value_function loss: 464.0461
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.7837
                       Mean reward: 609.15
               Mean episode length: 180.83
    Episode_Reward/reaching_object: 1.2727
     Episode_Reward/lifting_object: 124.6748
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.15s
                      Time elapsed: 00:50:48
                               ETA: 05:27:14

################################################################################
                    [1m Learning iteration 1344/10000 [0m                     

                       Computation: 42256 steps/s (collection: 2.227s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 461.6860
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.7924
                       Mean reward: 640.42
               Mean episode length: 186.11
    Episode_Reward/reaching_object: 1.3198
     Episode_Reward/lifting_object: 130.2266
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.33s
                      Time elapsed: 00:50:50
                               ETA: 05:27:12

################################################################################
                    [1m Learning iteration 1345/10000 [0m                     

                       Computation: 46139 steps/s (collection: 2.029s, learning 0.102s)
             Mean action noise std: 3.38
          Mean value_function loss: 425.7396
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.8015
                       Mean reward: 635.22
               Mean episode length: 184.41
    Episode_Reward/reaching_object: 1.3061
     Episode_Reward/lifting_object: 128.6943
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.13s
                      Time elapsed: 00:50:52
                               ETA: 05:27:09

################################################################################
                    [1m Learning iteration 1346/10000 [0m                     

                       Computation: 46296 steps/s (collection: 1.997s, learning 0.126s)
             Mean action noise std: 3.38
          Mean value_function loss: 393.2388
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.8078
                       Mean reward: 677.50
               Mean episode length: 196.06
    Episode_Reward/reaching_object: 1.3931
     Episode_Reward/lifting_object: 137.5259
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.12s
                      Time elapsed: 00:50:54
                               ETA: 05:27:06

################################################################################
                    [1m Learning iteration 1347/10000 [0m                     

                       Computation: 46254 steps/s (collection: 1.995s, learning 0.130s)
             Mean action noise std: 3.38
          Mean value_function loss: 449.2153
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.8146
                       Mean reward: 583.16
               Mean episode length: 173.77
    Episode_Reward/reaching_object: 1.2520
     Episode_Reward/lifting_object: 121.3646
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.13s
                      Time elapsed: 00:50:57
                               ETA: 05:27:03

################################################################################
                    [1m Learning iteration 1348/10000 [0m                     

                       Computation: 45600 steps/s (collection: 2.067s, learning 0.089s)
             Mean action noise std: 3.38
          Mean value_function loss: 481.7877
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 80.8242
                       Mean reward: 708.72
               Mean episode length: 200.31
    Episode_Reward/reaching_object: 1.3312
     Episode_Reward/lifting_object: 130.7359
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.16s
                      Time elapsed: 00:50:59
                               ETA: 05:27:00

################################################################################
                    [1m Learning iteration 1349/10000 [0m                     

                       Computation: 45291 steps/s (collection: 2.048s, learning 0.122s)
             Mean action noise std: 3.38
          Mean value_function loss: 456.2052
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.8309
                       Mean reward: 667.64
               Mean episode length: 190.91
    Episode_Reward/reaching_object: 1.3133
     Episode_Reward/lifting_object: 129.4477
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.17s
                      Time elapsed: 00:51:01
                               ETA: 05:26:57

################################################################################
                    [1m Learning iteration 1350/10000 [0m                     

                       Computation: 46894 steps/s (collection: 1.990s, learning 0.107s)
             Mean action noise std: 3.38
          Mean value_function loss: 381.6210
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.8407
                       Mean reward: 648.14
               Mean episode length: 190.00
    Episode_Reward/reaching_object: 1.3732
     Episode_Reward/lifting_object: 136.2495
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.10s
                      Time elapsed: 00:51:03
                               ETA: 05:26:54

################################################################################
                    [1m Learning iteration 1351/10000 [0m                     

                       Computation: 46598 steps/s (collection: 2.011s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 390.1199
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 80.8515
                       Mean reward: 642.17
               Mean episode length: 186.17
    Episode_Reward/reaching_object: 1.3360
     Episode_Reward/lifting_object: 131.5376
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.11s
                      Time elapsed: 00:51:05
                               ETA: 05:26:50

################################################################################
                    [1m Learning iteration 1352/10000 [0m                     

                       Computation: 46032 steps/s (collection: 2.034s, learning 0.102s)
             Mean action noise std: 3.39
          Mean value_function loss: 386.0627
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.8571
                       Mean reward: 720.43
               Mean episode length: 205.90
    Episode_Reward/reaching_object: 1.3688
     Episode_Reward/lifting_object: 135.4128
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.14s
                      Time elapsed: 00:51:07
                               ETA: 05:26:47

################################################################################
                    [1m Learning iteration 1353/10000 [0m                     

                       Computation: 45819 steps/s (collection: 2.044s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 373.5361
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.8717
                       Mean reward: 695.27
               Mean episode length: 199.46
    Episode_Reward/reaching_object: 1.4042
     Episode_Reward/lifting_object: 140.1023
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.15s
                      Time elapsed: 00:51:09
                               ETA: 05:26:44

################################################################################
                    [1m Learning iteration 1354/10000 [0m                     

                       Computation: 43310 steps/s (collection: 2.108s, learning 0.162s)
             Mean action noise std: 3.39
          Mean value_function loss: 370.2198
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.8867
                       Mean reward: 701.22
               Mean episode length: 200.91
    Episode_Reward/reaching_object: 1.4478
     Episode_Reward/lifting_object: 144.7609
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.27s
                      Time elapsed: 00:51:12
                               ETA: 05:26:42

################################################################################
                    [1m Learning iteration 1355/10000 [0m                     

                       Computation: 44542 steps/s (collection: 2.090s, learning 0.117s)
             Mean action noise std: 3.39
          Mean value_function loss: 347.4022
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.9012
                       Mean reward: 782.83
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.4433
     Episode_Reward/lifting_object: 144.4463
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.21s
                      Time elapsed: 00:51:14
                               ETA: 05:26:39

################################################################################
                    [1m Learning iteration 1356/10000 [0m                     

                       Computation: 45829 steps/s (collection: 2.054s, learning 0.091s)
             Mean action noise std: 3.39
          Mean value_function loss: 349.4286
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.9202
                       Mean reward: 656.84
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 1.3944
     Episode_Reward/lifting_object: 138.8447
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.14s
                      Time elapsed: 00:51:16
                               ETA: 05:26:36

################################################################################
                    [1m Learning iteration 1357/10000 [0m                     

                       Computation: 45746 steps/s (collection: 2.039s, learning 0.110s)
             Mean action noise std: 3.40
          Mean value_function loss: 395.6212
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.9339
                       Mean reward: 702.65
               Mean episode length: 202.75
    Episode_Reward/reaching_object: 1.3908
     Episode_Reward/lifting_object: 137.4105
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.15s
                      Time elapsed: 00:51:18
                               ETA: 05:26:33

################################################################################
                    [1m Learning iteration 1358/10000 [0m                     

                       Computation: 45383 steps/s (collection: 2.055s, learning 0.111s)
             Mean action noise std: 3.40
          Mean value_function loss: 379.2586
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.9439
                       Mean reward: 664.11
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 1.3847
     Episode_Reward/lifting_object: 137.7464
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.17s
                      Time elapsed: 00:51:20
                               ETA: 05:26:30

################################################################################
                    [1m Learning iteration 1359/10000 [0m                     

                       Computation: 44814 steps/s (collection: 2.070s, learning 0.124s)
             Mean action noise std: 3.40
          Mean value_function loss: 393.5524
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.9585
                       Mean reward: 716.25
               Mean episode length: 202.67
    Episode_Reward/reaching_object: 1.4428
     Episode_Reward/lifting_object: 144.1117
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.19s
                      Time elapsed: 00:51:22
                               ETA: 05:26:28

################################################################################
                    [1m Learning iteration 1360/10000 [0m                     

                       Computation: 43521 steps/s (collection: 2.145s, learning 0.114s)
             Mean action noise std: 3.40
          Mean value_function loss: 358.7263
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.9680
                       Mean reward: 701.49
               Mean episode length: 201.33
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 141.0322
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.26s
                      Time elapsed: 00:51:25
                               ETA: 05:26:25

################################################################################
                    [1m Learning iteration 1361/10000 [0m                     

                       Computation: 45788 steps/s (collection: 2.030s, learning 0.117s)
             Mean action noise std: 3.40
          Mean value_function loss: 366.7837
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 80.9801
                       Mean reward: 698.83
               Mean episode length: 200.86
    Episode_Reward/reaching_object: 1.4072
     Episode_Reward/lifting_object: 140.4328
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.15s
                      Time elapsed: 00:51:27
                               ETA: 05:26:22

################################################################################
                    [1m Learning iteration 1362/10000 [0m                     

                       Computation: 46046 steps/s (collection: 2.032s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 391.3279
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.9993
                       Mean reward: 702.15
               Mean episode length: 201.41
    Episode_Reward/reaching_object: 1.3736
     Episode_Reward/lifting_object: 136.5424
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.13s
                      Time elapsed: 00:51:29
                               ETA: 05:26:19

################################################################################
                    [1m Learning iteration 1363/10000 [0m                     

                       Computation: 42689 steps/s (collection: 2.192s, learning 0.111s)
             Mean action noise std: 3.41
          Mean value_function loss: 352.3623
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.0172
                       Mean reward: 676.36
               Mean episode length: 195.83
    Episode_Reward/reaching_object: 1.3929
     Episode_Reward/lifting_object: 138.2397
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.30s
                      Time elapsed: 00:51:31
                               ETA: 05:26:17

################################################################################
                    [1m Learning iteration 1364/10000 [0m                     

                       Computation: 44378 steps/s (collection: 2.064s, learning 0.151s)
             Mean action noise std: 3.41
          Mean value_function loss: 334.7035
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.0309
                       Mean reward: 694.02
               Mean episode length: 196.99
    Episode_Reward/reaching_object: 1.3651
     Episode_Reward/lifting_object: 136.0737
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.22s
                      Time elapsed: 00:51:34
                               ETA: 05:26:15

################################################################################
                    [1m Learning iteration 1365/10000 [0m                     

                       Computation: 44972 steps/s (collection: 2.060s, learning 0.126s)
             Mean action noise std: 3.41
          Mean value_function loss: 293.1383
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.0444
                       Mean reward: 716.18
               Mean episode length: 202.39
    Episode_Reward/reaching_object: 1.4272
     Episode_Reward/lifting_object: 143.5113
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.19s
                      Time elapsed: 00:51:36
                               ETA: 05:26:12

################################################################################
                    [1m Learning iteration 1366/10000 [0m                     

                       Computation: 46660 steps/s (collection: 2.013s, learning 0.094s)
             Mean action noise std: 3.42
          Mean value_function loss: 327.1914
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.0704
                       Mean reward: 729.98
               Mean episode length: 206.06
    Episode_Reward/reaching_object: 1.4473
     Episode_Reward/lifting_object: 145.1136
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.11s
                      Time elapsed: 00:51:38
                               ETA: 05:26:08

################################################################################
                    [1m Learning iteration 1367/10000 [0m                     

                       Computation: 46431 steps/s (collection: 2.024s, learning 0.093s)
             Mean action noise std: 3.42
          Mean value_function loss: 319.1019
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 81.0921
                       Mean reward: 705.89
               Mean episode length: 202.73
    Episode_Reward/reaching_object: 1.4152
     Episode_Reward/lifting_object: 142.0736
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.12s
                      Time elapsed: 00:51:40
                               ETA: 05:26:05

################################################################################
                    [1m Learning iteration 1368/10000 [0m                     

                       Computation: 46379 steps/s (collection: 2.017s, learning 0.102s)
             Mean action noise std: 3.42
          Mean value_function loss: 328.6906
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.0965
                       Mean reward: 759.38
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 1.4283
     Episode_Reward/lifting_object: 143.1946
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.12s
                      Time elapsed: 00:51:42
                               ETA: 05:26:02

################################################################################
                    [1m Learning iteration 1369/10000 [0m                     

                       Computation: 44614 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 3.42
          Mean value_function loss: 307.2875
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.1061
                       Mean reward: 768.17
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.4605
     Episode_Reward/lifting_object: 147.5912
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.20s
                      Time elapsed: 00:51:44
                               ETA: 05:25:59

################################################################################
                    [1m Learning iteration 1370/10000 [0m                     

                       Computation: 40558 steps/s (collection: 2.263s, learning 0.161s)
             Mean action noise std: 3.42
          Mean value_function loss: 367.0080
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.1219
                       Mean reward: 741.49
               Mean episode length: 206.80
    Episode_Reward/reaching_object: 1.4441
     Episode_Reward/lifting_object: 144.6572
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.42s
                      Time elapsed: 00:51:47
                               ETA: 05:25:58

################################################################################
                    [1m Learning iteration 1371/10000 [0m                     

                       Computation: 42106 steps/s (collection: 2.181s, learning 0.153s)
             Mean action noise std: 3.42
          Mean value_function loss: 444.4834
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.1409
                       Mean reward: 709.87
               Mean episode length: 200.21
    Episode_Reward/reaching_object: 1.4097
     Episode_Reward/lifting_object: 140.3280
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.33s
                      Time elapsed: 00:51:49
                               ETA: 05:25:56

################################################################################
                    [1m Learning iteration 1372/10000 [0m                     

                       Computation: 36922 steps/s (collection: 2.563s, learning 0.099s)
             Mean action noise std: 3.42
          Mean value_function loss: 380.0547
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.1533
                       Mean reward: 700.44
               Mean episode length: 201.67
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 139.7209
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.66s
                      Time elapsed: 00:51:52
                               ETA: 05:25:56

################################################################################
                    [1m Learning iteration 1373/10000 [0m                     

                       Computation: 45110 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 3.43
          Mean value_function loss: 359.7358
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.1697
                       Mean reward: 734.50
               Mean episode length: 205.86
    Episode_Reward/reaching_object: 1.4267
     Episode_Reward/lifting_object: 142.9676
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.18s
                      Time elapsed: 00:51:54
                               ETA: 05:25:54

################################################################################
                    [1m Learning iteration 1374/10000 [0m                     

                       Computation: 44792 steps/s (collection: 2.082s, learning 0.112s)
             Mean action noise std: 3.43
          Mean value_function loss: 372.8992
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.1809
                       Mean reward: 668.95
               Mean episode length: 191.21
    Episode_Reward/reaching_object: 1.3966
     Episode_Reward/lifting_object: 138.9594
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.19s
                      Time elapsed: 00:51:56
                               ETA: 05:25:51

################################################################################
                    [1m Learning iteration 1375/10000 [0m                     

                       Computation: 43419 steps/s (collection: 2.154s, learning 0.111s)
             Mean action noise std: 3.43
          Mean value_function loss: 424.9267
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 81.1886
                       Mean reward: 712.94
               Mean episode length: 201.11
    Episode_Reward/reaching_object: 1.3903
     Episode_Reward/lifting_object: 138.5979
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.26s
                      Time elapsed: 00:51:58
                               ETA: 05:25:49

################################################################################
                    [1m Learning iteration 1376/10000 [0m                     

                       Computation: 45636 steps/s (collection: 2.064s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 319.1020
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.2019
                       Mean reward: 742.73
               Mean episode length: 208.41
    Episode_Reward/reaching_object: 1.3964
     Episode_Reward/lifting_object: 139.2335
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.15s
                      Time elapsed: 00:52:00
                               ETA: 05:25:46

################################################################################
                    [1m Learning iteration 1377/10000 [0m                     

                       Computation: 44196 steps/s (collection: 2.114s, learning 0.110s)
             Mean action noise std: 3.43
          Mean value_function loss: 367.4133
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 81.2162
                       Mean reward: 677.02
               Mean episode length: 195.21
    Episode_Reward/reaching_object: 1.4424
     Episode_Reward/lifting_object: 144.3965
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.22s
                      Time elapsed: 00:52:03
                               ETA: 05:25:43

################################################################################
                    [1m Learning iteration 1378/10000 [0m                     

                       Computation: 38783 steps/s (collection: 2.399s, learning 0.136s)
             Mean action noise std: 3.43
          Mean value_function loss: 335.5155
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.2292
                       Mean reward: 729.01
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 1.4856
     Episode_Reward/lifting_object: 148.4505
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.53s
                      Time elapsed: 00:52:05
                               ETA: 05:25:43

################################################################################
                    [1m Learning iteration 1379/10000 [0m                     

                       Computation: 38601 steps/s (collection: 2.389s, learning 0.158s)
             Mean action noise std: 3.44
          Mean value_function loss: 316.6381
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.2459
                       Mean reward: 804.80
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 1.4652
     Episode_Reward/lifting_object: 146.9043
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.55s
                      Time elapsed: 00:52:08
                               ETA: 05:25:42

################################################################################
                    [1m Learning iteration 1380/10000 [0m                     

                       Computation: 45953 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 3.44
          Mean value_function loss: 329.4588
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.2612
                       Mean reward: 744.71
               Mean episode length: 208.15
    Episode_Reward/reaching_object: 1.4380
     Episode_Reward/lifting_object: 144.1526
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.14s
                      Time elapsed: 00:52:10
                               ETA: 05:25:39

################################################################################
                    [1m Learning iteration 1381/10000 [0m                     

                       Computation: 47231 steps/s (collection: 1.988s, learning 0.093s)
             Mean action noise std: 3.44
          Mean value_function loss: 287.3833
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.2756
                       Mean reward: 748.28
               Mean episode length: 208.91
    Episode_Reward/reaching_object: 1.4647
     Episode_Reward/lifting_object: 147.4167
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.08s
                      Time elapsed: 00:52:12
                               ETA: 05:25:36

################################################################################
                    [1m Learning iteration 1382/10000 [0m                     

                       Computation: 46230 steps/s (collection: 2.035s, learning 0.091s)
             Mean action noise std: 3.44
          Mean value_function loss: 320.0956
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.2855
                       Mean reward: 704.80
               Mean episode length: 200.69
    Episode_Reward/reaching_object: 1.5098
     Episode_Reward/lifting_object: 152.9822
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.13s
                      Time elapsed: 00:52:14
                               ETA: 05:25:32

################################################################################
                    [1m Learning iteration 1383/10000 [0m                     

                       Computation: 44124 steps/s (collection: 2.140s, learning 0.088s)
             Mean action noise std: 3.44
          Mean value_function loss: 323.3517
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.3004
                       Mean reward: 716.29
               Mean episode length: 203.79
    Episode_Reward/reaching_object: 1.4876
     Episode_Reward/lifting_object: 150.0417
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.23s
                      Time elapsed: 00:52:16
                               ETA: 05:25:30

################################################################################
                    [1m Learning iteration 1384/10000 [0m                     

                       Computation: 46078 steps/s (collection: 2.045s, learning 0.089s)
             Mean action noise std: 3.45
          Mean value_function loss: 354.2256
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.3223
                       Mean reward: 728.46
               Mean episode length: 207.05
    Episode_Reward/reaching_object: 1.4445
     Episode_Reward/lifting_object: 145.1076
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.13s
                      Time elapsed: 00:52:18
                               ETA: 05:25:27

################################################################################
                    [1m Learning iteration 1385/10000 [0m                     

                       Computation: 46872 steps/s (collection: 1.998s, learning 0.099s)
             Mean action noise std: 3.45
          Mean value_function loss: 352.3446
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.3397
                       Mean reward: 770.15
               Mean episode length: 213.82
    Episode_Reward/reaching_object: 1.4808
     Episode_Reward/lifting_object: 149.4872
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.10s
                      Time elapsed: 00:52:21
                               ETA: 05:25:24

################################################################################
                    [1m Learning iteration 1386/10000 [0m                     

                       Computation: 47168 steps/s (collection: 1.994s, learning 0.090s)
             Mean action noise std: 3.45
          Mean value_function loss: 308.8124
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.3547
                       Mean reward: 699.13
               Mean episode length: 196.59
    Episode_Reward/reaching_object: 1.3873
     Episode_Reward/lifting_object: 138.8177
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.08s
                      Time elapsed: 00:52:23
                               ETA: 05:25:20

################################################################################
                    [1m Learning iteration 1387/10000 [0m                     

                       Computation: 45465 steps/s (collection: 2.062s, learning 0.101s)
             Mean action noise std: 3.45
          Mean value_function loss: 297.1796
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.3656
                       Mean reward: 711.99
               Mean episode length: 204.01
    Episode_Reward/reaching_object: 1.4389
     Episode_Reward/lifting_object: 144.6559
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.16s
                      Time elapsed: 00:52:25
                               ETA: 05:25:17

################################################################################
                    [1m Learning iteration 1388/10000 [0m                     

                       Computation: 47347 steps/s (collection: 1.981s, learning 0.095s)
             Mean action noise std: 3.45
          Mean value_function loss: 305.1915
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.3749
                       Mean reward: 750.79
               Mean episode length: 209.14
    Episode_Reward/reaching_object: 1.4381
     Episode_Reward/lifting_object: 144.8252
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.08s
                      Time elapsed: 00:52:27
                               ETA: 05:25:14

################################################################################
                    [1m Learning iteration 1389/10000 [0m                     

                       Computation: 46978 steps/s (collection: 2.003s, learning 0.089s)
             Mean action noise std: 3.45
          Mean value_function loss: 335.3127
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.3887
                       Mean reward: 749.24
               Mean episode length: 211.33
    Episode_Reward/reaching_object: 1.4916
     Episode_Reward/lifting_object: 151.1351
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.09s
                      Time elapsed: 00:52:29
                               ETA: 05:25:10

################################################################################
                    [1m Learning iteration 1390/10000 [0m                     

                       Computation: 47132 steps/s (collection: 1.985s, learning 0.101s)
             Mean action noise std: 3.46
          Mean value_function loss: 346.0911
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.4081
                       Mean reward: 692.00
               Mean episode length: 196.60
    Episode_Reward/reaching_object: 1.4348
     Episode_Reward/lifting_object: 144.3374
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.09s
                      Time elapsed: 00:52:31
                               ETA: 05:25:07

################################################################################
                    [1m Learning iteration 1391/10000 [0m                     

                       Computation: 46747 steps/s (collection: 2.007s, learning 0.096s)
             Mean action noise std: 3.46
          Mean value_function loss: 311.3309
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.4326
                       Mean reward: 767.09
               Mean episode length: 213.68
    Episode_Reward/reaching_object: 1.4731
     Episode_Reward/lifting_object: 149.3464
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.10s
                      Time elapsed: 00:52:33
                               ETA: 05:25:04

################################################################################
                    [1m Learning iteration 1392/10000 [0m                     

                       Computation: 47083 steps/s (collection: 1.988s, learning 0.100s)
             Mean action noise std: 3.46
          Mean value_function loss: 284.1620
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.4570
                       Mean reward: 760.29
               Mean episode length: 213.47
    Episode_Reward/reaching_object: 1.4782
     Episode_Reward/lifting_object: 148.6166
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.09s
                      Time elapsed: 00:52:35
                               ETA: 05:25:00

################################################################################
                    [1m Learning iteration 1393/10000 [0m                     

                       Computation: 44707 steps/s (collection: 2.094s, learning 0.105s)
             Mean action noise std: 3.47
          Mean value_function loss: 319.1103
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.4723
                       Mean reward: 749.55
               Mean episode length: 211.36
    Episode_Reward/reaching_object: 1.4730
     Episode_Reward/lifting_object: 148.5451
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.20s
                      Time elapsed: 00:52:37
                               ETA: 05:24:58

################################################################################
                    [1m Learning iteration 1394/10000 [0m                     

                       Computation: 46383 steps/s (collection: 2.022s, learning 0.098s)
             Mean action noise std: 3.47
          Mean value_function loss: 307.5240
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.4910
                       Mean reward: 742.93
               Mean episode length: 207.70
    Episode_Reward/reaching_object: 1.4762
     Episode_Reward/lifting_object: 149.0362
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.12s
                      Time elapsed: 00:52:40
                               ETA: 05:24:55

################################################################################
                    [1m Learning iteration 1395/10000 [0m                     

                       Computation: 45864 steps/s (collection: 2.048s, learning 0.096s)
             Mean action noise std: 3.47
          Mean value_function loss: 336.0451
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.5041
                       Mean reward: 761.51
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 1.4691
     Episode_Reward/lifting_object: 148.2173
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.14s
                      Time elapsed: 00:52:42
                               ETA: 05:24:52

################################################################################
                    [1m Learning iteration 1396/10000 [0m                     

                       Computation: 46056 steps/s (collection: 2.040s, learning 0.095s)
             Mean action noise std: 3.47
          Mean value_function loss: 343.4071
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.5168
                       Mean reward: 711.87
               Mean episode length: 201.73
    Episode_Reward/reaching_object: 1.4777
     Episode_Reward/lifting_object: 149.8472
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.13s
                      Time elapsed: 00:52:44
                               ETA: 05:24:48

################################################################################
                    [1m Learning iteration 1397/10000 [0m                     

                       Computation: 46087 steps/s (collection: 2.029s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 289.2381
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.5367
                       Mean reward: 739.70
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.4339
     Episode_Reward/lifting_object: 144.3975
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.13s
                      Time elapsed: 00:52:46
                               ETA: 05:24:45

################################################################################
                    [1m Learning iteration 1398/10000 [0m                     

                       Computation: 46277 steps/s (collection: 2.031s, learning 0.093s)
             Mean action noise std: 3.48
          Mean value_function loss: 295.9987
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.5502
                       Mean reward: 705.41
               Mean episode length: 199.74
    Episode_Reward/reaching_object: 1.4515
     Episode_Reward/lifting_object: 146.5669
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.12s
                      Time elapsed: 00:52:48
                               ETA: 05:24:42

################################################################################
                    [1m Learning iteration 1399/10000 [0m                     

                       Computation: 46903 steps/s (collection: 2.004s, learning 0.092s)
             Mean action noise std: 3.48
          Mean value_function loss: 317.9697
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.5662
                       Mean reward: 774.88
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 1.4541
     Episode_Reward/lifting_object: 146.9201
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.10s
                      Time elapsed: 00:52:50
                               ETA: 05:24:39

################################################################################
                    [1m Learning iteration 1400/10000 [0m                     

                       Computation: 47070 steps/s (collection: 1.999s, learning 0.089s)
             Mean action noise std: 3.48
          Mean value_function loss: 301.4067
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.5834
                       Mean reward: 742.50
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 1.3992
     Episode_Reward/lifting_object: 140.3922
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.09s
                      Time elapsed: 00:52:52
                               ETA: 05:24:36

################################################################################
                    [1m Learning iteration 1401/10000 [0m                     

                       Computation: 46014 steps/s (collection: 2.035s, learning 0.101s)
             Mean action noise std: 3.48
          Mean value_function loss: 314.0814
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.5970
                       Mean reward: 789.17
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.4663
     Episode_Reward/lifting_object: 148.1560
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.14s
                      Time elapsed: 00:52:54
                               ETA: 05:24:33

################################################################################
                    [1m Learning iteration 1402/10000 [0m                     

                       Computation: 46998 steps/s (collection: 2.001s, learning 0.091s)
             Mean action noise std: 3.48
          Mean value_function loss: 285.7644
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.6071
                       Mean reward: 786.92
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 1.4319
     Episode_Reward/lifting_object: 144.8688
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.09s
                      Time elapsed: 00:52:57
                               ETA: 05:24:29

################################################################################
                    [1m Learning iteration 1403/10000 [0m                     

                       Computation: 45188 steps/s (collection: 2.048s, learning 0.128s)
             Mean action noise std: 3.48
          Mean value_function loss: 309.2373
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.6184
                       Mean reward: 753.33
               Mean episode length: 211.89
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 140.7840
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.18s
                      Time elapsed: 00:52:59
                               ETA: 05:24:26

################################################################################
                    [1m Learning iteration 1404/10000 [0m                     

                       Computation: 42909 steps/s (collection: 2.110s, learning 0.181s)
             Mean action noise std: 3.49
          Mean value_function loss: 311.2651
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.6316
                       Mean reward: 783.04
               Mean episode length: 217.29
    Episode_Reward/reaching_object: 1.4895
     Episode_Reward/lifting_object: 151.0370
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.29s
                      Time elapsed: 00:53:01
                               ETA: 05:24:24

################################################################################
                    [1m Learning iteration 1405/10000 [0m                     

                       Computation: 42530 steps/s (collection: 2.203s, learning 0.109s)
             Mean action noise std: 3.49
          Mean value_function loss: 317.0862
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.6507
                       Mean reward: 795.43
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 1.4664
     Episode_Reward/lifting_object: 148.6377
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.31s
                      Time elapsed: 00:53:03
                               ETA: 05:24:22

################################################################################
                    [1m Learning iteration 1406/10000 [0m                     

                       Computation: 44685 steps/s (collection: 2.084s, learning 0.116s)
             Mean action noise std: 3.49
          Mean value_function loss: 285.2231
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.6700
                       Mean reward: 729.24
               Mean episode length: 204.15
    Episode_Reward/reaching_object: 1.4234
     Episode_Reward/lifting_object: 143.6714
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.20s
                      Time elapsed: 00:53:06
                               ETA: 05:24:20

################################################################################
                    [1m Learning iteration 1407/10000 [0m                     

                       Computation: 43371 steps/s (collection: 2.143s, learning 0.123s)
             Mean action noise std: 3.49
          Mean value_function loss: 284.9188
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6893
                       Mean reward: 754.23
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 1.3591
     Episode_Reward/lifting_object: 135.5616
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.27s
                      Time elapsed: 00:53:08
                               ETA: 05:24:17

################################################################################
                    [1m Learning iteration 1408/10000 [0m                     

                       Computation: 46247 steps/s (collection: 2.028s, learning 0.098s)
             Mean action noise std: 3.49
          Mean value_function loss: 306.6759
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.7019
                       Mean reward: 676.45
               Mean episode length: 192.53
    Episode_Reward/reaching_object: 1.4230
     Episode_Reward/lifting_object: 143.2891
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.13s
                      Time elapsed: 00:53:10
                               ETA: 05:24:14

################################################################################
                    [1m Learning iteration 1409/10000 [0m                     

                       Computation: 46322 steps/s (collection: 2.015s, learning 0.107s)
             Mean action noise std: 3.50
          Mean value_function loss: 341.2874
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.7054
                       Mean reward: 757.77
               Mean episode length: 210.26
    Episode_Reward/reaching_object: 1.4307
     Episode_Reward/lifting_object: 143.6278
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.12s
                      Time elapsed: 00:53:12
                               ETA: 05:24:11

################################################################################
                    [1m Learning iteration 1410/10000 [0m                     

                       Computation: 46196 steps/s (collection: 2.034s, learning 0.094s)
             Mean action noise std: 3.50
          Mean value_function loss: 268.7492
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.7139
                       Mean reward: 750.87
               Mean episode length: 209.98
    Episode_Reward/reaching_object: 1.4544
     Episode_Reward/lifting_object: 145.9579
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.13s
                      Time elapsed: 00:53:14
                               ETA: 05:24:08

################################################################################
                    [1m Learning iteration 1411/10000 [0m                     

                       Computation: 46449 steps/s (collection: 2.020s, learning 0.097s)
             Mean action noise std: 3.50
          Mean value_function loss: 326.5587
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.7352
                       Mean reward: 753.30
               Mean episode length: 208.50
    Episode_Reward/reaching_object: 1.4712
     Episode_Reward/lifting_object: 148.2219
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.12s
                      Time elapsed: 00:53:16
                               ETA: 05:24:05

################################################################################
                    [1m Learning iteration 1412/10000 [0m                     

                       Computation: 44903 steps/s (collection: 2.090s, learning 0.099s)
             Mean action noise std: 3.50
          Mean value_function loss: 358.3716
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 81.7540
                       Mean reward: 752.77
               Mean episode length: 210.40
    Episode_Reward/reaching_object: 1.4368
     Episode_Reward/lifting_object: 144.4581
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.19s
                      Time elapsed: 00:53:18
                               ETA: 05:24:02

################################################################################
                    [1m Learning iteration 1413/10000 [0m                     

                       Computation: 45612 steps/s (collection: 2.056s, learning 0.099s)
             Mean action noise std: 3.50
          Mean value_function loss: 288.0938
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.7574
                       Mean reward: 779.70
               Mean episode length: 216.29
    Episode_Reward/reaching_object: 1.4630
     Episode_Reward/lifting_object: 146.6602
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.16s
                      Time elapsed: 00:53:21
                               ETA: 05:23:59

################################################################################
                    [1m Learning iteration 1414/10000 [0m                     

                       Computation: 45884 steps/s (collection: 2.031s, learning 0.112s)
             Mean action noise std: 3.50
          Mean value_function loss: 304.4019
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.7686
                       Mean reward: 747.86
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 1.4655
     Episode_Reward/lifting_object: 147.2102
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.14s
                      Time elapsed: 00:53:23
                               ETA: 05:23:56

################################################################################
                    [1m Learning iteration 1415/10000 [0m                     

                       Computation: 46118 steps/s (collection: 2.043s, learning 0.088s)
             Mean action noise std: 3.51
          Mean value_function loss: 317.4254
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.7884
                       Mean reward: 724.17
               Mean episode length: 203.04
    Episode_Reward/reaching_object: 1.4291
     Episode_Reward/lifting_object: 142.9997
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.13s
                      Time elapsed: 00:53:25
                               ETA: 05:23:53

################################################################################
                    [1m Learning iteration 1416/10000 [0m                     

                       Computation: 46637 steps/s (collection: 2.016s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 283.2173
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.8032
                       Mean reward: 755.41
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 1.4785
     Episode_Reward/lifting_object: 149.4224
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.11s
                      Time elapsed: 00:53:27
                               ETA: 05:23:50

################################################################################
                    [1m Learning iteration 1417/10000 [0m                     

                       Computation: 46615 steps/s (collection: 2.019s, learning 0.090s)
             Mean action noise std: 3.51
          Mean value_function loss: 249.9929
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.8194
                       Mean reward: 795.74
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 150.0535
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.11s
                      Time elapsed: 00:53:29
                               ETA: 05:23:47

################################################################################
                    [1m Learning iteration 1418/10000 [0m                     

                       Computation: 46937 steps/s (collection: 2.005s, learning 0.090s)
             Mean action noise std: 3.51
          Mean value_function loss: 287.4116
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.8463
                       Mean reward: 737.98
               Mean episode length: 207.33
    Episode_Reward/reaching_object: 1.4891
     Episode_Reward/lifting_object: 149.7501
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.09s
                      Time elapsed: 00:53:31
                               ETA: 05:23:44

################################################################################
                    [1m Learning iteration 1419/10000 [0m                     

                       Computation: 46243 steps/s (collection: 2.015s, learning 0.111s)
             Mean action noise std: 3.52
          Mean value_function loss: 254.2971
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.8621
                       Mean reward: 736.82
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 1.4841
     Episode_Reward/lifting_object: 149.3305
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.13s
                      Time elapsed: 00:53:33
                               ETA: 05:23:40

################################################################################
                    [1m Learning iteration 1420/10000 [0m                     

                       Computation: 46034 steps/s (collection: 2.018s, learning 0.118s)
             Mean action noise std: 3.52
          Mean value_function loss: 249.6727
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.8807
                       Mean reward: 765.59
               Mean episode length: 214.74
    Episode_Reward/reaching_object: 1.4911
     Episode_Reward/lifting_object: 150.1340
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.14s
                      Time elapsed: 00:53:35
                               ETA: 05:23:37

################################################################################
                    [1m Learning iteration 1421/10000 [0m                     

                       Computation: 46202 steps/s (collection: 2.014s, learning 0.114s)
             Mean action noise std: 3.52
          Mean value_function loss: 277.5364
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.8939
                       Mean reward: 736.27
               Mean episode length: 208.44
    Episode_Reward/reaching_object: 1.4913
     Episode_Reward/lifting_object: 150.6563
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.13s
                      Time elapsed: 00:53:38
                               ETA: 05:23:34

################################################################################
                    [1m Learning iteration 1422/10000 [0m                     

                       Computation: 46867 steps/s (collection: 2.007s, learning 0.090s)
             Mean action noise std: 3.52
          Mean value_function loss: 287.3984
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.9025
                       Mean reward: 793.43
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.4741
     Episode_Reward/lifting_object: 147.8310
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.10s
                      Time elapsed: 00:53:40
                               ETA: 05:23:31

################################################################################
                    [1m Learning iteration 1423/10000 [0m                     

                       Computation: 46812 steps/s (collection: 2.004s, learning 0.096s)
             Mean action noise std: 3.52
          Mean value_function loss: 255.1894
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.9113
                       Mean reward: 748.10
               Mean episode length: 210.61
    Episode_Reward/reaching_object: 1.5100
     Episode_Reward/lifting_object: 153.1424
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.10s
                      Time elapsed: 00:53:42
                               ETA: 05:23:28

################################################################################
                    [1m Learning iteration 1424/10000 [0m                     

                       Computation: 46252 steps/s (collection: 2.035s, learning 0.091s)
             Mean action noise std: 3.52
          Mean value_function loss: 306.5256
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.9247
                       Mean reward: 745.14
               Mean episode length: 209.54
    Episode_Reward/reaching_object: 1.4720
     Episode_Reward/lifting_object: 147.9009
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.13s
                      Time elapsed: 00:53:44
                               ETA: 05:23:25

################################################################################
                    [1m Learning iteration 1425/10000 [0m                     

                       Computation: 46290 steps/s (collection: 2.019s, learning 0.105s)
             Mean action noise std: 3.52
          Mean value_function loss: 313.3574
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.9299
                       Mean reward: 791.00
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 1.5050
     Episode_Reward/lifting_object: 151.6533
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.12s
                      Time elapsed: 00:53:46
                               ETA: 05:23:22

################################################################################
                    [1m Learning iteration 1426/10000 [0m                     

                       Computation: 46889 steps/s (collection: 1.982s, learning 0.114s)
             Mean action noise std: 3.52
          Mean value_function loss: 268.7623
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 81.9331
                       Mean reward: 764.50
               Mean episode length: 214.31
    Episode_Reward/reaching_object: 1.5008
     Episode_Reward/lifting_object: 151.8742
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.10s
                      Time elapsed: 00:53:48
                               ETA: 05:23:18

################################################################################
                    [1m Learning iteration 1427/10000 [0m                     

                       Computation: 46682 steps/s (collection: 2.001s, learning 0.105s)
             Mean action noise std: 3.52
          Mean value_function loss: 287.8323
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.9362
                       Mean reward: 753.36
               Mean episode length: 212.44
    Episode_Reward/reaching_object: 1.4509
     Episode_Reward/lifting_object: 146.0005
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.11s
                      Time elapsed: 00:53:50
                               ETA: 05:23:15

################################################################################
                    [1m Learning iteration 1428/10000 [0m                     

                       Computation: 46596 steps/s (collection: 2.021s, learning 0.089s)
             Mean action noise std: 3.53
          Mean value_function loss: 268.5643
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.9428
                       Mean reward: 783.54
               Mean episode length: 215.97
    Episode_Reward/reaching_object: 1.4934
     Episode_Reward/lifting_object: 151.0637
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.11s
                      Time elapsed: 00:53:52
                               ETA: 05:23:12

################################################################################
                    [1m Learning iteration 1429/10000 [0m                     

                       Computation: 45644 steps/s (collection: 2.061s, learning 0.093s)
             Mean action noise std: 3.53
          Mean value_function loss: 240.0103
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.9542
                       Mean reward: 797.96
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.4578
     Episode_Reward/lifting_object: 146.7482
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.15s
                      Time elapsed: 00:53:54
                               ETA: 05:23:09

################################################################################
                    [1m Learning iteration 1430/10000 [0m                     

                       Computation: 46917 steps/s (collection: 2.000s, learning 0.096s)
             Mean action noise std: 3.53
          Mean value_function loss: 235.4685
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.9695
                       Mean reward: 776.85
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 149.2912
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.10s
                      Time elapsed: 00:53:57
                               ETA: 05:23:06

################################################################################
                    [1m Learning iteration 1431/10000 [0m                     

                       Computation: 46580 steps/s (collection: 2.018s, learning 0.092s)
             Mean action noise std: 3.53
          Mean value_function loss: 255.7534
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.9820
                       Mean reward: 789.73
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 1.4956
     Episode_Reward/lifting_object: 151.0977
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.11s
                      Time elapsed: 00:53:59
                               ETA: 05:23:03

################################################################################
                    [1m Learning iteration 1432/10000 [0m                     

                       Computation: 46789 steps/s (collection: 2.013s, learning 0.088s)
             Mean action noise std: 3.53
          Mean value_function loss: 230.8667
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.9968
                       Mean reward: 815.37
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.5356
     Episode_Reward/lifting_object: 155.6975
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.10s
                      Time elapsed: 00:54:01
                               ETA: 05:22:59

################################################################################
                    [1m Learning iteration 1433/10000 [0m                     

                       Computation: 46562 steps/s (collection: 2.019s, learning 0.093s)
             Mean action noise std: 3.54
          Mean value_function loss: 244.9530
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.0028
                       Mean reward: 765.43
               Mean episode length: 213.98
    Episode_Reward/reaching_object: 1.5293
     Episode_Reward/lifting_object: 154.9578
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.11s
                      Time elapsed: 00:54:03
                               ETA: 05:22:56

################################################################################
                    [1m Learning iteration 1434/10000 [0m                     

                       Computation: 46119 steps/s (collection: 2.016s, learning 0.116s)
             Mean action noise std: 3.54
          Mean value_function loss: 255.4094
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.0152
                       Mean reward: 762.69
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.5395
     Episode_Reward/lifting_object: 156.4125
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.13s
                      Time elapsed: 00:54:05
                               ETA: 05:22:53

################################################################################
                    [1m Learning iteration 1435/10000 [0m                     

                       Computation: 46961 steps/s (collection: 1.982s, learning 0.111s)
             Mean action noise std: 3.54
          Mean value_function loss: 278.9556
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.0308
                       Mean reward: 797.18
               Mean episode length: 221.36
    Episode_Reward/reaching_object: 1.5153
     Episode_Reward/lifting_object: 153.7793
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.09s
                      Time elapsed: 00:54:07
                               ETA: 05:22:50

################################################################################
                    [1m Learning iteration 1436/10000 [0m                     

                       Computation: 45478 steps/s (collection: 2.058s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 315.4768
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.0406
                       Mean reward: 772.64
               Mean episode length: 212.28
    Episode_Reward/reaching_object: 1.4873
     Episode_Reward/lifting_object: 150.3982
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.16s
                      Time elapsed: 00:54:09
                               ETA: 05:22:47

################################################################################
                    [1m Learning iteration 1437/10000 [0m                     

                       Computation: 46918 steps/s (collection: 2.000s, learning 0.095s)
             Mean action noise std: 3.54
          Mean value_function loss: 263.4326
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.0515
                       Mean reward: 743.40
               Mean episode length: 207.94
    Episode_Reward/reaching_object: 1.4767
     Episode_Reward/lifting_object: 148.7251
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.10s
                      Time elapsed: 00:54:11
                               ETA: 05:22:44

################################################################################
                    [1m Learning iteration 1438/10000 [0m                     

                       Computation: 46737 steps/s (collection: 2.000s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 277.2176
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0676
                       Mean reward: 748.56
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 1.4656
     Episode_Reward/lifting_object: 147.7820
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.10s
                      Time elapsed: 00:54:13
                               ETA: 05:22:41

################################################################################
                    [1m Learning iteration 1439/10000 [0m                     

                       Computation: 46662 steps/s (collection: 1.992s, learning 0.115s)
             Mean action noise std: 3.55
          Mean value_function loss: 266.6323
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.0820
                       Mean reward: 738.47
               Mean episode length: 210.19
    Episode_Reward/reaching_object: 1.4967
     Episode_Reward/lifting_object: 151.4116
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.11s
                      Time elapsed: 00:54:16
                               ETA: 05:22:37

################################################################################
                    [1m Learning iteration 1440/10000 [0m                     

                       Computation: 46707 steps/s (collection: 1.997s, learning 0.108s)
             Mean action noise std: 3.55
          Mean value_function loss: 255.9944
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.0894
                       Mean reward: 742.60
               Mean episode length: 208.93
    Episode_Reward/reaching_object: 1.4860
     Episode_Reward/lifting_object: 150.2945
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.10s
                      Time elapsed: 00:54:18
                               ETA: 05:22:34

################################################################################
                    [1m Learning iteration 1441/10000 [0m                     

                       Computation: 46924 steps/s (collection: 1.982s, learning 0.113s)
             Mean action noise std: 3.55
          Mean value_function loss: 228.0767
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.1040
                       Mean reward: 769.83
               Mean episode length: 214.03
    Episode_Reward/reaching_object: 1.5102
     Episode_Reward/lifting_object: 152.8771
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.09s
                      Time elapsed: 00:54:20
                               ETA: 05:22:31

################################################################################
                    [1m Learning iteration 1442/10000 [0m                     

                       Computation: 47170 steps/s (collection: 1.969s, learning 0.115s)
             Mean action noise std: 3.55
          Mean value_function loss: 290.8718
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.1148
                       Mean reward: 733.38
               Mean episode length: 204.03
    Episode_Reward/reaching_object: 1.5371
     Episode_Reward/lifting_object: 156.4292
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.08s
                      Time elapsed: 00:54:22
                               ETA: 05:22:28

################################################################################
                    [1m Learning iteration 1443/10000 [0m                     

                       Computation: 46986 steps/s (collection: 1.982s, learning 0.110s)
             Mean action noise std: 3.55
          Mean value_function loss: 249.7635
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.1268
                       Mean reward: 754.44
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 1.5485
     Episode_Reward/lifting_object: 156.8072
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.09s
                      Time elapsed: 00:54:24
                               ETA: 05:22:24

################################################################################
                    [1m Learning iteration 1444/10000 [0m                     

                       Computation: 46837 steps/s (collection: 1.985s, learning 0.114s)
             Mean action noise std: 3.55
          Mean value_function loss: 242.4622
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.1388
                       Mean reward: 766.53
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.5035
     Episode_Reward/lifting_object: 152.0254
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.10s
                      Time elapsed: 00:54:26
                               ETA: 05:22:21

################################################################################
                    [1m Learning iteration 1445/10000 [0m                     

                       Computation: 47505 steps/s (collection: 1.976s, learning 0.094s)
             Mean action noise std: 3.56
          Mean value_function loss: 309.9814
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.1593
                       Mean reward: 767.26
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.5283
     Episode_Reward/lifting_object: 154.6364
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.07s
                      Time elapsed: 00:54:28
                               ETA: 05:22:18

################################################################################
                    [1m Learning iteration 1446/10000 [0m                     

                       Computation: 47006 steps/s (collection: 1.996s, learning 0.095s)
             Mean action noise std: 3.56
          Mean value_function loss: 259.7145
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.1754
                       Mean reward: 768.13
               Mean episode length: 214.14
    Episode_Reward/reaching_object: 1.5407
     Episode_Reward/lifting_object: 155.8291
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.09s
                      Time elapsed: 00:54:30
                               ETA: 05:22:15

################################################################################
                    [1m Learning iteration 1447/10000 [0m                     

                       Computation: 47025 steps/s (collection: 1.996s, learning 0.094s)
             Mean action noise std: 3.56
          Mean value_function loss: 237.7862
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 82.1893
                       Mean reward: 778.05
               Mean episode length: 213.32
    Episode_Reward/reaching_object: 1.5139
     Episode_Reward/lifting_object: 152.9289
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.09s
                      Time elapsed: 00:54:32
                               ETA: 05:22:11

################################################################################
                    [1m Learning iteration 1448/10000 [0m                     

                       Computation: 46121 steps/s (collection: 2.018s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 242.4074
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.1975
                       Mean reward: 732.46
               Mean episode length: 205.20
    Episode_Reward/reaching_object: 1.4439
     Episode_Reward/lifting_object: 145.0964
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.13s
                      Time elapsed: 00:54:34
                               ETA: 05:22:08

################################################################################
                    [1m Learning iteration 1449/10000 [0m                     

                       Computation: 46336 steps/s (collection: 2.009s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 256.5308
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.2091
                       Mean reward: 778.91
               Mean episode length: 215.98
    Episode_Reward/reaching_object: 1.4981
     Episode_Reward/lifting_object: 151.0815
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.12s
                      Time elapsed: 00:54:37
                               ETA: 05:22:05

################################################################################
                    [1m Learning iteration 1450/10000 [0m                     

                       Computation: 46876 steps/s (collection: 1.992s, learning 0.105s)
             Mean action noise std: 3.56
          Mean value_function loss: 251.9546
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.2238
                       Mean reward: 805.82
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 1.5155
     Episode_Reward/lifting_object: 152.4884
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.10s
                      Time elapsed: 00:54:39
                               ETA: 05:22:02

################################################################################
                    [1m Learning iteration 1451/10000 [0m                     

                       Computation: 46593 steps/s (collection: 2.008s, learning 0.102s)
             Mean action noise std: 3.57
          Mean value_function loss: 254.6476
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.2370
                       Mean reward: 734.34
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 1.4699
     Episode_Reward/lifting_object: 147.4830
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.11s
                      Time elapsed: 00:54:41
                               ETA: 05:21:59

################################################################################
                    [1m Learning iteration 1452/10000 [0m                     

                       Computation: 46878 steps/s (collection: 2.006s, learning 0.091s)
             Mean action noise std: 3.57
          Mean value_function loss: 249.2248
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 82.2533
                       Mean reward: 830.10
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.5287
     Episode_Reward/lifting_object: 154.5313
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.10s
                      Time elapsed: 00:54:43
                               ETA: 05:21:56

################################################################################
                    [1m Learning iteration 1453/10000 [0m                     

                       Computation: 47249 steps/s (collection: 1.990s, learning 0.090s)
             Mean action noise std: 3.57
          Mean value_function loss: 237.5922
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.2759
                       Mean reward: 747.52
               Mean episode length: 210.32
    Episode_Reward/reaching_object: 1.5255
     Episode_Reward/lifting_object: 154.1865
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.08s
                      Time elapsed: 00:54:45
                               ETA: 05:21:52

################################################################################
                    [1m Learning iteration 1454/10000 [0m                     

                       Computation: 47146 steps/s (collection: 1.988s, learning 0.098s)
             Mean action noise std: 3.57
          Mean value_function loss: 240.2973
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.2920
                       Mean reward: 809.20
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 1.5076
     Episode_Reward/lifting_object: 151.8867
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.09s
                      Time elapsed: 00:54:47
                               ETA: 05:21:49

################################################################################
                    [1m Learning iteration 1455/10000 [0m                     

                       Computation: 45910 steps/s (collection: 2.041s, learning 0.101s)
             Mean action noise std: 3.58
          Mean value_function loss: 219.1734
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.3043
                       Mean reward: 809.92
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.5397
     Episode_Reward/lifting_object: 155.5404
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.14s
                      Time elapsed: 00:54:49
                               ETA: 05:21:46

################################################################################
                    [1m Learning iteration 1456/10000 [0m                     

                       Computation: 46855 steps/s (collection: 1.995s, learning 0.103s)
             Mean action noise std: 3.58
          Mean value_function loss: 236.0040
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.3166
                       Mean reward: 710.47
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 149.9843
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.10s
                      Time elapsed: 00:54:51
                               ETA: 05:21:43

################################################################################
                    [1m Learning iteration 1457/10000 [0m                     

                       Computation: 45833 steps/s (collection: 2.046s, learning 0.099s)
             Mean action noise std: 3.58
          Mean value_function loss: 249.3832
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.3286
                       Mean reward: 780.08
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 1.5492
     Episode_Reward/lifting_object: 156.7365
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.14s
                      Time elapsed: 00:54:53
                               ETA: 05:21:40

################################################################################
                    [1m Learning iteration 1458/10000 [0m                     

                       Computation: 46382 steps/s (collection: 2.003s, learning 0.116s)
             Mean action noise std: 3.58
          Mean value_function loss: 276.1405
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.3406
                       Mean reward: 795.69
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 1.4947
     Episode_Reward/lifting_object: 150.4325
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.12s
                      Time elapsed: 00:54:56
                               ETA: 05:21:37

################################################################################
                    [1m Learning iteration 1459/10000 [0m                     

                       Computation: 47061 steps/s (collection: 1.995s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 226.2626
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.3492
                       Mean reward: 784.24
               Mean episode length: 216.61
    Episode_Reward/reaching_object: 1.5334
     Episode_Reward/lifting_object: 154.8939
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.09s
                      Time elapsed: 00:54:58
                               ETA: 05:21:34

################################################################################
                    [1m Learning iteration 1460/10000 [0m                     

                       Computation: 47200 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 208.0710
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.3581
                       Mean reward: 805.72
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 1.5752
     Episode_Reward/lifting_object: 160.0979
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.08s
                      Time elapsed: 00:55:00
                               ETA: 05:21:30

################################################################################
                    [1m Learning iteration 1461/10000 [0m                     

                       Computation: 45019 steps/s (collection: 2.094s, learning 0.090s)
             Mean action noise std: 3.58
          Mean value_function loss: 278.7185
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.3663
                       Mean reward: 793.53
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.5423
     Episode_Reward/lifting_object: 156.1276
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.18s
                      Time elapsed: 00:55:02
                               ETA: 05:21:28

################################################################################
                    [1m Learning iteration 1462/10000 [0m                     

                       Computation: 46644 steps/s (collection: 2.016s, learning 0.091s)
             Mean action noise std: 3.59
          Mean value_function loss: 287.3586
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.3719
                       Mean reward: 799.36
               Mean episode length: 218.08
    Episode_Reward/reaching_object: 1.4836
     Episode_Reward/lifting_object: 150.1461
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.11s
                      Time elapsed: 00:55:04
                               ETA: 05:21:24

################################################################################
                    [1m Learning iteration 1463/10000 [0m                     

                       Computation: 45774 steps/s (collection: 1.995s, learning 0.153s)
             Mean action noise std: 3.59
          Mean value_function loss: 215.9506
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.3830
                       Mean reward: 787.11
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 1.5182
     Episode_Reward/lifting_object: 153.4728
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.15s
                      Time elapsed: 00:55:06
                               ETA: 05:21:22

################################################################################
                    [1m Learning iteration 1464/10000 [0m                     

                       Computation: 38823 steps/s (collection: 2.397s, learning 0.135s)
             Mean action noise std: 3.59
          Mean value_function loss: 236.9844
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 82.3969
                       Mean reward: 776.19
               Mean episode length: 215.69
    Episode_Reward/reaching_object: 1.5731
     Episode_Reward/lifting_object: 159.5585
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.53s
                      Time elapsed: 00:55:09
                               ETA: 05:21:21

################################################################################
                    [1m Learning iteration 1465/10000 [0m                     

                       Computation: 40218 steps/s (collection: 2.267s, learning 0.177s)
             Mean action noise std: 3.59
          Mean value_function loss: 262.2479
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4125
                       Mean reward: 805.99
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.5355
     Episode_Reward/lifting_object: 155.5313
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.44s
                      Time elapsed: 00:55:11
                               ETA: 05:21:20

################################################################################
                    [1m Learning iteration 1466/10000 [0m                     

                       Computation: 37731 steps/s (collection: 2.488s, learning 0.117s)
             Mean action noise std: 3.59
          Mean value_function loss: 299.3373
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.4273
                       Mean reward: 755.00
               Mean episode length: 211.64
    Episode_Reward/reaching_object: 1.5513
     Episode_Reward/lifting_object: 157.0606
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.61s
                      Time elapsed: 00:55:14
                               ETA: 05:21:20

################################################################################
                    [1m Learning iteration 1467/10000 [0m                     

                       Computation: 45402 steps/s (collection: 2.069s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 241.7617
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.4503
                       Mean reward: 778.55
               Mean episode length: 215.26
    Episode_Reward/reaching_object: 1.5419
     Episode_Reward/lifting_object: 156.5758
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.17s
                      Time elapsed: 00:55:16
                               ETA: 05:21:17

################################################################################
                    [1m Learning iteration 1468/10000 [0m                     

                       Computation: 47195 steps/s (collection: 1.986s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 257.4466
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.4670
                       Mean reward: 784.35
               Mean episode length: 218.53
    Episode_Reward/reaching_object: 1.4983
     Episode_Reward/lifting_object: 151.2820
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.08s
                      Time elapsed: 00:55:18
                               ETA: 05:21:13

################################################################################
                    [1m Learning iteration 1469/10000 [0m                     

                       Computation: 46782 steps/s (collection: 1.997s, learning 0.104s)
             Mean action noise std: 3.60
          Mean value_function loss: 263.9309
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.4742
                       Mean reward: 714.27
               Mean episode length: 201.18
    Episode_Reward/reaching_object: 1.4897
     Episode_Reward/lifting_object: 150.5717
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.10s
                      Time elapsed: 00:55:20
                               ETA: 05:21:10

################################################################################
                    [1m Learning iteration 1470/10000 [0m                     

                       Computation: 44656 steps/s (collection: 2.027s, learning 0.175s)
             Mean action noise std: 3.60
          Mean value_function loss: 220.2133
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.4827
                       Mean reward: 720.77
               Mean episode length: 204.44
    Episode_Reward/reaching_object: 1.5386
     Episode_Reward/lifting_object: 155.6951
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.20s
                      Time elapsed: 00:55:22
                               ETA: 05:21:08

################################################################################
                    [1m Learning iteration 1471/10000 [0m                     

                       Computation: 41066 steps/s (collection: 2.259s, learning 0.135s)
             Mean action noise std: 3.60
          Mean value_function loss: 249.7650
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4982
                       Mean reward: 789.54
               Mean episode length: 216.94
    Episode_Reward/reaching_object: 1.5554
     Episode_Reward/lifting_object: 157.8951
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.39s
                      Time elapsed: 00:55:25
                               ETA: 05:21:06

################################################################################
                    [1m Learning iteration 1472/10000 [0m                     

                       Computation: 36154 steps/s (collection: 2.480s, learning 0.239s)
             Mean action noise std: 3.60
          Mean value_function loss: 229.7385
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.5129
                       Mean reward: 802.33
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.5312
     Episode_Reward/lifting_object: 154.6397
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.72s
                      Time elapsed: 00:55:27
                               ETA: 05:21:07

################################################################################
                    [1m Learning iteration 1473/10000 [0m                     

                       Computation: 40456 steps/s (collection: 2.304s, learning 0.126s)
             Mean action noise std: 3.61
          Mean value_function loss: 249.3860
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 82.5307
                       Mean reward: 767.50
               Mean episode length: 214.05
    Episode_Reward/reaching_object: 1.5402
     Episode_Reward/lifting_object: 155.8477
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.43s
                      Time elapsed: 00:55:30
                               ETA: 05:21:05

################################################################################
                    [1m Learning iteration 1474/10000 [0m                     

                       Computation: 44347 steps/s (collection: 2.124s, learning 0.093s)
             Mean action noise std: 3.61
          Mean value_function loss: 268.2229
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.5463
                       Mean reward: 739.77
               Mean episode length: 208.76
    Episode_Reward/reaching_object: 1.4988
     Episode_Reward/lifting_object: 150.7439
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.22s
                      Time elapsed: 00:55:32
                               ETA: 05:21:03

################################################################################
                    [1m Learning iteration 1475/10000 [0m                     

                       Computation: 45187 steps/s (collection: 2.077s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 221.0412
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.5586
                       Mean reward: 769.42
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 1.5338
     Episode_Reward/lifting_object: 155.1652
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.18s
                      Time elapsed: 00:55:34
                               ETA: 05:21:00

################################################################################
                    [1m Learning iteration 1476/10000 [0m                     

                       Computation: 34888 steps/s (collection: 2.655s, learning 0.163s)
             Mean action noise std: 3.61
          Mean value_function loss: 222.2448
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.5728
                       Mean reward: 799.16
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.5546
     Episode_Reward/lifting_object: 157.6763
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.82s
                      Time elapsed: 00:55:37
                               ETA: 05:21:01

################################################################################
                    [1m Learning iteration 1477/10000 [0m                     

                       Computation: 35338 steps/s (collection: 2.645s, learning 0.137s)
             Mean action noise std: 3.61
          Mean value_function loss: 191.2948
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.5900
                       Mean reward: 829.60
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 1.5750
     Episode_Reward/lifting_object: 159.5180
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.78s
                      Time elapsed: 00:55:40
                               ETA: 05:21:02

################################################################################
                    [1m Learning iteration 1478/10000 [0m                     

                       Computation: 37298 steps/s (collection: 2.439s, learning 0.197s)
             Mean action noise std: 3.61
          Mean value_function loss: 215.5650
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.6039
                       Mean reward: 784.88
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.5905
     Episode_Reward/lifting_object: 161.2233
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.64s
                      Time elapsed: 00:55:42
                               ETA: 05:21:02

################################################################################
                    [1m Learning iteration 1479/10000 [0m                     

                       Computation: 39279 steps/s (collection: 2.375s, learning 0.128s)
             Mean action noise std: 3.62
          Mean value_function loss: 232.7909
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.6143
                       Mean reward: 827.79
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.5583
     Episode_Reward/lifting_object: 157.4541
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.50s
                      Time elapsed: 00:55:45
                               ETA: 05:21:01

################################################################################
                    [1m Learning iteration 1480/10000 [0m                     

                       Computation: 44113 steps/s (collection: 2.137s, learning 0.092s)
             Mean action noise std: 3.62
          Mean value_function loss: 254.8074
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 82.6312
                       Mean reward: 789.57
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 1.5365
     Episode_Reward/lifting_object: 155.2021
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.23s
                      Time elapsed: 00:55:47
                               ETA: 05:20:58

################################################################################
                    [1m Learning iteration 1481/10000 [0m                     

                       Computation: 41532 steps/s (collection: 2.160s, learning 0.207s)
             Mean action noise std: 3.62
          Mean value_function loss: 256.7136
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.6497
                       Mean reward: 766.61
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.5636
     Episode_Reward/lifting_object: 158.1233
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.37s
                      Time elapsed: 00:55:50
                               ETA: 05:20:57

################################################################################
                    [1m Learning iteration 1482/10000 [0m                     

                       Computation: 44663 steps/s (collection: 2.099s, learning 0.102s)
             Mean action noise std: 3.62
          Mean value_function loss: 252.3136
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.6631
                       Mean reward: 789.32
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.5166
     Episode_Reward/lifting_object: 152.6926
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.20s
                      Time elapsed: 00:55:52
                               ETA: 05:20:54

################################################################################
                    [1m Learning iteration 1483/10000 [0m                     

                       Computation: 45178 steps/s (collection: 2.086s, learning 0.090s)
             Mean action noise std: 3.62
          Mean value_function loss: 265.5257
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.6766
                       Mean reward: 786.11
               Mean episode length: 216.91
    Episode_Reward/reaching_object: 1.5361
     Episode_Reward/lifting_object: 155.6139
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.18s
                      Time elapsed: 00:55:54
                               ETA: 05:20:51

################################################################################
                    [1m Learning iteration 1484/10000 [0m                     

                       Computation: 42948 steps/s (collection: 2.195s, learning 0.094s)
             Mean action noise std: 3.63
          Mean value_function loss: 262.4072
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.6930
                       Mean reward: 827.98
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.5572
     Episode_Reward/lifting_object: 158.0757
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.29s
                      Time elapsed: 00:55:56
                               ETA: 05:20:49

################################################################################
                    [1m Learning iteration 1485/10000 [0m                     

                       Computation: 41301 steps/s (collection: 2.248s, learning 0.132s)
             Mean action noise std: 3.63
          Mean value_function loss: 327.3128
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.7090
                       Mean reward: 794.11
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 1.4947
     Episode_Reward/lifting_object: 149.7993
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.38s
                      Time elapsed: 00:55:59
                               ETA: 05:20:48

################################################################################
                    [1m Learning iteration 1486/10000 [0m                     

                       Computation: 43441 steps/s (collection: 2.160s, learning 0.103s)
             Mean action noise std: 3.63
          Mean value_function loss: 299.9841
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.7266
                       Mean reward: 769.53
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 1.4985
     Episode_Reward/lifting_object: 151.6895
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.26s
                      Time elapsed: 00:56:01
                               ETA: 05:20:45

################################################################################
                    [1m Learning iteration 1487/10000 [0m                     

                       Computation: 41019 steps/s (collection: 2.283s, learning 0.113s)
             Mean action noise std: 3.63
          Mean value_function loss: 300.9078
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.7482
                       Mean reward: 754.35
               Mean episode length: 210.67
    Episode_Reward/reaching_object: 1.4781
     Episode_Reward/lifting_object: 148.0092
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.40s
                      Time elapsed: 00:56:03
                               ETA: 05:20:44

################################################################################
                    [1m Learning iteration 1488/10000 [0m                     

                       Computation: 44407 steps/s (collection: 2.069s, learning 0.145s)
             Mean action noise std: 3.64
          Mean value_function loss: 241.0076
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.7672
                       Mean reward: 807.37
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 1.5426
     Episode_Reward/lifting_object: 154.9594
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.21s
                      Time elapsed: 00:56:05
                               ETA: 05:20:41

################################################################################
                    [1m Learning iteration 1489/10000 [0m                     

                       Computation: 45499 steps/s (collection: 2.066s, learning 0.095s)
             Mean action noise std: 3.64
          Mean value_function loss: 247.0353
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.7749
                       Mean reward: 791.06
               Mean episode length: 219.07
    Episode_Reward/reaching_object: 1.5414
     Episode_Reward/lifting_object: 155.3810
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.16s
                      Time elapsed: 00:56:08
                               ETA: 05:20:39

################################################################################
                    [1m Learning iteration 1490/10000 [0m                     

                       Computation: 45794 steps/s (collection: 2.054s, learning 0.093s)
             Mean action noise std: 3.64
          Mean value_function loss: 290.3933
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.7917
                       Mean reward: 726.74
               Mean episode length: 206.24
    Episode_Reward/reaching_object: 1.5071
     Episode_Reward/lifting_object: 151.9608
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.15s
                      Time elapsed: 00:56:10
                               ETA: 05:20:36

################################################################################
                    [1m Learning iteration 1491/10000 [0m                     

                       Computation: 46020 steps/s (collection: 2.035s, learning 0.101s)
             Mean action noise std: 3.64
          Mean value_function loss: 238.4231
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.8091
                       Mean reward: 783.84
               Mean episode length: 218.21
    Episode_Reward/reaching_object: 1.5519
     Episode_Reward/lifting_object: 156.9682
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.14s
                      Time elapsed: 00:56:12
                               ETA: 05:20:33

################################################################################
                    [1m Learning iteration 1492/10000 [0m                     

                       Computation: 45998 steps/s (collection: 2.045s, learning 0.092s)
             Mean action noise std: 3.64
          Mean value_function loss: 224.0308
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.8269
                       Mean reward: 769.34
               Mean episode length: 214.29
    Episode_Reward/reaching_object: 1.5377
     Episode_Reward/lifting_object: 155.4872
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.14s
                      Time elapsed: 00:56:14
                               ETA: 05:20:30

################################################################################
                    [1m Learning iteration 1493/10000 [0m                     

                       Computation: 45561 steps/s (collection: 2.033s, learning 0.125s)
             Mean action noise std: 3.65
          Mean value_function loss: 226.7050
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.8456
                       Mean reward: 753.83
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 1.5085
     Episode_Reward/lifting_object: 151.6936
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.16s
                      Time elapsed: 00:56:16
                               ETA: 05:20:27

################################################################################
                    [1m Learning iteration 1494/10000 [0m                     

                       Computation: 39972 steps/s (collection: 2.330s, learning 0.130s)
             Mean action noise std: 3.65
          Mean value_function loss: 210.1179
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.8723
                       Mean reward: 813.61
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.5688
     Episode_Reward/lifting_object: 158.1465
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.46s
                      Time elapsed: 00:56:19
                               ETA: 05:20:26

################################################################################
                    [1m Learning iteration 1495/10000 [0m                     

                       Computation: 45571 steps/s (collection: 2.061s, learning 0.097s)
             Mean action noise std: 3.65
          Mean value_function loss: 212.6229
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.8955
                       Mean reward: 788.98
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 1.5474
     Episode_Reward/lifting_object: 156.1993
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.16s
                      Time elapsed: 00:56:21
                               ETA: 05:20:23

################################################################################
                    [1m Learning iteration 1496/10000 [0m                     

                       Computation: 45508 steps/s (collection: 2.054s, learning 0.107s)
             Mean action noise std: 3.65
          Mean value_function loss: 215.8996
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.9131
                       Mean reward: 843.15
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.5963
     Episode_Reward/lifting_object: 162.6371
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.16s
                      Time elapsed: 00:56:23
                               ETA: 05:20:20

################################################################################
                    [1m Learning iteration 1497/10000 [0m                     

                       Computation: 45345 steps/s (collection: 2.048s, learning 0.120s)
             Mean action noise std: 3.65
          Mean value_function loss: 235.7274
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.9234
                       Mean reward: 768.81
               Mean episode length: 213.23
    Episode_Reward/reaching_object: 1.5573
     Episode_Reward/lifting_object: 157.3197
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.17s
                      Time elapsed: 00:56:25
                               ETA: 05:20:17

################################################################################
                    [1m Learning iteration 1498/10000 [0m                     

                       Computation: 44920 steps/s (collection: 2.050s, learning 0.138s)
             Mean action noise std: 3.66
          Mean value_function loss: 237.7321
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.9360
                       Mean reward: 787.71
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 1.5332
     Episode_Reward/lifting_object: 154.1738
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.19s
                      Time elapsed: 00:56:27
                               ETA: 05:20:15

################################################################################
                    [1m Learning iteration 1499/10000 [0m                     

                       Computation: 44462 steps/s (collection: 2.115s, learning 0.096s)
             Mean action noise std: 3.66
          Mean value_function loss: 209.0423
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.9513
                       Mean reward: 807.65
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.5853
     Episode_Reward/lifting_object: 160.2039
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.21s
                      Time elapsed: 00:56:30
                               ETA: 05:20:12

################################################################################
                    [1m Learning iteration 1500/10000 [0m                     

                       Computation: 46061 steps/s (collection: 2.034s, learning 0.101s)
             Mean action noise std: 3.66
          Mean value_function loss: 236.1265
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.9646
                       Mean reward: 723.11
               Mean episode length: 203.00
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 153.5722
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.13s
                      Time elapsed: 00:56:32
                               ETA: 05:20:09

################################################################################
                    [1m Learning iteration 1501/10000 [0m                     

                       Computation: 44898 steps/s (collection: 2.095s, learning 0.094s)
             Mean action noise std: 3.66
          Mean value_function loss: 238.0754
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.9765
                       Mean reward: 795.35
               Mean episode length: 219.47
    Episode_Reward/reaching_object: 1.5661
     Episode_Reward/lifting_object: 157.5305
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.19s
                      Time elapsed: 00:56:34
                               ETA: 05:20:06

################################################################################
                    [1m Learning iteration 1502/10000 [0m                     

                       Computation: 45978 steps/s (collection: 2.046s, learning 0.092s)
             Mean action noise std: 3.66
          Mean value_function loss: 264.5415
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.9909
                       Mean reward: 756.29
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 1.5553
     Episode_Reward/lifting_object: 155.6469
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.14s
                      Time elapsed: 00:56:36
                               ETA: 05:20:04

################################################################################
                    [1m Learning iteration 1503/10000 [0m                     

                       Computation: 44314 steps/s (collection: 2.108s, learning 0.111s)
             Mean action noise std: 3.67
          Mean value_function loss: 182.6199
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 83.0081
                       Mean reward: 797.95
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 1.5704
     Episode_Reward/lifting_object: 158.2851
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.22s
                      Time elapsed: 00:56:38
                               ETA: 05:20:01

################################################################################
                    [1m Learning iteration 1504/10000 [0m                     

                       Computation: 45714 steps/s (collection: 2.052s, learning 0.098s)
             Mean action noise std: 3.67
          Mean value_function loss: 241.1694
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 83.0290
                       Mean reward: 827.53
               Mean episode length: 227.12
    Episode_Reward/reaching_object: 1.5858
     Episode_Reward/lifting_object: 159.3399
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.15s
                      Time elapsed: 00:56:40
                               ETA: 05:19:58

################################################################################
                    [1m Learning iteration 1505/10000 [0m                     

                       Computation: 44068 steps/s (collection: 2.096s, learning 0.135s)
             Mean action noise std: 3.67
          Mean value_function loss: 194.5594
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.0380
                       Mean reward: 799.88
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 1.5887
     Episode_Reward/lifting_object: 159.1797
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.23s
                      Time elapsed: 00:56:43
                               ETA: 05:19:56

################################################################################
                    [1m Learning iteration 1506/10000 [0m                     

                       Computation: 43418 steps/s (collection: 2.167s, learning 0.098s)
             Mean action noise std: 3.67
          Mean value_function loss: 223.7931
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.0556
                       Mean reward: 794.79
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.5751
     Episode_Reward/lifting_object: 158.5258
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.26s
                      Time elapsed: 00:56:45
                               ETA: 05:19:54

################################################################################
                    [1m Learning iteration 1507/10000 [0m                     

                       Computation: 44608 steps/s (collection: 2.097s, learning 0.107s)
             Mean action noise std: 3.67
          Mean value_function loss: 186.2371
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.0682
                       Mean reward: 746.59
               Mean episode length: 209.03
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 159.1417
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.20s
                      Time elapsed: 00:56:47
                               ETA: 05:19:51

################################################################################
                    [1m Learning iteration 1508/10000 [0m                     

                       Computation: 43582 steps/s (collection: 2.153s, learning 0.103s)
             Mean action noise std: 3.67
          Mean value_function loss: 226.8348
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.0756
                       Mean reward: 779.71
               Mean episode length: 214.95
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 157.7444
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.26s
                      Time elapsed: 00:56:49
                               ETA: 05:19:49

################################################################################
                    [1m Learning iteration 1509/10000 [0m                     

                       Computation: 44738 steps/s (collection: 2.088s, learning 0.110s)
             Mean action noise std: 3.68
          Mean value_function loss: 226.1419
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.0910
                       Mean reward: 784.35
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 1.5526
     Episode_Reward/lifting_object: 155.3832
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.20s
                      Time elapsed: 00:56:52
                               ETA: 05:19:46

################################################################################
                    [1m Learning iteration 1510/10000 [0m                     

                       Computation: 45601 steps/s (collection: 2.044s, learning 0.111s)
             Mean action noise std: 3.68
          Mean value_function loss: 207.6171
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.1096
                       Mean reward: 793.46
               Mean episode length: 218.56
    Episode_Reward/reaching_object: 1.5751
     Episode_Reward/lifting_object: 158.2254
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.16s
                      Time elapsed: 00:56:54
                               ETA: 05:19:43

################################################################################
                    [1m Learning iteration 1511/10000 [0m                     

                       Computation: 45245 steps/s (collection: 2.079s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 180.9587
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.1222
                       Mean reward: 842.26
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 1.6369
     Episode_Reward/lifting_object: 164.5477
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.17s
                      Time elapsed: 00:56:56
                               ETA: 05:19:40

################################################################################
                    [1m Learning iteration 1512/10000 [0m                     

                       Computation: 42244 steps/s (collection: 2.171s, learning 0.156s)
             Mean action noise std: 3.68
          Mean value_function loss: 222.4612
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.1364
                       Mean reward: 774.63
               Mean episode length: 215.16
    Episode_Reward/reaching_object: 1.5681
     Episode_Reward/lifting_object: 156.8516
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.33s
                      Time elapsed: 00:56:58
                               ETA: 05:19:39

################################################################################
                    [1m Learning iteration 1513/10000 [0m                     

                       Computation: 44323 steps/s (collection: 2.118s, learning 0.100s)
             Mean action noise std: 3.68
          Mean value_function loss: 219.7754
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 83.1508
                       Mean reward: 829.44
               Mean episode length: 228.02
    Episode_Reward/reaching_object: 1.5741
     Episode_Reward/lifting_object: 157.5623
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.22s
                      Time elapsed: 00:57:00
                               ETA: 05:19:36

################################################################################
                    [1m Learning iteration 1514/10000 [0m                     

                       Computation: 42127 steps/s (collection: 2.151s, learning 0.183s)
             Mean action noise std: 3.69
          Mean value_function loss: 189.4976
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.1631
                       Mean reward: 839.77
               Mean episode length: 228.82
    Episode_Reward/reaching_object: 1.6167
     Episode_Reward/lifting_object: 162.5184
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.33s
                      Time elapsed: 00:57:03
                               ETA: 05:19:34

################################################################################
                    [1m Learning iteration 1515/10000 [0m                     

                       Computation: 44416 steps/s (collection: 2.115s, learning 0.098s)
             Mean action noise std: 3.69
          Mean value_function loss: 196.8773
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.1796
                       Mean reward: 800.52
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 1.6035
     Episode_Reward/lifting_object: 161.1082
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.21s
                      Time elapsed: 00:57:05
                               ETA: 05:19:32

################################################################################
                    [1m Learning iteration 1516/10000 [0m                     

                       Computation: 43481 steps/s (collection: 2.134s, learning 0.127s)
             Mean action noise std: 3.69
          Mean value_function loss: 227.4517
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.1968
                       Mean reward: 838.63
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.5901
     Episode_Reward/lifting_object: 159.3618
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.26s
                      Time elapsed: 00:57:07
                               ETA: 05:19:29

################################################################################
                    [1m Learning iteration 1517/10000 [0m                     

                       Computation: 44713 steps/s (collection: 2.108s, learning 0.091s)
             Mean action noise std: 3.69
          Mean value_function loss: 237.4941
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.2148
                       Mean reward: 785.21
               Mean episode length: 216.67
    Episode_Reward/reaching_object: 1.5680
     Episode_Reward/lifting_object: 157.1140
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.20s
                      Time elapsed: 00:57:09
                               ETA: 05:19:27

################################################################################
                    [1m Learning iteration 1518/10000 [0m                     

                       Computation: 44011 steps/s (collection: 2.086s, learning 0.148s)
             Mean action noise std: 3.69
          Mean value_function loss: 231.8816
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.2255
                       Mean reward: 802.65
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.5571
     Episode_Reward/lifting_object: 156.0414
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.23s
                      Time elapsed: 00:57:12
                               ETA: 05:19:24

################################################################################
                    [1m Learning iteration 1519/10000 [0m                     

                       Computation: 44707 steps/s (collection: 2.051s, learning 0.148s)
             Mean action noise std: 3.69
          Mean value_function loss: 236.9846
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.2400
                       Mean reward: 805.81
               Mean episode length: 223.18
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 158.7005
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.20s
                      Time elapsed: 00:57:14
                               ETA: 05:19:22

################################################################################
                    [1m Learning iteration 1520/10000 [0m                     

                       Computation: 43928 steps/s (collection: 2.108s, learning 0.130s)
             Mean action noise std: 3.70
          Mean value_function loss: 253.4450
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.2582
                       Mean reward: 830.86
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.5508
     Episode_Reward/lifting_object: 154.8934
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.24s
                      Time elapsed: 00:57:16
                               ETA: 05:19:19

################################################################################
                    [1m Learning iteration 1521/10000 [0m                     

                       Computation: 38966 steps/s (collection: 2.350s, learning 0.173s)
             Mean action noise std: 3.70
          Mean value_function loss: 238.8391
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.2771
                       Mean reward: 777.26
               Mean episode length: 214.51
    Episode_Reward/reaching_object: 1.5626
     Episode_Reward/lifting_object: 156.5784
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.52s
                      Time elapsed: 00:57:19
                               ETA: 05:19:19

################################################################################
                    [1m Learning iteration 1522/10000 [0m                     

                       Computation: 44562 steps/s (collection: 2.092s, learning 0.114s)
             Mean action noise std: 3.70
          Mean value_function loss: 207.0590
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.2904
                       Mean reward: 774.42
               Mean episode length: 214.22
    Episode_Reward/reaching_object: 1.5531
     Episode_Reward/lifting_object: 155.1091
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.21s
                      Time elapsed: 00:57:21
                               ETA: 05:19:16

################################################################################
                    [1m Learning iteration 1523/10000 [0m                     

                       Computation: 40624 steps/s (collection: 2.271s, learning 0.148s)
             Mean action noise std: 3.70
          Mean value_function loss: 224.3499
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.3099
                       Mean reward: 799.14
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.5950
     Episode_Reward/lifting_object: 160.4892
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.42s
                      Time elapsed: 00:57:23
                               ETA: 05:19:15

################################################################################
                    [1m Learning iteration 1524/10000 [0m                     

                       Computation: 43728 steps/s (collection: 2.130s, learning 0.118s)
             Mean action noise std: 3.71
          Mean value_function loss: 220.4578
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.3268
                       Mean reward: 774.76
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 1.5847
     Episode_Reward/lifting_object: 158.7673
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.25s
                      Time elapsed: 00:57:25
                               ETA: 05:19:12

################################################################################
                    [1m Learning iteration 1525/10000 [0m                     

                       Computation: 44916 steps/s (collection: 2.066s, learning 0.123s)
             Mean action noise std: 3.71
          Mean value_function loss: 173.3513
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.3385
                       Mean reward: 805.01
               Mean episode length: 223.06
    Episode_Reward/reaching_object: 1.6088
     Episode_Reward/lifting_object: 161.5018
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.19s
                      Time elapsed: 00:57:28
                               ETA: 05:19:10

################################################################################
                    [1m Learning iteration 1526/10000 [0m                     

                       Computation: 43234 steps/s (collection: 2.114s, learning 0.160s)
             Mean action noise std: 3.71
          Mean value_function loss: 216.1857
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.3520
                       Mean reward: 784.43
               Mean episode length: 219.31
    Episode_Reward/reaching_object: 1.5638
     Episode_Reward/lifting_object: 156.3851
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.27s
                      Time elapsed: 00:57:30
                               ETA: 05:19:08

################################################################################
                    [1m Learning iteration 1527/10000 [0m                     

                       Computation: 43756 steps/s (collection: 2.126s, learning 0.121s)
             Mean action noise std: 3.71
          Mean value_function loss: 219.7492
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.3688
                       Mean reward: 764.13
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.5806
     Episode_Reward/lifting_object: 158.0125
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.25s
                      Time elapsed: 00:57:32
                               ETA: 05:19:05

################################################################################
                    [1m Learning iteration 1528/10000 [0m                     

                       Computation: 44795 steps/s (collection: 2.069s, learning 0.125s)
             Mean action noise std: 3.71
          Mean value_function loss: 219.8637
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.3839
                       Mean reward: 801.15
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.5884
     Episode_Reward/lifting_object: 159.1853
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.19s
                      Time elapsed: 00:57:34
                               ETA: 05:19:03

################################################################################
                    [1m Learning iteration 1529/10000 [0m                     

                       Computation: 44739 steps/s (collection: 2.101s, learning 0.096s)
             Mean action noise std: 3.72
          Mean value_function loss: 189.5880
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.3993
                       Mean reward: 786.52
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 1.5639
     Episode_Reward/lifting_object: 156.2280
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.20s
                      Time elapsed: 00:57:37
                               ETA: 05:19:00

################################################################################
                    [1m Learning iteration 1530/10000 [0m                     

                       Computation: 45550 steps/s (collection: 2.038s, learning 0.120s)
             Mean action noise std: 3.72
          Mean value_function loss: 210.4630
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 83.4121
                       Mean reward: 809.73
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.6000
     Episode_Reward/lifting_object: 160.1106
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.16s
                      Time elapsed: 00:57:39
                               ETA: 05:18:57

################################################################################
                    [1m Learning iteration 1531/10000 [0m                     

                       Computation: 44566 steps/s (collection: 2.076s, learning 0.130s)
             Mean action noise std: 3.72
          Mean value_function loss: 194.2136
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.4217
                       Mean reward: 783.83
               Mean episode length: 217.06
    Episode_Reward/reaching_object: 1.5696
     Episode_Reward/lifting_object: 156.3032
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.21s
                      Time elapsed: 00:57:41
                               ETA: 05:18:55

################################################################################
                    [1m Learning iteration 1532/10000 [0m                     

                       Computation: 41808 steps/s (collection: 2.193s, learning 0.158s)
             Mean action noise std: 3.72
          Mean value_function loss: 185.7046
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.4390
                       Mean reward: 798.19
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.6099
     Episode_Reward/lifting_object: 161.6148
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.35s
                      Time elapsed: 00:57:43
                               ETA: 05:18:53

################################################################################
                    [1m Learning iteration 1533/10000 [0m                     

                       Computation: 45014 steps/s (collection: 2.086s, learning 0.098s)
             Mean action noise std: 3.72
          Mean value_function loss: 174.6041
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 83.4491
                       Mean reward: 804.87
               Mean episode length: 221.49
    Episode_Reward/reaching_object: 1.6198
     Episode_Reward/lifting_object: 162.2687
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.18s
                      Time elapsed: 00:57:45
                               ETA: 05:18:50

################################################################################
                    [1m Learning iteration 1534/10000 [0m                     

                       Computation: 45175 steps/s (collection: 2.073s, learning 0.103s)
             Mean action noise std: 3.72
          Mean value_function loss: 194.9623
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 83.4647
                       Mean reward: 833.95
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.6067
     Episode_Reward/lifting_object: 160.8130
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.18s
                      Time elapsed: 00:57:48
                               ETA: 05:18:48

################################################################################
                    [1m Learning iteration 1535/10000 [0m                     

                       Computation: 45077 steps/s (collection: 2.064s, learning 0.116s)
             Mean action noise std: 3.73
          Mean value_function loss: 182.4635
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.4726
                       Mean reward: 798.44
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.6053
     Episode_Reward/lifting_object: 160.6327
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.18s
                      Time elapsed: 00:57:50
                               ETA: 05:18:45

################################################################################
                    [1m Learning iteration 1536/10000 [0m                     

                       Computation: 45233 steps/s (collection: 2.049s, learning 0.124s)
             Mean action noise std: 3.73
          Mean value_function loss: 222.8697
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.4836
                       Mean reward: 823.42
               Mean episode length: 225.59
    Episode_Reward/reaching_object: 1.6338
     Episode_Reward/lifting_object: 163.0601
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.17s
                      Time elapsed: 00:57:52
                               ETA: 05:18:42

################################################################################
                    [1m Learning iteration 1537/10000 [0m                     

                       Computation: 45037 steps/s (collection: 2.067s, learning 0.116s)
             Mean action noise std: 3.73
          Mean value_function loss: 203.7081
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.5027
                       Mean reward: 820.51
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 1.6025
     Episode_Reward/lifting_object: 160.3745
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.18s
                      Time elapsed: 00:57:54
                               ETA: 05:18:39

################################################################################
                    [1m Learning iteration 1538/10000 [0m                     

                       Computation: 40674 steps/s (collection: 2.301s, learning 0.116s)
             Mean action noise std: 3.73
          Mean value_function loss: 253.0506
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 83.5197
                       Mean reward: 797.57
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 1.5770
     Episode_Reward/lifting_object: 157.6604
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.42s
                      Time elapsed: 00:57:57
                               ETA: 05:18:38

################################################################################
                    [1m Learning iteration 1539/10000 [0m                     

                       Computation: 43912 steps/s (collection: 2.103s, learning 0.135s)
             Mean action noise std: 3.73
          Mean value_function loss: 223.3207
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.5282
                       Mean reward: 809.17
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.5809
     Episode_Reward/lifting_object: 157.1031
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.24s
                      Time elapsed: 00:57:59
                               ETA: 05:18:36

################################################################################
                    [1m Learning iteration 1540/10000 [0m                     

                       Computation: 44655 steps/s (collection: 2.107s, learning 0.095s)
             Mean action noise std: 3.73
          Mean value_function loss: 173.8325
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.5392
                       Mean reward: 802.67
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.6189
     Episode_Reward/lifting_object: 162.2092
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.20s
                      Time elapsed: 00:58:01
                               ETA: 05:18:33

################################################################################
                    [1m Learning iteration 1541/10000 [0m                     

                       Computation: 40978 steps/s (collection: 2.278s, learning 0.121s)
             Mean action noise std: 3.74
          Mean value_function loss: 211.2090
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.5500
                       Mean reward: 787.19
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 1.5784
     Episode_Reward/lifting_object: 157.6758
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.40s
                      Time elapsed: 00:58:03
                               ETA: 05:18:32

################################################################################
                    [1m Learning iteration 1542/10000 [0m                     

                       Computation: 41635 steps/s (collection: 2.184s, learning 0.177s)
             Mean action noise std: 3.74
          Mean value_function loss: 221.5606
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.5645
                       Mean reward: 764.97
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 158.3323
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.36s
                      Time elapsed: 00:58:06
                               ETA: 05:18:30

################################################################################
                    [1m Learning iteration 1543/10000 [0m                     

                       Computation: 41780 steps/s (collection: 2.244s, learning 0.109s)
             Mean action noise std: 3.74
          Mean value_function loss: 234.6087
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.5736
                       Mean reward: 794.18
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.5739
     Episode_Reward/lifting_object: 156.9512
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.35s
                      Time elapsed: 00:58:08
                               ETA: 05:18:28

################################################################################
                    [1m Learning iteration 1544/10000 [0m                     

                       Computation: 45204 steps/s (collection: 2.075s, learning 0.100s)
             Mean action noise std: 3.74
          Mean value_function loss: 248.8549
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.5886
                       Mean reward: 789.71
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.5425
     Episode_Reward/lifting_object: 153.6191
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.17s
                      Time elapsed: 00:58:10
                               ETA: 05:18:25

################################################################################
                    [1m Learning iteration 1545/10000 [0m                     

                       Computation: 45335 steps/s (collection: 2.048s, learning 0.121s)
             Mean action noise std: 3.74
          Mean value_function loss: 181.8651
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.5991
                       Mean reward: 773.59
               Mean episode length: 214.22
    Episode_Reward/reaching_object: 1.5903
     Episode_Reward/lifting_object: 159.3298
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.17s
                      Time elapsed: 00:58:13
                               ETA: 05:18:23

################################################################################
                    [1m Learning iteration 1546/10000 [0m                     

                       Computation: 45234 steps/s (collection: 2.063s, learning 0.111s)
             Mean action noise std: 3.75
          Mean value_function loss: 209.6115
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.6153
                       Mean reward: 768.50
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 1.5726
     Episode_Reward/lifting_object: 157.6352
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.17s
                      Time elapsed: 00:58:15
                               ETA: 05:18:20

################################################################################
                    [1m Learning iteration 1547/10000 [0m                     

                       Computation: 44348 steps/s (collection: 2.104s, learning 0.113s)
             Mean action noise std: 3.75
          Mean value_function loss: 225.0852
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.6421
                       Mean reward: 790.02
               Mean episode length: 216.93
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 157.5583
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.22s
                      Time elapsed: 00:58:17
                               ETA: 05:18:17

################################################################################
                    [1m Learning iteration 1548/10000 [0m                     

                       Computation: 44580 steps/s (collection: 2.103s, learning 0.102s)
             Mean action noise std: 3.75
          Mean value_function loss: 249.9679
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.6643
                       Mean reward: 759.82
               Mean episode length: 209.37
    Episode_Reward/reaching_object: 1.5321
     Episode_Reward/lifting_object: 153.5948
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.21s
                      Time elapsed: 00:58:19
                               ETA: 05:18:15

################################################################################
                    [1m Learning iteration 1549/10000 [0m                     

                       Computation: 45837 steps/s (collection: 2.046s, learning 0.099s)
             Mean action noise std: 3.75
          Mean value_function loss: 236.5140
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.6757
                       Mean reward: 801.39
               Mean episode length: 218.95
    Episode_Reward/reaching_object: 1.5596
     Episode_Reward/lifting_object: 156.9358
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.14s
                      Time elapsed: 00:58:21
                               ETA: 05:18:12

################################################################################
                    [1m Learning iteration 1550/10000 [0m                     

                       Computation: 45761 steps/s (collection: 2.056s, learning 0.092s)
             Mean action noise std: 3.75
          Mean value_function loss: 206.2316
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.6847
                       Mean reward: 799.55
               Mean episode length: 219.84
    Episode_Reward/reaching_object: 1.5819
     Episode_Reward/lifting_object: 158.8889
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.15s
                      Time elapsed: 00:58:23
                               ETA: 05:18:09

################################################################################
                    [1m Learning iteration 1551/10000 [0m                     

                       Computation: 44032 steps/s (collection: 2.142s, learning 0.091s)
             Mean action noise std: 3.76
          Mean value_function loss: 200.3477
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.6954
                       Mean reward: 851.80
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 1.6183
     Episode_Reward/lifting_object: 163.4416
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.23s
                      Time elapsed: 00:58:26
                               ETA: 05:18:07

################################################################################
                    [1m Learning iteration 1552/10000 [0m                     

                       Computation: 43550 steps/s (collection: 2.104s, learning 0.154s)
             Mean action noise std: 3.76
          Mean value_function loss: 208.4786
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.7078
                       Mean reward: 783.93
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 1.5806
     Episode_Reward/lifting_object: 158.6676
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.26s
                      Time elapsed: 00:58:28
                               ETA: 05:18:04

################################################################################
                    [1m Learning iteration 1553/10000 [0m                     

                       Computation: 44414 steps/s (collection: 2.108s, learning 0.105s)
             Mean action noise std: 3.76
          Mean value_function loss: 197.5657
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.7199
                       Mean reward: 819.83
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 1.5994
     Episode_Reward/lifting_object: 161.0299
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.21s
                      Time elapsed: 00:58:30
                               ETA: 05:18:02

################################################################################
                    [1m Learning iteration 1554/10000 [0m                     

                       Computation: 44617 steps/s (collection: 2.103s, learning 0.100s)
             Mean action noise std: 3.76
          Mean value_function loss: 213.8360
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.7402
                       Mean reward: 817.27
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.5920
     Episode_Reward/lifting_object: 160.2775
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.20s
                      Time elapsed: 00:58:32
                               ETA: 05:17:59

################################################################################
                    [1m Learning iteration 1555/10000 [0m                     

                       Computation: 44385 steps/s (collection: 2.099s, learning 0.116s)
             Mean action noise std: 3.76
          Mean value_function loss: 188.9715
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 83.7583
                       Mean reward: 842.51
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.5838
     Episode_Reward/lifting_object: 159.6773
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.21s
                      Time elapsed: 00:58:35
                               ETA: 05:17:57

################################################################################
                    [1m Learning iteration 1556/10000 [0m                     

                       Computation: 44526 steps/s (collection: 2.114s, learning 0.094s)
             Mean action noise std: 3.76
          Mean value_function loss: 188.7176
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.7679
                       Mean reward: 788.48
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 1.6303
     Episode_Reward/lifting_object: 164.6698
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.21s
                      Time elapsed: 00:58:37
                               ETA: 05:17:54

################################################################################
                    [1m Learning iteration 1557/10000 [0m                     

                       Computation: 44543 steps/s (collection: 2.087s, learning 0.120s)
             Mean action noise std: 3.77
          Mean value_function loss: 184.1779
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 83.7775
                       Mean reward: 841.21
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 1.5663
     Episode_Reward/lifting_object: 157.6110
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.21s
                      Time elapsed: 00:58:39
                               ETA: 05:17:52

################################################################################
                    [1m Learning iteration 1558/10000 [0m                     

                       Computation: 45295 steps/s (collection: 2.066s, learning 0.104s)
             Mean action noise std: 3.77
          Mean value_function loss: 187.6193
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.7925
                       Mean reward: 804.58
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 1.6299
     Episode_Reward/lifting_object: 164.7142
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.17s
                      Time elapsed: 00:58:41
                               ETA: 05:17:49

################################################################################
                    [1m Learning iteration 1559/10000 [0m                     

                       Computation: 44380 steps/s (collection: 2.120s, learning 0.095s)
             Mean action noise std: 3.77
          Mean value_function loss: 209.0556
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.8117
                       Mean reward: 757.95
               Mean episode length: 213.61
    Episode_Reward/reaching_object: 1.5969
     Episode_Reward/lifting_object: 160.6409
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.22s
                      Time elapsed: 00:58:43
                               ETA: 05:17:47

################################################################################
                    [1m Learning iteration 1560/10000 [0m                     

                       Computation: 45679 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 3.77
          Mean value_function loss: 163.7480
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.8262
                       Mean reward: 815.87
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.5986
     Episode_Reward/lifting_object: 161.8140
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.15s
                      Time elapsed: 00:58:45
                               ETA: 05:17:44

################################################################################
                    [1m Learning iteration 1561/10000 [0m                     

                       Computation: 45052 steps/s (collection: 2.080s, learning 0.102s)
             Mean action noise std: 3.77
          Mean value_function loss: 176.5307
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.8355
                       Mean reward: 786.46
               Mean episode length: 218.00
    Episode_Reward/reaching_object: 1.6211
     Episode_Reward/lifting_object: 163.6047
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.18s
                      Time elapsed: 00:58:48
                               ETA: 05:17:41

################################################################################
                    [1m Learning iteration 1562/10000 [0m                     

                       Computation: 45509 steps/s (collection: 2.068s, learning 0.093s)
             Mean action noise std: 3.78
          Mean value_function loss: 194.5071
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.8429
                       Mean reward: 778.03
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.5654
     Episode_Reward/lifting_object: 156.7208
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.16s
                      Time elapsed: 00:58:50
                               ETA: 05:17:38

################################################################################
                    [1m Learning iteration 1563/10000 [0m                     

                       Computation: 43933 steps/s (collection: 2.136s, learning 0.102s)
             Mean action noise std: 3.78
          Mean value_function loss: 180.1194
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.8595
                       Mean reward: 808.92
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 1.6306
     Episode_Reward/lifting_object: 164.8651
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.24s
                      Time elapsed: 00:58:52
                               ETA: 05:17:36

################################################################################
                    [1m Learning iteration 1564/10000 [0m                     

                       Computation: 44874 steps/s (collection: 2.090s, learning 0.101s)
             Mean action noise std: 3.78
          Mean value_function loss: 203.0038
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.8823
                       Mean reward: 832.79
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.5965
     Episode_Reward/lifting_object: 161.1841
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.19s
                      Time elapsed: 00:58:54
                               ETA: 05:17:33

################################################################################
                    [1m Learning iteration 1565/10000 [0m                     

                       Computation: 44931 steps/s (collection: 2.089s, learning 0.099s)
             Mean action noise std: 3.78
          Mean value_function loss: 221.1139
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.8954
                       Mean reward: 805.59
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 1.6033
     Episode_Reward/lifting_object: 160.9393
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.19s
                      Time elapsed: 00:58:56
                               ETA: 05:17:31

################################################################################
                    [1m Learning iteration 1566/10000 [0m                     

                       Computation: 44584 steps/s (collection: 2.101s, learning 0.104s)
             Mean action noise std: 3.78
          Mean value_function loss: 193.8394
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.9106
                       Mean reward: 786.73
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 1.6176
     Episode_Reward/lifting_object: 162.8469
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.20s
                      Time elapsed: 00:58:59
                               ETA: 05:17:28

################################################################################
                    [1m Learning iteration 1567/10000 [0m                     

                       Computation: 44672 steps/s (collection: 2.069s, learning 0.131s)
             Mean action noise std: 3.79
          Mean value_function loss: 201.6081
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.9257
                       Mean reward: 810.70
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.5848
     Episode_Reward/lifting_object: 159.2542
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.20s
                      Time elapsed: 00:59:01
                               ETA: 05:17:26

################################################################################
                    [1m Learning iteration 1568/10000 [0m                     

                       Computation: 44454 steps/s (collection: 2.076s, learning 0.136s)
             Mean action noise std: 3.79
          Mean value_function loss: 229.4948
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.9369
                       Mean reward: 782.68
               Mean episode length: 216.11
    Episode_Reward/reaching_object: 1.5866
     Episode_Reward/lifting_object: 159.8759
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.21s
                      Time elapsed: 00:59:03
                               ETA: 05:17:23

################################################################################
                    [1m Learning iteration 1569/10000 [0m                     

                       Computation: 44004 steps/s (collection: 2.110s, learning 0.124s)
             Mean action noise std: 3.79
          Mean value_function loss: 214.0301
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.9492
                       Mean reward: 796.68
               Mean episode length: 219.77
    Episode_Reward/reaching_object: 1.6085
     Episode_Reward/lifting_object: 161.9370
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.23s
                      Time elapsed: 00:59:05
                               ETA: 05:17:21

################################################################################
                    [1m Learning iteration 1570/10000 [0m                     

                       Computation: 44996 steps/s (collection: 2.074s, learning 0.111s)
             Mean action noise std: 3.79
          Mean value_function loss: 215.1086
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.9617
                       Mean reward: 804.28
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.6152
     Episode_Reward/lifting_object: 162.7327
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.18s
                      Time elapsed: 00:59:07
                               ETA: 05:17:18

################################################################################
                    [1m Learning iteration 1571/10000 [0m                     

                       Computation: 44293 steps/s (collection: 2.108s, learning 0.112s)
             Mean action noise std: 3.79
          Mean value_function loss: 196.4870
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.9746
                       Mean reward: 766.22
               Mean episode length: 211.09
    Episode_Reward/reaching_object: 1.5507
     Episode_Reward/lifting_object: 155.4706
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.22s
                      Time elapsed: 00:59:10
                               ETA: 05:17:15

################################################################################
                    [1m Learning iteration 1572/10000 [0m                     

                       Computation: 40535 steps/s (collection: 2.231s, learning 0.194s)
             Mean action noise std: 3.80
          Mean value_function loss: 157.6378
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.9923
                       Mean reward: 814.83
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.6257
     Episode_Reward/lifting_object: 164.8203
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.43s
                      Time elapsed: 00:59:12
                               ETA: 05:17:14

################################################################################
                    [1m Learning iteration 1573/10000 [0m                     

                       Computation: 44186 steps/s (collection: 2.118s, learning 0.107s)
             Mean action noise std: 3.80
          Mean value_function loss: 167.0310
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.0080
                       Mean reward: 834.07
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 1.6277
     Episode_Reward/lifting_object: 164.8797
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.22s
                      Time elapsed: 00:59:14
                               ETA: 05:17:12

################################################################################
                    [1m Learning iteration 1574/10000 [0m                     

                       Computation: 42638 steps/s (collection: 2.171s, learning 0.135s)
             Mean action noise std: 3.80
          Mean value_function loss: 185.0822
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 84.0249
                       Mean reward: 791.76
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 162.7046
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.31s
                      Time elapsed: 00:59:17
                               ETA: 05:17:10

################################################################################
                    [1m Learning iteration 1575/10000 [0m                     

                       Computation: 41147 steps/s (collection: 2.245s, learning 0.144s)
             Mean action noise std: 3.80
          Mean value_function loss: 180.1785
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.0428
                       Mean reward: 826.91
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.6388
     Episode_Reward/lifting_object: 166.1131
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.39s
                      Time elapsed: 00:59:19
                               ETA: 05:17:08

################################################################################
                    [1m Learning iteration 1576/10000 [0m                     

                       Computation: 43110 steps/s (collection: 2.178s, learning 0.103s)
             Mean action noise std: 3.80
          Mean value_function loss: 193.8937
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.0578
                       Mean reward: 772.41
               Mean episode length: 215.33
    Episode_Reward/reaching_object: 1.5615
     Episode_Reward/lifting_object: 156.6087
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.28s
                      Time elapsed: 00:59:21
                               ETA: 05:17:06

################################################################################
                    [1m Learning iteration 1577/10000 [0m                     

                       Computation: 43154 steps/s (collection: 2.145s, learning 0.133s)
             Mean action noise std: 3.81
          Mean value_function loss: 186.2778
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.0766
                       Mean reward: 830.00
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.6183
     Episode_Reward/lifting_object: 163.6999
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.28s
                      Time elapsed: 00:59:24
                               ETA: 05:17:04

################################################################################
                    [1m Learning iteration 1578/10000 [0m                     

                       Computation: 46050 steps/s (collection: 2.036s, learning 0.099s)
             Mean action noise std: 3.81
          Mean value_function loss: 154.4380
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.1010
                       Mean reward: 838.66
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.6219
     Episode_Reward/lifting_object: 164.8310
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.13s
                      Time elapsed: 00:59:26
                               ETA: 05:17:01

################################################################################
                    [1m Learning iteration 1579/10000 [0m                     

                       Computation: 44170 steps/s (collection: 2.126s, learning 0.100s)
             Mean action noise std: 3.81
          Mean value_function loss: 188.6107
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 84.1230
                       Mean reward: 800.45
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 1.6205
     Episode_Reward/lifting_object: 164.3319
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.23s
                      Time elapsed: 00:59:28
                               ETA: 05:16:58

################################################################################
                    [1m Learning iteration 1580/10000 [0m                     

                       Computation: 44390 steps/s (collection: 2.076s, learning 0.139s)
             Mean action noise std: 3.81
          Mean value_function loss: 202.0530
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.1357
                       Mean reward: 830.89
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.5715
     Episode_Reward/lifting_object: 158.8274
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.21s
                      Time elapsed: 00:59:30
                               ETA: 05:16:56

################################################################################
                    [1m Learning iteration 1581/10000 [0m                     

                       Computation: 44022 steps/s (collection: 2.119s, learning 0.114s)
             Mean action noise std: 3.82
          Mean value_function loss: 212.8275
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.1516
                       Mean reward: 797.97
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 160.5240
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.23s
                      Time elapsed: 00:59:32
                               ETA: 05:16:54

################################################################################
                    [1m Learning iteration 1582/10000 [0m                     

                       Computation: 45030 steps/s (collection: 2.077s, learning 0.106s)
             Mean action noise std: 3.82
          Mean value_function loss: 223.3301
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.1687
                       Mean reward: 791.17
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.5827
     Episode_Reward/lifting_object: 160.3026
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.18s
                      Time elapsed: 00:59:35
                               ETA: 05:16:51

################################################################################
                    [1m Learning iteration 1583/10000 [0m                     

                       Computation: 43335 steps/s (collection: 2.139s, learning 0.129s)
             Mean action noise std: 3.82
          Mean value_function loss: 158.5601
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.1927
                       Mean reward: 787.23
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.6222
     Episode_Reward/lifting_object: 164.1287
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.27s
                      Time elapsed: 00:59:37
                               ETA: 05:16:49

################################################################################
                    [1m Learning iteration 1584/10000 [0m                     

                       Computation: 45256 steps/s (collection: 2.066s, learning 0.107s)
             Mean action noise std: 3.82
          Mean value_function loss: 184.9175
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.2040
                       Mean reward: 798.38
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.5680
     Episode_Reward/lifting_object: 157.5757
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.17s
                      Time elapsed: 00:59:39
                               ETA: 05:16:46

################################################################################
                    [1m Learning iteration 1585/10000 [0m                     

                       Computation: 44275 steps/s (collection: 2.097s, learning 0.124s)
             Mean action noise std: 3.82
          Mean value_function loss: 168.6161
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.2118
                       Mean reward: 834.36
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 1.6310
     Episode_Reward/lifting_object: 166.0542
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.22s
                      Time elapsed: 00:59:41
                               ETA: 05:16:44

################################################################################
                    [1m Learning iteration 1586/10000 [0m                     

                       Computation: 43386 steps/s (collection: 2.169s, learning 0.097s)
             Mean action noise std: 3.83
          Mean value_function loss: 182.9031
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.2257
                       Mean reward: 822.65
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 1.5843
     Episode_Reward/lifting_object: 160.5039
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.27s
                      Time elapsed: 00:59:44
                               ETA: 05:16:41

################################################################################
                    [1m Learning iteration 1587/10000 [0m                     

                       Computation: 41758 steps/s (collection: 2.221s, learning 0.134s)
             Mean action noise std: 3.83
          Mean value_function loss: 187.7347
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.2377
                       Mean reward: 844.63
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.6240
     Episode_Reward/lifting_object: 164.2692
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.35s
                      Time elapsed: 00:59:46
                               ETA: 05:16:40

################################################################################
                    [1m Learning iteration 1588/10000 [0m                     

                       Computation: 41631 steps/s (collection: 2.204s, learning 0.157s)
             Mean action noise std: 3.83
          Mean value_function loss: 197.2092
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.2524
                       Mean reward: 819.14
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 1.5985
     Episode_Reward/lifting_object: 161.8395
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.36s
                      Time elapsed: 00:59:48
                               ETA: 05:16:38

################################################################################
                    [1m Learning iteration 1589/10000 [0m                     

                       Computation: 44389 steps/s (collection: 2.111s, learning 0.104s)
             Mean action noise std: 3.83
          Mean value_function loss: 165.3011
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.2690
                       Mean reward: 791.21
               Mean episode length: 221.14
    Episode_Reward/reaching_object: 1.6227
     Episode_Reward/lifting_object: 164.6801
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.21s
                      Time elapsed: 00:59:50
                               ETA: 05:16:35

################################################################################
                    [1m Learning iteration 1590/10000 [0m                     

                       Computation: 45066 steps/s (collection: 2.085s, learning 0.096s)
             Mean action noise std: 3.83
          Mean value_function loss: 187.2448
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.2858
                       Mean reward: 806.98
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.6340
     Episode_Reward/lifting_object: 166.7514
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.18s
                      Time elapsed: 00:59:53
                               ETA: 05:16:33

################################################################################
                    [1m Learning iteration 1591/10000 [0m                     

                       Computation: 45145 steps/s (collection: 2.066s, learning 0.112s)
             Mean action noise std: 3.84
          Mean value_function loss: 214.6700
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.3002
                       Mean reward: 821.64
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 1.5577
     Episode_Reward/lifting_object: 157.3835
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.18s
                      Time elapsed: 00:59:55
                               ETA: 05:16:30

################################################################################
                    [1m Learning iteration 1592/10000 [0m                     

                       Computation: 44097 steps/s (collection: 2.118s, learning 0.111s)
             Mean action noise std: 3.84
          Mean value_function loss: 201.0722
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.3135
                       Mean reward: 790.03
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 1.5532
     Episode_Reward/lifting_object: 157.4827
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.23s
                      Time elapsed: 00:59:57
                               ETA: 05:16:28

################################################################################
                    [1m Learning iteration 1593/10000 [0m                     

                       Computation: 44386 steps/s (collection: 2.088s, learning 0.127s)
             Mean action noise std: 3.84
          Mean value_function loss: 180.8902
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 84.3304
                       Mean reward: 828.57
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 1.5727
     Episode_Reward/lifting_object: 159.9478
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.21s
                      Time elapsed: 00:59:59
                               ETA: 05:16:25

################################################################################
                    [1m Learning iteration 1594/10000 [0m                     

                       Computation: 44997 steps/s (collection: 2.091s, learning 0.094s)
             Mean action noise std: 3.84
          Mean value_function loss: 185.1810
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.3367
                       Mean reward: 785.94
               Mean episode length: 216.04
    Episode_Reward/reaching_object: 1.5354
     Episode_Reward/lifting_object: 154.8311
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.18s
                      Time elapsed: 01:00:01
                               ETA: 05:16:22

################################################################################
                    [1m Learning iteration 1595/10000 [0m                     

                       Computation: 44283 steps/s (collection: 2.107s, learning 0.113s)
             Mean action noise std: 3.84
          Mean value_function loss: 158.6201
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.3492
                       Mean reward: 800.48
               Mean episode length: 221.23
    Episode_Reward/reaching_object: 1.6415
     Episode_Reward/lifting_object: 167.0048
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.22s
                      Time elapsed: 01:00:04
                               ETA: 05:16:20

################################################################################
                    [1m Learning iteration 1596/10000 [0m                     

                       Computation: 43805 steps/s (collection: 2.124s, learning 0.120s)
             Mean action noise std: 3.84
          Mean value_function loss: 162.5392
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.3643
                       Mean reward: 786.80
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 1.6144
     Episode_Reward/lifting_object: 163.7101
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.24s
                      Time elapsed: 01:00:06
                               ETA: 05:16:18

################################################################################
                    [1m Learning iteration 1597/10000 [0m                     

                       Computation: 44415 steps/s (collection: 2.090s, learning 0.124s)
             Mean action noise std: 3.85
          Mean value_function loss: 179.7009
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.3783
                       Mean reward: 838.61
               Mean episode length: 230.52
    Episode_Reward/reaching_object: 1.6149
     Episode_Reward/lifting_object: 163.6155
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.21s
                      Time elapsed: 01:00:08
                               ETA: 05:16:15

################################################################################
                    [1m Learning iteration 1598/10000 [0m                     

                       Computation: 44783 steps/s (collection: 2.093s, learning 0.102s)
             Mean action noise std: 3.85
          Mean value_function loss: 186.1691
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.4001
                       Mean reward: 777.15
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 1.6033
     Episode_Reward/lifting_object: 162.8305
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.20s
                      Time elapsed: 01:00:10
                               ETA: 05:16:13

################################################################################
                    [1m Learning iteration 1599/10000 [0m                     

                       Computation: 45516 steps/s (collection: 2.064s, learning 0.096s)
             Mean action noise std: 3.85
          Mean value_function loss: 216.3337
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.4241
                       Mean reward: 744.81
               Mean episode length: 208.00
    Episode_Reward/reaching_object: 1.5542
     Episode_Reward/lifting_object: 157.1385
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.16s
                      Time elapsed: 01:00:12
                               ETA: 05:16:10

################################################################################
                    [1m Learning iteration 1600/10000 [0m                     

                       Computation: 44395 steps/s (collection: 2.124s, learning 0.091s)
             Mean action noise std: 3.85
          Mean value_function loss: 235.9806
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.4374
                       Mean reward: 803.34
               Mean episode length: 219.72
    Episode_Reward/reaching_object: 1.6108
     Episode_Reward/lifting_object: 164.1796
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.21s
                      Time elapsed: 01:00:15
                               ETA: 05:16:07

################################################################################
                    [1m Learning iteration 1601/10000 [0m                     

                       Computation: 44497 steps/s (collection: 2.098s, learning 0.111s)
             Mean action noise std: 3.85
          Mean value_function loss: 216.0942
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.4469
                       Mean reward: 831.40
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 1.5719
     Episode_Reward/lifting_object: 159.3885
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.21s
                      Time elapsed: 01:00:17
                               ETA: 05:16:05

################################################################################
                    [1m Learning iteration 1602/10000 [0m                     

                       Computation: 45142 steps/s (collection: 2.087s, learning 0.091s)
             Mean action noise std: 3.85
          Mean value_function loss: 218.2253
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.4518
                       Mean reward: 807.09
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.6066
     Episode_Reward/lifting_object: 163.1081
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.18s
                      Time elapsed: 01:00:19
                               ETA: 05:16:02

################################################################################
                    [1m Learning iteration 1603/10000 [0m                     

                       Computation: 44160 steps/s (collection: 2.101s, learning 0.125s)
             Mean action noise std: 3.86
          Mean value_function loss: 237.8240
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.4582
                       Mean reward: 781.12
               Mean episode length: 216.27
    Episode_Reward/reaching_object: 1.5716
     Episode_Reward/lifting_object: 159.2039
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.23s
                      Time elapsed: 01:00:21
                               ETA: 05:16:00

################################################################################
                    [1m Learning iteration 1604/10000 [0m                     

                       Computation: 43882 steps/s (collection: 2.134s, learning 0.107s)
             Mean action noise std: 3.86
          Mean value_function loss: 202.3644
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.4647
                       Mean reward: 879.23
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.6142
     Episode_Reward/lifting_object: 164.2653
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0964
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.24s
                      Time elapsed: 01:00:24
                               ETA: 05:15:57

################################################################################
                    [1m Learning iteration 1605/10000 [0m                     

                       Computation: 45662 steps/s (collection: 2.053s, learning 0.100s)
             Mean action noise std: 3.86
          Mean value_function loss: 202.1497
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 84.4744
                       Mean reward: 741.26
               Mean episode length: 205.96
    Episode_Reward/reaching_object: 1.5553
     Episode_Reward/lifting_object: 157.3297
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.15s
                      Time elapsed: 01:00:26
                               ETA: 05:15:55

################################################################################
                    [1m Learning iteration 1606/10000 [0m                     

                       Computation: 43794 steps/s (collection: 2.147s, learning 0.098s)
             Mean action noise std: 3.86
          Mean value_function loss: 199.4824
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.4889
                       Mean reward: 826.37
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.5883
     Episode_Reward/lifting_object: 161.2558
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.24s
                      Time elapsed: 01:00:28
                               ETA: 05:15:52

################################################################################
                    [1m Learning iteration 1607/10000 [0m                     

                       Computation: 45027 steps/s (collection: 2.087s, learning 0.096s)
             Mean action noise std: 3.86
          Mean value_function loss: 209.8253
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.5008
                       Mean reward: 799.65
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 1.6330
     Episode_Reward/lifting_object: 166.6140
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.18s
                      Time elapsed: 01:00:30
                               ETA: 05:15:50

################################################################################
                    [1m Learning iteration 1608/10000 [0m                     

                       Computation: 45416 steps/s (collection: 2.061s, learning 0.104s)
             Mean action noise std: 3.86
          Mean value_function loss: 173.9289
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.5200
                       Mean reward: 774.57
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 1.5718
     Episode_Reward/lifting_object: 158.5794
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.16s
                      Time elapsed: 01:00:32
                               ETA: 05:15:47

################################################################################
                    [1m Learning iteration 1609/10000 [0m                     

                       Computation: 45247 steps/s (collection: 2.024s, learning 0.149s)
             Mean action noise std: 3.87
          Mean value_function loss: 197.9260
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.5406
                       Mean reward: 828.05
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.6184
     Episode_Reward/lifting_object: 164.5045
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.17s
                      Time elapsed: 01:00:34
                               ETA: 05:15:44

################################################################################
                    [1m Learning iteration 1610/10000 [0m                     

                       Computation: 45845 steps/s (collection: 2.013s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 146.7292
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.5616
                       Mean reward: 841.73
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 1.6297
     Episode_Reward/lifting_object: 166.0773
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.14s
                      Time elapsed: 01:00:37
                               ETA: 05:15:41

################################################################################
                    [1m Learning iteration 1611/10000 [0m                     

                       Computation: 46201 steps/s (collection: 2.026s, learning 0.102s)
             Mean action noise std: 3.87
          Mean value_function loss: 193.2408
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.5780
                       Mean reward: 805.54
               Mean episode length: 219.11
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 157.5356
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.13s
                      Time elapsed: 01:00:39
                               ETA: 05:15:38

################################################################################
                    [1m Learning iteration 1612/10000 [0m                     

                       Computation: 45838 steps/s (collection: 2.031s, learning 0.114s)
             Mean action noise std: 3.87
          Mean value_function loss: 205.7448
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.5935
                       Mean reward: 777.37
               Mean episode length: 212.67
    Episode_Reward/reaching_object: 1.6120
     Episode_Reward/lifting_object: 163.8954
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.14s
                      Time elapsed: 01:00:41
                               ETA: 05:15:35

################################################################################
                    [1m Learning iteration 1613/10000 [0m                     

                       Computation: 46507 steps/s (collection: 2.015s, learning 0.099s)
             Mean action noise std: 3.88
          Mean value_function loss: 186.7597
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 84.6089
                       Mean reward: 830.94
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.6268
     Episode_Reward/lifting_object: 164.9997
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.11s
                      Time elapsed: 01:00:43
                               ETA: 05:15:32

################################################################################
                    [1m Learning iteration 1614/10000 [0m                     

                       Computation: 46893 steps/s (collection: 1.994s, learning 0.102s)
             Mean action noise std: 3.88
          Mean value_function loss: 232.1017
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.6228
                       Mean reward: 832.77
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.5829
     Episode_Reward/lifting_object: 160.5042
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.10s
                      Time elapsed: 01:00:45
                               ETA: 05:15:29

################################################################################
                    [1m Learning iteration 1615/10000 [0m                     

                       Computation: 46416 steps/s (collection: 2.004s, learning 0.114s)
             Mean action noise std: 3.88
          Mean value_function loss: 195.0225
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.6373
                       Mean reward: 786.31
               Mean episode length: 217.37
    Episode_Reward/reaching_object: 1.5732
     Episode_Reward/lifting_object: 159.1290
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.12s
                      Time elapsed: 01:00:47
                               ETA: 05:15:26

################################################################################
                    [1m Learning iteration 1616/10000 [0m                     

                       Computation: 46085 steps/s (collection: 2.039s, learning 0.095s)
             Mean action noise std: 3.88
          Mean value_function loss: 172.5199
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.6521
                       Mean reward: 855.11
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.5882
     Episode_Reward/lifting_object: 161.3284
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0964
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.13s
                      Time elapsed: 01:00:49
                               ETA: 05:15:24

################################################################################
                    [1m Learning iteration 1617/10000 [0m                     

                       Computation: 47176 steps/s (collection: 1.962s, learning 0.122s)
             Mean action noise std: 3.88
          Mean value_function loss: 191.9269
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 84.6643
                       Mean reward: 842.54
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.5920
     Episode_Reward/lifting_object: 160.8893
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.08s
                      Time elapsed: 01:00:51
                               ETA: 05:15:20

################################################################################
                    [1m Learning iteration 1618/10000 [0m                     

                       Computation: 46881 steps/s (collection: 1.997s, learning 0.100s)
             Mean action noise std: 3.89
          Mean value_function loss: 184.0543
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.6727
                       Mean reward: 808.50
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 1.6148
     Episode_Reward/lifting_object: 164.0026
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.10s
                      Time elapsed: 01:00:54
                               ETA: 05:15:17

################################################################################
                    [1m Learning iteration 1619/10000 [0m                     

                       Computation: 46729 steps/s (collection: 1.992s, learning 0.112s)
             Mean action noise std: 3.89
          Mean value_function loss: 179.8209
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.6844
                       Mean reward: 828.94
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 1.6215
     Episode_Reward/lifting_object: 164.3928
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.10s
                      Time elapsed: 01:00:56
                               ETA: 05:15:14

################################################################################
                    [1m Learning iteration 1620/10000 [0m                     

                       Computation: 45895 steps/s (collection: 2.027s, learning 0.115s)
             Mean action noise std: 3.89
          Mean value_function loss: 189.6063
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.6984
                       Mean reward: 821.61
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 1.6016
     Episode_Reward/lifting_object: 162.2590
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.14s
                      Time elapsed: 01:00:58
                               ETA: 05:15:11

################################################################################
                    [1m Learning iteration 1621/10000 [0m                     

                       Computation: 47071 steps/s (collection: 1.984s, learning 0.105s)
             Mean action noise std: 3.89
          Mean value_function loss: 169.4986
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.7110
                       Mean reward: 812.37
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.6245
     Episode_Reward/lifting_object: 164.5598
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.09s
                      Time elapsed: 01:01:00
                               ETA: 05:15:08

################################################################################
                    [1m Learning iteration 1622/10000 [0m                     

                       Computation: 46347 steps/s (collection: 2.026s, learning 0.095s)
             Mean action noise std: 3.89
          Mean value_function loss: 167.8493
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.7249
                       Mean reward: 820.41
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 1.6649
     Episode_Reward/lifting_object: 168.9462
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.12s
                      Time elapsed: 01:01:02
                               ETA: 05:15:05

################################################################################
                    [1m Learning iteration 1623/10000 [0m                     

                       Computation: 47070 steps/s (collection: 1.996s, learning 0.092s)
             Mean action noise std: 3.90
          Mean value_function loss: 181.9819
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.7375
                       Mean reward: 830.50
               Mean episode length: 227.35
    Episode_Reward/reaching_object: 1.6404
     Episode_Reward/lifting_object: 166.0097
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.09s
                      Time elapsed: 01:01:04
                               ETA: 05:15:02

################################################################################
                    [1m Learning iteration 1624/10000 [0m                     

                       Computation: 45005 steps/s (collection: 2.025s, learning 0.159s)
             Mean action noise std: 3.90
          Mean value_function loss: 192.5896
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.7542
                       Mean reward: 795.08
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 1.5790
     Episode_Reward/lifting_object: 159.2517
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.18s
                      Time elapsed: 01:01:06
                               ETA: 05:15:00

################################################################################
                    [1m Learning iteration 1625/10000 [0m                     

                       Computation: 45893 steps/s (collection: 2.045s, learning 0.097s)
             Mean action noise std: 3.90
          Mean value_function loss: 187.6807
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.7735
                       Mean reward: 833.91
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.6348
     Episode_Reward/lifting_object: 165.7422
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.14s
                      Time elapsed: 01:01:08
                               ETA: 05:14:57

################################################################################
                    [1m Learning iteration 1626/10000 [0m                     

                       Computation: 42909 steps/s (collection: 2.106s, learning 0.185s)
             Mean action noise std: 3.90
          Mean value_function loss: 189.9922
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 84.7920
                       Mean reward: 861.80
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.5841
     Episode_Reward/lifting_object: 159.7016
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.29s
                      Time elapsed: 01:01:11
                               ETA: 05:14:55

################################################################################
                    [1m Learning iteration 1627/10000 [0m                     

                       Computation: 46931 steps/s (collection: 1.980s, learning 0.115s)
             Mean action noise std: 3.90
          Mean value_function loss: 145.9471
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.7988
                       Mean reward: 845.57
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 1.6332
     Episode_Reward/lifting_object: 165.8137
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.09s
                      Time elapsed: 01:01:13
                               ETA: 05:14:52

################################################################################
                    [1m Learning iteration 1628/10000 [0m                     

                       Computation: 45719 steps/s (collection: 2.032s, learning 0.118s)
             Mean action noise std: 3.91
          Mean value_function loss: 176.1182
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.8105
                       Mean reward: 845.08
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 1.6225
     Episode_Reward/lifting_object: 164.6406
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.15s
                      Time elapsed: 01:01:15
                               ETA: 05:14:49

################################################################################
                    [1m Learning iteration 1629/10000 [0m                     

                       Computation: 47429 steps/s (collection: 1.983s, learning 0.090s)
             Mean action noise std: 3.91
          Mean value_function loss: 191.2234
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.8263
                       Mean reward: 854.72
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 1.6218
     Episode_Reward/lifting_object: 164.4164
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.07s
                      Time elapsed: 01:01:17
                               ETA: 05:14:46

################################################################################
                    [1m Learning iteration 1630/10000 [0m                     

                       Computation: 47151 steps/s (collection: 1.993s, learning 0.092s)
             Mean action noise std: 3.91
          Mean value_function loss: 202.8117
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.8412
                       Mean reward: 787.12
               Mean episode length: 217.77
    Episode_Reward/reaching_object: 1.6173
     Episode_Reward/lifting_object: 163.0725
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.08s
                      Time elapsed: 01:01:19
                               ETA: 05:14:42

################################################################################
                    [1m Learning iteration 1631/10000 [0m                     

                       Computation: 47683 steps/s (collection: 1.962s, learning 0.100s)
             Mean action noise std: 3.91
          Mean value_function loss: 170.5469
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.8623
                       Mean reward: 867.11
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.6770
     Episode_Reward/lifting_object: 170.7759
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.06s
                      Time elapsed: 01:01:21
                               ETA: 05:14:39

################################################################################
                    [1m Learning iteration 1632/10000 [0m                     

                       Computation: 46100 steps/s (collection: 2.023s, learning 0.109s)
             Mean action noise std: 3.91
          Mean value_function loss: 188.6803
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.8810
                       Mean reward: 833.98
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.6143
     Episode_Reward/lifting_object: 164.0784
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.13s
                      Time elapsed: 01:01:23
                               ETA: 05:14:36

################################################################################
                    [1m Learning iteration 1633/10000 [0m                     

                       Computation: 47138 steps/s (collection: 1.981s, learning 0.104s)
             Mean action noise std: 3.92
          Mean value_function loss: 172.7168
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 84.8958
                       Mean reward: 842.69
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.5854
     Episode_Reward/lifting_object: 160.2193
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.09s
                      Time elapsed: 01:01:25
                               ETA: 05:14:33

################################################################################
                    [1m Learning iteration 1634/10000 [0m                     

                       Computation: 47288 steps/s (collection: 1.989s, learning 0.089s)
             Mean action noise std: 3.92
          Mean value_function loss: 165.0810
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.9066
                       Mean reward: 856.92
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.6489
     Episode_Reward/lifting_object: 167.7035
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.08s
                      Time elapsed: 01:01:27
                               ETA: 05:14:30

################################################################################
                    [1m Learning iteration 1635/10000 [0m                     

                       Computation: 47367 steps/s (collection: 1.987s, learning 0.089s)
             Mean action noise std: 3.92
          Mean value_function loss: 174.4615
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.9201
                       Mean reward: 875.63
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 1.6498
     Episode_Reward/lifting_object: 167.4879
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.08s
                      Time elapsed: 01:01:30
                               ETA: 05:14:27

################################################################################
                    [1m Learning iteration 1636/10000 [0m                     

                       Computation: 44176 steps/s (collection: 2.084s, learning 0.142s)
             Mean action noise std: 3.92
          Mean value_function loss: 203.7811
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.9318
                       Mean reward: 809.28
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.6078
     Episode_Reward/lifting_object: 161.9584
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.23s
                      Time elapsed: 01:01:32
                               ETA: 05:14:24

################################################################################
                    [1m Learning iteration 1637/10000 [0m                     

                       Computation: 47105 steps/s (collection: 1.971s, learning 0.115s)
             Mean action noise std: 3.92
          Mean value_function loss: 171.8956
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.9454
                       Mean reward: 804.35
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 1.6430
     Episode_Reward/lifting_object: 166.6334
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.09s
                      Time elapsed: 01:01:34
                               ETA: 05:14:21

################################################################################
                    [1m Learning iteration 1638/10000 [0m                     

                       Computation: 46188 steps/s (collection: 2.008s, learning 0.121s)
             Mean action noise std: 3.92
          Mean value_function loss: 168.2519
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 84.9604
                       Mean reward: 819.09
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 160.3968
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.13s
                      Time elapsed: 01:01:36
                               ETA: 05:14:18

################################################################################
                    [1m Learning iteration 1639/10000 [0m                     

                       Computation: 47325 steps/s (collection: 1.980s, learning 0.098s)
             Mean action noise std: 3.93
          Mean value_function loss: 181.9251
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.9693
                       Mean reward: 805.80
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 1.5887
     Episode_Reward/lifting_object: 159.9784
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.08s
                      Time elapsed: 01:01:38
                               ETA: 05:14:15

################################################################################
                    [1m Learning iteration 1640/10000 [0m                     

                       Computation: 47895 steps/s (collection: 1.952s, learning 0.101s)
             Mean action noise std: 3.93
          Mean value_function loss: 170.2452
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.9894
                       Mean reward: 860.45
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.6285
     Episode_Reward/lifting_object: 164.8892
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.05s
                      Time elapsed: 01:01:40
                               ETA: 05:14:12

################################################################################
                    [1m Learning iteration 1641/10000 [0m                     

                       Computation: 46297 steps/s (collection: 2.027s, learning 0.096s)
             Mean action noise std: 3.93
          Mean value_function loss: 194.0517
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.0057
                       Mean reward: 791.60
               Mean episode length: 218.74
    Episode_Reward/reaching_object: 1.5989
     Episode_Reward/lifting_object: 161.1689
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.12s
                      Time elapsed: 01:01:42
                               ETA: 05:14:09

################################################################################
                    [1m Learning iteration 1642/10000 [0m                     

                       Computation: 48014 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 3.93
          Mean value_function loss: 181.3850
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.0198
                       Mean reward: 829.64
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 1.6135
     Episode_Reward/lifting_object: 162.9817
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.05s
                      Time elapsed: 01:01:44
                               ETA: 05:14:06

################################################################################
                    [1m Learning iteration 1643/10000 [0m                     

                       Computation: 46503 steps/s (collection: 1.974s, learning 0.140s)
             Mean action noise std: 3.93
          Mean value_function loss: 208.5632
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.0365
                       Mean reward: 789.11
               Mean episode length: 216.19
    Episode_Reward/reaching_object: 1.5837
     Episode_Reward/lifting_object: 159.9637
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.11s
                      Time elapsed: 01:01:46
                               ETA: 05:14:03

################################################################################
                    [1m Learning iteration 1644/10000 [0m                     

                       Computation: 45825 steps/s (collection: 2.055s, learning 0.090s)
             Mean action noise std: 3.94
          Mean value_function loss: 214.0313
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 85.0515
                       Mean reward: 773.05
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 1.6198
     Episode_Reward/lifting_object: 163.2110
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.15s
                      Time elapsed: 01:01:48
                               ETA: 05:14:00

################################################################################
                    [1m Learning iteration 1645/10000 [0m                     

                       Computation: 47306 steps/s (collection: 1.986s, learning 0.092s)
             Mean action noise std: 3.94
          Mean value_function loss: 175.6292
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 85.0569
                       Mean reward: 833.79
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.6342
     Episode_Reward/lifting_object: 165.0374
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.08s
                      Time elapsed: 01:01:51
                               ETA: 05:13:57

################################################################################
                    [1m Learning iteration 1646/10000 [0m                     

                       Computation: 47096 steps/s (collection: 1.983s, learning 0.104s)
             Mean action noise std: 3.94
          Mean value_function loss: 223.3154
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.0614
                       Mean reward: 844.71
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 1.5851
     Episode_Reward/lifting_object: 159.4171
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.09s
                      Time elapsed: 01:01:53
                               ETA: 05:13:54

################################################################################
                    [1m Learning iteration 1647/10000 [0m                     

                       Computation: 46496 steps/s (collection: 1.997s, learning 0.118s)
             Mean action noise std: 3.94
          Mean value_function loss: 201.4070
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.0759
                       Mean reward: 780.66
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 159.7605
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.11s
                      Time elapsed: 01:01:55
                               ETA: 05:13:51

################################################################################
                    [1m Learning iteration 1648/10000 [0m                     

                       Computation: 47727 steps/s (collection: 1.963s, learning 0.096s)
             Mean action noise std: 3.94
          Mean value_function loss: 192.9472
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.0912
                       Mean reward: 840.02
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 160.1319
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.06s
                      Time elapsed: 01:01:57
                               ETA: 05:13:47

################################################################################
                    [1m Learning iteration 1649/10000 [0m                     

                       Computation: 46475 steps/s (collection: 1.994s, learning 0.121s)
             Mean action noise std: 3.94
          Mean value_function loss: 180.6853
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.1092
                       Mean reward: 843.71
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.6122
     Episode_Reward/lifting_object: 163.0237
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.12s
                      Time elapsed: 01:01:59
                               ETA: 05:13:44

################################################################################
                    [1m Learning iteration 1650/10000 [0m                     

                       Computation: 47947 steps/s (collection: 1.958s, learning 0.093s)
             Mean action noise std: 3.95
          Mean value_function loss: 197.5518
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 85.1181
                       Mean reward: 813.16
               Mean episode length: 223.98
    Episode_Reward/reaching_object: 1.6148
     Episode_Reward/lifting_object: 163.1041
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.05s
                      Time elapsed: 01:02:01
                               ETA: 05:13:41

################################################################################
                    [1m Learning iteration 1651/10000 [0m                     

                       Computation: 44586 steps/s (collection: 2.051s, learning 0.154s)
             Mean action noise std: 3.95
          Mean value_function loss: 193.9042
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.1309
                       Mean reward: 850.97
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 1.6279
     Episode_Reward/lifting_object: 164.4878
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.20s
                      Time elapsed: 01:02:03
                               ETA: 05:13:39

################################################################################
                    [1m Learning iteration 1652/10000 [0m                     

                       Computation: 45038 steps/s (collection: 2.074s, learning 0.109s)
             Mean action noise std: 3.95
          Mean value_function loss: 193.1445
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.1466
                       Mean reward: 802.57
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.6088
     Episode_Reward/lifting_object: 163.0780
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.18s
                      Time elapsed: 01:02:05
                               ETA: 05:13:36

################################################################################
                    [1m Learning iteration 1653/10000 [0m                     

                       Computation: 44217 steps/s (collection: 2.131s, learning 0.092s)
             Mean action noise std: 3.95
          Mean value_function loss: 187.7848
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.1657
                       Mean reward: 844.45
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 1.6179
     Episode_Reward/lifting_object: 164.0737
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.22s
                      Time elapsed: 01:02:08
                               ETA: 05:13:34

################################################################################
                    [1m Learning iteration 1654/10000 [0m                     

                       Computation: 45978 steps/s (collection: 2.012s, learning 0.126s)
             Mean action noise std: 3.95
          Mean value_function loss: 205.8382
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.1841
                       Mean reward: 867.57
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.6222
     Episode_Reward/lifting_object: 164.6381
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.14s
                      Time elapsed: 01:02:10
                               ETA: 05:13:31

################################################################################
                    [1m Learning iteration 1655/10000 [0m                     

                       Computation: 43043 steps/s (collection: 2.197s, learning 0.087s)
             Mean action noise std: 3.96
          Mean value_function loss: 214.9229
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.1983
                       Mean reward: 767.76
               Mean episode length: 216.04
    Episode_Reward/reaching_object: 1.5913
     Episode_Reward/lifting_object: 160.5843
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.28s
                      Time elapsed: 01:02:12
                               ETA: 05:13:29

################################################################################
                    [1m Learning iteration 1656/10000 [0m                     

                       Computation: 47019 steps/s (collection: 2.002s, learning 0.089s)
             Mean action noise std: 3.96
          Mean value_function loss: 204.0361
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.2118
                       Mean reward: 770.67
               Mean episode length: 214.84
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 160.0352
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.09s
                      Time elapsed: 01:02:14
                               ETA: 05:13:26

################################################################################
                    [1m Learning iteration 1657/10000 [0m                     

                       Computation: 46276 steps/s (collection: 2.026s, learning 0.099s)
             Mean action noise std: 3.96
          Mean value_function loss: 196.8021
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 85.2248
                       Mean reward: 823.18
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.6028
     Episode_Reward/lifting_object: 162.3019
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.12s
                      Time elapsed: 01:02:16
                               ETA: 05:13:23

################################################################################
                    [1m Learning iteration 1658/10000 [0m                     

                       Computation: 47328 steps/s (collection: 1.981s, learning 0.097s)
             Mean action noise std: 3.96
          Mean value_function loss: 198.1608
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.2371
                       Mean reward: 831.26
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 158.8487
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.08s
                      Time elapsed: 01:02:18
                               ETA: 05:13:20

################################################################################
                    [1m Learning iteration 1659/10000 [0m                     

                       Computation: 47081 steps/s (collection: 1.980s, learning 0.108s)
             Mean action noise std: 3.96
          Mean value_function loss: 208.7022
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 85.2567
                       Mean reward: 821.55
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.5985
     Episode_Reward/lifting_object: 162.2682
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.09s
                      Time elapsed: 01:02:20
                               ETA: 05:13:16

################################################################################
                    [1m Learning iteration 1660/10000 [0m                     

                       Computation: 46227 steps/s (collection: 2.007s, learning 0.120s)
             Mean action noise std: 3.96
          Mean value_function loss: 200.3445
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 85.2674
                       Mean reward: 828.84
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.5635
     Episode_Reward/lifting_object: 158.2574
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.13s
                      Time elapsed: 01:02:23
                               ETA: 05:13:14

################################################################################
                    [1m Learning iteration 1661/10000 [0m                     

                       Computation: 47330 steps/s (collection: 1.986s, learning 0.091s)
             Mean action noise std: 3.97
          Mean value_function loss: 216.9372
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.2774
                       Mean reward: 805.56
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.6007
     Episode_Reward/lifting_object: 162.6521
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.08s
                      Time elapsed: 01:02:25
                               ETA: 05:13:10

################################################################################
                    [1m Learning iteration 1662/10000 [0m                     

                       Computation: 44855 steps/s (collection: 2.036s, learning 0.156s)
             Mean action noise std: 3.97
          Mean value_function loss: 221.1965
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.2890
                       Mean reward: 800.65
               Mean episode length: 218.37
    Episode_Reward/reaching_object: 1.6098
     Episode_Reward/lifting_object: 163.1940
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.19s
                      Time elapsed: 01:02:27
                               ETA: 05:13:08

################################################################################
                    [1m Learning iteration 1663/10000 [0m                     

                       Computation: 46291 steps/s (collection: 1.996s, learning 0.128s)
             Mean action noise std: 3.97
          Mean value_function loss: 189.7085
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.3066
                       Mean reward: 816.41
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 161.3848
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.12s
                      Time elapsed: 01:02:29
                               ETA: 05:13:05

################################################################################
                    [1m Learning iteration 1664/10000 [0m                     

                       Computation: 47347 steps/s (collection: 1.954s, learning 0.122s)
             Mean action noise std: 3.97
          Mean value_function loss: 239.1772
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.3292
                       Mean reward: 843.12
               Mean episode length: 229.10
    Episode_Reward/reaching_object: 1.5807
     Episode_Reward/lifting_object: 160.4309
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.08s
                      Time elapsed: 01:02:31
                               ETA: 05:13:02

################################################################################
                    [1m Learning iteration 1665/10000 [0m                     

                       Computation: 47055 steps/s (collection: 1.971s, learning 0.119s)
             Mean action noise std: 3.98
          Mean value_function loss: 194.8145
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.3460
                       Mean reward: 854.88
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 1.5755
     Episode_Reward/lifting_object: 159.3849
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.09s
                      Time elapsed: 01:02:33
                               ETA: 05:12:59

################################################################################
                    [1m Learning iteration 1666/10000 [0m                     

                       Computation: 26992 steps/s (collection: 3.529s, learning 0.113s)
             Mean action noise std: 3.98
          Mean value_function loss: 196.4711
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.3647
                       Mean reward: 809.64
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 1.5921
     Episode_Reward/lifting_object: 161.5150
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.64s
                      Time elapsed: 01:02:37
                               ETA: 05:13:03

################################################################################
                    [1m Learning iteration 1667/10000 [0m                     

                       Computation: 14358 steps/s (collection: 6.721s, learning 0.126s)
             Mean action noise std: 3.98
          Mean value_function loss: 178.5668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.3766
                       Mean reward: 780.05
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 1.6100
     Episode_Reward/lifting_object: 163.8656
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.85s
                      Time elapsed: 01:02:44
                               ETA: 05:13:24

################################################################################
                    [1m Learning iteration 1668/10000 [0m                     

                       Computation: 13761 steps/s (collection: 6.983s, learning 0.161s)
             Mean action noise std: 3.98
          Mean value_function loss: 183.7776
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.3870
                       Mean reward: 852.10
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 161.2455
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.14s
                      Time elapsed: 01:02:51
                               ETA: 05:13:46

################################################################################
                    [1m Learning iteration 1669/10000 [0m                     

                       Computation: 14146 steps/s (collection: 6.827s, learning 0.122s)
             Mean action noise std: 3.98
          Mean value_function loss: 177.2548
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.3993
                       Mean reward: 852.08
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.6659
     Episode_Reward/lifting_object: 169.6468
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.95s
                      Time elapsed: 01:02:58
                               ETA: 05:14:07

################################################################################
                    [1m Learning iteration 1670/10000 [0m                     

                       Computation: 13942 steps/s (collection: 6.910s, learning 0.140s)
             Mean action noise std: 3.98
          Mean value_function loss: 172.3522
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.4157
                       Mean reward: 781.62
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 155.1356
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.05s
                      Time elapsed: 01:03:05
                               ETA: 05:14:29

################################################################################
                    [1m Learning iteration 1671/10000 [0m                     

                       Computation: 13958 steps/s (collection: 6.898s, learning 0.144s)
             Mean action noise std: 3.99
          Mean value_function loss: 190.3337
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 85.4278
                       Mean reward: 836.26
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.6366
     Episode_Reward/lifting_object: 165.9728
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.04s
                      Time elapsed: 01:03:12
                               ETA: 05:14:51

################################################################################
                    [1m Learning iteration 1672/10000 [0m                     

                       Computation: 14249 steps/s (collection: 6.767s, learning 0.132s)
             Mean action noise std: 3.99
          Mean value_function loss: 204.6270
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.4471
                       Mean reward: 853.51
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.6225
     Episode_Reward/lifting_object: 164.5781
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.90s
                      Time elapsed: 01:03:19
                               ETA: 05:15:11

################################################################################
                    [1m Learning iteration 1673/10000 [0m                     

                       Computation: 13955 steps/s (collection: 6.923s, learning 0.122s)
             Mean action noise std: 3.99
          Mean value_function loss: 167.8277
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.4666
                       Mean reward: 838.44
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.5924
     Episode_Reward/lifting_object: 161.7896
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.04s
                      Time elapsed: 01:03:26
                               ETA: 05:15:33

################################################################################
                    [1m Learning iteration 1674/10000 [0m                     

                       Computation: 13842 steps/s (collection: 6.977s, learning 0.125s)
             Mean action noise std: 3.99
          Mean value_function loss: 193.7347
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.4795
                       Mean reward: 812.24
               Mean episode length: 221.62
    Episode_Reward/reaching_object: 1.6129
     Episode_Reward/lifting_object: 163.5429
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.10s
                      Time elapsed: 01:03:33
                               ETA: 05:15:55

################################################################################
                    [1m Learning iteration 1675/10000 [0m                     

                       Computation: 21465 steps/s (collection: 4.454s, learning 0.126s)
             Mean action noise std: 3.99
          Mean value_function loss: 186.3638
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.4911
                       Mean reward: 786.06
               Mean episode length: 218.32
    Episode_Reward/reaching_object: 1.5945
     Episode_Reward/lifting_object: 161.3984
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.58s
                      Time elapsed: 01:03:37
                               ETA: 05:16:04

################################################################################
                    [1m Learning iteration 1676/10000 [0m                     

                       Computation: 47926 steps/s (collection: 1.960s, learning 0.092s)
             Mean action noise std: 4.00
          Mean value_function loss: 214.9967
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.4984
                       Mean reward: 909.08
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 1.6501
     Episode_Reward/lifting_object: 167.7973
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.05s
                      Time elapsed: 01:03:39
                               ETA: 05:16:00

################################################################################
                    [1m Learning iteration 1677/10000 [0m                     

                       Computation: 46652 steps/s (collection: 2.021s, learning 0.086s)
             Mean action noise std: 4.00
          Mean value_function loss: 205.8352
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.5080
                       Mean reward: 846.27
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 1.5827
     Episode_Reward/lifting_object: 160.4393
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.11s
                      Time elapsed: 01:03:42
                               ETA: 05:15:57

################################################################################
                    [1m Learning iteration 1678/10000 [0m                     

                       Computation: 48022 steps/s (collection: 1.957s, learning 0.090s)
             Mean action noise std: 4.00
          Mean value_function loss: 203.1685
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.5192
                       Mean reward: 873.66
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 1.6000
     Episode_Reward/lifting_object: 161.9363
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.05s
                      Time elapsed: 01:03:44
                               ETA: 05:15:54

################################################################################
                    [1m Learning iteration 1679/10000 [0m                     

                       Computation: 45426 steps/s (collection: 2.033s, learning 0.131s)
             Mean action noise std: 4.00
          Mean value_function loss: 183.5610
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.5355
                       Mean reward: 806.30
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 159.0832
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.16s
                      Time elapsed: 01:03:46
                               ETA: 05:15:51

################################################################################
                    [1m Learning iteration 1680/10000 [0m                     

                       Computation: 47661 steps/s (collection: 1.970s, learning 0.092s)
             Mean action noise std: 4.00
          Mean value_function loss: 176.3093
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.5496
                       Mean reward: 831.12
               Mean episode length: 225.70
    Episode_Reward/reaching_object: 1.5843
     Episode_Reward/lifting_object: 161.2894
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1029
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.06s
                      Time elapsed: 01:03:48
                               ETA: 05:15:48

################################################################################
                    [1m Learning iteration 1681/10000 [0m                     

                       Computation: 48174 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 4.01
          Mean value_function loss: 169.9704
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.5641
                       Mean reward: 874.42
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 1.6249
     Episode_Reward/lifting_object: 165.8714
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.04s
                      Time elapsed: 01:03:50
                               ETA: 05:15:44

################################################################################
                    [1m Learning iteration 1682/10000 [0m                     

                       Computation: 46822 steps/s (collection: 1.992s, learning 0.107s)
             Mean action noise std: 4.01
          Mean value_function loss: 156.4431
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.5825
                       Mean reward: 838.72
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 1.5995
     Episode_Reward/lifting_object: 162.9245
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.10s
                      Time elapsed: 01:03:52
                               ETA: 05:15:41

################################################################################
                    [1m Learning iteration 1683/10000 [0m                     

                       Computation: 47946 steps/s (collection: 1.959s, learning 0.092s)
             Mean action noise std: 4.01
          Mean value_function loss: 206.7958
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 85.5964
                       Mean reward: 820.19
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.5988
     Episode_Reward/lifting_object: 162.1367
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.05s
                      Time elapsed: 01:03:54
                               ETA: 05:15:38

################################################################################
                    [1m Learning iteration 1684/10000 [0m                     

                       Computation: 47925 steps/s (collection: 1.961s, learning 0.090s)
             Mean action noise std: 4.01
          Mean value_function loss: 187.2553
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.6134
                       Mean reward: 827.33
               Mean episode length: 226.39
    Episode_Reward/reaching_object: 1.6005
     Episode_Reward/lifting_object: 163.0239
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.05s
                      Time elapsed: 01:03:56
                               ETA: 05:15:34

################################################################################
                    [1m Learning iteration 1685/10000 [0m                     

                       Computation: 48107 steps/s (collection: 1.954s, learning 0.090s)
             Mean action noise std: 4.01
          Mean value_function loss: 224.1987
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.6266
                       Mean reward: 790.18
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 1.6077
     Episode_Reward/lifting_object: 163.7592
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.04s
                      Time elapsed: 01:03:58
                               ETA: 05:15:31

################################################################################
                    [1m Learning iteration 1686/10000 [0m                     

                       Computation: 48119 steps/s (collection: 1.948s, learning 0.095s)
             Mean action noise std: 4.02
          Mean value_function loss: 197.0410
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.6392
                       Mean reward: 817.38
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 1.5758
     Episode_Reward/lifting_object: 160.1095
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.04s
                      Time elapsed: 01:04:00
                               ETA: 05:15:27

################################################################################
                    [1m Learning iteration 1687/10000 [0m                     

                       Computation: 47431 steps/s (collection: 1.968s, learning 0.105s)
             Mean action noise std: 4.02
          Mean value_function loss: 206.1148
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.6539
                       Mean reward: 798.95
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 1.5459
     Episode_Reward/lifting_object: 155.7942
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.07s
                      Time elapsed: 01:04:02
                               ETA: 05:15:24

################################################################################
                    [1m Learning iteration 1688/10000 [0m                     

                       Computation: 47658 steps/s (collection: 1.963s, learning 0.100s)
             Mean action noise std: 4.02
          Mean value_function loss: 202.1198
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.6637
                       Mean reward: 829.91
               Mean episode length: 229.28
    Episode_Reward/reaching_object: 1.6163
     Episode_Reward/lifting_object: 165.2552
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.06s
                      Time elapsed: 01:04:04
                               ETA: 05:15:21

################################################################################
                    [1m Learning iteration 1689/10000 [0m                     

                       Computation: 46859 steps/s (collection: 1.988s, learning 0.110s)
             Mean action noise std: 4.02
          Mean value_function loss: 189.1277
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 85.6675
                       Mean reward: 858.09
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.5929
     Episode_Reward/lifting_object: 161.6410
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.10s
                      Time elapsed: 01:04:06
                               ETA: 05:15:18

################################################################################
                    [1m Learning iteration 1690/10000 [0m                     

                       Computation: 45556 steps/s (collection: 2.039s, learning 0.119s)
             Mean action noise std: 4.02
          Mean value_function loss: 185.0704
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.6798
                       Mean reward: 765.40
               Mean episode length: 211.62
    Episode_Reward/reaching_object: 1.5560
     Episode_Reward/lifting_object: 157.8296
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.16s
                      Time elapsed: 01:04:09
                               ETA: 05:15:15

################################################################################
                    [1m Learning iteration 1691/10000 [0m                     

                       Computation: 44873 steps/s (collection: 2.047s, learning 0.144s)
             Mean action noise std: 4.02
          Mean value_function loss: 186.4444
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.6950
                       Mean reward: 760.53
               Mean episode length: 213.04
    Episode_Reward/reaching_object: 1.5657
     Episode_Reward/lifting_object: 159.3922
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.19s
                      Time elapsed: 01:04:11
                               ETA: 05:15:12

################################################################################
                    [1m Learning iteration 1692/10000 [0m                     

                       Computation: 46429 steps/s (collection: 1.968s, learning 0.150s)
             Mean action noise std: 4.03
          Mean value_function loss: 207.7387
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 85.7074
                       Mean reward: 832.35
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.6107
     Episode_Reward/lifting_object: 164.2874
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.12s
                      Time elapsed: 01:04:13
                               ETA: 05:15:09

################################################################################
                    [1m Learning iteration 1693/10000 [0m                     

                       Computation: 45458 steps/s (collection: 1.998s, learning 0.165s)
             Mean action noise std: 4.03
          Mean value_function loss: 189.8475
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.7216
                       Mean reward: 810.84
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.5705
     Episode_Reward/lifting_object: 159.2611
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.16s
                      Time elapsed: 01:04:15
                               ETA: 05:15:06

################################################################################
                    [1m Learning iteration 1694/10000 [0m                     

                       Computation: 45483 steps/s (collection: 2.068s, learning 0.093s)
             Mean action noise std: 4.03
          Mean value_function loss: 194.9181
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 85.7335
                       Mean reward: 816.85
               Mean episode length: 226.98
    Episode_Reward/reaching_object: 1.6187
     Episode_Reward/lifting_object: 165.0470
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.16s
                      Time elapsed: 01:04:17
                               ETA: 05:15:03

################################################################################
                    [1m Learning iteration 1695/10000 [0m                     

                       Computation: 47390 steps/s (collection: 1.984s, learning 0.090s)
             Mean action noise std: 4.03
          Mean value_function loss: 169.2840
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.7396
                       Mean reward: 831.14
               Mean episode length: 226.20
    Episode_Reward/reaching_object: 1.6414
     Episode_Reward/lifting_object: 167.9588
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.07s
                      Time elapsed: 01:04:19
                               ETA: 05:15:00

################################################################################
                    [1m Learning iteration 1696/10000 [0m                     

                       Computation: 47878 steps/s (collection: 1.959s, learning 0.094s)
             Mean action noise std: 4.03
          Mean value_function loss: 153.0456
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.7527
                       Mean reward: 859.76
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.5970
     Episode_Reward/lifting_object: 163.3663
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.05s
                      Time elapsed: 01:04:21
                               ETA: 05:14:57

################################################################################
                    [1m Learning iteration 1697/10000 [0m                     

                       Computation: 47660 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 4.03
          Mean value_function loss: 165.8803
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.7654
                       Mean reward: 872.32
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.6122
     Episode_Reward/lifting_object: 164.9750
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.06s
                      Time elapsed: 01:04:23
                               ETA: 05:14:53

################################################################################
                    [1m Learning iteration 1698/10000 [0m                     

                       Computation: 45849 steps/s (collection: 1.991s, learning 0.154s)
             Mean action noise std: 4.04
          Mean value_function loss: 180.5780
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.7826
                       Mean reward: 890.55
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.6310
     Episode_Reward/lifting_object: 166.6198
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.14s
                      Time elapsed: 01:04:26
                               ETA: 05:14:50

################################################################################
                    [1m Learning iteration 1699/10000 [0m                     

                       Computation: 46388 steps/s (collection: 1.989s, learning 0.130s)
             Mean action noise std: 4.04
          Mean value_function loss: 186.1578
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.7900
                       Mean reward: 831.46
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.5938
     Episode_Reward/lifting_object: 162.5200
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.12s
                      Time elapsed: 01:04:28
                               ETA: 05:14:47

################################################################################
                    [1m Learning iteration 1700/10000 [0m                     

                       Computation: 47205 steps/s (collection: 1.976s, learning 0.107s)
             Mean action noise std: 4.04
          Mean value_function loss: 185.6752
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.8052
                       Mean reward: 799.42
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 1.6071
     Episode_Reward/lifting_object: 164.1620
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.08s
                      Time elapsed: 01:04:30
                               ETA: 05:14:44

################################################################################
                    [1m Learning iteration 1701/10000 [0m                     

                       Computation: 46462 steps/s (collection: 2.012s, learning 0.104s)
             Mean action noise std: 4.04
          Mean value_function loss: 190.3962
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.8236
                       Mean reward: 841.29
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.5820
     Episode_Reward/lifting_object: 160.9923
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.12s
                      Time elapsed: 01:04:32
                               ETA: 05:14:41

################################################################################
                    [1m Learning iteration 1702/10000 [0m                     

                       Computation: 47476 steps/s (collection: 1.976s, learning 0.095s)
             Mean action noise std: 4.04
          Mean value_function loss: 188.0259
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.8335
                       Mean reward: 839.30
               Mean episode length: 228.21
    Episode_Reward/reaching_object: 1.6180
     Episode_Reward/lifting_object: 165.4841
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.07s
                      Time elapsed: 01:04:34
                               ETA: 05:14:38

################################################################################
                    [1m Learning iteration 1703/10000 [0m                     

                       Computation: 46372 steps/s (collection: 2.027s, learning 0.093s)
             Mean action noise std: 4.05
          Mean value_function loss: 184.6277
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.8430
                       Mean reward: 847.19
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.6131
     Episode_Reward/lifting_object: 164.7359
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.12s
                      Time elapsed: 01:04:36
                               ETA: 05:14:35

################################################################################
                    [1m Learning iteration 1704/10000 [0m                     

                       Computation: 46142 steps/s (collection: 2.033s, learning 0.098s)
             Mean action noise std: 4.05
          Mean value_function loss: 207.8627
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 85.8608
                       Mean reward: 794.17
               Mean episode length: 218.58
    Episode_Reward/reaching_object: 1.5807
     Episode_Reward/lifting_object: 160.3348
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.13s
                      Time elapsed: 01:04:38
                               ETA: 05:14:32

################################################################################
                    [1m Learning iteration 1705/10000 [0m                     

                       Computation: 47342 steps/s (collection: 1.979s, learning 0.097s)
             Mean action noise std: 4.05
          Mean value_function loss: 182.6215
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.8763
                       Mean reward: 840.54
               Mean episode length: 227.68
    Episode_Reward/reaching_object: 1.6319
     Episode_Reward/lifting_object: 167.2175
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.08s
                      Time elapsed: 01:04:40
                               ETA: 05:14:29

################################################################################
                    [1m Learning iteration 1706/10000 [0m                     

                       Computation: 47320 steps/s (collection: 1.987s, learning 0.090s)
             Mean action noise std: 4.05
          Mean value_function loss: 203.0315
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.8923
                       Mean reward: 810.71
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.6059
     Episode_Reward/lifting_object: 163.7138
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.08s
                      Time elapsed: 01:04:42
                               ETA: 05:14:25

################################################################################
                    [1m Learning iteration 1707/10000 [0m                     

                       Computation: 47402 steps/s (collection: 1.982s, learning 0.092s)
             Mean action noise std: 4.06
          Mean value_function loss: 209.0782
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.9169
                       Mean reward: 798.98
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 160.2797
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.07s
                      Time elapsed: 01:04:44
                               ETA: 05:14:22

################################################################################
                    [1m Learning iteration 1708/10000 [0m                     

                       Computation: 46945 steps/s (collection: 1.968s, learning 0.126s)
             Mean action noise std: 4.06
          Mean value_function loss: 156.6408
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.9293
                       Mean reward: 843.03
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 164.1214
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.09s
                      Time elapsed: 01:04:46
                               ETA: 05:14:19

################################################################################
                    [1m Learning iteration 1709/10000 [0m                     

                       Computation: 46901 steps/s (collection: 1.970s, learning 0.126s)
             Mean action noise std: 4.06
          Mean value_function loss: 207.0323
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 85.9406
                       Mean reward: 752.03
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.5739
     Episode_Reward/lifting_object: 160.0027
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.10s
                      Time elapsed: 01:04:49
                               ETA: 05:14:16

################################################################################
                    [1m Learning iteration 1710/10000 [0m                     

                       Computation: 45820 steps/s (collection: 1.999s, learning 0.146s)
             Mean action noise std: 4.06
          Mean value_function loss: 169.1713
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 85.9469
                       Mean reward: 850.74
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 1.6195
     Episode_Reward/lifting_object: 165.7299
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.15s
                      Time elapsed: 01:04:51
                               ETA: 05:14:13

################################################################################
                    [1m Learning iteration 1711/10000 [0m                     

                       Computation: 47494 steps/s (collection: 1.968s, learning 0.102s)
             Mean action noise std: 4.06
          Mean value_function loss: 183.1692
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.9576
                       Mean reward: 831.48
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 1.6300
     Episode_Reward/lifting_object: 166.7802
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.07s
                      Time elapsed: 01:04:53
                               ETA: 05:14:10

################################################################################
                    [1m Learning iteration 1712/10000 [0m                     

                       Computation: 46187 steps/s (collection: 2.031s, learning 0.097s)
             Mean action noise std: 4.07
          Mean value_function loss: 168.8735
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.9776
                       Mean reward: 865.05
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.6221
     Episode_Reward/lifting_object: 165.6094
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.13s
                      Time elapsed: 01:04:55
                               ETA: 05:14:07

################################################################################
                    [1m Learning iteration 1713/10000 [0m                     

                       Computation: 47166 steps/s (collection: 1.980s, learning 0.105s)
             Mean action noise std: 4.07
          Mean value_function loss: 169.0584
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.0011
                       Mean reward: 833.59
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.6011
     Episode_Reward/lifting_object: 163.6062
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.08s
                      Time elapsed: 01:04:57
                               ETA: 05:14:03

################################################################################
                    [1m Learning iteration 1714/10000 [0m                     

                       Computation: 46996 steps/s (collection: 1.986s, learning 0.106s)
             Mean action noise std: 4.07
          Mean value_function loss: 178.7435
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 86.0197
                       Mean reward: 867.41
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.6043
     Episode_Reward/lifting_object: 164.0862
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.09s
                      Time elapsed: 01:04:59
                               ETA: 05:14:00

################################################################################
                    [1m Learning iteration 1715/10000 [0m                     

                       Computation: 47263 steps/s (collection: 1.982s, learning 0.098s)
             Mean action noise std: 4.07
          Mean value_function loss: 203.4799
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.0326
                       Mean reward: 836.96
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 1.6220
     Episode_Reward/lifting_object: 166.3583
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.08s
                      Time elapsed: 01:05:01
                               ETA: 05:13:57

################################################################################
                    [1m Learning iteration 1716/10000 [0m                     

                       Computation: 47464 steps/s (collection: 1.969s, learning 0.103s)
             Mean action noise std: 4.07
          Mean value_function loss: 178.1658
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.0436
                       Mean reward: 803.50
               Mean episode length: 219.20
    Episode_Reward/reaching_object: 1.5965
     Episode_Reward/lifting_object: 163.5012
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.07s
                      Time elapsed: 01:05:03
                               ETA: 05:13:54

################################################################################
                    [1m Learning iteration 1717/10000 [0m                     

                       Computation: 44918 steps/s (collection: 2.070s, learning 0.119s)
             Mean action noise std: 4.08
          Mean value_function loss: 194.6569
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.0594
                       Mean reward: 861.58
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.6392
     Episode_Reward/lifting_object: 167.9269
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.19s
                      Time elapsed: 01:05:05
                               ETA: 05:13:51

################################################################################
                    [1m Learning iteration 1718/10000 [0m                     

                       Computation: 45245 steps/s (collection: 2.052s, learning 0.120s)
             Mean action noise std: 4.08
          Mean value_function loss: 194.2622
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 86.0705
                       Mean reward: 823.59
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.6094
     Episode_Reward/lifting_object: 164.1532
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.17s
                      Time elapsed: 01:05:08
                               ETA: 05:13:48

################################################################################
                    [1m Learning iteration 1719/10000 [0m                     

                       Computation: 45712 steps/s (collection: 2.017s, learning 0.134s)
             Mean action noise std: 4.08
          Mean value_function loss: 183.4387
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.0770
                       Mean reward: 856.79
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.6412
     Episode_Reward/lifting_object: 168.3036
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.15s
                      Time elapsed: 01:05:10
                               ETA: 05:13:46

################################################################################
                    [1m Learning iteration 1720/10000 [0m                     

                       Computation: 46257 steps/s (collection: 2.016s, learning 0.109s)
             Mean action noise std: 4.08
          Mean value_function loss: 178.4240
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.0886
                       Mean reward: 843.54
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.6213
     Episode_Reward/lifting_object: 166.0330
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.13s
                      Time elapsed: 01:05:12
                               ETA: 05:13:43

################################################################################
                    [1m Learning iteration 1721/10000 [0m                     

                       Computation: 45869 steps/s (collection: 2.035s, learning 0.108s)
             Mean action noise std: 4.08
          Mean value_function loss: 187.0748
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.0997
                       Mean reward: 754.71
               Mean episode length: 209.22
    Episode_Reward/reaching_object: 1.6013
     Episode_Reward/lifting_object: 163.2787
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.14s
                      Time elapsed: 01:05:14
                               ETA: 05:13:40

################################################################################
                    [1m Learning iteration 1722/10000 [0m                     

                       Computation: 46595 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 4.08
          Mean value_function loss: 171.9142
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.1111
                       Mean reward: 845.23
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.6395
     Episode_Reward/lifting_object: 168.1795
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.11s
                      Time elapsed: 01:05:16
                               ETA: 05:13:37

################################################################################
                    [1m Learning iteration 1723/10000 [0m                     

                       Computation: 45940 steps/s (collection: 2.034s, learning 0.106s)
             Mean action noise std: 4.08
          Mean value_function loss: 156.9204
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.1222
                       Mean reward: 839.98
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.6414
     Episode_Reward/lifting_object: 168.3459
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.14s
                      Time elapsed: 01:05:18
                               ETA: 05:13:34

################################################################################
                    [1m Learning iteration 1724/10000 [0m                     

                       Computation: 45736 steps/s (collection: 2.049s, learning 0.101s)
             Mean action noise std: 4.09
          Mean value_function loss: 159.0934
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.1356
                       Mean reward: 817.47
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.6243
     Episode_Reward/lifting_object: 166.5588
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.15s
                      Time elapsed: 01:05:20
                               ETA: 05:13:31

################################################################################
                    [1m Learning iteration 1725/10000 [0m                     

                       Computation: 47268 steps/s (collection: 1.989s, learning 0.091s)
             Mean action noise std: 4.09
          Mean value_function loss: 140.8736
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.1522
                       Mean reward: 870.50
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.6639
     Episode_Reward/lifting_object: 170.9232
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.08s
                      Time elapsed: 01:05:23
                               ETA: 05:13:28

################################################################################
                    [1m Learning iteration 1726/10000 [0m                     

                       Computation: 46818 steps/s (collection: 2.001s, learning 0.098s)
             Mean action noise std: 4.09
          Mean value_function loss: 162.1097
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.1719
                       Mean reward: 805.40
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.6157
     Episode_Reward/lifting_object: 164.6490
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.10s
                      Time elapsed: 01:05:25
                               ETA: 05:13:25

################################################################################
                    [1m Learning iteration 1727/10000 [0m                     

                       Computation: 46634 steps/s (collection: 1.984s, learning 0.124s)
             Mean action noise std: 4.09
          Mean value_function loss: 192.3772
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.1837
                       Mean reward: 809.35
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 1.6365
     Episode_Reward/lifting_object: 167.1918
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.11s
                      Time elapsed: 01:05:27
                               ETA: 05:13:21

################################################################################
                    [1m Learning iteration 1728/10000 [0m                     

                       Computation: 46231 steps/s (collection: 2.012s, learning 0.114s)
             Mean action noise std: 4.09
          Mean value_function loss: 135.5374
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.1954
                       Mean reward: 850.33
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.6667
     Episode_Reward/lifting_object: 170.8783
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.13s
                      Time elapsed: 01:05:29
                               ETA: 05:13:18

################################################################################
                    [1m Learning iteration 1729/10000 [0m                     

                       Computation: 44807 steps/s (collection: 2.102s, learning 0.092s)
             Mean action noise std: 4.10
          Mean value_function loss: 179.5587
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.2075
                       Mean reward: 820.22
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.6122
     Episode_Reward/lifting_object: 163.5882
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.19s
                      Time elapsed: 01:05:31
                               ETA: 05:13:16

################################################################################
                    [1m Learning iteration 1730/10000 [0m                     

                       Computation: 47232 steps/s (collection: 1.989s, learning 0.092s)
             Mean action noise std: 4.10
          Mean value_function loss: 174.9530
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.2245
                       Mean reward: 835.15
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 1.6126
     Episode_Reward/lifting_object: 164.0020
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.08s
                      Time elapsed: 01:05:33
                               ETA: 05:13:13

################################################################################
                    [1m Learning iteration 1731/10000 [0m                     

                       Computation: 46391 steps/s (collection: 2.019s, learning 0.100s)
             Mean action noise std: 4.10
          Mean value_function loss: 191.6662
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.2360
                       Mean reward: 840.37
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.6101
     Episode_Reward/lifting_object: 164.3596
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.12s
                      Time elapsed: 01:05:35
                               ETA: 05:13:10

################################################################################
                    [1m Learning iteration 1732/10000 [0m                     

                       Computation: 45906 steps/s (collection: 2.032s, learning 0.109s)
             Mean action noise std: 4.10
          Mean value_function loss: 196.0103
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.2538
                       Mean reward: 804.04
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 1.5945
     Episode_Reward/lifting_object: 162.0509
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.14s
                      Time elapsed: 01:05:37
                               ETA: 05:13:07

################################################################################
                    [1m Learning iteration 1733/10000 [0m                     

                       Computation: 47105 steps/s (collection: 1.996s, learning 0.091s)
             Mean action noise std: 4.10
          Mean value_function loss: 157.6242
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 86.2692
                       Mean reward: 833.62
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.6123
     Episode_Reward/lifting_object: 164.3251
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.09s
                      Time elapsed: 01:05:39
                               ETA: 05:13:04

################################################################################
                    [1m Learning iteration 1734/10000 [0m                     

                       Computation: 45822 steps/s (collection: 2.032s, learning 0.113s)
             Mean action noise std: 4.11
          Mean value_function loss: 151.3202
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.2820
                       Mean reward: 846.10
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 1.6458
     Episode_Reward/lifting_object: 168.5565
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.15s
                      Time elapsed: 01:05:42
                               ETA: 05:13:01

################################################################################
                    [1m Learning iteration 1735/10000 [0m                     

                       Computation: 46683 steps/s (collection: 1.975s, learning 0.131s)
             Mean action noise std: 4.11
          Mean value_function loss: 171.8356
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.2976
                       Mean reward: 831.08
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.6364
     Episode_Reward/lifting_object: 167.3819
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.11s
                      Time elapsed: 01:05:44
                               ETA: 05:12:58

################################################################################
                    [1m Learning iteration 1736/10000 [0m                     

                       Computation: 44945 steps/s (collection: 2.080s, learning 0.107s)
             Mean action noise std: 4.11
          Mean value_function loss: 197.3369
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.3130
                       Mean reward: 840.68
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 1.6540
     Episode_Reward/lifting_object: 169.5998
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.19s
                      Time elapsed: 01:05:46
                               ETA: 05:12:55

################################################################################
                    [1m Learning iteration 1737/10000 [0m                     

                       Computation: 46215 steps/s (collection: 2.018s, learning 0.109s)
             Mean action noise std: 4.11
          Mean value_function loss: 153.3115
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.3280
                       Mean reward: 880.08
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.6242
     Episode_Reward/lifting_object: 166.0628
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.13s
                      Time elapsed: 01:05:48
                               ETA: 05:12:52

################################################################################
                    [1m Learning iteration 1738/10000 [0m                     

                       Computation: 44122 steps/s (collection: 2.118s, learning 0.110s)
             Mean action noise std: 4.11
          Mean value_function loss: 171.3062
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.3409
                       Mean reward: 856.25
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.6575
     Episode_Reward/lifting_object: 170.4229
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.23s
                      Time elapsed: 01:05:50
                               ETA: 05:12:50

################################################################################
                    [1m Learning iteration 1739/10000 [0m                     

                       Computation: 46361 steps/s (collection: 2.010s, learning 0.111s)
             Mean action noise std: 4.12
          Mean value_function loss: 166.6218
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.3576
                       Mean reward: 817.85
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.6194
     Episode_Reward/lifting_object: 165.7278
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1120
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.12s
                      Time elapsed: 01:05:52
                               ETA: 05:12:47

################################################################################
                    [1m Learning iteration 1740/10000 [0m                     

                       Computation: 45057 steps/s (collection: 2.045s, learning 0.137s)
             Mean action noise std: 4.12
          Mean value_function loss: 168.8059
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.3682
                       Mean reward: 859.81
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.6288
     Episode_Reward/lifting_object: 167.1045
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.18s
                      Time elapsed: 01:05:55
                               ETA: 05:12:44

################################################################################
                    [1m Learning iteration 1741/10000 [0m                     

                       Computation: 46083 steps/s (collection: 2.020s, learning 0.113s)
             Mean action noise std: 4.12
          Mean value_function loss: 151.3583
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.3756
                       Mean reward: 838.23
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 1.6394
     Episode_Reward/lifting_object: 167.7736
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.13s
                      Time elapsed: 01:05:57
                               ETA: 05:12:41

################################################################################
                    [1m Learning iteration 1742/10000 [0m                     

                       Computation: 46982 steps/s (collection: 1.981s, learning 0.112s)
             Mean action noise std: 4.12
          Mean value_function loss: 187.6097
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.3847
                       Mean reward: 853.00
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.6312
     Episode_Reward/lifting_object: 166.9010
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.09s
                      Time elapsed: 01:05:59
                               ETA: 05:12:38

################################################################################
                    [1m Learning iteration 1743/10000 [0m                     

                       Computation: 47098 steps/s (collection: 1.984s, learning 0.103s)
             Mean action noise std: 4.12
          Mean value_function loss: 166.5413
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 86.3949
                       Mean reward: 827.15
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.5932
     Episode_Reward/lifting_object: 162.8448
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.09s
                      Time elapsed: 01:06:01
                               ETA: 05:12:35

################################################################################
                    [1m Learning iteration 1744/10000 [0m                     

                       Computation: 46049 steps/s (collection: 2.021s, learning 0.114s)
             Mean action noise std: 4.12
          Mean value_function loss: 167.3839
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.3991
                       Mean reward: 814.36
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.5994
     Episode_Reward/lifting_object: 162.8164
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.13s
                      Time elapsed: 01:06:03
                               ETA: 05:12:32

################################################################################
                    [1m Learning iteration 1745/10000 [0m                     

                       Computation: 41924 steps/s (collection: 2.193s, learning 0.152s)
             Mean action noise std: 4.13
          Mean value_function loss: 154.9274
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.4084
                       Mean reward: 827.18
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.6480
     Episode_Reward/lifting_object: 169.0516
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.34s
                      Time elapsed: 01:06:05
                               ETA: 05:12:30

################################################################################
                    [1m Learning iteration 1746/10000 [0m                     

                       Computation: 45282 steps/s (collection: 2.079s, learning 0.092s)
             Mean action noise std: 4.13
          Mean value_function loss: 158.4391
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.4209
                       Mean reward: 854.80
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.6192
     Episode_Reward/lifting_object: 165.3372
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.17s
                      Time elapsed: 01:06:08
                               ETA: 05:12:27

################################################################################
                    [1m Learning iteration 1747/10000 [0m                     

                       Computation: 46008 steps/s (collection: 2.008s, learning 0.129s)
             Mean action noise std: 4.13
          Mean value_function loss: 173.3083
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.4326
                       Mean reward: 848.66
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 1.6498
     Episode_Reward/lifting_object: 169.5705
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.14s
                      Time elapsed: 01:06:10
                               ETA: 05:12:24

################################################################################
                    [1m Learning iteration 1748/10000 [0m                     

                       Computation: 45928 steps/s (collection: 2.049s, learning 0.092s)
             Mean action noise std: 4.13
          Mean value_function loss: 158.9332
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 86.4488
                       Mean reward: 842.22
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 1.6428
     Episode_Reward/lifting_object: 168.0274
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.14s
                      Time elapsed: 01:06:12
                               ETA: 05:12:21

################################################################################
                    [1m Learning iteration 1749/10000 [0m                     

                       Computation: 44922 steps/s (collection: 2.072s, learning 0.116s)
             Mean action noise std: 4.13
          Mean value_function loss: 162.8842
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.4661
                       Mean reward: 843.14
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 1.6072
     Episode_Reward/lifting_object: 164.2318
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.19s
                      Time elapsed: 01:06:14
                               ETA: 05:12:19

################################################################################
                    [1m Learning iteration 1750/10000 [0m                     

                       Computation: 43524 steps/s (collection: 2.165s, learning 0.094s)
             Mean action noise std: 4.13
          Mean value_function loss: 177.3943
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.4784
                       Mean reward: 827.74
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 1.6443
     Episode_Reward/lifting_object: 168.5958
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.26s
                      Time elapsed: 01:06:16
                               ETA: 05:12:16

################################################################################
                    [1m Learning iteration 1751/10000 [0m                     

                       Computation: 46438 steps/s (collection: 2.008s, learning 0.109s)
             Mean action noise std: 4.14
          Mean value_function loss: 175.1050
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.4879
                       Mean reward: 810.98
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 1.6136
     Episode_Reward/lifting_object: 165.7849
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.12s
                      Time elapsed: 01:06:18
                               ETA: 05:12:13

################################################################################
                    [1m Learning iteration 1752/10000 [0m                     

                       Computation: 46969 steps/s (collection: 1.996s, learning 0.097s)
             Mean action noise std: 4.14
          Mean value_function loss: 160.0717
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.5026
                       Mean reward: 854.63
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.6524
     Episode_Reward/lifting_object: 169.5064
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.09s
                      Time elapsed: 01:06:20
                               ETA: 05:12:10

################################################################################
                    [1m Learning iteration 1753/10000 [0m                     

                       Computation: 46541 steps/s (collection: 2.014s, learning 0.099s)
             Mean action noise std: 4.14
          Mean value_function loss: 176.0777
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.5169
                       Mean reward: 799.79
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.6124
     Episode_Reward/lifting_object: 164.9456
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.11s
                      Time elapsed: 01:06:23
                               ETA: 05:12:07

################################################################################
                    [1m Learning iteration 1754/10000 [0m                     

                       Computation: 46880 steps/s (collection: 1.987s, learning 0.110s)
             Mean action noise std: 4.14
          Mean value_function loss: 155.6673
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.5304
                       Mean reward: 872.07
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.6534
     Episode_Reward/lifting_object: 170.1246
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.10s
                      Time elapsed: 01:06:25
                               ETA: 05:12:04

################################################################################
                    [1m Learning iteration 1755/10000 [0m                     

                       Computation: 47402 steps/s (collection: 1.967s, learning 0.107s)
             Mean action noise std: 4.14
          Mean value_function loss: 161.5650
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.5388
                       Mean reward: 850.00
               Mean episode length: 230.58
    Episode_Reward/reaching_object: 1.6169
     Episode_Reward/lifting_object: 165.7876
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.07s
                      Time elapsed: 01:06:27
                               ETA: 05:12:01

################################################################################
                    [1m Learning iteration 1756/10000 [0m                     

                       Computation: 42524 steps/s (collection: 2.201s, learning 0.111s)
             Mean action noise std: 4.15
          Mean value_function loss: 161.1367
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.5510
                       Mean reward: 826.64
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.6111
     Episode_Reward/lifting_object: 165.3538
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.31s
                      Time elapsed: 01:06:29
                               ETA: 05:11:59

################################################################################
                    [1m Learning iteration 1757/10000 [0m                     

                       Computation: 44260 steps/s (collection: 2.132s, learning 0.089s)
             Mean action noise std: 4.15
          Mean value_function loss: 172.5969
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.5686
                       Mean reward: 851.20
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.6227
     Episode_Reward/lifting_object: 166.1549
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.22s
                      Time elapsed: 01:06:31
                               ETA: 05:11:56

################################################################################
                    [1m Learning iteration 1758/10000 [0m                     

                       Computation: 46188 steps/s (collection: 2.033s, learning 0.096s)
             Mean action noise std: 4.15
          Mean value_function loss: 174.5229
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.5900
                       Mean reward: 792.09
               Mean episode length: 217.50
    Episode_Reward/reaching_object: 1.6087
     Episode_Reward/lifting_object: 164.5175
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.1126
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.13s
                      Time elapsed: 01:06:33
                               ETA: 05:11:53

################################################################################
                    [1m Learning iteration 1759/10000 [0m                     

                       Computation: 46851 steps/s (collection: 2.012s, learning 0.086s)
             Mean action noise std: 4.15
          Mean value_function loss: 172.4422
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.6062
                       Mean reward: 889.86
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 1.6105
     Episode_Reward/lifting_object: 164.1968
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.10s
                      Time elapsed: 01:06:35
                               ETA: 05:11:50

################################################################################
                    [1m Learning iteration 1760/10000 [0m                     

                       Computation: 47162 steps/s (collection: 1.992s, learning 0.093s)
             Mean action noise std: 4.15
          Mean value_function loss: 193.9778
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.6156
                       Mean reward: 850.60
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.6121
     Episode_Reward/lifting_object: 164.7654
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1135
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.08s
                      Time elapsed: 01:06:38
                               ETA: 05:11:47

################################################################################
                    [1m Learning iteration 1761/10000 [0m                     

                       Computation: 46888 steps/s (collection: 1.994s, learning 0.103s)
             Mean action noise std: 4.16
          Mean value_function loss: 158.9980
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.6278
                       Mean reward: 846.04
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.6068
     Episode_Reward/lifting_object: 164.5949
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.10s
                      Time elapsed: 01:06:40
                               ETA: 05:11:44

################################################################################
                    [1m Learning iteration 1762/10000 [0m                     

                       Computation: 47244 steps/s (collection: 1.984s, learning 0.097s)
             Mean action noise std: 4.16
          Mean value_function loss: 123.1466
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.6406
                       Mean reward: 880.00
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.6288
     Episode_Reward/lifting_object: 167.2821
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.08s
                      Time elapsed: 01:06:42
                               ETA: 05:11:41

################################################################################
                    [1m Learning iteration 1763/10000 [0m                     

                       Computation: 47203 steps/s (collection: 1.984s, learning 0.099s)
             Mean action noise std: 4.16
          Mean value_function loss: 169.5500
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.6469
                       Mean reward: 863.50
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.6589
     Episode_Reward/lifting_object: 170.1032
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.08s
                      Time elapsed: 01:06:44
                               ETA: 05:11:38

################################################################################
                    [1m Learning iteration 1764/10000 [0m                     

                       Computation: 45975 steps/s (collection: 1.979s, learning 0.159s)
             Mean action noise std: 4.16
          Mean value_function loss: 157.8681
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.6574
                       Mean reward: 783.20
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 1.5730
     Episode_Reward/lifting_object: 160.7745
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.1114
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.14s
                      Time elapsed: 01:06:46
                               ETA: 05:11:35

################################################################################
                    [1m Learning iteration 1765/10000 [0m                     

                       Computation: 46816 steps/s (collection: 1.970s, learning 0.130s)
             Mean action noise std: 4.16
          Mean value_function loss: 161.2524
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.6755
                       Mean reward: 799.53
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 1.6392
     Episode_Reward/lifting_object: 168.0954
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.10s
                      Time elapsed: 01:06:48
                               ETA: 05:11:32

################################################################################
                    [1m Learning iteration 1766/10000 [0m                     

                       Computation: 47295 steps/s (collection: 1.964s, learning 0.115s)
             Mean action noise std: 4.17
          Mean value_function loss: 169.8635
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.6926
                       Mean reward: 826.32
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.6125
     Episode_Reward/lifting_object: 165.3087
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.08s
                      Time elapsed: 01:06:50
                               ETA: 05:11:29

################################################################################
                    [1m Learning iteration 1767/10000 [0m                     

                       Computation: 48005 steps/s (collection: 1.960s, learning 0.088s)
             Mean action noise std: 4.17
          Mean value_function loss: 184.0628
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.7081
                       Mean reward: 807.62
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.6141
     Episode_Reward/lifting_object: 164.9715
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.1135
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.05s
                      Time elapsed: 01:06:52
                               ETA: 05:11:25

################################################################################
                    [1m Learning iteration 1768/10000 [0m                     

                       Computation: 46124 steps/s (collection: 2.001s, learning 0.130s)
             Mean action noise std: 4.17
          Mean value_function loss: 180.7671
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.7210
                       Mean reward: 815.84
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.6059
     Episode_Reward/lifting_object: 164.6932
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.13s
                      Time elapsed: 01:06:54
                               ETA: 05:11:22

################################################################################
                    [1m Learning iteration 1769/10000 [0m                     

                       Computation: 44687 steps/s (collection: 2.095s, learning 0.105s)
             Mean action noise std: 4.17
          Mean value_function loss: 164.7735
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.7384
                       Mean reward: 885.96
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 1.6412
     Episode_Reward/lifting_object: 168.9385
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1161
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.20s
                      Time elapsed: 01:06:57
                               ETA: 05:11:20

################################################################################
                    [1m Learning iteration 1770/10000 [0m                     

                       Computation: 46330 steps/s (collection: 2.028s, learning 0.094s)
             Mean action noise std: 4.17
          Mean value_function loss: 173.2048
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.7491
                       Mean reward: 882.61
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 1.6422
     Episode_Reward/lifting_object: 168.7873
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1163
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.12s
                      Time elapsed: 01:06:59
                               ETA: 05:11:17

################################################################################
                    [1m Learning iteration 1771/10000 [0m                     

                       Computation: 45945 steps/s (collection: 2.031s, learning 0.109s)
             Mean action noise std: 4.17
          Mean value_function loss: 151.6260
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.7586
                       Mean reward: 819.75
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.5772
     Episode_Reward/lifting_object: 161.9528
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.14s
                      Time elapsed: 01:07:01
                               ETA: 05:11:14

################################################################################
                    [1m Learning iteration 1772/10000 [0m                     

                       Computation: 44003 steps/s (collection: 2.121s, learning 0.113s)
             Mean action noise std: 4.18
          Mean value_function loss: 192.1168
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.7713
                       Mean reward: 830.06
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.6232
     Episode_Reward/lifting_object: 166.2788
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1144
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.23s
                      Time elapsed: 01:07:03
                               ETA: 05:11:12

################################################################################
                    [1m Learning iteration 1773/10000 [0m                     

                       Computation: 45168 steps/s (collection: 2.077s, learning 0.100s)
             Mean action noise std: 4.18
          Mean value_function loss: 167.6426
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.7834
                       Mean reward: 799.70
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 1.5813
     Episode_Reward/lifting_object: 161.8526
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1126
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.18s
                      Time elapsed: 01:07:05
                               ETA: 05:11:09

################################################################################
                    [1m Learning iteration 1774/10000 [0m                     

                       Computation: 45469 steps/s (collection: 2.049s, learning 0.113s)
             Mean action noise std: 4.18
          Mean value_function loss: 131.0701
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 86.7979
                       Mean reward: 852.42
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.6440
     Episode_Reward/lifting_object: 169.6712
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.16s
                      Time elapsed: 01:07:07
                               ETA: 05:11:06

################################################################################
                    [1m Learning iteration 1775/10000 [0m                     

                       Computation: 46940 steps/s (collection: 1.996s, learning 0.098s)
             Mean action noise std: 4.18
          Mean value_function loss: 172.3843
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 86.8011
                       Mean reward: 856.75
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.6499
     Episode_Reward/lifting_object: 170.2267
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1163
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.09s
                      Time elapsed: 01:07:09
                               ETA: 05:11:03

################################################################################
                    [1m Learning iteration 1776/10000 [0m                     

                       Computation: 47649 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 4.18
          Mean value_function loss: 146.9337
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.8061
                       Mean reward: 861.28
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.6503
     Episode_Reward/lifting_object: 169.9312
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.06s
                      Time elapsed: 01:07:12
                               ETA: 05:11:00

################################################################################
                    [1m Learning iteration 1777/10000 [0m                     

                       Computation: 47597 steps/s (collection: 1.972s, learning 0.093s)
             Mean action noise std: 4.18
          Mean value_function loss: 169.8064
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.8162
                       Mean reward: 829.96
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 1.6410
     Episode_Reward/lifting_object: 169.2129
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.07s
                      Time elapsed: 01:07:14
                               ETA: 05:10:57

################################################################################
                    [1m Learning iteration 1778/10000 [0m                     

                       Computation: 46983 steps/s (collection: 1.996s, learning 0.097s)
             Mean action noise std: 4.18
          Mean value_function loss: 131.2660
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.8296
                       Mean reward: 835.17
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.6545
     Episode_Reward/lifting_object: 170.2632
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.09s
                      Time elapsed: 01:07:16
                               ETA: 05:10:54

################################################################################
                    [1m Learning iteration 1779/10000 [0m                     

                       Computation: 45663 steps/s (collection: 2.055s, learning 0.098s)
             Mean action noise std: 4.19
          Mean value_function loss: 156.3519
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.8407
                       Mean reward: 838.63
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 1.6430
     Episode_Reward/lifting_object: 169.2948
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.15s
                      Time elapsed: 01:07:18
                               ETA: 05:10:51

################################################################################
                    [1m Learning iteration 1780/10000 [0m                     

                       Computation: 45509 steps/s (collection: 2.044s, learning 0.117s)
             Mean action noise std: 4.19
          Mean value_function loss: 137.2782
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 86.8514
                       Mean reward: 867.81
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 1.6333
     Episode_Reward/lifting_object: 168.3148
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.16s
                      Time elapsed: 01:07:20
                               ETA: 05:10:48

################################################################################
                    [1m Learning iteration 1781/10000 [0m                     

                       Computation: 46389 steps/s (collection: 2.025s, learning 0.094s)
             Mean action noise std: 4.19
          Mean value_function loss: 136.3483
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.8564
                       Mean reward: 862.86
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.6330
     Episode_Reward/lifting_object: 167.6349
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.12s
                      Time elapsed: 01:07:22
                               ETA: 05:10:45

################################################################################
                    [1m Learning iteration 1782/10000 [0m                     

                       Computation: 46809 steps/s (collection: 2.003s, learning 0.097s)
             Mean action noise std: 4.19
          Mean value_function loss: 140.7901
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.8665
                       Mean reward: 865.21
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.6458
     Episode_Reward/lifting_object: 169.2356
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.10s
                      Time elapsed: 01:07:24
                               ETA: 05:10:42

################################################################################
                    [1m Learning iteration 1783/10000 [0m                     

                       Computation: 46659 steps/s (collection: 2.003s, learning 0.104s)
             Mean action noise std: 4.19
          Mean value_function loss: 165.8988
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.8783
                       Mean reward: 872.14
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.6475
     Episode_Reward/lifting_object: 169.6927
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.11s
                      Time elapsed: 01:07:26
                               ETA: 05:10:39

################################################################################
                    [1m Learning iteration 1784/10000 [0m                     

                       Computation: 46286 steps/s (collection: 2.003s, learning 0.121s)
             Mean action noise std: 4.19
          Mean value_function loss: 134.6379
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.8944
                       Mean reward: 829.51
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.6314
     Episode_Reward/lifting_object: 167.3320
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1159
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.12s
                      Time elapsed: 01:07:28
                               ETA: 05:10:36

################################################################################
                    [1m Learning iteration 1785/10000 [0m                     

                       Computation: 46445 steps/s (collection: 2.017s, learning 0.099s)
             Mean action noise std: 4.20
          Mean value_function loss: 148.5346
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.9074
                       Mean reward: 880.92
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.6541
     Episode_Reward/lifting_object: 170.0881
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.12s
                      Time elapsed: 01:07:31
                               ETA: 05:10:33

################################################################################
                    [1m Learning iteration 1786/10000 [0m                     

                       Computation: 45820 steps/s (collection: 2.049s, learning 0.097s)
             Mean action noise std: 4.20
          Mean value_function loss: 147.9919
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.9183
                       Mean reward: 867.01
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.6695
     Episode_Reward/lifting_object: 171.9408
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.15s
                      Time elapsed: 01:07:33
                               ETA: 05:10:30

################################################################################
                    [1m Learning iteration 1787/10000 [0m                     

                       Computation: 46420 steps/s (collection: 2.019s, learning 0.099s)
             Mean action noise std: 4.20
          Mean value_function loss: 181.6122
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.9318
                       Mean reward: 819.05
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.5582
     Episode_Reward/lifting_object: 159.4490
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1120
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.12s
                      Time elapsed: 01:07:35
                               ETA: 05:10:27

################################################################################
                    [1m Learning iteration 1788/10000 [0m                     

                       Computation: 46824 steps/s (collection: 1.995s, learning 0.104s)
             Mean action noise std: 4.20
          Mean value_function loss: 147.7675
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.9452
                       Mean reward: 862.59
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 1.6497
     Episode_Reward/lifting_object: 169.9002
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.10s
                      Time elapsed: 01:07:37
                               ETA: 05:10:24

################################################################################
                    [1m Learning iteration 1789/10000 [0m                     

                       Computation: 46998 steps/s (collection: 1.996s, learning 0.096s)
             Mean action noise std: 4.20
          Mean value_function loss: 141.6979
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 86.9589
                       Mean reward: 827.44
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.6153
     Episode_Reward/lifting_object: 166.5090
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.09s
                      Time elapsed: 01:07:39
                               ETA: 05:10:21

################################################################################
                    [1m Learning iteration 1790/10000 [0m                     

                       Computation: 46048 steps/s (collection: 2.033s, learning 0.102s)
             Mean action noise std: 4.20
          Mean value_function loss: 141.1622
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.9704
                       Mean reward: 853.42
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.6257
     Episode_Reward/lifting_object: 167.1616
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.13s
                      Time elapsed: 01:07:41
                               ETA: 05:10:18

################################################################################
                    [1m Learning iteration 1791/10000 [0m                     

                       Computation: 46273 steps/s (collection: 2.013s, learning 0.111s)
             Mean action noise std: 4.21
          Mean value_function loss: 140.2154
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.9853
                       Mean reward: 878.66
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 1.6400
     Episode_Reward/lifting_object: 169.2638
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.12s
                      Time elapsed: 01:07:43
                               ETA: 05:10:15

################################################################################
                    [1m Learning iteration 1792/10000 [0m                     

                       Computation: 46593 steps/s (collection: 2.015s, learning 0.095s)
             Mean action noise std: 4.21
          Mean value_function loss: 157.1026
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 87.0000
                       Mean reward: 850.71
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.6474
     Episode_Reward/lifting_object: 169.5320
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.11s
                      Time elapsed: 01:07:45
                               ETA: 05:10:12

################################################################################
                    [1m Learning iteration 1793/10000 [0m                     

                       Computation: 45492 steps/s (collection: 2.056s, learning 0.105s)
             Mean action noise std: 4.21
          Mean value_function loss: 152.0759
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.0093
                       Mean reward: 868.12
               Mean episode length: 235.21
    Episode_Reward/reaching_object: 1.6561
     Episode_Reward/lifting_object: 170.8099
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.16s
                      Time elapsed: 01:07:48
                               ETA: 05:10:10

################################################################################
                    [1m Learning iteration 1794/10000 [0m                     

                       Computation: 47030 steps/s (collection: 2.000s, learning 0.090s)
             Mean action noise std: 4.21
          Mean value_function loss: 140.3686
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.0201
                       Mean reward: 825.12
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 1.6443
     Episode_Reward/lifting_object: 169.4811
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.09s
                      Time elapsed: 01:07:50
                               ETA: 05:10:06

################################################################################
                    [1m Learning iteration 1795/10000 [0m                     

                       Computation: 47531 steps/s (collection: 1.968s, learning 0.101s)
             Mean action noise std: 4.21
          Mean value_function loss: 156.8991
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.0359
                       Mean reward: 876.55
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.6317
     Episode_Reward/lifting_object: 168.4229
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.07s
                      Time elapsed: 01:07:52
                               ETA: 05:10:03

################################################################################
                    [1m Learning iteration 1796/10000 [0m                     

                       Computation: 44464 steps/s (collection: 2.095s, learning 0.116s)
             Mean action noise std: 4.22
          Mean value_function loss: 141.6860
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.0457
                       Mean reward: 844.77
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 1.6378
     Episode_Reward/lifting_object: 168.5790
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.21s
                      Time elapsed: 01:07:54
                               ETA: 05:10:01

################################################################################
                    [1m Learning iteration 1797/10000 [0m                     

                       Computation: 46493 steps/s (collection: 1.997s, learning 0.117s)
             Mean action noise std: 4.22
          Mean value_function loss: 153.5122
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.0551
                       Mean reward: 876.43
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 1.6705
     Episode_Reward/lifting_object: 172.5055
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.11s
                      Time elapsed: 01:07:56
                               ETA: 05:09:58

################################################################################
                    [1m Learning iteration 1798/10000 [0m                     

                       Computation: 45437 steps/s (collection: 2.058s, learning 0.105s)
             Mean action noise std: 4.22
          Mean value_function loss: 189.9333
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.0633
                       Mean reward: 810.73
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.6229
     Episode_Reward/lifting_object: 166.9221
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.16s
                      Time elapsed: 01:07:58
                               ETA: 05:09:55

################################################################################
                    [1m Learning iteration 1799/10000 [0m                     

                       Computation: 48140 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 4.22
          Mean value_function loss: 219.5380
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 87.0753
                       Mean reward: 812.97
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.6474
     Episode_Reward/lifting_object: 169.5213
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.04s
                      Time elapsed: 01:08:00
                               ETA: 05:09:52

################################################################################
                    [1m Learning iteration 1800/10000 [0m                     

                       Computation: 46578 steps/s (collection: 1.949s, learning 0.162s)
             Mean action noise std: 4.22
          Mean value_function loss: 207.3905
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.0863
                       Mean reward: 840.46
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.5699
     Episode_Reward/lifting_object: 160.2477
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.11s
                      Time elapsed: 01:08:02
                               ETA: 05:09:49

################################################################################
                    [1m Learning iteration 1801/10000 [0m                     

                       Computation: 45544 steps/s (collection: 2.041s, learning 0.118s)
             Mean action noise std: 4.22
          Mean value_function loss: 192.6405
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 87.0988
                       Mean reward: 818.78
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.6111
     Episode_Reward/lifting_object: 165.9175
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.16s
                      Time elapsed: 01:08:05
                               ETA: 05:09:46

################################################################################
                    [1m Learning iteration 1802/10000 [0m                     

                       Computation: 47242 steps/s (collection: 1.979s, learning 0.102s)
             Mean action noise std: 4.23
          Mean value_function loss: 198.1878
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 87.1070
                       Mean reward: 841.62
               Mean episode length: 228.33
    Episode_Reward/reaching_object: 1.6160
     Episode_Reward/lifting_object: 166.1927
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.08s
                      Time elapsed: 01:08:07
                               ETA: 05:09:43

################################################################################
                    [1m Learning iteration 1803/10000 [0m                     

                       Computation: 46078 steps/s (collection: 2.029s, learning 0.104s)
             Mean action noise std: 4.23
          Mean value_function loss: 180.4267
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 87.1158
                       Mean reward: 834.69
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.6389
     Episode_Reward/lifting_object: 168.4928
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.13s
                      Time elapsed: 01:08:09
                               ETA: 05:09:40

################################################################################
                    [1m Learning iteration 1804/10000 [0m                     

                       Computation: 45389 steps/s (collection: 2.064s, learning 0.102s)
             Mean action noise std: 4.23
          Mean value_function loss: 154.8888
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 87.1286
                       Mean reward: 834.57
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.6293
     Episode_Reward/lifting_object: 167.6701
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.17s
                      Time elapsed: 01:08:11
                               ETA: 05:09:37

################################################################################
                    [1m Learning iteration 1805/10000 [0m                     

                       Computation: 46829 steps/s (collection: 1.997s, learning 0.103s)
             Mean action noise std: 4.23
          Mean value_function loss: 148.3208
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 87.1370
                       Mean reward: 830.17
               Mean episode length: 224.98
    Episode_Reward/reaching_object: 1.6282
     Episode_Reward/lifting_object: 168.0455
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.10s
                      Time elapsed: 01:08:13
                               ETA: 05:09:34

################################################################################
                    [1m Learning iteration 1806/10000 [0m                     

                       Computation: 47249 steps/s (collection: 1.979s, learning 0.101s)
             Mean action noise std: 4.23
          Mean value_function loss: 166.9154
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.1431
                       Mean reward: 817.41
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.5960
     Episode_Reward/lifting_object: 164.2141
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.08s
                      Time elapsed: 01:08:15
                               ETA: 05:09:31

################################################################################
                    [1m Learning iteration 1807/10000 [0m                     

                       Computation: 47283 steps/s (collection: 1.974s, learning 0.105s)
             Mean action noise std: 4.23
          Mean value_function loss: 141.1320
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.1581
                       Mean reward: 852.24
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 1.6177
     Episode_Reward/lifting_object: 166.2391
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.08s
                      Time elapsed: 01:08:17
                               ETA: 05:09:28

################################################################################
                    [1m Learning iteration 1808/10000 [0m                     

                       Computation: 47000 steps/s (collection: 1.995s, learning 0.097s)
             Mean action noise std: 4.24
          Mean value_function loss: 165.8901
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.1792
                       Mean reward: 905.66
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 1.6663
     Episode_Reward/lifting_object: 171.9282
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.09s
                      Time elapsed: 01:08:19
                               ETA: 05:09:25

################################################################################
                    [1m Learning iteration 1809/10000 [0m                     

                       Computation: 47453 steps/s (collection: 1.943s, learning 0.129s)
             Mean action noise std: 4.24
          Mean value_function loss: 171.1694
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 87.2014
                       Mean reward: 793.43
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.6074
     Episode_Reward/lifting_object: 165.1624
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1171
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.07s
                      Time elapsed: 01:08:21
                               ETA: 05:09:22

################################################################################
                    [1m Learning iteration 1810/10000 [0m                     

                       Computation: 47665 steps/s (collection: 1.945s, learning 0.117s)
             Mean action noise std: 4.24
          Mean value_function loss: 111.8627
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.2151
                       Mean reward: 882.70
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.6495
     Episode_Reward/lifting_object: 170.4403
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1196
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.06s
                      Time elapsed: 01:08:23
                               ETA: 05:09:19

################################################################################
                    [1m Learning iteration 1811/10000 [0m                     

                       Computation: 47717 steps/s (collection: 1.968s, learning 0.093s)
             Mean action noise std: 4.24
          Mean value_function loss: 136.0079
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.2298
                       Mean reward: 847.47
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.6446
     Episode_Reward/lifting_object: 169.1288
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.06s
                      Time elapsed: 01:08:25
                               ETA: 05:09:15

################################################################################
                    [1m Learning iteration 1812/10000 [0m                     

                       Computation: 47340 steps/s (collection: 1.970s, learning 0.107s)
             Mean action noise std: 4.25
          Mean value_function loss: 138.1910
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.2437
                       Mean reward: 840.68
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.6260
     Episode_Reward/lifting_object: 167.5481
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.08s
                      Time elapsed: 01:08:28
                               ETA: 05:09:12

################################################################################
                    [1m Learning iteration 1813/10000 [0m                     

                       Computation: 47025 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 4.25
          Mean value_function loss: 151.2914
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 87.2535
                       Mean reward: 812.03
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.6184
     Episode_Reward/lifting_object: 166.1205
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.09s
                      Time elapsed: 01:08:30
                               ETA: 05:09:09

################################################################################
                    [1m Learning iteration 1814/10000 [0m                     

                       Computation: 45988 steps/s (collection: 2.021s, learning 0.116s)
             Mean action noise std: 4.25
          Mean value_function loss: 156.7723
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.2664
                       Mean reward: 860.03
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.6528
     Episode_Reward/lifting_object: 170.0157
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.14s
                      Time elapsed: 01:08:32
                               ETA: 05:09:06

################################################################################
                    [1m Learning iteration 1815/10000 [0m                     

                       Computation: 46848 steps/s (collection: 1.991s, learning 0.108s)
             Mean action noise std: 4.25
          Mean value_function loss: 163.9150
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.2807
                       Mean reward: 857.88
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 1.6273
     Episode_Reward/lifting_object: 167.6178
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.10s
                      Time elapsed: 01:08:34
                               ETA: 05:09:03

################################################################################
                    [1m Learning iteration 1816/10000 [0m                     

                       Computation: 47139 steps/s (collection: 1.986s, learning 0.099s)
             Mean action noise std: 4.25
          Mean value_function loss: 164.4341
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.2933
                       Mean reward: 825.85
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 1.6243
     Episode_Reward/lifting_object: 166.8875
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.09s
                      Time elapsed: 01:08:36
                               ETA: 05:09:00

################################################################################
                    [1m Learning iteration 1817/10000 [0m                     

                       Computation: 46016 steps/s (collection: 2.005s, learning 0.131s)
             Mean action noise std: 4.25
          Mean value_function loss: 162.1591
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 87.3038
                       Mean reward: 826.24
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.6339
     Episode_Reward/lifting_object: 168.0901
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.14s
                      Time elapsed: 01:08:38
                               ETA: 05:08:58

################################################################################
                    [1m Learning iteration 1818/10000 [0m                     

                       Computation: 45700 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 4.26
          Mean value_function loss: 142.4336
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.3121
                       Mean reward: 863.10
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.6489
     Episode_Reward/lifting_object: 169.3943
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1208
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.15s
                      Time elapsed: 01:08:40
                               ETA: 05:08:55

################################################################################
                    [1m Learning iteration 1819/10000 [0m                     

                       Computation: 46897 steps/s (collection: 1.991s, learning 0.106s)
             Mean action noise std: 4.26
          Mean value_function loss: 155.2987
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 87.3196
                       Mean reward: 867.63
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.6453
     Episode_Reward/lifting_object: 169.5588
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1205
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.10s
                      Time elapsed: 01:08:42
                               ETA: 05:08:52

################################################################################
                    [1m Learning iteration 1820/10000 [0m                     

                       Computation: 43853 steps/s (collection: 2.113s, learning 0.128s)
             Mean action noise std: 4.26
          Mean value_function loss: 146.8265
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.3280
                       Mean reward: 830.50
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.6319
     Episode_Reward/lifting_object: 168.0612
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.24s
                      Time elapsed: 01:08:45
                               ETA: 05:08:49

################################################################################
                    [1m Learning iteration 1821/10000 [0m                     

                       Computation: 47220 steps/s (collection: 1.986s, learning 0.096s)
             Mean action noise std: 4.26
          Mean value_function loss: 161.6490
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.3471
                       Mean reward: 821.01
               Mean episode length: 225.25
    Episode_Reward/reaching_object: 1.6235
     Episode_Reward/lifting_object: 166.7426
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.08s
                      Time elapsed: 01:08:47
                               ETA: 05:08:46

################################################################################
                    [1m Learning iteration 1822/10000 [0m                     

                       Computation: 47865 steps/s (collection: 1.958s, learning 0.096s)
             Mean action noise std: 4.26
          Mean value_function loss: 120.6397
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.3695
                       Mean reward: 864.54
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 1.6517
     Episode_Reward/lifting_object: 170.2191
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.05s
                      Time elapsed: 01:08:49
                               ETA: 05:08:43

################################################################################
                    [1m Learning iteration 1823/10000 [0m                     

                       Computation: 47112 steps/s (collection: 1.978s, learning 0.109s)
             Mean action noise std: 4.27
          Mean value_function loss: 159.8617
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.3853
                       Mean reward: 818.65
               Mean episode length: 223.96
    Episode_Reward/reaching_object: 1.5958
     Episode_Reward/lifting_object: 163.9654
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.09s
                      Time elapsed: 01:08:51
                               ETA: 05:08:40

################################################################################
                    [1m Learning iteration 1824/10000 [0m                     

                       Computation: 47424 steps/s (collection: 1.975s, learning 0.098s)
             Mean action noise std: 4.27
          Mean value_function loss: 177.2869
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.3996
                       Mean reward: 842.16
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.6498
     Episode_Reward/lifting_object: 169.4543
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.07s
                      Time elapsed: 01:08:53
                               ETA: 05:08:37

################################################################################
                    [1m Learning iteration 1825/10000 [0m                     

                       Computation: 46704 steps/s (collection: 2.007s, learning 0.098s)
             Mean action noise std: 4.27
          Mean value_function loss: 165.6168
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.4112
                       Mean reward: 830.11
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.6203
     Episode_Reward/lifting_object: 166.6274
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1190
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.10s
                      Time elapsed: 01:08:55
                               ETA: 05:08:34

################################################################################
                    [1m Learning iteration 1826/10000 [0m                     

                       Computation: 46297 steps/s (collection: 2.023s, learning 0.101s)
             Mean action noise std: 4.27
          Mean value_function loss: 138.6102
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 87.4217
                       Mean reward: 867.11
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.6462
     Episode_Reward/lifting_object: 169.7731
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.12s
                      Time elapsed: 01:08:57
                               ETA: 05:08:31

################################################################################
                    [1m Learning iteration 1827/10000 [0m                     

                       Computation: 47285 steps/s (collection: 1.972s, learning 0.107s)
             Mean action noise std: 4.27
          Mean value_function loss: 180.7883
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.4341
                       Mean reward: 833.60
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 1.5985
     Episode_Reward/lifting_object: 163.6982
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.08s
                      Time elapsed: 01:08:59
                               ETA: 05:08:28

################################################################################
                    [1m Learning iteration 1828/10000 [0m                     

                       Computation: 47010 steps/s (collection: 1.980s, learning 0.112s)
             Mean action noise std: 4.28
          Mean value_function loss: 147.7934
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.4486
                       Mean reward: 811.49
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 1.6007
     Episode_Reward/lifting_object: 164.3352
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.09s
                      Time elapsed: 01:09:01
                               ETA: 05:08:25

################################################################################
                    [1m Learning iteration 1829/10000 [0m                     

                       Computation: 46494 steps/s (collection: 1.986s, learning 0.129s)
             Mean action noise std: 4.28
          Mean value_function loss: 159.9048
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 87.4613
                       Mean reward: 856.10
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.6550
     Episode_Reward/lifting_object: 170.2178
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.11s
                      Time elapsed: 01:09:03
                               ETA: 05:08:22

################################################################################
                    [1m Learning iteration 1830/10000 [0m                     

                       Computation: 45119 steps/s (collection: 2.027s, learning 0.152s)
             Mean action noise std: 4.28
          Mean value_function loss: 153.7723
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.4720
                       Mean reward: 849.67
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.6484
     Episode_Reward/lifting_object: 170.4315
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.18s
                      Time elapsed: 01:09:06
                               ETA: 05:08:19

################################################################################
                    [1m Learning iteration 1831/10000 [0m                     

                       Computation: 45048 steps/s (collection: 2.074s, learning 0.108s)
             Mean action noise std: 4.28
          Mean value_function loss: 162.4800
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.4778
                       Mean reward: 868.76
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.6672
     Episode_Reward/lifting_object: 172.2038
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1229
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.18s
                      Time elapsed: 01:09:08
                               ETA: 05:08:17

################################################################################
                    [1m Learning iteration 1832/10000 [0m                     

                       Computation: 45136 steps/s (collection: 2.078s, learning 0.100s)
             Mean action noise std: 4.28
          Mean value_function loss: 196.7105
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.4875
                       Mean reward: 855.12
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 1.6288
     Episode_Reward/lifting_object: 167.6984
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1209
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.18s
                      Time elapsed: 01:09:10
                               ETA: 05:08:14

################################################################################
                    [1m Learning iteration 1833/10000 [0m                     

                       Computation: 43679 steps/s (collection: 2.158s, learning 0.092s)
             Mean action noise std: 4.28
          Mean value_function loss: 161.8464
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.5092
                       Mean reward: 845.34
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.6237
     Episode_Reward/lifting_object: 166.5854
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1208
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.25s
                      Time elapsed: 01:09:12
                               ETA: 05:08:12

################################################################################
                    [1m Learning iteration 1834/10000 [0m                     

                       Computation: 43069 steps/s (collection: 2.111s, learning 0.171s)
             Mean action noise std: 4.29
          Mean value_function loss: 185.9387
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 87.5249
                       Mean reward: 823.69
               Mean episode length: 222.86
    Episode_Reward/reaching_object: 1.5940
     Episode_Reward/lifting_object: 163.1443
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.28s
                      Time elapsed: 01:09:14
                               ETA: 05:08:09

################################################################################
                    [1m Learning iteration 1835/10000 [0m                     

                       Computation: 43344 steps/s (collection: 2.109s, learning 0.159s)
             Mean action noise std: 4.29
          Mean value_function loss: 199.1783
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.5306
                       Mean reward: 843.17
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 1.5833
     Episode_Reward/lifting_object: 162.2415
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.27s
                      Time elapsed: 01:09:17
                               ETA: 05:08:07

################################################################################
                    [1m Learning iteration 1836/10000 [0m                     

                       Computation: 45299 steps/s (collection: 2.056s, learning 0.115s)
             Mean action noise std: 4.29
          Mean value_function loss: 203.2247
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.5372
                       Mean reward: 794.89
               Mean episode length: 218.56
    Episode_Reward/reaching_object: 1.5672
     Episode_Reward/lifting_object: 160.6808
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.17s
                      Time elapsed: 01:09:19
                               ETA: 05:08:05

################################################################################
                    [1m Learning iteration 1837/10000 [0m                     

                       Computation: 45941 steps/s (collection: 2.043s, learning 0.097s)
             Mean action noise std: 4.29
          Mean value_function loss: 137.8133
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.5456
                       Mean reward: 854.30
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.6619
     Episode_Reward/lifting_object: 171.5564
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1236
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.14s
                      Time elapsed: 01:09:21
                               ETA: 05:08:02

################################################################################
                    [1m Learning iteration 1838/10000 [0m                     

                       Computation: 45001 steps/s (collection: 2.071s, learning 0.114s)
             Mean action noise std: 4.29
          Mean value_function loss: 199.9477
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.5595
                       Mean reward: 807.08
               Mean episode length: 221.03
    Episode_Reward/reaching_object: 1.6212
     Episode_Reward/lifting_object: 166.9716
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1208
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.18s
                      Time elapsed: 01:09:23
                               ETA: 05:07:59

################################################################################
                    [1m Learning iteration 1839/10000 [0m                     

                       Computation: 45396 steps/s (collection: 2.060s, learning 0.105s)
             Mean action noise std: 4.29
          Mean value_function loss: 203.9005
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.5713
                       Mean reward: 805.16
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.6077
     Episode_Reward/lifting_object: 164.7266
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.17s
                      Time elapsed: 01:09:25
                               ETA: 05:07:56

################################################################################
                    [1m Learning iteration 1840/10000 [0m                     

                       Computation: 44348 steps/s (collection: 2.075s, learning 0.142s)
             Mean action noise std: 4.29
          Mean value_function loss: 178.2898
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.5842
                       Mean reward: 811.92
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.5621
     Episode_Reward/lifting_object: 159.8332
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.22s
                      Time elapsed: 01:09:28
                               ETA: 05:07:54

################################################################################
                    [1m Learning iteration 1841/10000 [0m                     

                       Computation: 43212 steps/s (collection: 2.153s, learning 0.122s)
             Mean action noise std: 4.30
          Mean value_function loss: 222.7030
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 87.5978
                       Mean reward: 806.52
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.5826
     Episode_Reward/lifting_object: 162.4592
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.27s
                      Time elapsed: 01:09:30
                               ETA: 05:07:52

################################################################################
                    [1m Learning iteration 1842/10000 [0m                     

                       Computation: 45079 steps/s (collection: 2.077s, learning 0.104s)
             Mean action noise std: 4.30
          Mean value_function loss: 142.2831
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.6101
                       Mean reward: 853.25
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 1.5950
     Episode_Reward/lifting_object: 164.1733
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.18s
                      Time elapsed: 01:09:32
                               ETA: 05:07:49

################################################################################
                    [1m Learning iteration 1843/10000 [0m                     

                       Computation: 44365 steps/s (collection: 2.060s, learning 0.156s)
             Mean action noise std: 4.30
          Mean value_function loss: 167.4616
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 87.6192
                       Mean reward: 843.64
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.6352
     Episode_Reward/lifting_object: 168.5240
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.22s
                      Time elapsed: 01:09:34
                               ETA: 05:07:47

################################################################################
                    [1m Learning iteration 1844/10000 [0m                     

                       Computation: 45262 steps/s (collection: 2.076s, learning 0.096s)
             Mean action noise std: 4.30
          Mean value_function loss: 154.7898
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.6219
                       Mean reward: 823.46
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.6239
     Episode_Reward/lifting_object: 166.6896
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1216
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.17s
                      Time elapsed: 01:09:36
                               ETA: 05:07:44

################################################################################
                    [1m Learning iteration 1845/10000 [0m                     

                       Computation: 44736 steps/s (collection: 2.024s, learning 0.173s)
             Mean action noise std: 4.30
          Mean value_function loss: 159.9318
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.6285
                       Mean reward: 829.52
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 1.6378
     Episode_Reward/lifting_object: 168.4919
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.20s
                      Time elapsed: 01:09:39
                               ETA: 05:07:41

################################################################################
                    [1m Learning iteration 1846/10000 [0m                     

                       Computation: 44772 steps/s (collection: 2.082s, learning 0.114s)
             Mean action noise std: 4.30
          Mean value_function loss: 145.4722
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 87.6380
                       Mean reward: 836.20
               Mean episode length: 227.03
    Episode_Reward/reaching_object: 1.6268
     Episode_Reward/lifting_object: 167.4375
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.20s
                      Time elapsed: 01:09:41
                               ETA: 05:07:39

################################################################################
                    [1m Learning iteration 1847/10000 [0m                     

                       Computation: 46188 steps/s (collection: 2.015s, learning 0.113s)
             Mean action noise std: 4.30
          Mean value_function loss: 204.6333
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.6475
                       Mean reward: 822.01
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 1.6006
     Episode_Reward/lifting_object: 164.0338
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.13s
                      Time elapsed: 01:09:43
                               ETA: 05:07:36

################################################################################
                    [1m Learning iteration 1848/10000 [0m                     

                       Computation: 45133 steps/s (collection: 2.053s, learning 0.125s)
             Mean action noise std: 4.31
          Mean value_function loss: 163.1370
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.6591
                       Mean reward: 821.27
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 1.5924
     Episode_Reward/lifting_object: 163.4442
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.18s
                      Time elapsed: 01:09:45
                               ETA: 05:07:33

################################################################################
                    [1m Learning iteration 1849/10000 [0m                     

                       Computation: 46586 steps/s (collection: 1.977s, learning 0.134s)
             Mean action noise std: 4.31
          Mean value_function loss: 150.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.6632
                       Mean reward: 831.42
               Mean episode length: 226.97
    Episode_Reward/reaching_object: 1.6272
     Episode_Reward/lifting_object: 166.8061
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1229
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.11s
                      Time elapsed: 01:09:47
                               ETA: 05:07:30

################################################################################
                    [1m Learning iteration 1850/10000 [0m                     

                       Computation: 45110 steps/s (collection: 2.055s, learning 0.124s)
             Mean action noise std: 4.31
          Mean value_function loss: 162.7005
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.6733
                       Mean reward: 863.59
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.6203
     Episode_Reward/lifting_object: 165.9578
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1217
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.18s
                      Time elapsed: 01:09:49
                               ETA: 05:07:28

################################################################################
                    [1m Learning iteration 1851/10000 [0m                     

                       Computation: 43257 steps/s (collection: 2.123s, learning 0.150s)
             Mean action noise std: 4.31
          Mean value_function loss: 124.1136
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.6886
                       Mean reward: 871.95
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 1.6333
     Episode_Reward/lifting_object: 168.2427
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.27s
                      Time elapsed: 01:09:52
                               ETA: 05:07:25

################################################################################
                    [1m Learning iteration 1852/10000 [0m                     

                       Computation: 45210 steps/s (collection: 2.058s, learning 0.116s)
             Mean action noise std: 4.31
          Mean value_function loss: 165.2709
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.7011
                       Mean reward: 773.66
               Mean episode length: 212.24
    Episode_Reward/reaching_object: 1.5882
     Episode_Reward/lifting_object: 163.2938
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.17s
                      Time elapsed: 01:09:54
                               ETA: 05:07:23

################################################################################
                    [1m Learning iteration 1853/10000 [0m                     

                       Computation: 46919 steps/s (collection: 2.003s, learning 0.092s)
             Mean action noise std: 4.31
          Mean value_function loss: 141.3185
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 87.7183
                       Mean reward: 870.44
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.6437
     Episode_Reward/lifting_object: 169.1743
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.10s
                      Time elapsed: 01:09:56
                               ETA: 05:07:20

################################################################################
                    [1m Learning iteration 1854/10000 [0m                     

                       Computation: 46602 steps/s (collection: 2.009s, learning 0.101s)
             Mean action noise std: 4.32
          Mean value_function loss: 187.9957
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.7309
                       Mean reward: 780.80
               Mean episode length: 215.03
    Episode_Reward/reaching_object: 1.6121
     Episode_Reward/lifting_object: 165.4559
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1216
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.11s
                      Time elapsed: 01:09:58
                               ETA: 05:07:17

################################################################################
                    [1m Learning iteration 1855/10000 [0m                     

                       Computation: 46661 steps/s (collection: 2.012s, learning 0.095s)
             Mean action noise std: 4.32
          Mean value_function loss: 186.5378
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 87.7420
                       Mean reward: 806.76
               Mean episode length: 219.93
    Episode_Reward/reaching_object: 1.5898
     Episode_Reward/lifting_object: 162.3986
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.11s
                      Time elapsed: 01:10:00
                               ETA: 05:07:14

################################################################################
                    [1m Learning iteration 1856/10000 [0m                     

                       Computation: 46540 steps/s (collection: 2.009s, learning 0.104s)
             Mean action noise std: 4.32
          Mean value_function loss: 205.3579
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.7479
                       Mean reward: 870.13
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.6370
     Episode_Reward/lifting_object: 168.1483
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.11s
                      Time elapsed: 01:10:02
                               ETA: 05:07:11

################################################################################
                    [1m Learning iteration 1857/10000 [0m                     

                       Computation: 45663 steps/s (collection: 2.044s, learning 0.109s)
             Mean action noise std: 4.32
          Mean value_function loss: 156.0589
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 87.7554
                       Mean reward: 851.69
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.6166
     Episode_Reward/lifting_object: 166.1474
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.15s
                      Time elapsed: 01:10:04
                               ETA: 05:07:08

################################################################################
                    [1m Learning iteration 1858/10000 [0m                     

                       Computation: 45242 steps/s (collection: 2.079s, learning 0.094s)
             Mean action noise std: 4.32
          Mean value_function loss: 171.4746
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.7646
                       Mean reward: 828.76
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.6212
     Episode_Reward/lifting_object: 166.3598
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.17s
                      Time elapsed: 01:10:07
                               ETA: 05:07:06

################################################################################
                    [1m Learning iteration 1859/10000 [0m                     

                       Computation: 44591 steps/s (collection: 2.036s, learning 0.169s)
             Mean action noise std: 4.32
          Mean value_function loss: 172.2043
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.7788
                       Mean reward: 818.44
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 1.6400
     Episode_Reward/lifting_object: 168.1812
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1242
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.20s
                      Time elapsed: 01:10:09
                               ETA: 05:07:03

################################################################################
                    [1m Learning iteration 1860/10000 [0m                     

                       Computation: 45568 steps/s (collection: 2.028s, learning 0.130s)
             Mean action noise std: 4.33
          Mean value_function loss: 153.4388
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.7918
                       Mean reward: 811.80
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 1.6427
     Episode_Reward/lifting_object: 169.1600
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.16s
                      Time elapsed: 01:10:11
                               ETA: 05:07:00

################################################################################
                    [1m Learning iteration 1861/10000 [0m                     

                       Computation: 45879 steps/s (collection: 2.029s, learning 0.114s)
             Mean action noise std: 4.33
          Mean value_function loss: 147.5388
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.8095
                       Mean reward: 840.69
               Mean episode length: 227.99
    Episode_Reward/reaching_object: 1.6609
     Episode_Reward/lifting_object: 171.1454
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1260
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.14s
                      Time elapsed: 01:10:13
                               ETA: 05:06:58

################################################################################
                    [1m Learning iteration 1862/10000 [0m                     

                       Computation: 44804 steps/s (collection: 2.040s, learning 0.154s)
             Mean action noise std: 4.33
          Mean value_function loss: 119.8396
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.8205
                       Mean reward: 884.22
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.6408
     Episode_Reward/lifting_object: 168.7757
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.19s
                      Time elapsed: 01:10:15
                               ETA: 05:06:55

################################################################################
                    [1m Learning iteration 1863/10000 [0m                     

                       Computation: 47012 steps/s (collection: 1.995s, learning 0.096s)
             Mean action noise std: 4.33
          Mean value_function loss: 150.3088
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.8346
                       Mean reward: 788.17
               Mean episode length: 216.17
    Episode_Reward/reaching_object: 1.6378
     Episode_Reward/lifting_object: 167.9548
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1248
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.09s
                      Time elapsed: 01:10:17
                               ETA: 05:06:52

################################################################################
                    [1m Learning iteration 1864/10000 [0m                     

                       Computation: 46267 steps/s (collection: 2.025s, learning 0.100s)
             Mean action noise std: 4.33
          Mean value_function loss: 147.4230
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.8478
                       Mean reward: 845.75
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.6202
     Episode_Reward/lifting_object: 166.2415
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1235
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.12s
                      Time elapsed: 01:10:20
                               ETA: 05:06:49

################################################################################
                    [1m Learning iteration 1865/10000 [0m                     

                       Computation: 43482 steps/s (collection: 2.120s, learning 0.140s)
             Mean action noise std: 4.33
          Mean value_function loss: 165.3779
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 87.8546
                       Mean reward: 837.80
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.6472
     Episode_Reward/lifting_object: 169.4041
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1253
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.26s
                      Time elapsed: 01:10:22
                               ETA: 05:06:47

################################################################################
                    [1m Learning iteration 1866/10000 [0m                     

                       Computation: 45080 steps/s (collection: 2.057s, learning 0.124s)
             Mean action noise std: 4.34
          Mean value_function loss: 170.5286
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.8613
                       Mean reward: 887.33
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 1.6300
     Episode_Reward/lifting_object: 167.5547
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1243
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.18s
                      Time elapsed: 01:10:24
                               ETA: 05:06:44

################################################################################
                    [1m Learning iteration 1867/10000 [0m                     

                       Computation: 43941 steps/s (collection: 2.108s, learning 0.129s)
             Mean action noise std: 4.34
          Mean value_function loss: 172.6007
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.8733
                       Mean reward: 771.95
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 1.6237
     Episode_Reward/lifting_object: 166.7712
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.24s
                      Time elapsed: 01:10:26
                               ETA: 05:06:42

################################################################################
                    [1m Learning iteration 1868/10000 [0m                     

                       Computation: 44808 steps/s (collection: 2.070s, learning 0.124s)
             Mean action noise std: 4.34
          Mean value_function loss: 164.9360
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 87.8863
                       Mean reward: 857.51
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 1.6377
     Episode_Reward/lifting_object: 168.8281
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.19s
                      Time elapsed: 01:10:28
                               ETA: 05:06:39

################################################################################
                    [1m Learning iteration 1869/10000 [0m                     

                       Computation: 45483 steps/s (collection: 2.053s, learning 0.109s)
             Mean action noise std: 4.34
          Mean value_function loss: 184.1661
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.8889
                       Mean reward: 824.15
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.5614
     Episode_Reward/lifting_object: 160.3262
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.16s
                      Time elapsed: 01:10:31
                               ETA: 05:06:37

################################################################################
                    [1m Learning iteration 1870/10000 [0m                     

                       Computation: 44548 steps/s (collection: 2.069s, learning 0.138s)
             Mean action noise std: 4.34
          Mean value_function loss: 185.4516
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 87.8929
                       Mean reward: 854.25
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.5813
     Episode_Reward/lifting_object: 162.4199
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.21s
                      Time elapsed: 01:10:33
                               ETA: 05:06:34

################################################################################
                    [1m Learning iteration 1871/10000 [0m                     

                       Computation: 44988 steps/s (collection: 2.064s, learning 0.121s)
             Mean action noise std: 4.34
          Mean value_function loss: 193.2511
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.8990
                       Mean reward: 833.50
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.6059
     Episode_Reward/lifting_object: 164.9026
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1235
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.19s
                      Time elapsed: 01:10:35
                               ETA: 05:06:31

################################################################################
                    [1m Learning iteration 1872/10000 [0m                     

                       Computation: 44555 steps/s (collection: 2.092s, learning 0.115s)
             Mean action noise std: 4.34
          Mean value_function loss: 169.7258
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.9095
                       Mean reward: 792.38
               Mean episode length: 216.11
    Episode_Reward/reaching_object: 1.6001
     Episode_Reward/lifting_object: 164.6118
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.21s
                      Time elapsed: 01:10:37
                               ETA: 05:06:29

################################################################################
                    [1m Learning iteration 1873/10000 [0m                     

                       Computation: 45263 steps/s (collection: 2.057s, learning 0.115s)
             Mean action noise std: 4.35
          Mean value_function loss: 191.2295
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 87.9231
                       Mean reward: 817.20
               Mean episode length: 223.54
    Episode_Reward/reaching_object: 1.6019
     Episode_Reward/lifting_object: 164.4823
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.17s
                      Time elapsed: 01:10:39
                               ETA: 05:06:26

################################################################################
                    [1m Learning iteration 1874/10000 [0m                     

                       Computation: 45223 steps/s (collection: 2.052s, learning 0.122s)
             Mean action noise std: 4.35
          Mean value_function loss: 130.4210
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.9406
                       Mean reward: 846.86
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.6428
     Episode_Reward/lifting_object: 169.3006
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.17s
                      Time elapsed: 01:10:41
                               ETA: 05:06:24

################################################################################
                    [1m Learning iteration 1875/10000 [0m                     

                       Computation: 45082 steps/s (collection: 2.077s, learning 0.104s)
             Mean action noise std: 4.35
          Mean value_function loss: 127.2430
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 87.9610
                       Mean reward: 877.09
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.6865
     Episode_Reward/lifting_object: 174.2727
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1292
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.18s
                      Time elapsed: 01:10:44
                               ETA: 05:06:21

################################################################################
                    [1m Learning iteration 1876/10000 [0m                     

                       Computation: 45409 steps/s (collection: 2.044s, learning 0.121s)
             Mean action noise std: 4.35
          Mean value_function loss: 132.0771
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.9702
                       Mean reward: 846.84
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 1.6488
     Episode_Reward/lifting_object: 170.4048
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.16s
                      Time elapsed: 01:10:46
                               ETA: 05:06:18

################################################################################
                    [1m Learning iteration 1877/10000 [0m                     

                       Computation: 45567 steps/s (collection: 2.051s, learning 0.107s)
             Mean action noise std: 4.35
          Mean value_function loss: 168.7583
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.9805
                       Mean reward: 872.42
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.6544
     Episode_Reward/lifting_object: 171.2968
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1269
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.16s
                      Time elapsed: 01:10:48
                               ETA: 05:06:16

################################################################################
                    [1m Learning iteration 1878/10000 [0m                     

                       Computation: 46461 steps/s (collection: 2.008s, learning 0.108s)
             Mean action noise std: 4.36
          Mean value_function loss: 156.2791
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.9901
                       Mean reward: 873.56
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.6226
     Episode_Reward/lifting_object: 167.4142
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1248
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.12s
                      Time elapsed: 01:10:50
                               ETA: 05:06:13

################################################################################
                    [1m Learning iteration 1879/10000 [0m                     

                       Computation: 46359 steps/s (collection: 2.021s, learning 0.099s)
             Mean action noise std: 4.36
          Mean value_function loss: 146.3602
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.0047
                       Mean reward: 846.34
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 1.6467
     Episode_Reward/lifting_object: 170.0340
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.12s
                      Time elapsed: 01:10:52
                               ETA: 05:06:10

################################################################################
                    [1m Learning iteration 1880/10000 [0m                     

                       Computation: 46806 steps/s (collection: 2.004s, learning 0.096s)
             Mean action noise std: 4.36
          Mean value_function loss: 150.7944
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 88.0163
                       Mean reward: 920.63
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 1.6524
     Episode_Reward/lifting_object: 170.5319
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1275
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.10s
                      Time elapsed: 01:10:54
                               ETA: 05:06:07

################################################################################
                    [1m Learning iteration 1881/10000 [0m                     

                       Computation: 45964 steps/s (collection: 2.046s, learning 0.093s)
             Mean action noise std: 4.36
          Mean value_function loss: 144.9488
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.0229
                       Mean reward: 863.65
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 1.6299
     Episode_Reward/lifting_object: 167.8140
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.14s
                      Time elapsed: 01:10:56
                               ETA: 05:06:04

################################################################################
                    [1m Learning iteration 1882/10000 [0m                     

                       Computation: 45794 steps/s (collection: 2.048s, learning 0.099s)
             Mean action noise std: 4.36
          Mean value_function loss: 133.2256
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 88.0352
                       Mean reward: 875.00
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 1.6385
     Episode_Reward/lifting_object: 168.8094
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.15s
                      Time elapsed: 01:10:59
                               ETA: 05:06:01

################################################################################
                    [1m Learning iteration 1883/10000 [0m                     

                       Computation: 44199 steps/s (collection: 2.048s, learning 0.176s)
             Mean action noise std: 4.37
          Mean value_function loss: 138.3360
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 88.0530
                       Mean reward: 827.22
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.6524
     Episode_Reward/lifting_object: 170.8155
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.22s
                      Time elapsed: 01:11:01
                               ETA: 05:05:59

################################################################################
                    [1m Learning iteration 1884/10000 [0m                     

                       Computation: 43912 steps/s (collection: 2.101s, learning 0.137s)
             Mean action noise std: 4.37
          Mean value_function loss: 158.2559
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.0664
                       Mean reward: 808.39
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.6044
     Episode_Reward/lifting_object: 164.8954
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.24s
                      Time elapsed: 01:11:03
                               ETA: 05:05:57

################################################################################
                    [1m Learning iteration 1885/10000 [0m                     

                       Computation: 44352 steps/s (collection: 2.118s, learning 0.098s)
             Mean action noise std: 4.37
          Mean value_function loss: 152.9027
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 88.0811
                       Mean reward: 809.43
               Mean episode length: 220.23
    Episode_Reward/reaching_object: 1.6154
     Episode_Reward/lifting_object: 166.2399
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1248
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.22s
                      Time elapsed: 01:11:05
                               ETA: 05:05:54

################################################################################
                    [1m Learning iteration 1886/10000 [0m                     

                       Computation: 45553 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 4.37
          Mean value_function loss: 160.6232
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 88.0900
                       Mean reward: 787.90
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 1.6366
     Episode_Reward/lifting_object: 169.0008
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1264
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.16s
                      Time elapsed: 01:11:07
                               ETA: 05:05:51

################################################################################
                    [1m Learning iteration 1887/10000 [0m                     

                       Computation: 45407 steps/s (collection: 2.051s, learning 0.114s)
             Mean action noise std: 4.37
          Mean value_function loss: 149.4721
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.0931
                       Mean reward: 852.28
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.6370
     Episode_Reward/lifting_object: 169.1101
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.16s
                      Time elapsed: 01:11:10
                               ETA: 05:05:49

################################################################################
                    [1m Learning iteration 1888/10000 [0m                     

                       Computation: 46292 steps/s (collection: 2.028s, learning 0.095s)
             Mean action noise std: 4.37
          Mean value_function loss: 147.6832
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.1024
                       Mean reward: 847.65
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 1.6588
     Episode_Reward/lifting_object: 171.8008
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1280
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.12s
                      Time elapsed: 01:11:12
                               ETA: 05:05:46

################################################################################
                    [1m Learning iteration 1889/10000 [0m                     

                       Computation: 44617 steps/s (collection: 2.036s, learning 0.167s)
             Mean action noise std: 4.38
          Mean value_function loss: 144.3229
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.1177
                       Mean reward: 869.42
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.6599
     Episode_Reward/lifting_object: 171.4483
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.20s
                      Time elapsed: 01:11:14
                               ETA: 05:05:43

################################################################################
                    [1m Learning iteration 1890/10000 [0m                     

                       Computation: 44762 steps/s (collection: 2.096s, learning 0.100s)
             Mean action noise std: 4.38
          Mean value_function loss: 198.7110
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.1358
                       Mean reward: 862.99
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.6151
     Episode_Reward/lifting_object: 167.0092
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.20s
                      Time elapsed: 01:11:16
                               ETA: 05:05:41

################################################################################
                    [1m Learning iteration 1891/10000 [0m                     

                       Computation: 45270 steps/s (collection: 2.057s, learning 0.115s)
             Mean action noise std: 4.38
          Mean value_function loss: 159.0028
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.1526
                       Mean reward: 838.44
               Mean episode length: 225.57
    Episode_Reward/reaching_object: 1.6167
     Episode_Reward/lifting_object: 166.7700
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1260
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.17s
                      Time elapsed: 01:11:18
                               ETA: 05:05:38

################################################################################
                    [1m Learning iteration 1892/10000 [0m                     

                       Computation: 46236 steps/s (collection: 2.032s, learning 0.094s)
             Mean action noise std: 4.38
          Mean value_function loss: 197.1300
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 88.1636
                       Mean reward: 814.71
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.5890
     Episode_Reward/lifting_object: 163.4011
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.13s
                      Time elapsed: 01:11:20
                               ETA: 05:05:35

################################################################################
                    [1m Learning iteration 1893/10000 [0m                     

                       Computation: 45990 steps/s (collection: 2.030s, learning 0.107s)
             Mean action noise std: 4.38
          Mean value_function loss: 126.7982
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.1765
                       Mean reward: 877.96
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 1.6454
     Episode_Reward/lifting_object: 170.3770
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1276
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.14s
                      Time elapsed: 01:11:23
                               ETA: 05:05:33

################################################################################
                    [1m Learning iteration 1894/10000 [0m                     

                       Computation: 45206 steps/s (collection: 2.063s, learning 0.112s)
             Mean action noise std: 4.39
          Mean value_function loss: 113.4822
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.1820
                       Mean reward: 901.43
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.6608
     Episode_Reward/lifting_object: 171.9291
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.17s
                      Time elapsed: 01:11:25
                               ETA: 05:05:30

################################################################################
                    [1m Learning iteration 1895/10000 [0m                     

                       Computation: 44905 steps/s (collection: 2.052s, learning 0.137s)
             Mean action noise std: 4.39
          Mean value_function loss: 156.4983
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.1918
                       Mean reward: 852.84
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 1.6325
     Episode_Reward/lifting_object: 168.6472
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1275
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.19s
                      Time elapsed: 01:11:27
                               ETA: 05:05:27

################################################################################
                    [1m Learning iteration 1896/10000 [0m                     

                       Computation: 45973 steps/s (collection: 2.045s, learning 0.093s)
             Mean action noise std: 4.39
          Mean value_function loss: 144.9003
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.2018
                       Mean reward: 871.66
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.6281
     Episode_Reward/lifting_object: 168.1505
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1273
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.14s
                      Time elapsed: 01:11:29
                               ETA: 05:05:25

################################################################################
                    [1m Learning iteration 1897/10000 [0m                     

                       Computation: 46687 steps/s (collection: 2.011s, learning 0.095s)
             Mean action noise std: 4.39
          Mean value_function loss: 132.8015
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.2101
                       Mean reward: 871.87
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.6474
     Episode_Reward/lifting_object: 170.1905
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1281
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.11s
                      Time elapsed: 01:11:31
                               ETA: 05:05:22

################################################################################
                    [1m Learning iteration 1898/10000 [0m                     

                       Computation: 45015 steps/s (collection: 2.051s, learning 0.133s)
             Mean action noise std: 4.39
          Mean value_function loss: 139.8506
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.2195
                       Mean reward: 874.27
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 1.6500
     Episode_Reward/lifting_object: 170.9764
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.18s
                      Time elapsed: 01:11:33
                               ETA: 05:05:19

################################################################################
                    [1m Learning iteration 1899/10000 [0m                     

                       Computation: 45587 steps/s (collection: 2.055s, learning 0.101s)
             Mean action noise std: 4.39
          Mean value_function loss: 148.3210
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.2324
                       Mean reward: 859.62
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.6074
     Episode_Reward/lifting_object: 166.5419
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1263
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.16s
                      Time elapsed: 01:11:36
                               ETA: 05:05:16

################################################################################
                    [1m Learning iteration 1900/10000 [0m                     

                       Computation: 45937 steps/s (collection: 2.023s, learning 0.117s)
             Mean action noise std: 4.39
          Mean value_function loss: 151.1526
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.2484
                       Mean reward: 836.42
               Mean episode length: 226.83
    Episode_Reward/reaching_object: 1.6252
     Episode_Reward/lifting_object: 167.8675
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1267
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.14s
                      Time elapsed: 01:11:38
                               ETA: 05:05:14

################################################################################
                    [1m Learning iteration 1901/10000 [0m                     

                       Computation: 44417 steps/s (collection: 2.115s, learning 0.099s)
             Mean action noise std: 4.40
          Mean value_function loss: 102.1203
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.2596
                       Mean reward: 823.19
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.6399
     Episode_Reward/lifting_object: 169.7496
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.21s
                      Time elapsed: 01:11:40
                               ETA: 05:05:11

################################################################################
                    [1m Learning iteration 1902/10000 [0m                     

                       Computation: 46114 steps/s (collection: 2.031s, learning 0.101s)
             Mean action noise std: 4.40
          Mean value_function loss: 138.1334
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 88.2769
                       Mean reward: 831.71
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.6363
     Episode_Reward/lifting_object: 169.5835
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1281
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.13s
                      Time elapsed: 01:11:42
                               ETA: 05:05:08

################################################################################
                    [1m Learning iteration 1903/10000 [0m                     

                       Computation: 46506 steps/s (collection: 2.019s, learning 0.095s)
             Mean action noise std: 4.40
          Mean value_function loss: 141.9526
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 88.2951
                       Mean reward: 837.83
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.6496
     Episode_Reward/lifting_object: 170.8271
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.11s
                      Time elapsed: 01:11:44
                               ETA: 05:05:05

################################################################################
                    [1m Learning iteration 1904/10000 [0m                     

                       Computation: 46144 steps/s (collection: 2.036s, learning 0.094s)
             Mean action noise std: 4.40
          Mean value_function loss: 166.2695
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.3074
                       Mean reward: 851.69
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 1.5976
     Episode_Reward/lifting_object: 164.9081
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1257
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.13s
                      Time elapsed: 01:11:46
                               ETA: 05:05:03

################################################################################
                    [1m Learning iteration 1905/10000 [0m                     

                       Computation: 46626 steps/s (collection: 2.010s, learning 0.098s)
             Mean action noise std: 4.41
          Mean value_function loss: 139.5870
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.3163
                       Mean reward: 845.44
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 1.6495
     Episode_Reward/lifting_object: 170.7300
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.11s
                      Time elapsed: 01:11:48
                               ETA: 05:05:00

################################################################################
                    [1m Learning iteration 1906/10000 [0m                     

                       Computation: 45067 steps/s (collection: 2.057s, learning 0.124s)
             Mean action noise std: 4.41
          Mean value_function loss: 138.3454
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.3285
                       Mean reward: 867.55
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.6076
     Episode_Reward/lifting_object: 165.8001
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.18s
                      Time elapsed: 01:11:51
                               ETA: 05:04:57

################################################################################
                    [1m Learning iteration 1907/10000 [0m                     

                       Computation: 46112 steps/s (collection: 2.013s, learning 0.119s)
             Mean action noise std: 4.41
          Mean value_function loss: 163.5403
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.3420
                       Mean reward: 800.49
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 1.6295
     Episode_Reward/lifting_object: 167.9713
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1281
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.13s
                      Time elapsed: 01:11:53
                               ETA: 05:04:54

################################################################################
                    [1m Learning iteration 1908/10000 [0m                     

                       Computation: 45691 steps/s (collection: 2.051s, learning 0.100s)
             Mean action noise std: 4.41
          Mean value_function loss: 166.5015
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 88.3611
                       Mean reward: 882.26
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 1.6519
     Episode_Reward/lifting_object: 170.2365
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1297
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.15s
                      Time elapsed: 01:11:55
                               ETA: 05:04:52

################################################################################
                    [1m Learning iteration 1909/10000 [0m                     

                       Computation: 44257 steps/s (collection: 2.070s, learning 0.151s)
             Mean action noise std: 4.41
          Mean value_function loss: 147.1826
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 88.3771
                       Mean reward: 852.85
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 1.6451
     Episode_Reward/lifting_object: 170.2133
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1292
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.22s
                      Time elapsed: 01:11:57
                               ETA: 05:04:49

################################################################################
                    [1m Learning iteration 1910/10000 [0m                     

                       Computation: 45102 steps/s (collection: 2.041s, learning 0.139s)
             Mean action noise std: 4.42
          Mean value_function loss: 174.4841
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.3880
                       Mean reward: 850.41
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.6251
     Episode_Reward/lifting_object: 167.8331
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1281
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.18s
                      Time elapsed: 01:11:59
                               ETA: 05:04:47

################################################################################
                    [1m Learning iteration 1911/10000 [0m                     

                       Computation: 46174 steps/s (collection: 2.036s, learning 0.093s)
             Mean action noise std: 4.42
          Mean value_function loss: 178.9439
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.4030
                       Mean reward: 765.39
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 1.5840
     Episode_Reward/lifting_object: 162.8924
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1260
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.13s
                      Time elapsed: 01:12:01
                               ETA: 05:04:44

################################################################################
                    [1m Learning iteration 1912/10000 [0m                     

                       Computation: 45298 steps/s (collection: 2.067s, learning 0.103s)
             Mean action noise std: 4.42
          Mean value_function loss: 133.3860
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.4219
                       Mean reward: 854.31
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.6495
     Episode_Reward/lifting_object: 170.5342
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.17s
                      Time elapsed: 01:12:04
                               ETA: 05:04:41

################################################################################
                    [1m Learning iteration 1913/10000 [0m                     

                       Computation: 44814 steps/s (collection: 2.085s, learning 0.109s)
             Mean action noise std: 4.42
          Mean value_function loss: 181.5100
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 88.4391
                       Mean reward: 821.54
               Mean episode length: 222.60
    Episode_Reward/reaching_object: 1.6199
     Episode_Reward/lifting_object: 167.0782
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1287
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.19s
                      Time elapsed: 01:12:06
                               ETA: 05:04:39

################################################################################
                    [1m Learning iteration 1914/10000 [0m                     

                       Computation: 45211 steps/s (collection: 2.078s, learning 0.096s)
             Mean action noise std: 4.42
          Mean value_function loss: 159.0800
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 88.4506
                       Mean reward: 857.85
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.6409
     Episode_Reward/lifting_object: 169.3578
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.17s
                      Time elapsed: 01:12:08
                               ETA: 05:04:36

################################################################################
                    [1m Learning iteration 1915/10000 [0m                     

                       Computation: 46777 steps/s (collection: 1.988s, learning 0.114s)
             Mean action noise std: 4.42
          Mean value_function loss: 160.5380
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 88.4570
                       Mean reward: 816.51
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.6158
     Episode_Reward/lifting_object: 166.6002
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.10s
                      Time elapsed: 01:12:10
                               ETA: 05:04:33

################################################################################
                    [1m Learning iteration 1916/10000 [0m                     

                       Computation: 44182 steps/s (collection: 2.067s, learning 0.158s)
             Mean action noise std: 4.43
          Mean value_function loss: 134.8957
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.4675
                       Mean reward: 817.38
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 1.6240
     Episode_Reward/lifting_object: 167.8503
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.22s
                      Time elapsed: 01:12:12
                               ETA: 05:04:31

################################################################################
                    [1m Learning iteration 1917/10000 [0m                     

                       Computation: 44862 steps/s (collection: 2.061s, learning 0.131s)
             Mean action noise std: 4.43
          Mean value_function loss: 144.9225
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.4835
                       Mean reward: 781.37
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 1.6194
     Episode_Reward/lifting_object: 167.4174
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1293
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.19s
                      Time elapsed: 01:12:14
                               ETA: 05:04:28

################################################################################
                    [1m Learning iteration 1918/10000 [0m                     

                       Computation: 45948 steps/s (collection: 2.012s, learning 0.128s)
             Mean action noise std: 4.43
          Mean value_function loss: 118.6985
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.5001
                       Mean reward: 874.81
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 1.6599
     Episode_Reward/lifting_object: 171.7770
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1324
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.14s
                      Time elapsed: 01:12:17
                               ETA: 05:04:25

################################################################################
                    [1m Learning iteration 1919/10000 [0m                     

                       Computation: 45647 steps/s (collection: 1.989s, learning 0.165s)
             Mean action noise std: 4.43
          Mean value_function loss: 148.3922
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 88.5157
                       Mean reward: 883.54
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.6366
     Episode_Reward/lifting_object: 169.4848
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.15s
                      Time elapsed: 01:12:19
                               ETA: 05:04:23

################################################################################
                    [1m Learning iteration 1920/10000 [0m                     

                       Computation: 45779 steps/s (collection: 2.038s, learning 0.109s)
             Mean action noise std: 4.44
          Mean value_function loss: 125.2582
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 88.5342
                       Mean reward: 860.25
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.6278
     Episode_Reward/lifting_object: 168.1061
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.15s
                      Time elapsed: 01:12:21
                               ETA: 05:04:20

################################################################################
                    [1m Learning iteration 1921/10000 [0m                     

                       Computation: 46549 steps/s (collection: 2.020s, learning 0.092s)
             Mean action noise std: 4.44
          Mean value_function loss: 144.3419
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.5462
                       Mean reward: 867.62
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 1.6619
     Episode_Reward/lifting_object: 171.9578
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1314
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.11s
                      Time elapsed: 01:12:23
                               ETA: 05:04:17

################################################################################
                    [1m Learning iteration 1922/10000 [0m                     

                       Computation: 46246 steps/s (collection: 2.009s, learning 0.116s)
             Mean action noise std: 4.44
          Mean value_function loss: 133.9396
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 88.5544
                       Mean reward: 840.30
               Mean episode length: 226.96
    Episode_Reward/reaching_object: 1.6310
     Episode_Reward/lifting_object: 168.3235
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.13s
                      Time elapsed: 01:12:25
                               ETA: 05:04:14

################################################################################
                    [1m Learning iteration 1923/10000 [0m                     

                       Computation: 46761 steps/s (collection: 2.002s, learning 0.100s)
             Mean action noise std: 4.44
          Mean value_function loss: 156.2280
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.5646
                       Mean reward: 842.07
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 1.6447
     Episode_Reward/lifting_object: 169.7700
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1312
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.10s
                      Time elapsed: 01:12:27
                               ETA: 05:04:11

################################################################################
                    [1m Learning iteration 1924/10000 [0m                     

                       Computation: 46451 steps/s (collection: 2.011s, learning 0.105s)
             Mean action noise std: 4.44
          Mean value_function loss: 184.0139
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 88.5808
                       Mean reward: 863.62
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.6493
     Episode_Reward/lifting_object: 170.4454
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1316
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.12s
                      Time elapsed: 01:12:29
                               ETA: 05:04:08

################################################################################
                    [1m Learning iteration 1925/10000 [0m                     

                       Computation: 45059 steps/s (collection: 2.019s, learning 0.163s)
             Mean action noise std: 4.45
          Mean value_function loss: 201.7508
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.5880
                       Mean reward: 813.17
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 1.5821
     Episode_Reward/lifting_object: 162.7394
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.18s
                      Time elapsed: 01:12:31
                               ETA: 05:04:06

################################################################################
                    [1m Learning iteration 1926/10000 [0m                     

                       Computation: 45452 steps/s (collection: 2.018s, learning 0.145s)
             Mean action noise std: 4.45
          Mean value_function loss: 188.2229
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.5980
                       Mean reward: 813.03
               Mean episode length: 222.02
    Episode_Reward/reaching_object: 1.5945
     Episode_Reward/lifting_object: 163.5845
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.16s
                      Time elapsed: 01:12:34
                               ETA: 05:04:03

################################################################################
                    [1m Learning iteration 1927/10000 [0m                     

                       Computation: 46341 steps/s (collection: 2.000s, learning 0.121s)
             Mean action noise std: 4.45
          Mean value_function loss: 151.7714
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 88.6092
                       Mean reward: 866.67
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 1.6413
     Episode_Reward/lifting_object: 169.2064
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1317
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.12s
                      Time elapsed: 01:12:36
                               ETA: 05:04:00

################################################################################
                    [1m Learning iteration 1928/10000 [0m                     

                       Computation: 45962 steps/s (collection: 2.020s, learning 0.119s)
             Mean action noise std: 4.45
          Mean value_function loss: 199.6884
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.6193
                       Mean reward: 835.27
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 1.6456
     Episode_Reward/lifting_object: 170.0154
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.14s
                      Time elapsed: 01:12:38
                               ETA: 05:03:57

################################################################################
                    [1m Learning iteration 1929/10000 [0m                     

                       Computation: 46459 steps/s (collection: 2.008s, learning 0.108s)
             Mean action noise std: 4.45
          Mean value_function loss: 141.5447
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.6321
                       Mean reward: 878.80
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.6280
     Episode_Reward/lifting_object: 168.4826
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.12s
                      Time elapsed: 01:12:40
                               ETA: 05:03:55

################################################################################
                    [1m Learning iteration 1930/10000 [0m                     

                       Computation: 46487 steps/s (collection: 1.999s, learning 0.116s)
             Mean action noise std: 4.46
          Mean value_function loss: 169.8031
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.6496
                       Mean reward: 836.58
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.6280
     Episode_Reward/lifting_object: 167.9365
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.11s
                      Time elapsed: 01:12:42
                               ETA: 05:03:52

################################################################################
                    [1m Learning iteration 1931/10000 [0m                     

                       Computation: 46097 steps/s (collection: 2.037s, learning 0.096s)
             Mean action noise std: 4.46
          Mean value_function loss: 142.4843
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.6666
                       Mean reward: 862.37
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 1.6395
     Episode_Reward/lifting_object: 169.9134
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.13s
                      Time elapsed: 01:12:44
                               ETA: 05:03:49

################################################################################
                    [1m Learning iteration 1932/10000 [0m                     

                       Computation: 46384 steps/s (collection: 2.014s, learning 0.106s)
             Mean action noise std: 4.46
          Mean value_function loss: 137.3991
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.6746
                       Mean reward: 857.45
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.6475
     Episode_Reward/lifting_object: 169.7409
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1316
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.12s
                      Time elapsed: 01:12:46
                               ETA: 05:03:46

################################################################################
                    [1m Learning iteration 1933/10000 [0m                     

                       Computation: 46821 steps/s (collection: 1.992s, learning 0.108s)
             Mean action noise std: 4.46
          Mean value_function loss: 118.2579
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 88.6869
                       Mean reward: 867.42
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.6890
     Episode_Reward/lifting_object: 175.1111
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1347
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.10s
                      Time elapsed: 01:12:48
                               ETA: 05:03:43

################################################################################
                    [1m Learning iteration 1934/10000 [0m                     

                       Computation: 45542 steps/s (collection: 2.007s, learning 0.152s)
             Mean action noise std: 4.46
          Mean value_function loss: 158.3796
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.6993
                       Mean reward: 836.58
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 1.6182
     Episode_Reward/lifting_object: 166.5613
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1305
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.16s
                      Time elapsed: 01:12:51
                               ETA: 05:03:41

################################################################################
                    [1m Learning iteration 1935/10000 [0m                     

                       Computation: 46572 steps/s (collection: 2.007s, learning 0.104s)
             Mean action noise std: 4.46
          Mean value_function loss: 153.6375
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 88.7066
                       Mean reward: 879.93
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.6403
     Episode_Reward/lifting_object: 169.1589
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1324
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.11s
                      Time elapsed: 01:12:53
                               ETA: 05:03:38

################################################################################
                    [1m Learning iteration 1936/10000 [0m                     

                       Computation: 45749 steps/s (collection: 2.035s, learning 0.114s)
             Mean action noise std: 4.46
          Mean value_function loss: 147.6529
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.7145
                       Mean reward: 835.25
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.6602
     Episode_Reward/lifting_object: 171.6158
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1330
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.15s
                      Time elapsed: 01:12:55
                               ETA: 05:03:35

################################################################################
                    [1m Learning iteration 1937/10000 [0m                     

                       Computation: 46753 steps/s (collection: 2.007s, learning 0.096s)
             Mean action noise std: 4.47
          Mean value_function loss: 129.1946
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.7253
                       Mean reward: 835.17
               Mean episode length: 225.70
    Episode_Reward/reaching_object: 1.6742
     Episode_Reward/lifting_object: 173.2665
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1343
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.10s
                      Time elapsed: 01:12:57
                               ETA: 05:03:32

################################################################################
                    [1m Learning iteration 1938/10000 [0m                     

                       Computation: 46786 steps/s (collection: 2.001s, learning 0.100s)
             Mean action noise std: 4.47
          Mean value_function loss: 160.3585
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.7372
                       Mean reward: 864.47
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.6781
     Episode_Reward/lifting_object: 173.6006
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1343
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.10s
                      Time elapsed: 01:12:59
                               ETA: 05:03:29

################################################################################
                    [1m Learning iteration 1939/10000 [0m                     

                       Computation: 46177 steps/s (collection: 2.037s, learning 0.092s)
             Mean action noise std: 4.47
          Mean value_function loss: 138.0408
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.7517
                       Mean reward: 861.43
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 1.6284
     Episode_Reward/lifting_object: 167.4937
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.13s
                      Time elapsed: 01:13:01
                               ETA: 05:03:26

################################################################################
                    [1m Learning iteration 1940/10000 [0m                     

                       Computation: 45685 steps/s (collection: 2.043s, learning 0.109s)
             Mean action noise std: 4.47
          Mean value_function loss: 131.0106
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.7684
                       Mean reward: 844.94
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.6682
     Episode_Reward/lifting_object: 172.6784
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1342
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.15s
                      Time elapsed: 01:13:03
                               ETA: 05:03:24

################################################################################
                    [1m Learning iteration 1941/10000 [0m                     

                       Computation: 46407 steps/s (collection: 2.014s, learning 0.104s)
             Mean action noise std: 4.48
          Mean value_function loss: 121.8487
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.7838
                       Mean reward: 848.79
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.6917
     Episode_Reward/lifting_object: 175.5543
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1355
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.12s
                      Time elapsed: 01:13:06
                               ETA: 05:03:21

################################################################################
                    [1m Learning iteration 1942/10000 [0m                     

                       Computation: 46573 steps/s (collection: 2.001s, learning 0.109s)
             Mean action noise std: 4.48
          Mean value_function loss: 105.5920
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.7970
                       Mean reward: 901.10
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 1.6607
     Episode_Reward/lifting_object: 171.6729
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.11s
                      Time elapsed: 01:13:08
                               ETA: 05:03:18

################################################################################
                    [1m Learning iteration 1943/10000 [0m                     

                       Computation: 46739 steps/s (collection: 1.988s, learning 0.116s)
             Mean action noise std: 4.48
          Mean value_function loss: 138.3154
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 88.8088
                       Mean reward: 890.09
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.6461
     Episode_Reward/lifting_object: 169.8229
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1330
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.10s
                      Time elapsed: 01:13:10
                               ETA: 05:03:15

################################################################################
                    [1m Learning iteration 1944/10000 [0m                     

                       Computation: 46214 steps/s (collection: 2.003s, learning 0.125s)
             Mean action noise std: 4.48
          Mean value_function loss: 152.2399
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.8167
                       Mean reward: 837.34
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 1.6779
     Episode_Reward/lifting_object: 173.3453
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.13s
                      Time elapsed: 01:13:12
                               ETA: 05:03:12

################################################################################
                    [1m Learning iteration 1945/10000 [0m                     

                       Computation: 46318 steps/s (collection: 2.011s, learning 0.112s)
             Mean action noise std: 4.48
          Mean value_function loss: 190.4209
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.8354
                       Mean reward: 892.65
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 1.6451
     Episode_Reward/lifting_object: 169.8350
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1328
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.12s
                      Time elapsed: 01:13:14
                               ETA: 05:03:09

################################################################################
                    [1m Learning iteration 1946/10000 [0m                     

                       Computation: 46138 steps/s (collection: 2.006s, learning 0.125s)
             Mean action noise std: 4.49
          Mean value_function loss: 125.1086
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.8544
                       Mean reward: 862.16
               Mean episode length: 231.43
    Episode_Reward/reaching_object: 1.6300
     Episode_Reward/lifting_object: 168.2186
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1321
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.13s
                      Time elapsed: 01:13:16
                               ETA: 05:03:07

################################################################################
                    [1m Learning iteration 1947/10000 [0m                     

                       Computation: 46798 steps/s (collection: 2.003s, learning 0.097s)
             Mean action noise std: 4.49
          Mean value_function loss: 133.2359
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 88.8665
                       Mean reward: 861.05
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.6606
     Episode_Reward/lifting_object: 171.8474
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.10s
                      Time elapsed: 01:13:18
                               ETA: 05:03:04

################################################################################
                    [1m Learning iteration 1948/10000 [0m                     

                       Computation: 46394 steps/s (collection: 2.008s, learning 0.111s)
             Mean action noise std: 4.49
          Mean value_function loss: 106.1940
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 88.8766
                       Mean reward: 874.48
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.6450
     Episode_Reward/lifting_object: 169.0740
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1335
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.12s
                      Time elapsed: 01:13:20
                               ETA: 05:03:01

################################################################################
                    [1m Learning iteration 1949/10000 [0m                     

                       Computation: 45420 steps/s (collection: 2.043s, learning 0.121s)
             Mean action noise std: 4.49
          Mean value_function loss: 135.5411
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.8929
                       Mean reward: 890.24
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.6528
     Episode_Reward/lifting_object: 170.5492
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1344
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.16s
                      Time elapsed: 01:13:22
                               ETA: 05:02:58

################################################################################
                    [1m Learning iteration 1950/10000 [0m                     

                       Computation: 46274 steps/s (collection: 2.013s, learning 0.111s)
             Mean action noise std: 4.49
          Mean value_function loss: 152.9333
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 88.9082
                       Mean reward: 823.64
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 1.6491
     Episode_Reward/lifting_object: 170.3322
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1338
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.12s
                      Time elapsed: 01:13:25
                               ETA: 05:02:55

################################################################################
                    [1m Learning iteration 1951/10000 [0m                     

                       Computation: 45482 steps/s (collection: 2.012s, learning 0.150s)
             Mean action noise std: 4.50
          Mean value_function loss: 146.2412
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.9196
                       Mean reward: 871.55
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.6463
     Episode_Reward/lifting_object: 169.9301
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1332
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.16s
                      Time elapsed: 01:13:27
                               ETA: 05:02:53

################################################################################
                    [1m Learning iteration 1952/10000 [0m                     

                       Computation: 44452 steps/s (collection: 2.103s, learning 0.108s)
             Mean action noise std: 4.50
          Mean value_function loss: 139.1960
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 88.9405
                       Mean reward: 840.59
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 1.6689
     Episode_Reward/lifting_object: 172.8632
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.21s
                      Time elapsed: 01:13:29
                               ETA: 05:02:50

################################################################################
                    [1m Learning iteration 1953/10000 [0m                     

                       Computation: 46461 steps/s (collection: 2.003s, learning 0.113s)
             Mean action noise std: 4.50
          Mean value_function loss: 131.3890
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.9546
                       Mean reward: 880.91
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.6699
     Episode_Reward/lifting_object: 172.9870
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.12s
                      Time elapsed: 01:13:31
                               ETA: 05:02:47

################################################################################
                    [1m Learning iteration 1954/10000 [0m                     

                       Computation: 46305 steps/s (collection: 2.029s, learning 0.094s)
             Mean action noise std: 4.50
          Mean value_function loss: 165.7643
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 88.9685
                       Mean reward: 898.29
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 1.6579
     Episode_Reward/lifting_object: 170.8993
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.12s
                      Time elapsed: 01:13:33
                               ETA: 05:02:45

################################################################################
                    [1m Learning iteration 1955/10000 [0m                     

                       Computation: 46858 steps/s (collection: 2.003s, learning 0.095s)
             Mean action noise std: 4.51
          Mean value_function loss: 126.2558
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.9814
                       Mean reward: 880.35
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.6770
     Episode_Reward/lifting_object: 173.6977
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1363
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.10s
                      Time elapsed: 01:13:35
                               ETA: 05:02:42

################################################################################
                    [1m Learning iteration 1956/10000 [0m                     

                       Computation: 46988 steps/s (collection: 1.993s, learning 0.099s)
             Mean action noise std: 4.51
          Mean value_function loss: 146.1960
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.9945
                       Mean reward: 853.33
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.6599
     Episode_Reward/lifting_object: 172.1073
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.09s
                      Time elapsed: 01:13:37
                               ETA: 05:02:39

################################################################################
                    [1m Learning iteration 1957/10000 [0m                     

                       Computation: 47060 steps/s (collection: 1.997s, learning 0.092s)
             Mean action noise std: 4.51
          Mean value_function loss: 125.6881
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 89.0097
                       Mean reward: 876.34
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 1.6769
     Episode_Reward/lifting_object: 173.4006
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1369
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.09s
                      Time elapsed: 01:13:39
                               ETA: 05:02:36

################################################################################
                    [1m Learning iteration 1958/10000 [0m                     

                       Computation: 47079 steps/s (collection: 1.995s, learning 0.094s)
             Mean action noise std: 4.51
          Mean value_function loss: 138.8017
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 89.0204
                       Mean reward: 857.23
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 1.6355
     Episode_Reward/lifting_object: 169.2212
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1342
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.09s
                      Time elapsed: 01:13:42
                               ETA: 05:02:33

################################################################################
                    [1m Learning iteration 1959/10000 [0m                     

                       Computation: 46041 steps/s (collection: 1.990s, learning 0.146s)
             Mean action noise std: 4.52
          Mean value_function loss: 163.8042
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 89.0350
                       Mean reward: 891.48
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 1.6723
     Episode_Reward/lifting_object: 173.4140
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.14s
                      Time elapsed: 01:13:44
                               ETA: 05:02:30

################################################################################
                    [1m Learning iteration 1960/10000 [0m                     

                       Computation: 45641 steps/s (collection: 2.035s, learning 0.119s)
             Mean action noise std: 4.52
          Mean value_function loss: 137.3063
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 89.0479
                       Mean reward: 861.79
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.6504
     Episode_Reward/lifting_object: 171.0396
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1355
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.15s
                      Time elapsed: 01:13:46
                               ETA: 05:02:27

################################################################################
                    [1m Learning iteration 1961/10000 [0m                     

                       Computation: 45607 steps/s (collection: 2.048s, learning 0.107s)
             Mean action noise std: 4.52
          Mean value_function loss: 181.0900
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 89.0565
                       Mean reward: 847.09
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.6291
     Episode_Reward/lifting_object: 168.4729
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.16s
                      Time elapsed: 01:13:48
                               ETA: 05:02:25

################################################################################
                    [1m Learning iteration 1962/10000 [0m                     

                       Computation: 46426 steps/s (collection: 2.022s, learning 0.095s)
             Mean action noise std: 4.52
          Mean value_function loss: 135.8709
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 89.0661
                       Mean reward: 837.99
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.6466
     Episode_Reward/lifting_object: 169.6516
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1356
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.12s
                      Time elapsed: 01:13:50
                               ETA: 05:02:22

################################################################################
                    [1m Learning iteration 1963/10000 [0m                     

                       Computation: 47383 steps/s (collection: 1.985s, learning 0.090s)
             Mean action noise std: 4.52
          Mean value_function loss: 121.5051
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 89.0771
                       Mean reward: 907.38
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 1.6756
     Episode_Reward/lifting_object: 174.0145
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.07s
                      Time elapsed: 01:13:52
                               ETA: 05:02:19

################################################################################
                    [1m Learning iteration 1964/10000 [0m                     

                       Computation: 47464 steps/s (collection: 1.976s, learning 0.095s)
             Mean action noise std: 4.52
          Mean value_function loss: 169.4550
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 89.0825
                       Mean reward: 895.68
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 1.6519
     Episode_Reward/lifting_object: 170.4323
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1360
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.07s
                      Time elapsed: 01:13:54
                               ETA: 05:02:16

################################################################################
                    [1m Learning iteration 1965/10000 [0m                     

                       Computation: 46037 steps/s (collection: 2.021s, learning 0.115s)
             Mean action noise std: 4.52
          Mean value_function loss: 137.6956
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 89.0937
                       Mean reward: 860.18
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.6616
     Episode_Reward/lifting_object: 171.7635
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1365
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.14s
                      Time elapsed: 01:13:56
                               ETA: 05:02:13

################################################################################
                    [1m Learning iteration 1966/10000 [0m                     

                       Computation: 46291 steps/s (collection: 2.009s, learning 0.115s)
             Mean action noise std: 4.53
          Mean value_function loss: 132.3252
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 89.1051
                       Mean reward: 882.05
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.6905
     Episode_Reward/lifting_object: 175.0951
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.12s
                      Time elapsed: 01:13:59
                               ETA: 05:02:10

################################################################################
                    [1m Learning iteration 1967/10000 [0m                     

                       Computation: 46568 steps/s (collection: 1.980s, learning 0.131s)
             Mean action noise std: 4.53
          Mean value_function loss: 140.3530
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 89.1206
                       Mean reward: 877.34
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.6654
     Episode_Reward/lifting_object: 172.0811
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.11s
                      Time elapsed: 01:14:01
                               ETA: 05:02:07

################################################################################
                    [1m Learning iteration 1968/10000 [0m                     

                       Computation: 45505 steps/s (collection: 2.013s, learning 0.147s)
             Mean action noise std: 4.53
          Mean value_function loss: 158.3320
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 89.1366
                       Mean reward: 856.63
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.6479
     Episode_Reward/lifting_object: 170.1456
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1359
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.16s
                      Time elapsed: 01:14:03
                               ETA: 05:02:05

################################################################################
                    [1m Learning iteration 1969/10000 [0m                     

                       Computation: 45099 steps/s (collection: 2.071s, learning 0.109s)
             Mean action noise std: 4.53
          Mean value_function loss: 148.0501
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 89.1513
                       Mean reward: 829.52
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.6085
     Episode_Reward/lifting_object: 166.0901
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1338
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.18s
                      Time elapsed: 01:14:05
                               ETA: 05:02:02

################################################################################
                    [1m Learning iteration 1970/10000 [0m                     

                       Computation: 47594 steps/s (collection: 1.974s, learning 0.091s)
             Mean action noise std: 4.54
          Mean value_function loss: 187.1443
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 89.1668
                       Mean reward: 860.13
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.6329
     Episode_Reward/lifting_object: 168.3370
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1351
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.07s
                      Time elapsed: 01:14:07
                               ETA: 05:01:59

################################################################################
                    [1m Learning iteration 1971/10000 [0m                     

                       Computation: 47202 steps/s (collection: 1.990s, learning 0.093s)
             Mean action noise std: 4.54
          Mean value_function loss: 125.7340
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 89.1835
                       Mean reward: 846.82
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.6643
     Episode_Reward/lifting_object: 171.8044
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1379
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.08s
                      Time elapsed: 01:14:09
                               ETA: 05:01:56

################################################################################
                    [1m Learning iteration 1972/10000 [0m                     

                       Computation: 47361 steps/s (collection: 1.978s, learning 0.098s)
             Mean action noise std: 4.54
          Mean value_function loss: 143.9855
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 89.1966
                       Mean reward: 831.05
               Mean episode length: 226.40
    Episode_Reward/reaching_object: 1.6446
     Episode_Reward/lifting_object: 169.5625
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.08s
                      Time elapsed: 01:14:11
                               ETA: 05:01:53

################################################################################
                    [1m Learning iteration 1973/10000 [0m                     

                       Computation: 46395 steps/s (collection: 2.021s, learning 0.098s)
             Mean action noise std: 4.54
          Mean value_function loss: 119.6167
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 89.2091
                       Mean reward: 890.72
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 1.6542
     Episode_Reward/lifting_object: 171.5748
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.12s
                      Time elapsed: 01:14:13
                               ETA: 05:01:50

################################################################################
                    [1m Learning iteration 1974/10000 [0m                     

                       Computation: 47509 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 4.55
          Mean value_function loss: 135.0643
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 89.2256
                       Mean reward: 860.03
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 1.6549
     Episode_Reward/lifting_object: 170.9848
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1385
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.07s
                      Time elapsed: 01:14:15
                               ETA: 05:01:47

################################################################################
                    [1m Learning iteration 1975/10000 [0m                     

                       Computation: 47339 steps/s (collection: 1.980s, learning 0.097s)
             Mean action noise std: 4.55
          Mean value_function loss: 184.3893
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 89.2350
                       Mean reward: 866.96
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.6569
     Episode_Reward/lifting_object: 170.9810
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1375
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.08s
                      Time elapsed: 01:14:17
                               ETA: 05:01:44

################################################################################
                    [1m Learning iteration 1976/10000 [0m                     

                       Computation: 47610 steps/s (collection: 1.971s, learning 0.094s)
             Mean action noise std: 4.55
          Mean value_function loss: 166.1648
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 89.2378
                       Mean reward: 840.59
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.6239
     Episode_Reward/lifting_object: 167.5355
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1364
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.06s
                      Time elapsed: 01:14:20
                               ETA: 05:01:41

################################################################################
                    [1m Learning iteration 1977/10000 [0m                     

                       Computation: 47659 steps/s (collection: 1.973s, learning 0.090s)
             Mean action noise std: 4.55
          Mean value_function loss: 182.6889
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 89.2493
                       Mean reward: 801.31
               Mean episode length: 217.02
    Episode_Reward/reaching_object: 1.6329
     Episode_Reward/lifting_object: 168.8710
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1365
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.06s
                      Time elapsed: 01:14:22
                               ETA: 05:01:38

################################################################################
                    [1m Learning iteration 1978/10000 [0m                     

                       Computation: 47873 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 4.55
          Mean value_function loss: 201.2528
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 89.2701
                       Mean reward: 846.42
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.5884
     Episode_Reward/lifting_object: 163.8907
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1328
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.05s
                      Time elapsed: 01:14:24
                               ETA: 05:01:35

################################################################################
                    [1m Learning iteration 1979/10000 [0m                     

                       Computation: 46244 steps/s (collection: 1.993s, learning 0.133s)
             Mean action noise std: 4.55
          Mean value_function loss: 184.1494
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 89.2858
                       Mean reward: 873.15
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 1.6152
     Episode_Reward/lifting_object: 166.8320
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1350
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.13s
                      Time elapsed: 01:14:26
                               ETA: 05:01:33

################################################################################
                    [1m Learning iteration 1980/10000 [0m                     

                       Computation: 47248 steps/s (collection: 1.979s, learning 0.102s)
             Mean action noise std: 4.56
          Mean value_function loss: 150.2568
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 89.2969
                       Mean reward: 842.43
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 1.6214
     Episode_Reward/lifting_object: 167.0977
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1371
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.08s
                      Time elapsed: 01:14:28
                               ETA: 05:01:30

################################################################################
                    [1m Learning iteration 1981/10000 [0m                     

                       Computation: 47972 steps/s (collection: 1.933s, learning 0.117s)
             Mean action noise std: 4.56
          Mean value_function loss: 165.9680
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 89.3112
                       Mean reward: 859.32
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.6514
     Episode_Reward/lifting_object: 171.0005
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1384
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.05s
                      Time elapsed: 01:14:30
                               ETA: 05:01:26

################################################################################
                    [1m Learning iteration 1982/10000 [0m                     

                       Computation: 47565 steps/s (collection: 1.932s, learning 0.135s)
             Mean action noise std: 4.56
          Mean value_function loss: 160.9206
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 89.3235
                       Mean reward: 854.18
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.6352
     Episode_Reward/lifting_object: 168.7664
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1371
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.07s
                      Time elapsed: 01:14:32
                               ETA: 05:01:23

################################################################################
                    [1m Learning iteration 1983/10000 [0m                     

                       Computation: 48413 steps/s (collection: 1.907s, learning 0.124s)
             Mean action noise std: 4.56
          Mean value_function loss: 147.7332
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 89.3340
                       Mean reward: 851.58
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 1.6248
     Episode_Reward/lifting_object: 167.8609
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1364
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.03s
                      Time elapsed: 01:14:34
                               ETA: 05:01:20

################################################################################
                    [1m Learning iteration 1984/10000 [0m                     

                       Computation: 48022 steps/s (collection: 1.939s, learning 0.108s)
             Mean action noise std: 4.56
          Mean value_function loss: 149.3116
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 89.3404
                       Mean reward: 834.08
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.6328
     Episode_Reward/lifting_object: 168.9201
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1373
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.05s
                      Time elapsed: 01:14:36
                               ETA: 05:01:17

################################################################################
                    [1m Learning iteration 1985/10000 [0m                     

                       Computation: 46969 steps/s (collection: 1.985s, learning 0.108s)
             Mean action noise std: 4.56
          Mean value_function loss: 153.2783
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 89.3509
                       Mean reward: 860.75
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 1.6419
     Episode_Reward/lifting_object: 169.9392
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1386
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.09s
                      Time elapsed: 01:14:38
                               ETA: 05:01:14

################################################################################
                    [1m Learning iteration 1986/10000 [0m                     

                       Computation: 47883 steps/s (collection: 1.948s, learning 0.105s)
             Mean action noise std: 4.57
          Mean value_function loss: 165.4875
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 89.3640
                       Mean reward: 835.63
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.6476
     Episode_Reward/lifting_object: 170.1329
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1391
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.05s
                      Time elapsed: 01:14:40
                               ETA: 05:01:11

################################################################################
                    [1m Learning iteration 1987/10000 [0m                     

                       Computation: 46117 steps/s (collection: 1.981s, learning 0.151s)
             Mean action noise std: 4.57
          Mean value_function loss: 153.2627
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 89.3875
                       Mean reward: 857.82
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.6380
     Episode_Reward/lifting_object: 169.1780
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1379
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.13s
                      Time elapsed: 01:14:42
                               ETA: 05:01:08

################################################################################
                    [1m Learning iteration 1988/10000 [0m                     

                       Computation: 47720 steps/s (collection: 1.967s, learning 0.093s)
             Mean action noise std: 4.57
          Mean value_function loss: 138.9163
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 89.4026
                       Mean reward: 883.11
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.6343
     Episode_Reward/lifting_object: 168.4005
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1379
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.06s
                      Time elapsed: 01:14:44
                               ETA: 05:01:05

################################################################################
                    [1m Learning iteration 1989/10000 [0m                     

                       Computation: 48280 steps/s (collection: 1.925s, learning 0.111s)
             Mean action noise std: 4.57
          Mean value_function loss: 163.4223
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 89.4137
                       Mean reward: 816.46
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.6186
     Episode_Reward/lifting_object: 166.4860
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1377
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.04s
                      Time elapsed: 01:14:46
                               ETA: 05:01:02

################################################################################
                    [1m Learning iteration 1990/10000 [0m                     

                       Computation: 48359 steps/s (collection: 1.935s, learning 0.098s)
             Mean action noise std: 4.58
          Mean value_function loss: 108.8435
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 89.4308
                       Mean reward: 856.43
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.6816
     Episode_Reward/lifting_object: 174.1382
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1422
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.03s
                      Time elapsed: 01:14:48
                               ETA: 05:00:59

################################################################################
                    [1m Learning iteration 1991/10000 [0m                     

                       Computation: 48843 steps/s (collection: 1.917s, learning 0.096s)
             Mean action noise std: 4.58
          Mean value_function loss: 167.2305
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 89.4466
                       Mean reward: 781.07
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 1.6138
     Episode_Reward/lifting_object: 166.3502
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.01s
                      Time elapsed: 01:14:50
                               ETA: 05:00:56

################################################################################
                    [1m Learning iteration 1992/10000 [0m                     

                       Computation: 46225 steps/s (collection: 1.989s, learning 0.138s)
             Mean action noise std: 4.58
          Mean value_function loss: 133.2102
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 89.4544
                       Mean reward: 859.51
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.6541
     Episode_Reward/lifting_object: 170.9056
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1397
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.13s
                      Time elapsed: 01:14:53
                               ETA: 05:00:53

################################################################################
                    [1m Learning iteration 1993/10000 [0m                     

                       Computation: 47908 steps/s (collection: 1.943s, learning 0.109s)
             Mean action noise std: 4.58
          Mean value_function loss: 148.0838
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 89.4629
                       Mean reward: 881.73
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.6556
     Episode_Reward/lifting_object: 171.3569
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1402
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.05s
                      Time elapsed: 01:14:55
                               ETA: 05:00:50

################################################################################
                    [1m Learning iteration 1994/10000 [0m                     

                       Computation: 48397 steps/s (collection: 1.938s, learning 0.093s)
             Mean action noise std: 4.58
          Mean value_function loss: 128.1004
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 89.4695
                       Mean reward: 875.68
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 1.6639
     Episode_Reward/lifting_object: 172.4599
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1416
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.03s
                      Time elapsed: 01:14:57
                               ETA: 05:00:47

################################################################################
                    [1m Learning iteration 1995/10000 [0m                     

                       Computation: 48508 steps/s (collection: 1.927s, learning 0.100s)
             Mean action noise std: 4.58
          Mean value_function loss: 144.2234
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 89.4783
                       Mean reward: 868.12
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 1.6553
     Episode_Reward/lifting_object: 171.0285
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1398
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.03s
                      Time elapsed: 01:14:59
                               ETA: 05:00:44

################################################################################
                    [1m Learning iteration 1996/10000 [0m                     

                       Computation: 48670 steps/s (collection: 1.925s, learning 0.095s)
             Mean action noise std: 4.59
          Mean value_function loss: 125.0327
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 89.4915
                       Mean reward: 871.22
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.6601
     Episode_Reward/lifting_object: 171.4877
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1410
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.02s
                      Time elapsed: 01:15:01
                               ETA: 05:00:41

################################################################################
                    [1m Learning iteration 1997/10000 [0m                     

                       Computation: 47503 steps/s (collection: 1.948s, learning 0.122s)
             Mean action noise std: 4.59
          Mean value_function loss: 105.7246
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 89.5018
                       Mean reward: 873.68
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.6668
     Episode_Reward/lifting_object: 172.9635
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1411
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.07s
                      Time elapsed: 01:15:03
                               ETA: 05:00:38

################################################################################
                    [1m Learning iteration 1998/10000 [0m                     

                       Computation: 47612 steps/s (collection: 1.941s, learning 0.124s)
             Mean action noise std: 4.59
          Mean value_function loss: 97.3476
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 89.5126
                       Mean reward: 874.30
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.7024
     Episode_Reward/lifting_object: 176.5792
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1438
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.06s
                      Time elapsed: 01:15:05
                               ETA: 05:00:35

################################################################################
                    [1m Learning iteration 1999/10000 [0m                     

                       Computation: 46456 steps/s (collection: 1.973s, learning 0.143s)
             Mean action noise std: 4.59
          Mean value_function loss: 119.7258
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 89.5228
                       Mean reward: 900.30
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.7072
     Episode_Reward/lifting_object: 177.0018
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.12s
                      Time elapsed: 01:15:07
                               ETA: 05:00:32


