################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 17867 steps/s (collection: 5.267s, learning 0.235s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 11.3622
                       Mean reward: 0.00
               Mean episode length: 21.21
    Episode_Reward/reaching_object: 0.0002
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.50s
                      Time elapsed: 00:00:05
                               ETA: 03:03:23

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 31449 steps/s (collection: 2.973s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 11.3978
                       Mean reward: 0.01
               Mean episode length: 45.83
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.13s
                      Time elapsed: 00:00:08
                               ETA: 02:23:43

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 30175 steps/s (collection: 3.094s, learning 0.164s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 11.4186
                       Mean reward: 0.01
               Mean episode length: 69.78
    Episode_Reward/reaching_object: 0.0019
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.26s
                      Time elapsed: 00:00:11
                               ETA: 02:11:55

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 29804 steps/s (collection: 3.150s, learning 0.148s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 11.4168
                       Mean reward: 0.01
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.0029
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 3.30s
                      Time elapsed: 00:00:15
                               ETA: 02:06:20

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 30711 steps/s (collection: 3.062s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 11.4219
                       Mean reward: 0.02
               Mean episode length: 117.74
    Episode_Reward/reaching_object: 0.0042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.20s
                      Time elapsed: 00:00:18
                               ETA: 02:02:19

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 31350 steps/s (collection: 3.000s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 11.4126
                       Mean reward: 0.02
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.0049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.14s
                      Time elapsed: 00:00:21
                               ETA: 01:59:15

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 30409 steps/s (collection: 3.064s, learning 0.169s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 11.4029
                       Mean reward: 0.03
               Mean episode length: 165.77
    Episode_Reward/reaching_object: 0.0063
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.23s
                      Time elapsed: 00:00:24
                               ETA: 01:57:31

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 28679 steps/s (collection: 3.264s, learning 0.164s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 11.3895
                       Mean reward: 0.04
               Mean episode length: 189.36
    Episode_Reward/reaching_object: 0.0083
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.43s
                      Time elapsed: 00:00:28
                               ETA: 01:57:00

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 25721 steps/s (collection: 3.706s, learning 0.116s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 11.3810
                       Mean reward: 0.04
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 0.0104
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.82s
                      Time elapsed: 00:00:32
                               ETA: 01:58:03

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 123557 steps/s (collection: 0.670s, learning 0.126s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 11.3833
                       Mean reward: 0.08
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.0147
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.80s
                      Time elapsed: 00:00:32
                               ETA: 01:48:50

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 123689 steps/s (collection: 0.706s, learning 0.088s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 11.3928
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0216
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.79s
                      Time elapsed: 00:00:33
                               ETA: 01:41:17

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 115225 steps/s (collection: 0.746s, learning 0.108s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0670
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 11.4053
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0296
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.85s
                      Time elapsed: 00:00:34
                               ETA: 01:35:09

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 125974 steps/s (collection: 0.693s, learning 0.088s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0737
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.4220
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0379
     Episode_Reward/lifting_object: 0.0214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.78s
                      Time elapsed: 00:00:35
                               ETA: 01:29:46

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 119123 steps/s (collection: 0.740s, learning 0.085s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.1132
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.4364
                       Mean reward: 0.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0472
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.83s
                      Time elapsed: 00:00:36
                               ETA: 01:25:16

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 119708 steps/s (collection: 0.726s, learning 0.095s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1375
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.4780
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0610
     Episode_Reward/lifting_object: 0.0586
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.82s
                      Time elapsed: 00:00:36
                               ETA: 01:21:21

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 124155 steps/s (collection: 0.702s, learning 0.090s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.3445
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.4902
                       Mean reward: 0.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0718
     Episode_Reward/lifting_object: 0.0226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.79s
                      Time elapsed: 00:00:37
                               ETA: 01:17:52

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 107405 steps/s (collection: 0.828s, learning 0.087s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.4980
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.5144
                       Mean reward: 0.55
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.0873
     Episode_Reward/lifting_object: 0.0324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.92s
                      Time elapsed: 00:00:38
                               ETA: 01:15:02

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 112875 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.5250
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.5867
                       Mean reward: 0.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1060
     Episode_Reward/lifting_object: 0.0850
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.87s
                      Time elapsed: 00:00:39
                               ETA: 01:12:26

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 109990 steps/s (collection: 0.789s, learning 0.105s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2125
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.6440
                       Mean reward: 0.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1217
     Episode_Reward/lifting_object: 0.1237
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.89s
                      Time elapsed: 00:00:40
                               ETA: 01:10:08

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 114167 steps/s (collection: 0.773s, learning 0.089s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.4738
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.6946
                       Mean reward: 1.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1382
     Episode_Reward/lifting_object: 0.1028
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.86s
                      Time elapsed: 00:00:41
                               ETA: 01:08:01

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 110185 steps/s (collection: 0.803s, learning 0.089s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.6049
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.7664
                       Mean reward: 1.21
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.1476
     Episode_Reward/lifting_object: 0.1649
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.89s
                      Time elapsed: 00:00:42
                               ETA: 01:06:09

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 114381 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.5178
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.8371
                       Mean reward: 2.89
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.1554
     Episode_Reward/lifting_object: 0.2659
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.86s
                      Time elapsed: 00:00:42
                               ETA: 01:04:24

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 112949 steps/s (collection: 0.743s, learning 0.127s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.6254
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.9073
                       Mean reward: 2.11
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.1563
     Episode_Reward/lifting_object: 0.1617
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.87s
                      Time elapsed: 00:00:43
                               ETA: 01:02:49

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 111724 steps/s (collection: 0.751s, learning 0.129s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.8482
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.0058
                       Mean reward: 1.58
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.1662
     Episode_Reward/lifting_object: 0.2416
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.88s
                      Time elapsed: 00:00:44
                               ETA: 01:01:22

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 115845 steps/s (collection: 0.758s, learning 0.091s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.6404
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.1032
                       Mean reward: 1.96
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.1652
     Episode_Reward/lifting_object: 0.3488
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.85s
                      Time elapsed: 00:00:45
                               ETA: 01:00:00

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 116283 steps/s (collection: 0.743s, learning 0.103s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.5928
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.1677
                       Mean reward: 2.11
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.1636
     Episode_Reward/lifting_object: 0.2581
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.85s
                      Time elapsed: 00:00:46
                               ETA: 00:58:44

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 107477 steps/s (collection: 0.825s, learning 0.090s)
             Mean action noise std: 1.12
          Mean value_function loss: 3.6097
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.2407
                       Mean reward: 2.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1592
     Episode_Reward/lifting_object: 0.2072
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.91s
                      Time elapsed: 00:00:47
                               ETA: 00:57:39

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 114304 steps/s (collection: 0.757s, learning 0.103s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.5648
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.2567
                       Mean reward: 1.52
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.1571
     Episode_Reward/lifting_object: 0.1931
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.86s
                      Time elapsed: 00:00:48
                               ETA: 00:56:34

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 117690 steps/s (collection: 0.741s, learning 0.095s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.5892
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.3036
                       Mean reward: 3.11
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.1554
     Episode_Reward/lifting_object: 0.3775
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.84s
                      Time elapsed: 00:00:49
                               ETA: 00:55:32

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 111658 steps/s (collection: 0.762s, learning 0.118s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.7763
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.3796
                       Mean reward: 3.51
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.1577
     Episode_Reward/lifting_object: 0.4431
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.88s
                      Time elapsed: 00:00:49
                               ETA: 00:54:37

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 114220 steps/s (collection: 0.744s, learning 0.117s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.8143
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 12.4496
                       Mean reward: 2.88
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.1472
     Episode_Reward/lifting_object: 0.3755
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.86s
                      Time elapsed: 00:00:50
                               ETA: 00:53:45

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 106350 steps/s (collection: 0.797s, learning 0.128s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.5965
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.4954
                       Mean reward: 3.37
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.1466
     Episode_Reward/lifting_object: 0.4673
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.92s
                      Time elapsed: 00:00:51
                               ETA: 00:52:59

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 108670 steps/s (collection: 0.820s, learning 0.085s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.6063
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 12.5360
                       Mean reward: 2.54
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.1479
     Episode_Reward/lifting_object: 0.3329
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.90s
                      Time elapsed: 00:00:52
                               ETA: 00:52:15

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 112561 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 1.17
          Mean value_function loss: 1.0440
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.5814
                       Mean reward: -0.74
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.1471
     Episode_Reward/lifting_object: 0.2090
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.87s
                      Time elapsed: 00:00:53
                               ETA: 00:51:32

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 115780 steps/s (collection: 0.760s, learning 0.089s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.8427
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 12.6794
                       Mean reward: 2.67
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.1485
     Episode_Reward/lifting_object: 0.5347
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.85s
                      Time elapsed: 00:00:54
                               ETA: 00:50:50

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 110229 steps/s (collection: 0.783s, learning 0.109s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.8034
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 12.7661
                       Mean reward: 1.95
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.1473
     Episode_Reward/lifting_object: 0.3321
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.89s
                      Time elapsed: 00:00:55
                               ETA: 00:50:12

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 113711 steps/s (collection: 0.765s, learning 0.099s)
             Mean action noise std: 1.21
          Mean value_function loss: 1.1385
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 12.8176
                       Mean reward: 3.35
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.1544
     Episode_Reward/lifting_object: 0.4093
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.86s
                      Time elapsed: 00:00:56
                               ETA: 00:49:35

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 109380 steps/s (collection: 0.791s, learning 0.108s)
             Mean action noise std: 1.22
          Mean value_function loss: 1.8787
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.8957
                       Mean reward: 2.96
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.1537
     Episode_Reward/lifting_object: 0.4132
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.90s
                      Time elapsed: 00:00:56
                               ETA: 00:49:02

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 92113 steps/s (collection: 0.923s, learning 0.144s)
             Mean action noise std: 1.24
          Mean value_function loss: 1.0156
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.0152
                       Mean reward: 4.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1558
     Episode_Reward/lifting_object: 0.5341
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.07s
                      Time elapsed: 00:00:58
                               ETA: 00:48:39

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 104907 steps/s (collection: 0.811s, learning 0.126s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.6187
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.0833
                       Mean reward: 4.16
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.1537
     Episode_Reward/lifting_object: 0.5707
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.94s
                      Time elapsed: 00:00:58
                               ETA: 00:48:10

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 107748 steps/s (collection: 0.809s, learning 0.104s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.7830
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.1568
                       Mean reward: 4.81
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 0.1537
     Episode_Reward/lifting_object: 0.6632
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.91s
                      Time elapsed: 00:00:59
                               ETA: 00:47:42

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 98660 steps/s (collection: 0.862s, learning 0.135s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.9470
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.2480
                       Mean reward: 3.02
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.1562
     Episode_Reward/lifting_object: 0.4925
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.00s
                      Time elapsed: 00:01:00
                               ETA: 00:47:19

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 107689 steps/s (collection: 0.793s, learning 0.120s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.5211
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.3177
                       Mean reward: 4.06
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.1548
     Episode_Reward/lifting_object: 0.5252
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.91s
                      Time elapsed: 00:01:01
                               ETA: 00:46:53

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 102640 steps/s (collection: 0.841s, learning 0.117s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.1013
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.3638
                       Mean reward: 2.38
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.1498
     Episode_Reward/lifting_object: 0.4730
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.96s
                      Time elapsed: 00:01:02
                               ETA: 00:46:30

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 99759 steps/s (collection: 0.859s, learning 0.126s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.9173
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 13.4180
                       Mean reward: 3.56
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.1476
     Episode_Reward/lifting_object: 0.3825
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.99s
                      Time elapsed: 00:01:03
                               ETA: 00:46:10

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 106461 steps/s (collection: 0.828s, learning 0.095s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.8014
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 13.4841
                       Mean reward: 3.24
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.1489
     Episode_Reward/lifting_object: 0.4798
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.92s
                      Time elapsed: 00:01:04
                               ETA: 00:45:47

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 103065 steps/s (collection: 0.854s, learning 0.100s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.7809
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 13.5390
                       Mean reward: 2.59
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.1504
     Episode_Reward/lifting_object: 0.4361
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.95s
                      Time elapsed: 00:01:05
                               ETA: 00:45:27

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 99100 steps/s (collection: 0.883s, learning 0.109s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.8909
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.5784
                       Mean reward: 2.89
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.1493
     Episode_Reward/lifting_object: 0.4069
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.99s
                      Time elapsed: 00:01:06
                               ETA: 00:45:09

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 90970 steps/s (collection: 0.952s, learning 0.129s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.9654
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.6127
                       Mean reward: 3.77
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.1496
     Episode_Reward/lifting_object: 0.4644
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.08s
                      Time elapsed: 00:01:07
                               ETA: 00:44:56

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 100049 steps/s (collection: 0.874s, learning 0.109s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.2600
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.6442
                       Mean reward: 2.56
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.1588
     Episode_Reward/lifting_object: 0.5945
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.98s
                      Time elapsed: 00:01:08
                               ETA: 00:44:39

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 99944 steps/s (collection: 0.870s, learning 0.114s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.2295
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 13.7012
                       Mean reward: 4.35
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.1589
     Episode_Reward/lifting_object: 0.5306
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.98s
                      Time elapsed: 00:01:09
                               ETA: 00:44:22

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 98063 steps/s (collection: 0.837s, learning 0.165s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.3988
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.7735
                       Mean reward: 2.73
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.1630
     Episode_Reward/lifting_object: 0.5116
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.00s
                      Time elapsed: 00:01:10
                               ETA: 00:44:07

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 96813 steps/s (collection: 0.879s, learning 0.137s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.2592
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.8162
                       Mean reward: 2.66
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.1585
     Episode_Reward/lifting_object: 0.4783
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.02s
                      Time elapsed: 00:01:11
                               ETA: 00:43:53

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 112565 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.4157
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.8574
                       Mean reward: 4.64
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.1660
     Episode_Reward/lifting_object: 0.6818
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.87s
                      Time elapsed: 00:01:12
                               ETA: 00:43:35

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 108316 steps/s (collection: 0.800s, learning 0.108s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.6971
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.9060
                       Mean reward: 4.38
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.1635
     Episode_Reward/lifting_object: 0.6613
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.91s
                      Time elapsed: 00:01:13
                               ETA: 00:43:18

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 117180 steps/s (collection: 0.750s, learning 0.089s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.7894
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.9747
                       Mean reward: 4.00
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.1673
     Episode_Reward/lifting_object: 0.6972
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.84s
                      Time elapsed: 00:01:14
                               ETA: 00:42:59

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 111832 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.3833
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.0450
                       Mean reward: 4.90
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.1746
     Episode_Reward/lifting_object: 0.7642
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.88s
                      Time elapsed: 00:01:15
                               ETA: 00:42:43

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 108765 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.9714
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.0924
                       Mean reward: 4.16
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.1806
     Episode_Reward/lifting_object: 0.7831
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.90s
                      Time elapsed: 00:01:16
                               ETA: 00:42:28

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 107512 steps/s (collection: 0.794s, learning 0.121s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.1718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.1351
                       Mean reward: 4.57
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 0.1797
     Episode_Reward/lifting_object: 0.8069
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.91s
                      Time elapsed: 00:01:16
                               ETA: 00:42:13

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 110284 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.2825
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.1721
                       Mean reward: 4.90
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 0.1868
     Episode_Reward/lifting_object: 0.8265
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.89s
                      Time elapsed: 00:01:17
                               ETA: 00:41:59

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 98010 steps/s (collection: 0.852s, learning 0.151s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.5792
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.1873
                       Mean reward: 5.80
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.1876
     Episode_Reward/lifting_object: 0.9999
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.00s
                      Time elapsed: 00:01:18
                               ETA: 00:41:48

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 101668 steps/s (collection: 0.823s, learning 0.144s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.9063
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.2001
                       Mean reward: 6.22
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.1993
     Episode_Reward/lifting_object: 1.1240
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.97s
                      Time elapsed: 00:01:19
                               ETA: 00:41:36

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 101668 steps/s (collection: 0.840s, learning 0.127s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.5809
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.2387
                       Mean reward: 7.09
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.2037
     Episode_Reward/lifting_object: 1.2023
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.97s
                      Time elapsed: 00:01:20
                               ETA: 00:41:25

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 98250 steps/s (collection: 0.804s, learning 0.196s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.4665
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.2697
                       Mean reward: 6.96
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 0.2026
     Episode_Reward/lifting_object: 1.0934
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.00s
                      Time elapsed: 00:01:21
                               ETA: 00:41:15

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 109862 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.2758
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.2889
                       Mean reward: 6.85
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.2113
     Episode_Reward/lifting_object: 1.2366
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.89s
                      Time elapsed: 00:01:22
                               ETA: 00:41:03

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 100783 steps/s (collection: 0.812s, learning 0.163s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.1043
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.3142
                       Mean reward: 8.12
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.2091
     Episode_Reward/lifting_object: 1.2883
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.98s
                      Time elapsed: 00:01:23
                               ETA: 00:40:53

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 103543 steps/s (collection: 0.857s, learning 0.093s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.8574
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.3405
                       Mean reward: 8.54
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 0.2118
     Episode_Reward/lifting_object: 1.2241
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.95s
                      Time elapsed: 00:01:24
                               ETA: 00:40:42

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 113532 steps/s (collection: 0.780s, learning 0.086s)
             Mean action noise std: 1.47
          Mean value_function loss: 5.1004
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3711
                       Mean reward: 7.48
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.2182
     Episode_Reward/lifting_object: 1.1793
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.87s
                      Time elapsed: 00:01:25
                               ETA: 00:40:30

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 108997 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 1.48
          Mean value_function loss: 4.1556
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.4322
                       Mean reward: 8.14
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.2164
     Episode_Reward/lifting_object: 1.5502
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.90s
                      Time elapsed: 00:01:26
                               ETA: 00:40:19

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 112377 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 1.48
          Mean value_function loss: 4.1583
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.4717
                       Mean reward: 10.91
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.2105
     Episode_Reward/lifting_object: 1.8319
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.87s
                      Time elapsed: 00:01:27
                               ETA: 00:40:07

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 106574 steps/s (collection: 0.814s, learning 0.109s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.5689
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.4800
                       Mean reward: 9.04
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.2146
     Episode_Reward/lifting_object: 1.5440
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.92s
                      Time elapsed: 00:01:28
                               ETA: 00:39:57

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 111270 steps/s (collection: 0.791s, learning 0.092s)
             Mean action noise std: 1.49
          Mean value_function loss: 5.2653
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.4881
                       Mean reward: 10.55
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.2151
     Episode_Reward/lifting_object: 2.0571
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.88s
                      Time elapsed: 00:01:29
                               ETA: 00:39:46

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 110219 steps/s (collection: 0.799s, learning 0.093s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.4938
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.5142
                       Mean reward: 7.32
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 0.2165
     Episode_Reward/lifting_object: 1.8990
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.89s
                      Time elapsed: 00:01:29
                               ETA: 00:39:36

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 105881 steps/s (collection: 0.787s, learning 0.141s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.3120
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.5418
                       Mean reward: 10.08
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.2146
     Episode_Reward/lifting_object: 1.8916
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.93s
                      Time elapsed: 00:01:30
                               ETA: 00:39:26

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 107147 steps/s (collection: 0.819s, learning 0.098s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.0766
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 14.5561
                       Mean reward: 9.76
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.2167
     Episode_Reward/lifting_object: 1.8253
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.92s
                      Time elapsed: 00:01:31
                               ETA: 00:39:17

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 111810 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.4508
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.5613
                       Mean reward: 11.37
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.2211
     Episode_Reward/lifting_object: 2.2680
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.88s
                      Time elapsed: 00:01:32
                               ETA: 00:39:07

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 108448 steps/s (collection: 0.795s, learning 0.112s)
             Mean action noise std: 1.51
          Mean value_function loss: 9.7747
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.5822
                       Mean reward: 5.94
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: 1.6598
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.91s
                      Time elapsed: 00:01:33
                               ETA: 00:38:58

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 109556 steps/s (collection: 0.799s, learning 0.098s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.4167
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.6325
                       Mean reward: 10.30
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 0.2158
     Episode_Reward/lifting_object: 1.9416
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.90s
                      Time elapsed: 00:01:34
                               ETA: 00:38:49

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 106973 steps/s (collection: 0.827s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 4.5182
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.6569
                       Mean reward: 10.14
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.2173
     Episode_Reward/lifting_object: 1.8910
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.92s
                      Time elapsed: 00:01:35
                               ETA: 00:38:41

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 113066 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.5106
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.6683
                       Mean reward: 9.66
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.2153
     Episode_Reward/lifting_object: 2.0275
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.87s
                      Time elapsed: 00:01:36
                               ETA: 00:38:32

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 103852 steps/s (collection: 0.856s, learning 0.091s)
             Mean action noise std: 1.53
          Mean value_function loss: 4.2080
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.6994
                       Mean reward: 8.10
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.2203
     Episode_Reward/lifting_object: 1.8757
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.95s
                      Time elapsed: 00:01:37
                               ETA: 00:38:24

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 108239 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 1.53
          Mean value_function loss: 7.9084
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7207
                       Mean reward: 11.27
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.2214
     Episode_Reward/lifting_object: 1.8574
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.91s
                      Time elapsed: 00:01:38
                               ETA: 00:38:16

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 99844 steps/s (collection: 0.849s, learning 0.135s)
             Mean action noise std: 1.53
          Mean value_function loss: 10.2484
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.7396
                       Mean reward: 12.27
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 0.2195
     Episode_Reward/lifting_object: 2.3502
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.98s
                      Time elapsed: 00:01:39
                               ETA: 00:38:10

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 108150 steps/s (collection: 0.807s, learning 0.102s)
             Mean action noise std: 1.54
          Mean value_function loss: 8.1995
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.7525
                       Mean reward: 11.48
               Mean episode length: 241.45
    Episode_Reward/reaching_object: 0.2154
     Episode_Reward/lifting_object: 2.0251
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.91s
                      Time elapsed: 00:01:40
                               ETA: 00:38:02

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 109622 steps/s (collection: 0.789s, learning 0.108s)
             Mean action noise std: 1.54
          Mean value_function loss: 11.4699
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7614
                       Mean reward: 13.33
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 0.2094
     Episode_Reward/lifting_object: 2.2251
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.90s
                      Time elapsed: 00:01:40
                               ETA: 00:37:55

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 102555 steps/s (collection: 0.817s, learning 0.141s)
             Mean action noise std: 1.54
          Mean value_function loss: 7.6397
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7776
                       Mean reward: 9.67
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.2120
     Episode_Reward/lifting_object: 2.0957
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.96s
                      Time elapsed: 00:01:41
                               ETA: 00:37:48

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 109380 steps/s (collection: 0.792s, learning 0.107s)
             Mean action noise std: 1.54
          Mean value_function loss: 13.4179
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.7873
                       Mean reward: 11.32
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.2126
     Episode_Reward/lifting_object: 2.3615
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.90s
                      Time elapsed: 00:01:42
                               ETA: 00:37:41

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 93265 steps/s (collection: 0.924s, learning 0.131s)
             Mean action noise std: 1.55
          Mean value_function loss: 9.3966
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8037
                       Mean reward: 11.74
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.2105
     Episode_Reward/lifting_object: 2.1518
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.05s
                      Time elapsed: 00:01:43
                               ETA: 00:37:37

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 113971 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 9.6404
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8081
                       Mean reward: 12.79
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.2146
     Episode_Reward/lifting_object: 2.8402
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.86s
                      Time elapsed: 00:01:44
                               ETA: 00:37:29

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 114265 steps/s (collection: 0.776s, learning 0.084s)
             Mean action noise std: 1.55
          Mean value_function loss: 13.4652
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.8201
                       Mean reward: 9.33
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 0.2076
     Episode_Reward/lifting_object: 2.8327
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.86s
                      Time elapsed: 00:01:45
                               ETA: 00:37:21

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 104823 steps/s (collection: 0.845s, learning 0.093s)
             Mean action noise std: 1.56
          Mean value_function loss: 11.8535
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8639
                       Mean reward: 16.12
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 0.2162
     Episode_Reward/lifting_object: 2.9257
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.94s
                      Time elapsed: 00:01:46
                               ETA: 00:37:15

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 111978 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 1.56
          Mean value_function loss: 6.9709
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8921
                       Mean reward: 16.06
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.2128
     Episode_Reward/lifting_object: 2.8056
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.88s
                      Time elapsed: 00:01:47
                               ETA: 00:37:08

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 108430 steps/s (collection: 0.810s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 8.6413
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9093
                       Mean reward: 18.85
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.2083
     Episode_Reward/lifting_object: 2.5747
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.91s
                      Time elapsed: 00:01:48
                               ETA: 00:37:01

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 114352 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 11.8005
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9312
                       Mean reward: 16.77
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.2209
     Episode_Reward/lifting_object: 3.1010
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.86s
                      Time elapsed: 00:01:49
                               ETA: 00:36:54

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 107640 steps/s (collection: 0.807s, learning 0.106s)
             Mean action noise std: 1.58
          Mean value_function loss: 16.0171
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9480
                       Mean reward: 12.54
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.2129
     Episode_Reward/lifting_object: 2.8809
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.91s
                      Time elapsed: 00:01:50
                               ETA: 00:36:48

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 113212 steps/s (collection: 0.781s, learning 0.087s)
             Mean action noise std: 1.58
          Mean value_function loss: 10.2130
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.9600
                       Mean reward: 20.26
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.2159
     Episode_Reward/lifting_object: 3.0818
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.87s
                      Time elapsed: 00:01:50
                               ETA: 00:36:41

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 99773 steps/s (collection: 0.807s, learning 0.179s)
             Mean action noise std: 1.58
          Mean value_function loss: 12.1813
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9664
                       Mean reward: 18.46
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: 3.5096
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.99s
                      Time elapsed: 00:01:51
                               ETA: 00:36:36

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 110965 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 1.58
          Mean value_function loss: 9.3807
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.9739
                       Mean reward: 17.47
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 0.2195
     Episode_Reward/lifting_object: 3.3137
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.89s
                      Time elapsed: 00:01:52
                               ETA: 00:36:30

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 113684 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.8690
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9869
                       Mean reward: 16.05
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.2202
     Episode_Reward/lifting_object: 3.2250
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.86s
                      Time elapsed: 00:01:53
                               ETA: 00:36:23

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 114342 steps/s (collection: 0.772s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 22.6462
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.0058
                       Mean reward: 19.61
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.2277
     Episode_Reward/lifting_object: 3.4283
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.86s
                      Time elapsed: 00:01:54
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 112563 steps/s (collection: 0.787s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.4539
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.0183
                       Mean reward: 17.78
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.2288
     Episode_Reward/lifting_object: 3.4282
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.87s
                      Time elapsed: 00:01:55
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 107845 steps/s (collection: 0.791s, learning 0.120s)
             Mean action noise std: 1.59
          Mean value_function loss: 12.4225
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.0346
                       Mean reward: 19.89
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 0.2234
     Episode_Reward/lifting_object: 3.4723
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.91s
                      Time elapsed: 00:01:56
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 110436 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 1.60
          Mean value_function loss: 18.6220
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.0513
                       Mean reward: 19.38
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.2266
     Episode_Reward/lifting_object: 3.9364
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.89s
                      Time elapsed: 00:01:57
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 111426 steps/s (collection: 0.772s, learning 0.110s)
             Mean action noise std: 1.60
          Mean value_function loss: 16.4716
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.0706
                       Mean reward: 20.50
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.2274
     Episode_Reward/lifting_object: 3.5169
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.88s
                      Time elapsed: 00:01:58
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 104586 steps/s (collection: 0.778s, learning 0.162s)
             Mean action noise std: 1.60
          Mean value_function loss: 12.5434
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.0817
                       Mean reward: 18.25
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 0.2193
     Episode_Reward/lifting_object: 2.8730
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.94s
                      Time elapsed: 00:01:59
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 100191 steps/s (collection: 0.874s, learning 0.107s)
             Mean action noise std: 1.60
          Mean value_function loss: 14.6254
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.0870
                       Mean reward: 21.70
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 0.2272
     Episode_Reward/lifting_object: 3.6546
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.98s
                      Time elapsed: 00:02:00
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 103635 steps/s (collection: 0.792s, learning 0.157s)
             Mean action noise std: 1.61
          Mean value_function loss: 19.7337
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.0956
                       Mean reward: 22.77
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.2271
     Episode_Reward/lifting_object: 3.4771
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.95s
                      Time elapsed: 00:02:00
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 112210 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 25.7518
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.1135
                       Mean reward: 19.01
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 0.2225
     Episode_Reward/lifting_object: 3.7285
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.88s
                      Time elapsed: 00:02:01
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 107661 steps/s (collection: 0.799s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 29.8200
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.1333
                       Mean reward: 21.71
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.2271
     Episode_Reward/lifting_object: 3.7955
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.91s
                      Time elapsed: 00:02:02
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 110565 steps/s (collection: 0.794s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.1396
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.1665
                       Mean reward: 21.21
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 0.2256
     Episode_Reward/lifting_object: 3.2137
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.89s
                      Time elapsed: 00:02:03
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 103215 steps/s (collection: 0.865s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 23.9914
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1972
                       Mean reward: 19.88
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 0.2263
     Episode_Reward/lifting_object: 3.9971
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.95s
                      Time elapsed: 00:02:04
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 96978 steps/s (collection: 0.869s, learning 0.145s)
             Mean action noise std: 1.63
          Mean value_function loss: 21.2995
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.2226
                       Mean reward: 21.42
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.2271
     Episode_Reward/lifting_object: 3.8759
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.01s
                      Time elapsed: 00:02:05
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 103991 steps/s (collection: 0.833s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 23.6577
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2342
                       Mean reward: 18.03
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.2207
     Episode_Reward/lifting_object: 4.1621
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.95s
                      Time elapsed: 00:02:06
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 104170 steps/s (collection: 0.812s, learning 0.132s)
             Mean action noise std: 1.64
          Mean value_function loss: 20.4771
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2555
                       Mean reward: 20.23
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 0.2199
     Episode_Reward/lifting_object: 3.4671
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.94s
                      Time elapsed: 00:02:07
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 108207 steps/s (collection: 0.797s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 24.4350
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.2771
                       Mean reward: 17.98
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 0.2217
     Episode_Reward/lifting_object: 3.1673
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.91s
                      Time elapsed: 00:02:08
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 112417 steps/s (collection: 0.791s, learning 0.083s)
             Mean action noise std: 1.65
          Mean value_function loss: 19.4520
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.2908
                       Mean reward: 19.43
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 0.2181
     Episode_Reward/lifting_object: 4.0984
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.87s
                      Time elapsed: 00:02:09
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 104426 steps/s (collection: 0.827s, learning 0.114s)
             Mean action noise std: 1.65
          Mean value_function loss: 21.6737
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3071
                       Mean reward: 22.41
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 0.2219
     Episode_Reward/lifting_object: 4.0419
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.94s
                      Time elapsed: 00:02:10
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 113017 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 1.65
          Mean value_function loss: 28.4799
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.3226
                       Mean reward: 25.92
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 0.2209
     Episode_Reward/lifting_object: 4.3395
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.87s
                      Time elapsed: 00:02:11
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 106227 steps/s (collection: 0.827s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 24.1614
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.3288
                       Mean reward: 19.89
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.2246
     Episode_Reward/lifting_object: 4.3929
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.93s
                      Time elapsed: 00:02:12
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 113278 steps/s (collection: 0.770s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 16.8052
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3332
                       Mean reward: 22.15
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 0.2186
     Episode_Reward/lifting_object: 4.7255
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.87s
                      Time elapsed: 00:02:12
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 104858 steps/s (collection: 0.784s, learning 0.154s)
             Mean action noise std: 1.66
          Mean value_function loss: 22.3333
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.3466
                       Mean reward: 30.23
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.2254
     Episode_Reward/lifting_object: 4.8825
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.94s
                      Time elapsed: 00:02:13
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 108284 steps/s (collection: 0.802s, learning 0.106s)
             Mean action noise std: 1.66
          Mean value_function loss: 20.2778
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.3649
                       Mean reward: 29.35
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 0.2246
     Episode_Reward/lifting_object: 5.2020
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.91s
                      Time elapsed: 00:02:14
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 114770 steps/s (collection: 0.764s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 29.8196
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.3777
                       Mean reward: 30.33
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 0.2163
     Episode_Reward/lifting_object: 5.0113
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.86s
                      Time elapsed: 00:02:15
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 110005 steps/s (collection: 0.792s, learning 0.102s)
             Mean action noise std: 1.67
          Mean value_function loss: 28.1264
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.3976
                       Mean reward: 28.95
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 0.2241
     Episode_Reward/lifting_object: 4.6392
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.89s
                      Time elapsed: 00:02:16
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 109891 steps/s (collection: 0.805s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.5452
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4143
                       Mean reward: 27.54
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 0.2189
     Episode_Reward/lifting_object: 5.2391
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.89s
                      Time elapsed: 00:02:17
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 109434 steps/s (collection: 0.798s, learning 0.100s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.6669
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.4201
                       Mean reward: 26.90
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 0.2196
     Episode_Reward/lifting_object: 4.7144
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.90s
                      Time elapsed: 00:02:18
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 104846 steps/s (collection: 0.831s, learning 0.107s)
             Mean action noise std: 1.68
          Mean value_function loss: 27.6958
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.4299
                       Mean reward: 24.80
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.2223
     Episode_Reward/lifting_object: 4.6194
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.94s
                      Time elapsed: 00:02:19
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 98458 steps/s (collection: 0.874s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 19.3304
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4445
                       Mean reward: 23.16
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.2170
     Episode_Reward/lifting_object: 4.9746
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.00s
                      Time elapsed: 00:02:20
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 101665 steps/s (collection: 0.858s, learning 0.109s)
             Mean action noise std: 1.68
          Mean value_function loss: 17.2170
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.4530
                       Mean reward: 30.25
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 0.2203
     Episode_Reward/lifting_object: 5.2272
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.97s
                      Time elapsed: 00:02:21
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 110643 steps/s (collection: 0.800s, learning 0.089s)
             Mean action noise std: 1.68
          Mean value_function loss: 19.8467
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.4562
                       Mean reward: 29.64
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 0.2266
     Episode_Reward/lifting_object: 5.8973
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.89s
                      Time elapsed: 00:02:22
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 101060 steps/s (collection: 0.838s, learning 0.135s)
             Mean action noise std: 1.68
          Mean value_function loss: 17.1917
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4603
                       Mean reward: 22.92
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 0.2258
     Episode_Reward/lifting_object: 5.3680
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.97s
                      Time elapsed: 00:02:23
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 107975 steps/s (collection: 0.804s, learning 0.107s)
             Mean action noise std: 1.69
          Mean value_function loss: 18.4561
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4701
                       Mean reward: 28.20
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 0.2261
     Episode_Reward/lifting_object: 4.9795
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.91s
                      Time elapsed: 00:02:23
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 99123 steps/s (collection: 0.826s, learning 0.166s)
             Mean action noise std: 1.69
          Mean value_function loss: 16.6115
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.4736
                       Mean reward: 27.28
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 0.2171
     Episode_Reward/lifting_object: 5.5611
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.99s
                      Time elapsed: 00:02:24
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 104925 steps/s (collection: 0.829s, learning 0.107s)
             Mean action noise std: 1.69
          Mean value_function loss: 37.8038
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.4778
                       Mean reward: 25.48
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.2256
     Episode_Reward/lifting_object: 5.3899
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.94s
                      Time elapsed: 00:02:25
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 110288 steps/s (collection: 0.775s, learning 0.117s)
             Mean action noise std: 1.69
          Mean value_function loss: 33.1450
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4924
                       Mean reward: 24.39
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 0.2190
     Episode_Reward/lifting_object: 5.6276
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.89s
                      Time elapsed: 00:02:26
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 113120 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 1.70
          Mean value_function loss: 32.8505
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.5103
                       Mean reward: 33.20
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.2241
     Episode_Reward/lifting_object: 5.8171
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.87s
                      Time elapsed: 00:02:27
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 110525 steps/s (collection: 0.801s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 21.3210
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5194
                       Mean reward: 25.91
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 0.2270
     Episode_Reward/lifting_object: 5.7910
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.89s
                      Time elapsed: 00:02:28
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 103933 steps/s (collection: 0.822s, learning 0.124s)
             Mean action noise std: 1.70
          Mean value_function loss: 40.3830
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5239
                       Mean reward: 33.83
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 0.2221
     Episode_Reward/lifting_object: 6.5173
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.95s
                      Time elapsed: 00:02:29
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 104144 steps/s (collection: 0.857s, learning 0.087s)
             Mean action noise std: 1.70
          Mean value_function loss: 27.6994
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5286
                       Mean reward: 35.11
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 0.2165
     Episode_Reward/lifting_object: 6.2459
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.94s
                      Time elapsed: 00:02:30
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 108194 steps/s (collection: 0.796s, learning 0.113s)
             Mean action noise std: 1.70
          Mean value_function loss: 21.4572
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5381
                       Mean reward: 29.02
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 0.2176
     Episode_Reward/lifting_object: 6.5132
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.91s
                      Time elapsed: 00:02:31
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 111643 steps/s (collection: 0.786s, learning 0.095s)
             Mean action noise std: 1.70
          Mean value_function loss: 27.4276
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5451
                       Mean reward: 33.65
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 0.2172
     Episode_Reward/lifting_object: 6.2287
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.88s
                      Time elapsed: 00:02:32
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 104268 steps/s (collection: 0.831s, learning 0.112s)
             Mean action noise std: 1.70
          Mean value_function loss: 30.5817
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5489
                       Mean reward: 31.17
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 0.2129
     Episode_Reward/lifting_object: 5.9930
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.94s
                      Time elapsed: 00:02:33
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 106813 steps/s (collection: 0.812s, learning 0.109s)
             Mean action noise std: 1.70
          Mean value_function loss: 45.9857
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5549
                       Mean reward: 28.62
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 0.2180
     Episode_Reward/lifting_object: 6.6297
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.92s
                      Time elapsed: 00:02:34
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 111275 steps/s (collection: 0.778s, learning 0.106s)
             Mean action noise std: 1.71
          Mean value_function loss: 42.9547
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5619
                       Mean reward: 32.97
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.2202
     Episode_Reward/lifting_object: 5.9005
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.88s
                      Time elapsed: 00:02:34
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 104375 steps/s (collection: 0.820s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 30.1284
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.5703
                       Mean reward: 28.42
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 0.2166
     Episode_Reward/lifting_object: 6.6239
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.94s
                      Time elapsed: 00:02:35
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 106544 steps/s (collection: 0.796s, learning 0.127s)
             Mean action noise std: 1.71
          Mean value_function loss: 26.4874
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.5792
                       Mean reward: 32.29
               Mean episode length: 227.87
    Episode_Reward/reaching_object: 0.2101
     Episode_Reward/lifting_object: 6.8138
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.92s
                      Time elapsed: 00:02:36
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 109739 steps/s (collection: 0.771s, learning 0.125s)
             Mean action noise std: 1.71
          Mean value_function loss: 22.3636
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5864
                       Mean reward: 40.89
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 0.2181
     Episode_Reward/lifting_object: 7.1980
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.90s
                      Time elapsed: 00:02:37
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 109246 steps/s (collection: 0.806s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 29.2298
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.5848
                       Mean reward: 31.48
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 0.2226
     Episode_Reward/lifting_object: 6.2025
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.90s
                      Time elapsed: 00:02:38
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 112235 steps/s (collection: 0.780s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 27.8044
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.5882
                       Mean reward: 33.89
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 0.2156
     Episode_Reward/lifting_object: 6.4037
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.88s
                      Time elapsed: 00:02:39
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 111583 steps/s (collection: 0.790s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 28.4509
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5949
                       Mean reward: 30.49
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 0.2161
     Episode_Reward/lifting_object: 7.0737
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.88s
                      Time elapsed: 00:02:40
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 108194 steps/s (collection: 0.816s, learning 0.093s)
             Mean action noise std: 1.71
          Mean value_function loss: 27.5222
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6003
                       Mean reward: 35.84
               Mean episode length: 219.18
    Episode_Reward/reaching_object: 0.2180
     Episode_Reward/lifting_object: 6.9323
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.91s
                      Time elapsed: 00:02:41
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 108524 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 22.4015
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6025
                       Mean reward: 41.29
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.2196
     Episode_Reward/lifting_object: 7.2305
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.91s
                      Time elapsed: 00:02:42
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 110392 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 27.6938
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.6097
                       Mean reward: 26.17
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 0.2136
     Episode_Reward/lifting_object: 5.6507
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.89s
                      Time elapsed: 00:02:43
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 108698 steps/s (collection: 0.781s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 32.9084
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6208
                       Mean reward: 33.27
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 0.2264
     Episode_Reward/lifting_object: 6.7898
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.90s
                      Time elapsed: 00:02:43
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 111322 steps/s (collection: 0.763s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 36.4964
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6275
                       Mean reward: 32.92
               Mean episode length: 227.71
    Episode_Reward/reaching_object: 0.2196
     Episode_Reward/lifting_object: 5.9482
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.88s
                      Time elapsed: 00:02:44
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 109108 steps/s (collection: 0.792s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 34.5773
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6337
                       Mean reward: 30.24
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 0.2215
     Episode_Reward/lifting_object: 6.8430
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.90s
                      Time elapsed: 00:02:45
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 113662 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 29.5923
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.6409
                       Mean reward: 35.95
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 0.2222
     Episode_Reward/lifting_object: 6.6117
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.86s
                      Time elapsed: 00:02:46
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 111843 steps/s (collection: 0.794s, learning 0.085s)
             Mean action noise std: 1.73
          Mean value_function loss: 31.4906
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6478
                       Mean reward: 37.00
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 0.2202
     Episode_Reward/lifting_object: 6.2385
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.88s
                      Time elapsed: 00:02:47
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 110089 steps/s (collection: 0.806s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 36.5069
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6481
                       Mean reward: 49.61
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 0.2190
     Episode_Reward/lifting_object: 8.2529
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.89s
                      Time elapsed: 00:02:48
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 109126 steps/s (collection: 0.802s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 32.6793
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.6451
                       Mean reward: 36.38
               Mean episode length: 208.98
    Episode_Reward/reaching_object: 0.2184
     Episode_Reward/lifting_object: 6.5640
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.90s
                      Time elapsed: 00:02:49
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 111699 steps/s (collection: 0.772s, learning 0.108s)
             Mean action noise std: 1.73
          Mean value_function loss: 43.1122
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6468
                       Mean reward: 43.64
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 0.2254
     Episode_Reward/lifting_object: 7.3016
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.88s
                      Time elapsed: 00:02:50
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 107364 steps/s (collection: 0.818s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.9661
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6501
                       Mean reward: 53.27
               Mean episode length: 212.69
    Episode_Reward/reaching_object: 0.2101
     Episode_Reward/lifting_object: 7.8213
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.92s
                      Time elapsed: 00:02:51
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 114352 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 36.9254
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6524
                       Mean reward: 38.05
               Mean episode length: 223.23
    Episode_Reward/reaching_object: 0.2132
     Episode_Reward/lifting_object: 7.7666
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.86s
                      Time elapsed: 00:02:51
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 108457 steps/s (collection: 0.778s, learning 0.129s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.0790
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6525
                       Mean reward: 29.62
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 0.2175
     Episode_Reward/lifting_object: 7.3230
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.91s
                      Time elapsed: 00:02:52
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 109746 steps/s (collection: 0.774s, learning 0.122s)
             Mean action noise std: 1.73
          Mean value_function loss: 32.1763
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6533
                       Mean reward: 49.29
               Mean episode length: 221.16
    Episode_Reward/reaching_object: 0.2178
     Episode_Reward/lifting_object: 9.2251
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.90s
                      Time elapsed: 00:02:53
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 112623 steps/s (collection: 0.781s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.1436
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.6547
                       Mean reward: 47.15
               Mean episode length: 218.89
    Episode_Reward/reaching_object: 0.2110
     Episode_Reward/lifting_object: 8.9591
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.87s
                      Time elapsed: 00:02:54
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 113325 steps/s (collection: 0.782s, learning 0.086s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.4516
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6567
                       Mean reward: 30.01
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.2122
     Episode_Reward/lifting_object: 8.1714
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.87s
                      Time elapsed: 00:02:55
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 112720 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 36.0198
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6605
                       Mean reward: 41.68
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 0.2133
     Episode_Reward/lifting_object: 7.5532
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.87s
                      Time elapsed: 00:02:56
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 111512 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.3439
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6737
                       Mean reward: 35.09
               Mean episode length: 232.27
    Episode_Reward/reaching_object: 0.2070
     Episode_Reward/lifting_object: 7.4407
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.88s
                      Time elapsed: 00:02:57
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 111638 steps/s (collection: 0.796s, learning 0.085s)
             Mean action noise std: 1.74
          Mean value_function loss: 41.7979
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.6903
                       Mean reward: 46.77
               Mean episode length: 215.88
    Episode_Reward/reaching_object: 0.2092
     Episode_Reward/lifting_object: 8.1201
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.88s
                      Time elapsed: 00:02:58
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 112738 steps/s (collection: 0.788s, learning 0.084s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.5415
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.6923
                       Mean reward: 36.67
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 0.2074
     Episode_Reward/lifting_object: 7.5563
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.87s
                      Time elapsed: 00:02:58
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 108144 steps/s (collection: 0.791s, learning 0.118s)
             Mean action noise std: 1.74
          Mean value_function loss: 38.2747
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6921
                       Mean reward: 41.82
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 0.2117
     Episode_Reward/lifting_object: 7.7055
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.91s
                      Time elapsed: 00:02:59
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 109976 steps/s (collection: 0.769s, learning 0.124s)
             Mean action noise std: 1.74
          Mean value_function loss: 46.5113
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.6939
                       Mean reward: 39.68
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.2150
     Episode_Reward/lifting_object: 8.6770
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.89s
                      Time elapsed: 00:03:00
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 107798 steps/s (collection: 0.792s, learning 0.120s)
             Mean action noise std: 1.74
          Mean value_function loss: 43.0851
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6949
                       Mean reward: 42.71
               Mean episode length: 225.80
    Episode_Reward/reaching_object: 0.2187
     Episode_Reward/lifting_object: 7.6747
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.91s
                      Time elapsed: 00:03:01
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 111405 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 47.5315
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6964
                       Mean reward: 51.31
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.2216
     Episode_Reward/lifting_object: 9.3886
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.88s
                      Time elapsed: 00:03:02
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 110464 steps/s (collection: 0.803s, learning 0.087s)
             Mean action noise std: 1.74
          Mean value_function loss: 51.6670
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6952
                       Mean reward: 45.55
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 0.2169
     Episode_Reward/lifting_object: 8.9619
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.89s
                      Time elapsed: 00:03:03
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 104738 steps/s (collection: 0.823s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 49.8144
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.6959
                       Mean reward: 54.37
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 0.2133
     Episode_Reward/lifting_object: 9.9004
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.94s
                      Time elapsed: 00:03:04
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 103601 steps/s (collection: 0.835s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 54.9597
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7004
                       Mean reward: 51.87
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 0.2119
     Episode_Reward/lifting_object: 9.3588
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.95s
                      Time elapsed: 00:03:05
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 108019 steps/s (collection: 0.823s, learning 0.087s)
             Mean action noise std: 1.74
          Mean value_function loss: 56.4942
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6952
                       Mean reward: 62.98
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 0.2221
     Episode_Reward/lifting_object: 11.5144
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.91s
                      Time elapsed: 00:03:06
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 109032 steps/s (collection: 0.814s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 57.4853
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6921
                       Mean reward: 48.62
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.2196
     Episode_Reward/lifting_object: 9.9890
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.90s
                      Time elapsed: 00:03:07
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 110789 steps/s (collection: 0.802s, learning 0.085s)
             Mean action noise std: 1.74
          Mean value_function loss: 59.5316
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6955
                       Mean reward: 71.82
               Mean episode length: 215.47
    Episode_Reward/reaching_object: 0.2280
     Episode_Reward/lifting_object: 12.2431
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.89s
                      Time elapsed: 00:03:08
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 104714 steps/s (collection: 0.818s, learning 0.120s)
             Mean action noise std: 1.74
          Mean value_function loss: 63.5299
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.6954
                       Mean reward: 61.76
               Mean episode length: 224.26
    Episode_Reward/reaching_object: 0.2245
     Episode_Reward/lifting_object: 10.7419
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.94s
                      Time elapsed: 00:03:09
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 108537 steps/s (collection: 0.801s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 64.4141
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6961
                       Mean reward: 63.04
               Mean episode length: 226.61
    Episode_Reward/reaching_object: 0.2280
     Episode_Reward/lifting_object: 11.6681
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.91s
                      Time elapsed: 00:03:09
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 107217 steps/s (collection: 0.787s, learning 0.130s)
             Mean action noise std: 1.74
          Mean value_function loss: 70.1712
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6937
                       Mean reward: 51.87
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 0.2311
     Episode_Reward/lifting_object: 12.5499
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.92s
                      Time elapsed: 00:03:10
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 106458 steps/s (collection: 0.805s, learning 0.119s)
             Mean action noise std: 1.74
          Mean value_function loss: 79.2388
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.6944
                       Mean reward: 63.87
               Mean episode length: 209.99
    Episode_Reward/reaching_object: 0.2379
     Episode_Reward/lifting_object: 13.7954
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.92s
                      Time elapsed: 00:03:11
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 112969 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 83.7533
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6976
                       Mean reward: 70.60
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 0.2339
     Episode_Reward/lifting_object: 12.8235
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.87s
                      Time elapsed: 00:03:12
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 102202 steps/s (collection: 0.855s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 101.2570
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6983
                       Mean reward: 45.06
               Mean episode length: 218.04
    Episode_Reward/reaching_object: 0.2319
     Episode_Reward/lifting_object: 12.1887
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.96s
                      Time elapsed: 00:03:13
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 113993 steps/s (collection: 0.779s, learning 0.084s)
             Mean action noise std: 1.74
          Mean value_function loss: 96.2616
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 15.7019
                       Mean reward: 65.49
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 0.2378
     Episode_Reward/lifting_object: 11.8794
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.86s
                      Time elapsed: 00:03:14
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 102613 steps/s (collection: 0.841s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 95.6609
               Mean surrogate loss: 0.0223
                 Mean entropy loss: 15.7023
                       Mean reward: 64.70
               Mean episode length: 223.00
    Episode_Reward/reaching_object: 0.2379
     Episode_Reward/lifting_object: 14.0928
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.96s
                      Time elapsed: 00:03:15
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 112633 steps/s (collection: 0.784s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 106.3164
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.7025
                       Mean reward: 64.60
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.2442
     Episode_Reward/lifting_object: 14.8062
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.87s
                      Time elapsed: 00:03:16
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 108064 steps/s (collection: 0.822s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 123.3010
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 15.7032
                       Mean reward: 67.86
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.2426
     Episode_Reward/lifting_object: 16.1621
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.91s
                      Time elapsed: 00:03:17
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 108090 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 132.0693
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 15.7038
                       Mean reward: 97.98
               Mean episode length: 216.98
    Episode_Reward/reaching_object: 0.2515
     Episode_Reward/lifting_object: 17.8480
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.91s
                      Time elapsed: 00:03:18
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 109805 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 1.74
          Mean value_function loss: 135.1197
               Mean surrogate loss: 0.0187
                 Mean entropy loss: 15.7041
                       Mean reward: 125.63
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 22.1820
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.90s
                      Time elapsed: 00:03:18
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 114099 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 126.8221
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.7046
                       Mean reward: 91.11
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.2669
     Episode_Reward/lifting_object: 22.3580
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.86s
                      Time elapsed: 00:03:19
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 108477 steps/s (collection: 0.771s, learning 0.136s)
             Mean action noise std: 1.74
          Mean value_function loss: 148.6261
               Mean surrogate loss: 0.0203
                 Mean entropy loss: 15.7059
                       Mean reward: 154.62
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 0.2801
     Episode_Reward/lifting_object: 27.2837
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.91s
                      Time elapsed: 00:03:20
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 106856 steps/s (collection: 0.787s, learning 0.133s)
             Mean action noise std: 1.74
          Mean value_function loss: 142.7671
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 15.7064
                       Mean reward: 127.65
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 0.2673
     Episode_Reward/lifting_object: 25.0512
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.92s
                      Time elapsed: 00:03:21
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 110546 steps/s (collection: 0.798s, learning 0.091s)
             Mean action noise std: 1.74
          Mean value_function loss: 157.1290
               Mean surrogate loss: 0.0179
                 Mean entropy loss: 15.7066
                       Mean reward: 162.88
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 0.2871
     Episode_Reward/lifting_object: 32.7077
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.89s
                      Time elapsed: 00:03:22
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 110598 steps/s (collection: 0.782s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 142.0121
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 15.7066
                       Mean reward: 172.92
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 0.2907
     Episode_Reward/lifting_object: 32.1048
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.89s
                      Time elapsed: 00:03:23
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 105392 steps/s (collection: 0.840s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 144.7261
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7071
                       Mean reward: 189.24
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 33.1196
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.93s
                      Time elapsed: 00:03:24
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 110687 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 160.6131
               Mean surrogate loss: 0.0155
                 Mean entropy loss: 15.7083
                       Mean reward: 171.11
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 0.2934
     Episode_Reward/lifting_object: 35.4346
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.89s
                      Time elapsed: 00:03:25
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 109144 steps/s (collection: 0.804s, learning 0.097s)
             Mean action noise std: 1.74
          Mean value_function loss: 173.5736
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 15.7091
                       Mean reward: 171.07
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 0.2920
     Episode_Reward/lifting_object: 36.5823
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.90s
                      Time elapsed: 00:03:26
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 109744 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 1.74
          Mean value_function loss: 166.4403
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 15.7097
                       Mean reward: 174.84
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.2853
     Episode_Reward/lifting_object: 37.4483
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.90s
                      Time elapsed: 00:03:27
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 108917 steps/s (collection: 0.793s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 162.5957
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.7111
                       Mean reward: 158.78
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.2677
     Episode_Reward/lifting_object: 30.1706
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.90s
                      Time elapsed: 00:03:27
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 99115 steps/s (collection: 0.901s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 181.4306
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 15.7155
                       Mean reward: 203.89
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 0.2982
     Episode_Reward/lifting_object: 39.5841
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.99s
                      Time elapsed: 00:03:28
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 106533 steps/s (collection: 0.808s, learning 0.114s)
             Mean action noise std: 1.75
          Mean value_function loss: 154.5058
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 15.7167
                       Mean reward: 170.89
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 33.0807
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.92s
                      Time elapsed: 00:03:29
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 112104 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 1.75
          Mean value_function loss: 173.7186
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 15.7168
                       Mean reward: 141.87
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 0.2769
     Episode_Reward/lifting_object: 34.1048
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.88s
                      Time elapsed: 00:03:30
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 112807 steps/s (collection: 0.761s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 167.7191
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.7169
                       Mean reward: 168.40
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 35.1259
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.87s
                      Time elapsed: 00:03:31
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 111974 steps/s (collection: 0.793s, learning 0.085s)
             Mean action noise std: 1.75
          Mean value_function loss: 169.8990
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 15.7165
                       Mean reward: 207.54
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: 35.9506
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.88s
                      Time elapsed: 00:03:32
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 109949 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 192.0236
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 15.7167
                       Mean reward: 169.31
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 0.2938
     Episode_Reward/lifting_object: 38.4714
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.89s
                      Time elapsed: 00:03:33
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 110887 steps/s (collection: 0.798s, learning 0.089s)
             Mean action noise std: 1.75
          Mean value_function loss: 180.7454
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7167
                       Mean reward: 208.40
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.3056
     Episode_Reward/lifting_object: 41.5892
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.89s
                      Time elapsed: 00:03:34
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 98037 steps/s (collection: 0.830s, learning 0.173s)
             Mean action noise std: 1.75
          Mean value_function loss: 188.7082
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7155
                       Mean reward: 208.05
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 0.3029
     Episode_Reward/lifting_object: 41.5647
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.00s
                      Time elapsed: 00:03:35
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 111321 steps/s (collection: 0.799s, learning 0.084s)
             Mean action noise std: 1.75
          Mean value_function loss: 202.6106
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7176
                       Mean reward: 226.56
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 0.2851
     Episode_Reward/lifting_object: 38.6274
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.88s
                      Time elapsed: 00:03:36
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 108423 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 212.0470
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7172
                       Mean reward: 224.61
               Mean episode length: 231.01
    Episode_Reward/reaching_object: 0.3002
     Episode_Reward/lifting_object: 43.1841
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.91s
                      Time elapsed: 00:03:37
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 109301 steps/s (collection: 0.805s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 228.4387
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7158
                       Mean reward: 259.89
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 38.4426
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.90s
                      Time elapsed: 00:03:37
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 109725 steps/s (collection: 0.771s, learning 0.125s)
             Mean action noise std: 1.75
          Mean value_function loss: 257.6213
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7133
                       Mean reward: 218.08
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 38.9919
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.90s
                      Time elapsed: 00:03:38
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 109203 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 239.8577
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7124
                       Mean reward: 277.92
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: 47.8128
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.90s
                      Time elapsed: 00:03:39
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 94887 steps/s (collection: 0.836s, learning 0.200s)
             Mean action noise std: 1.75
          Mean value_function loss: 267.7973
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7140
                       Mean reward: 204.17
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 41.0574
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.04s
                      Time elapsed: 00:03:40
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 113199 steps/s (collection: 0.777s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 242.3383
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7167
                       Mean reward: 247.54
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.3251
     Episode_Reward/lifting_object: 48.1867
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.87s
                      Time elapsed: 00:03:41
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 110718 steps/s (collection: 0.801s, learning 0.087s)
             Mean action noise std: 1.75
          Mean value_function loss: 249.5251
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7149
                       Mean reward: 200.45
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 45.2096
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.89s
                      Time elapsed: 00:03:42
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 112493 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 247.4853
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7138
                       Mean reward: 239.49
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 0.3324
     Episode_Reward/lifting_object: 50.6781
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.87s
                      Time elapsed: 00:03:43
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 110233 steps/s (collection: 0.807s, learning 0.085s)
             Mean action noise std: 1.75
          Mean value_function loss: 236.3998
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7174
                       Mean reward: 282.60
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.3468
     Episode_Reward/lifting_object: 55.0991
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.89s
                      Time elapsed: 00:03:44
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 109807 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 232.7355
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7215
                       Mean reward: 333.65
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 0.3414
     Episode_Reward/lifting_object: 55.2476
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.90s
                      Time elapsed: 00:03:45
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 110901 steps/s (collection: 0.795s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 266.7408
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7222
                       Mean reward: 306.02
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.3378
     Episode_Reward/lifting_object: 53.1710
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.89s
                      Time elapsed: 00:03:46
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 110291 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 247.2306
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7215
                       Mean reward: 315.61
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.3641
     Episode_Reward/lifting_object: 60.2902
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.89s
                      Time elapsed: 00:03:47
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 103641 steps/s (collection: 0.825s, learning 0.124s)
             Mean action noise std: 1.75
          Mean value_function loss: 266.2094
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7207
                       Mean reward: 273.45
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.3637
     Episode_Reward/lifting_object: 58.7153
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.95s
                      Time elapsed: 00:03:47
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 108694 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 1.75
          Mean value_function loss: 270.7058
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7231
                       Mean reward: 241.18
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 0.3304
     Episode_Reward/lifting_object: 50.0881
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.90s
                      Time elapsed: 00:03:48
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 106605 steps/s (collection: 0.770s, learning 0.152s)
             Mean action noise std: 1.75
          Mean value_function loss: 254.9138
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7284
                       Mean reward: 288.67
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 57.6919
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.92s
                      Time elapsed: 00:03:49
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 109839 steps/s (collection: 0.792s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 236.5415
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7320
                       Mean reward: 272.71
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 0.3491
     Episode_Reward/lifting_object: 55.9553
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.89s
                      Time elapsed: 00:03:50
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 110381 steps/s (collection: 0.778s, learning 0.112s)
             Mean action noise std: 1.75
          Mean value_function loss: 254.5058
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7340
                       Mean reward: 296.99
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 0.3484
     Episode_Reward/lifting_object: 54.5026
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.89s
                      Time elapsed: 00:03:51
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 110054 steps/s (collection: 0.805s, learning 0.088s)
             Mean action noise std: 1.75
          Mean value_function loss: 254.3532
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7362
                       Mean reward: 211.85
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 0.3190
     Episode_Reward/lifting_object: 46.7304
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.89s
                      Time elapsed: 00:03:52
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 113464 steps/s (collection: 0.768s, learning 0.099s)
             Mean action noise std: 1.75
          Mean value_function loss: 270.9253
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7377
                       Mean reward: 269.28
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 0.3389
     Episode_Reward/lifting_object: 53.3961
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.87s
                      Time elapsed: 00:03:53
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 113525 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 248.6488
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7384
                       Mean reward: 317.08
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.3654
     Episode_Reward/lifting_object: 57.7944
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.87s
                      Time elapsed: 00:03:54
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 109855 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 274.2855
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7411
                       Mean reward: 266.01
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 0.3301
     Episode_Reward/lifting_object: 49.8176
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.89s
                      Time elapsed: 00:03:55
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 112010 steps/s (collection: 0.788s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 269.7599
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7480
                       Mean reward: 268.21
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 0.3517
     Episode_Reward/lifting_object: 56.5957
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.88s
                      Time elapsed: 00:03:55
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 110036 steps/s (collection: 0.801s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 254.2211
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7544
                       Mean reward: 300.87
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 0.3638
     Episode_Reward/lifting_object: 58.8014
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.89s
                      Time elapsed: 00:03:56
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 112200 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 256.8230
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7604
                       Mean reward: 284.24
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 60.2356
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.88s
                      Time elapsed: 00:03:57
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 110535 steps/s (collection: 0.774s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 268.4098
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7621
                       Mean reward: 276.66
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.3622
     Episode_Reward/lifting_object: 59.1669
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.89s
                      Time elapsed: 00:03:58
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 106343 steps/s (collection: 0.822s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 305.8366
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7654
                       Mean reward: 292.64
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3603
     Episode_Reward/lifting_object: 57.6754
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.92s
                      Time elapsed: 00:03:59
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 113645 steps/s (collection: 0.779s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 294.9434
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7672
                       Mean reward: 343.64
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: 61.0448
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.87s
                      Time elapsed: 00:04:00
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 111608 steps/s (collection: 0.795s, learning 0.086s)
             Mean action noise std: 1.76
          Mean value_function loss: 301.3687
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7698
                       Mean reward: 334.73
               Mean episode length: 235.06
    Episode_Reward/reaching_object: 0.3850
     Episode_Reward/lifting_object: 63.9457
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.88s
                      Time elapsed: 00:04:01
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 108306 steps/s (collection: 0.810s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 335.0923
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7709
                       Mean reward: 325.88
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 0.3825
     Episode_Reward/lifting_object: 65.0617
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.91s
                      Time elapsed: 00:04:02
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 112296 steps/s (collection: 0.787s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 309.2615
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7739
                       Mean reward: 350.67
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.3422
     Episode_Reward/lifting_object: 54.1417
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.88s
                      Time elapsed: 00:04:03
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 104696 steps/s (collection: 0.809s, learning 0.130s)
             Mean action noise std: 1.76
          Mean value_function loss: 320.8571
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7766
                       Mean reward: 289.77
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 0.3730
     Episode_Reward/lifting_object: 61.9631
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.94s
                      Time elapsed: 00:04:04
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 107304 steps/s (collection: 0.800s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 300.8185
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7770
                       Mean reward: 346.16
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 0.3627
     Episode_Reward/lifting_object: 59.4732
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.92s
                      Time elapsed: 00:04:04
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 106046 steps/s (collection: 0.817s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 336.5451
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7774
                       Mean reward: 329.67
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 0.3832
     Episode_Reward/lifting_object: 63.7641
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.93s
                      Time elapsed: 00:04:05
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 110217 steps/s (collection: 0.789s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 312.6492
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7780
                       Mean reward: 352.11
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 0.4006
     Episode_Reward/lifting_object: 68.7559
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.89s
                      Time elapsed: 00:04:06
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 106576 steps/s (collection: 0.808s, learning 0.114s)
             Mean action noise std: 1.76
          Mean value_function loss: 329.4872
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7786
                       Mean reward: 324.60
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 0.4396
     Episode_Reward/lifting_object: 78.6321
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.92s
                      Time elapsed: 00:04:07
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 112104 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 317.4074
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7815
                       Mean reward: 446.39
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 0.4306
     Episode_Reward/lifting_object: 75.4225
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.88s
                      Time elapsed: 00:04:08
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 109114 steps/s (collection: 0.807s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 319.1321
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7844
                       Mean reward: 344.82
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.4185
     Episode_Reward/lifting_object: 74.7279
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.90s
                      Time elapsed: 00:04:09
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 113325 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 316.5459
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7858
                       Mean reward: 394.91
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.4491
     Episode_Reward/lifting_object: 80.3724
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.87s
                      Time elapsed: 00:04:10
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 111561 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 308.1806
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7858
                       Mean reward: 298.47
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.3978
     Episode_Reward/lifting_object: 67.2864
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.88s
                      Time elapsed: 00:04:11
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 110030 steps/s (collection: 0.807s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 329.5984
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7875
                       Mean reward: 357.45
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.4420
     Episode_Reward/lifting_object: 80.8410
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.89s
                      Time elapsed: 00:04:12
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 113843 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 315.5723
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7899
                       Mean reward: 379.71
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.4112
     Episode_Reward/lifting_object: 73.0961
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.86s
                      Time elapsed: 00:04:12
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 111926 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 283.1050
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7913
                       Mean reward: 418.68
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.4650
     Episode_Reward/lifting_object: 86.6467
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.88s
                      Time elapsed: 00:04:13
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 112475 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 288.8633
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.7936
                       Mean reward: 351.87
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 0.4422
     Episode_Reward/lifting_object: 78.7852
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.87s
                      Time elapsed: 00:04:14
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 106860 steps/s (collection: 0.798s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 306.2965
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.7983
                       Mean reward: 439.93
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 0.4554
     Episode_Reward/lifting_object: 84.4072
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.92s
                      Time elapsed: 00:04:15
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 101174 steps/s (collection: 0.864s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 312.8732
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8006
                       Mean reward: 422.30
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 0.4518
     Episode_Reward/lifting_object: 83.5293
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.97s
                      Time elapsed: 00:04:16
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 114483 steps/s (collection: 0.774s, learning 0.085s)
             Mean action noise std: 1.77
          Mean value_function loss: 296.3974
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8005
                       Mean reward: 465.41
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 0.4686
     Episode_Reward/lifting_object: 87.3790
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.86s
                      Time elapsed: 00:04:17
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 111198 steps/s (collection: 0.794s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 293.6926
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8013
                       Mean reward: 411.60
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.4676
     Episode_Reward/lifting_object: 85.7289
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.88s
                      Time elapsed: 00:04:18
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 109845 steps/s (collection: 0.805s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 284.2227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8038
                       Mean reward: 478.89
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.4836
     Episode_Reward/lifting_object: 90.6134
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.89s
                      Time elapsed: 00:04:19
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 108857 steps/s (collection: 0.817s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 314.6734
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8043
                       Mean reward: 434.64
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 0.4790
     Episode_Reward/lifting_object: 88.8725
      Episode_Reward/object_height: 0.0342
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.90s
                      Time elapsed: 00:04:20
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 111791 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 295.0893
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8029
                       Mean reward: 468.95
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 0.4644
     Episode_Reward/lifting_object: 87.4623
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.88s
                      Time elapsed: 00:04:21
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 110351 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 304.9813
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8065
                       Mean reward: 435.31
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.4876
     Episode_Reward/lifting_object: 90.7442
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.89s
                      Time elapsed: 00:04:21
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 114611 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 309.2583
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8113
                       Mean reward: 460.18
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.4956
     Episode_Reward/lifting_object: 94.4034
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.86s
                      Time elapsed: 00:04:22
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 115097 steps/s (collection: 0.765s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 310.8754
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8134
                       Mean reward: 492.60
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 0.4919
     Episode_Reward/lifting_object: 91.1067
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.85s
                      Time elapsed: 00:04:23
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 112460 steps/s (collection: 0.791s, learning 0.083s)
             Mean action noise std: 1.77
          Mean value_function loss: 342.9028
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.8134
                       Mean reward: 475.97
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 0.4933
     Episode_Reward/lifting_object: 94.5104
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.87s
                      Time elapsed: 00:04:24
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 110244 steps/s (collection: 0.806s, learning 0.085s)
             Mean action noise std: 1.77
          Mean value_function loss: 350.1452
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8137
                       Mean reward: 445.65
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 0.4657
     Episode_Reward/lifting_object: 87.3220
      Episode_Reward/object_height: 0.0343
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.89s
                      Time elapsed: 00:04:25
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 113584 steps/s (collection: 0.769s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 358.5296
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8133
                       Mean reward: 465.33
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.4673
     Episode_Reward/lifting_object: 86.2166
      Episode_Reward/object_height: 0.0339
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.87s
                      Time elapsed: 00:04:26
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 112360 steps/s (collection: 0.781s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 344.9500
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8151
                       Mean reward: 486.09
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.4758
     Episode_Reward/lifting_object: 89.2579
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.87s
                      Time elapsed: 00:04:27
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 111613 steps/s (collection: 0.788s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 340.0051
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.8145
                       Mean reward: 439.38
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.5057
     Episode_Reward/lifting_object: 96.7919
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.88s
                      Time elapsed: 00:04:28
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 112183 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 317.6230
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8169
                       Mean reward: 454.59
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 0.4768
     Episode_Reward/lifting_object: 89.5824
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.88s
                      Time elapsed: 00:04:28
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 99747 steps/s (collection: 0.854s, learning 0.131s)
             Mean action noise std: 1.78
          Mean value_function loss: 354.8645
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8213
                       Mean reward: 471.63
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.4710
     Episode_Reward/lifting_object: 87.8049
      Episode_Reward/object_height: 0.0344
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.99s
                      Time elapsed: 00:04:29
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 104309 steps/s (collection: 0.807s, learning 0.136s)
             Mean action noise std: 1.78
          Mean value_function loss: 370.7561
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.8252
                       Mean reward: 487.16
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.5128
     Episode_Reward/lifting_object: 99.5805
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.94s
                      Time elapsed: 00:04:30
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 109654 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 359.2885
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8273
                       Mean reward: 460.80
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.5122
     Episode_Reward/lifting_object: 95.9452
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.90s
                      Time elapsed: 00:04:31
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 109878 steps/s (collection: 0.770s, learning 0.125s)
             Mean action noise std: 1.78
          Mean value_function loss: 351.1955
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8311
                       Mean reward: 540.67
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.5269
     Episode_Reward/lifting_object: 101.6567
      Episode_Reward/object_height: 0.0402
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.89s
                      Time elapsed: 00:04:32
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 107356 steps/s (collection: 0.825s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 341.5131
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8337
                       Mean reward: 461.46
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 0.5249
     Episode_Reward/lifting_object: 103.2783
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.92s
                      Time elapsed: 00:04:33
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 108871 steps/s (collection: 0.815s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 319.6128
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8358
                       Mean reward: 594.50
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 101.3495
      Episode_Reward/object_height: 0.0402
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.90s
                      Time elapsed: 00:04:34
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 112902 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 328.9501
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 15.8389
                       Mean reward: 526.65
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.5008
     Episode_Reward/lifting_object: 95.4194
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.87s
                      Time elapsed: 00:04:35
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 110779 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 335.3307
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8403
                       Mean reward: 530.10
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 106.3153
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.89s
                      Time elapsed: 00:04:36
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 113360 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 341.1435
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8414
                       Mean reward: 497.89
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.5289
     Episode_Reward/lifting_object: 103.4717
      Episode_Reward/object_height: 0.0413
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.87s
                      Time elapsed: 00:04:37
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 113401 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 366.7789
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8415
                       Mean reward: 541.06
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.5468
     Episode_Reward/lifting_object: 108.0224
      Episode_Reward/object_height: 0.0428
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.87s
                      Time elapsed: 00:04:37
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 110782 steps/s (collection: 0.779s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 357.8543
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.8429
                       Mean reward: 575.04
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.5506
     Episode_Reward/lifting_object: 110.0368
      Episode_Reward/object_height: 0.0438
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.89s
                      Time elapsed: 00:04:38
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 109898 steps/s (collection: 0.762s, learning 0.133s)
             Mean action noise std: 1.78
          Mean value_function loss: 402.1849
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8461
                       Mean reward: 573.10
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 110.0526
      Episode_Reward/object_height: 0.0436
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.89s
                      Time elapsed: 00:04:39
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 112007 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 368.5194
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8483
                       Mean reward: 506.45
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.5421
     Episode_Reward/lifting_object: 107.9054
      Episode_Reward/object_height: 0.0430
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.88s
                      Time elapsed: 00:04:40
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 114120 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 365.1284
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8503
                       Mean reward: 540.98
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.5481
     Episode_Reward/lifting_object: 106.8376
      Episode_Reward/object_height: 0.0422
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.86s
                      Time elapsed: 00:04:41
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 113233 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 342.0341
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8519
                       Mean reward: 538.98
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.5422
     Episode_Reward/lifting_object: 105.4724
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.87s
                      Time elapsed: 00:04:42
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 113267 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 367.4087
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.8542
                       Mean reward: 569.92
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.5456
     Episode_Reward/lifting_object: 106.2286
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.87s
                      Time elapsed: 00:04:43
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 106846 steps/s (collection: 0.821s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 366.5974
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8561
                       Mean reward: 536.29
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 112.2082
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.92s
                      Time elapsed: 00:04:44
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 110843 steps/s (collection: 0.778s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 342.7987
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8580
                       Mean reward: 573.08
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 113.7732
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.89s
                      Time elapsed: 00:04:44
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 107187 steps/s (collection: 0.789s, learning 0.128s)
             Mean action noise std: 1.79
          Mean value_function loss: 330.3034
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8588
                       Mean reward: 547.55
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.5599
     Episode_Reward/lifting_object: 112.9906
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.92s
                      Time elapsed: 00:04:45
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 113607 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 323.7302
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8599
                       Mean reward: 571.78
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.5546
     Episode_Reward/lifting_object: 110.2971
      Episode_Reward/object_height: 0.0436
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.87s
                      Time elapsed: 00:04:46
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 111510 steps/s (collection: 0.795s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 318.0836
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8617
                       Mean reward: 591.30
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.5951
     Episode_Reward/lifting_object: 120.4629
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.88s
                      Time elapsed: 00:04:47
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 108040 steps/s (collection: 0.815s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 311.8138
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8660
                       Mean reward: 613.42
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 119.0907
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.91s
                      Time elapsed: 00:04:48
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 108512 steps/s (collection: 0.799s, learning 0.107s)
             Mean action noise std: 1.79
          Mean value_function loss: 328.5058
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8684
                       Mean reward: 560.23
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 109.6349
      Episode_Reward/object_height: 0.0432
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.91s
                      Time elapsed: 00:04:49
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 109824 steps/s (collection: 0.782s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 353.6574
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8669
                       Mean reward: 601.35
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.5729
     Episode_Reward/lifting_object: 115.0646
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.90s
                      Time elapsed: 00:04:50
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 108974 steps/s (collection: 0.800s, learning 0.102s)
             Mean action noise std: 1.79
          Mean value_function loss: 315.1924
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8683
                       Mean reward: 611.39
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.5865
     Episode_Reward/lifting_object: 119.2576
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.90s
                      Time elapsed: 00:04:51
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 109748 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 332.8404
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8728
                       Mean reward: 613.54
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.5667
     Episode_Reward/lifting_object: 113.7939
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.90s
                      Time elapsed: 00:04:52
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 106639 steps/s (collection: 0.815s, learning 0.107s)
             Mean action noise std: 1.79
          Mean value_function loss: 332.2504
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8754
                       Mean reward: 541.02
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.5616
     Episode_Reward/lifting_object: 113.5645
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.92s
                      Time elapsed: 00:04:53
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 111920 steps/s (collection: 0.793s, learning 0.086s)
             Mean action noise std: 1.79
          Mean value_function loss: 294.4718
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8788
                       Mean reward: 581.57
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 119.0915
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.88s
                      Time elapsed: 00:04:53
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 108212 steps/s (collection: 0.787s, learning 0.122s)
             Mean action noise std: 1.79
          Mean value_function loss: 305.3617
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.8815
                       Mean reward: 593.44
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 0.5765
     Episode_Reward/lifting_object: 117.4842
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.91s
                      Time elapsed: 00:04:54
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 106525 steps/s (collection: 0.797s, learning 0.126s)
             Mean action noise std: 1.79
          Mean value_function loss: 327.1821
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8832
                       Mean reward: 575.38
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.5782
     Episode_Reward/lifting_object: 117.0768
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.92s
                      Time elapsed: 00:04:55
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 108461 steps/s (collection: 0.788s, learning 0.118s)
             Mean action noise std: 1.79
          Mean value_function loss: 301.1571
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8862
                       Mean reward: 593.87
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 114.9012
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.91s
                      Time elapsed: 00:04:56
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 110541 steps/s (collection: 0.784s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 300.1201
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.8885
                       Mean reward: 639.91
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.5804
     Episode_Reward/lifting_object: 116.7650
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.89s
                      Time elapsed: 00:04:57
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 114227 steps/s (collection: 0.767s, learning 0.094s)
             Mean action noise std: 1.80
          Mean value_function loss: 342.0707
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.8932
                       Mean reward: 536.02
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.5667
     Episode_Reward/lifting_object: 115.6481
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.86s
                      Time elapsed: 00:04:58
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 112949 steps/s (collection: 0.784s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 285.3446
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.8960
                       Mean reward: 608.72
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.5767
     Episode_Reward/lifting_object: 118.7600
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.87s
                      Time elapsed: 00:04:59
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 110775 steps/s (collection: 0.798s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 342.7306
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8966
                       Mean reward: 611.55
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 0.5945
     Episode_Reward/lifting_object: 122.3038
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.89s
                      Time elapsed: 00:05:00
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 108736 steps/s (collection: 0.783s, learning 0.121s)
             Mean action noise std: 1.80
          Mean value_function loss: 306.1131
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 15.9007
                       Mean reward: 614.08
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.6081
     Episode_Reward/lifting_object: 125.1021
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.90s
                      Time elapsed: 00:05:01
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 108742 steps/s (collection: 0.801s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 304.0889
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.9041
                       Mean reward: 611.21
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: 118.0204
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.90s
                      Time elapsed: 00:05:02
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 104193 steps/s (collection: 0.818s, learning 0.125s)
             Mean action noise std: 1.80
          Mean value_function loss: 320.6471
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9077
                       Mean reward: 665.03
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.5888
     Episode_Reward/lifting_object: 120.8197
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.94s
                      Time elapsed: 00:05:02
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 102301 steps/s (collection: 0.822s, learning 0.139s)
             Mean action noise std: 1.80
          Mean value_function loss: 295.1034
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9055
                       Mean reward: 598.27
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.5800
     Episode_Reward/lifting_object: 118.2402
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.96s
                      Time elapsed: 00:05:03
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 102183 steps/s (collection: 0.820s, learning 0.142s)
             Mean action noise std: 1.80
          Mean value_function loss: 290.6802
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9021
                       Mean reward: 624.04
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.6252
     Episode_Reward/lifting_object: 130.5426
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.96s
                      Time elapsed: 00:05:04
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 100093 steps/s (collection: 0.809s, learning 0.173s)
             Mean action noise std: 1.80
          Mean value_function loss: 273.8475
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.9023
                       Mean reward: 627.15
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 126.4106
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.98s
                      Time elapsed: 00:05:05
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 110135 steps/s (collection: 0.793s, learning 0.100s)
             Mean action noise std: 1.80
          Mean value_function loss: 266.7521
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.9044
                       Mean reward: 634.65
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.6069
     Episode_Reward/lifting_object: 124.2029
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.89s
                      Time elapsed: 00:05:06
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 109467 steps/s (collection: 0.796s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 289.3094
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.9088
                       Mean reward: 619.79
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.6018
     Episode_Reward/lifting_object: 123.4926
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.90s
                      Time elapsed: 00:05:07
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 109712 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 269.1706
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.9136
                       Mean reward: 610.39
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.5965
     Episode_Reward/lifting_object: 123.6186
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.90s
                      Time elapsed: 00:05:08
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 113402 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 303.4789
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.9146
                       Mean reward: 697.40
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 0.6106
     Episode_Reward/lifting_object: 128.1816
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.87s
                      Time elapsed: 00:05:09
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 109286 steps/s (collection: 0.798s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 243.9606
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9152
                       Mean reward: 617.64
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.5974
     Episode_Reward/lifting_object: 122.4931
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.90s
                      Time elapsed: 00:05:10
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 109402 steps/s (collection: 0.806s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 277.0998
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.9134
                       Mean reward: 594.04
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 121.6283
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.90s
                      Time elapsed: 00:05:11
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 112561 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 264.0240
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.9135
                       Mean reward: 566.65
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.5912
     Episode_Reward/lifting_object: 122.5079
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.87s
                      Time elapsed: 00:05:12
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 111089 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 1.80
          Mean value_function loss: 270.2819
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9165
                       Mean reward: 522.48
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.5821
     Episode_Reward/lifting_object: 119.3382
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.88s
                      Time elapsed: 00:05:12
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 113206 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 277.9290
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.9202
                       Mean reward: 667.50
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.5880
     Episode_Reward/lifting_object: 121.1334
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.87s
                      Time elapsed: 00:05:13
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 111845 steps/s (collection: 0.757s, learning 0.122s)
             Mean action noise std: 1.80
          Mean value_function loss: 257.8697
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.9224
                       Mean reward: 600.16
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 0.5792
     Episode_Reward/lifting_object: 118.7697
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.88s
                      Time elapsed: 00:05:14
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 111667 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 268.9453
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9239
                       Mean reward: 625.03
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.6023
     Episode_Reward/lifting_object: 123.9057
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.88s
                      Time elapsed: 00:05:15
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 112811 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 285.4991
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9288
                       Mean reward: 615.17
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.6140
     Episode_Reward/lifting_object: 126.9836
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.87s
                      Time elapsed: 00:05:16
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 107978 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 265.2814
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9348
                       Mean reward: 639.71
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 0.6185
     Episode_Reward/lifting_object: 129.4419
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.91s
                      Time elapsed: 00:05:17
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 114792 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 257.0188
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.9392
                       Mean reward: 584.42
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.5961
     Episode_Reward/lifting_object: 121.8396
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.86s
                      Time elapsed: 00:05:18
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 109314 steps/s (collection: 0.790s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 246.4677
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.9412
                       Mean reward: 656.15
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 125.8450
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.90s
                      Time elapsed: 00:05:19
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 107147 steps/s (collection: 0.821s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 244.5199
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9427
                       Mean reward: 608.81
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.5934
     Episode_Reward/lifting_object: 121.5448
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.92s
                      Time elapsed: 00:05:20
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 107557 steps/s (collection: 0.806s, learning 0.108s)
             Mean action noise std: 1.81
          Mean value_function loss: 258.0706
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.9423
                       Mean reward: 610.90
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 119.7123
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.91s
                      Time elapsed: 00:05:20
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 106298 steps/s (collection: 0.807s, learning 0.118s)
             Mean action noise std: 1.81
          Mean value_function loss: 211.2169
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.9441
                       Mean reward: 559.51
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 0.5646
     Episode_Reward/lifting_object: 114.5515
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.92s
                      Time elapsed: 00:05:21
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 107365 steps/s (collection: 0.772s, learning 0.143s)
             Mean action noise std: 1.81
          Mean value_function loss: 234.0165
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.9455
                       Mean reward: 606.03
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.5653
     Episode_Reward/lifting_object: 113.7634
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.92s
                      Time elapsed: 00:05:22
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 109284 steps/s (collection: 0.791s, learning 0.108s)
             Mean action noise std: 1.81
          Mean value_function loss: 222.5449
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 15.9455
                       Mean reward: 638.01
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.5947
     Episode_Reward/lifting_object: 122.1088
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.90s
                      Time elapsed: 00:05:23
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 109778 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 1.81
          Mean value_function loss: 210.4064
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 15.9461
                       Mean reward: 661.90
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 119.8222
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.90s
                      Time elapsed: 00:05:24
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 43408 steps/s (collection: 2.165s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 228.2655
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9468
                       Mean reward: 685.89
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.6251
     Episode_Reward/lifting_object: 130.9390
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.26s
                      Time elapsed: 00:05:26
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 30652 steps/s (collection: 3.065s, learning 0.142s)
             Mean action noise std: 1.81
          Mean value_function loss: 242.8015
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.9491
                       Mean reward: 646.65
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.6267
     Episode_Reward/lifting_object: 131.4378
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 3.21s
                      Time elapsed: 00:05:30
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 30268 steps/s (collection: 3.136s, learning 0.112s)
             Mean action noise std: 1.81
          Mean value_function loss: 247.1650
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9519
                       Mean reward: 669.94
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 128.8546
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.25s
                      Time elapsed: 00:05:33
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 31551 steps/s (collection: 3.003s, learning 0.113s)
             Mean action noise std: 1.81
          Mean value_function loss: 251.1778
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9532
                       Mean reward: 639.93
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.6285
     Episode_Reward/lifting_object: 131.8753
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.12s
                      Time elapsed: 00:05:36
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 31200 steps/s (collection: 3.033s, learning 0.117s)
             Mean action noise std: 1.81
          Mean value_function loss: 225.3289
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9550
                       Mean reward: 666.13
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 133.9132
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.15s
                      Time elapsed: 00:05:39
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 30062 steps/s (collection: 3.124s, learning 0.146s)
             Mean action noise std: 1.82
          Mean value_function loss: 238.8020
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.9570
                       Mean reward: 707.60
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.6556
     Episode_Reward/lifting_object: 138.5921
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.27s
                      Time elapsed: 00:05:42
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 29625 steps/s (collection: 3.181s, learning 0.138s)
             Mean action noise std: 1.82
          Mean value_function loss: 231.4592
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 15.9592
                       Mean reward: 694.01
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.6349
     Episode_Reward/lifting_object: 132.6633
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.32s
                      Time elapsed: 00:05:46
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 31358 steps/s (collection: 3.015s, learning 0.120s)
             Mean action noise std: 1.82
          Mean value_function loss: 265.1509
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9604
                       Mean reward: 686.53
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.6418
     Episode_Reward/lifting_object: 133.6715
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.13s
                      Time elapsed: 00:05:49
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 21831 steps/s (collection: 4.381s, learning 0.122s)
             Mean action noise std: 1.82
          Mean value_function loss: 219.9990
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9610
                       Mean reward: 708.50
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 133.1246
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.50s
                      Time elapsed: 00:05:53
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 105356 steps/s (collection: 0.813s, learning 0.120s)
             Mean action noise std: 1.82
          Mean value_function loss: 256.2565
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.9651
                       Mean reward: 651.33
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.6384
     Episode_Reward/lifting_object: 132.9370
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.93s
                      Time elapsed: 00:05:54
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 116505 steps/s (collection: 0.750s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 244.9556
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 15.9699
                       Mean reward: 628.29
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 133.8367
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.84s
                      Time elapsed: 00:05:55
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 108582 steps/s (collection: 0.800s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 234.7333
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9713
                       Mean reward: 652.28
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.6462
     Episode_Reward/lifting_object: 135.0587
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.91s
                      Time elapsed: 00:05:56
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 112987 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 240.4763
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9743
                       Mean reward: 678.31
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.6537
     Episode_Reward/lifting_object: 138.7318
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.87s
                      Time elapsed: 00:05:57
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 107388 steps/s (collection: 0.825s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 268.5800
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.9784
                       Mean reward: 684.29
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.6551
     Episode_Reward/lifting_object: 137.2235
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.92s
                      Time elapsed: 00:05:58
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 107529 steps/s (collection: 0.782s, learning 0.132s)
             Mean action noise std: 1.82
          Mean value_function loss: 224.9728
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.9820
                       Mean reward: 682.63
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: 137.2470
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.91s
                      Time elapsed: 00:05:59
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 109963 steps/s (collection: 0.756s, learning 0.138s)
             Mean action noise std: 1.82
          Mean value_function loss: 221.5300
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.9834
                       Mean reward: 713.50
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.6632
     Episode_Reward/lifting_object: 140.4878
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.89s
                      Time elapsed: 00:06:00
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 114392 steps/s (collection: 0.762s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 268.4550
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 15.9836
                       Mean reward: 614.78
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.6216
     Episode_Reward/lifting_object: 128.7441
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.86s
                      Time elapsed: 00:06:00
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 110016 steps/s (collection: 0.779s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 258.8875
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9845
                       Mean reward: 589.27
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.6117
     Episode_Reward/lifting_object: 126.7303
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.89s
                      Time elapsed: 00:06:01
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 109353 steps/s (collection: 0.768s, learning 0.131s)
             Mean action noise std: 1.82
          Mean value_function loss: 274.2227
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9857
                       Mean reward: 661.22
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.6276
     Episode_Reward/lifting_object: 130.9594
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.90s
                      Time elapsed: 00:06:02
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 94637 steps/s (collection: 0.915s, learning 0.124s)
             Mean action noise std: 1.82
          Mean value_function loss: 282.8562
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9851
                       Mean reward: 655.41
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.6318
     Episode_Reward/lifting_object: 130.2488
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.04s
                      Time elapsed: 00:06:03
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 110857 steps/s (collection: 0.799s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 281.8625
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9862
                       Mean reward: 639.66
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 131.1783
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.89s
                      Time elapsed: 00:06:04
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 107719 steps/s (collection: 0.805s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 323.2659
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9878
                       Mean reward: 634.49
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 0.6292
     Episode_Reward/lifting_object: 129.7855
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.91s
                      Time elapsed: 00:06:05
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 111439 steps/s (collection: 0.785s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 298.9360
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9897
                       Mean reward: 707.22
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.6471
     Episode_Reward/lifting_object: 135.9971
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.88s
                      Time elapsed: 00:06:06
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 112393 steps/s (collection: 0.785s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 267.6716
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.9934
                       Mean reward: 690.74
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.6499
     Episode_Reward/lifting_object: 134.8504
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.87s
                      Time elapsed: 00:06:07
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 110993 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 1.82
          Mean value_function loss: 278.3011
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9944
                       Mean reward: 654.72
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.6412
     Episode_Reward/lifting_object: 130.7298
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.89s
                      Time elapsed: 00:06:08
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 104708 steps/s (collection: 0.791s, learning 0.148s)
             Mean action noise std: 1.83
          Mean value_function loss: 242.5353
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9964
                       Mean reward: 587.75
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.6075
     Episode_Reward/lifting_object: 121.1276
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.94s
                      Time elapsed: 00:06:09
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 113990 steps/s (collection: 0.764s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 262.5173
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.9980
                       Mean reward: 605.33
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 121.3542
      Episode_Reward/object_height: 0.0442
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.86s
                      Time elapsed: 00:06:10
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 112681 steps/s (collection: 0.764s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 220.8685
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9946
                       Mean reward: 620.39
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.5953
     Episode_Reward/lifting_object: 118.8710
      Episode_Reward/object_height: 0.0434
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.87s
                      Time elapsed: 00:06:10
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 111809 steps/s (collection: 0.795s, learning 0.085s)
             Mean action noise std: 1.83
          Mean value_function loss: 220.8873
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9942
                       Mean reward: 613.95
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.6283
     Episode_Reward/lifting_object: 129.2896
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.88s
                      Time elapsed: 00:06:11
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 113107 steps/s (collection: 0.785s, learning 0.084s)
             Mean action noise std: 1.83
          Mean value_function loss: 228.4999
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.9959
                       Mean reward: 683.20
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.6436
     Episode_Reward/lifting_object: 133.8742
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.87s
                      Time elapsed: 00:06:12
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 101397 steps/s (collection: 0.861s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 234.9255
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9979
                       Mean reward: 672.34
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.6401
     Episode_Reward/lifting_object: 133.5580
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.97s
                      Time elapsed: 00:06:13
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 113474 steps/s (collection: 0.778s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 240.7956
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 16.0001
                       Mean reward: 673.89
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.6429
     Episode_Reward/lifting_object: 134.3488
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.87s
                      Time elapsed: 00:06:14
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 115726 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 218.4423
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0014
                       Mean reward: 679.75
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.6449
     Episode_Reward/lifting_object: 134.5355
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.85s
                      Time elapsed: 00:06:15
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 108807 steps/s (collection: 0.766s, learning 0.137s)
             Mean action noise std: 1.83
          Mean value_function loss: 225.9006
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0032
                       Mean reward: 644.23
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.6418
     Episode_Reward/lifting_object: 134.8010
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.90s
                      Time elapsed: 00:06:16
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 115873 steps/s (collection: 0.755s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 209.4286
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0042
                       Mean reward: 706.81
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 133.9451
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.85s
                      Time elapsed: 00:06:17
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 112587 steps/s (collection: 0.754s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 190.6525
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0047
                       Mean reward: 690.32
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.6533
     Episode_Reward/lifting_object: 136.4974
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.87s
                      Time elapsed: 00:06:17
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 113545 steps/s (collection: 0.782s, learning 0.084s)
             Mean action noise std: 1.83
          Mean value_function loss: 191.5864
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0068
                       Mean reward: 745.61
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.6584
     Episode_Reward/lifting_object: 140.9474
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.87s
                      Time elapsed: 00:06:18
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 115016 steps/s (collection: 0.764s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 198.0192
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0120
                       Mean reward: 695.97
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.6605
     Episode_Reward/lifting_object: 141.0854
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.85s
                      Time elapsed: 00:06:19
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 112963 steps/s (collection: 0.776s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 217.5564
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.0137
                       Mean reward: 657.73
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.6520
     Episode_Reward/lifting_object: 136.8723
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.87s
                      Time elapsed: 00:06:20
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 108340 steps/s (collection: 0.794s, learning 0.114s)
             Mean action noise std: 1.83
          Mean value_function loss: 215.3908
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.0166
                       Mean reward: 723.16
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.6609
     Episode_Reward/lifting_object: 140.8208
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.91s
                      Time elapsed: 00:06:21
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 108116 steps/s (collection: 0.809s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 200.7796
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0210
                       Mean reward: 677.85
               Mean episode length: 243.67
    Episode_Reward/reaching_object: 0.6432
     Episode_Reward/lifting_object: 136.8291
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.91s
                      Time elapsed: 00:06:22
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 110599 steps/s (collection: 0.760s, learning 0.129s)
             Mean action noise std: 1.83
          Mean value_function loss: 211.6598
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.0247
                       Mean reward: 713.25
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.6631
     Episode_Reward/lifting_object: 142.6622
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.89s
                      Time elapsed: 00:06:23
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 106510 steps/s (collection: 0.772s, learning 0.151s)
             Mean action noise std: 1.84
          Mean value_function loss: 190.6710
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.0299
                       Mean reward: 708.77
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 0.6607
     Episode_Reward/lifting_object: 139.9438
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.92s
                      Time elapsed: 00:06:24
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 107542 steps/s (collection: 0.765s, learning 0.149s)
             Mean action noise std: 1.84
          Mean value_function loss: 194.7075
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0337
                       Mean reward: 736.64
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.6680
     Episode_Reward/lifting_object: 143.1117
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.91s
                      Time elapsed: 00:06:25
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 110349 steps/s (collection: 0.804s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 216.9484
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.0368
                       Mean reward: 744.28
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.6899
     Episode_Reward/lifting_object: 149.3885
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.89s
                      Time elapsed: 00:06:25
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 116172 steps/s (collection: 0.752s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 192.4480
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0383
                       Mean reward: 725.31
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.6657
     Episode_Reward/lifting_object: 141.4580
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.85s
                      Time elapsed: 00:06:26
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 109250 steps/s (collection: 0.816s, learning 0.084s)
             Mean action noise std: 1.84
          Mean value_function loss: 188.0160
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.0412
                       Mean reward: 739.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6772
     Episode_Reward/lifting_object: 145.9850
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.90s
                      Time elapsed: 00:06:27
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 112585 steps/s (collection: 0.786s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 185.3533
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0427
                       Mean reward: 743.38
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.6747
     Episode_Reward/lifting_object: 145.2449
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.87s
                      Time elapsed: 00:06:28
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 111965 steps/s (collection: 0.785s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 198.9682
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0429
                       Mean reward: 693.90
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.6543
     Episode_Reward/lifting_object: 137.9929
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.88s
                      Time elapsed: 00:06:29
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 110874 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 195.6397
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0449
                       Mean reward: 716.17
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.6616
     Episode_Reward/lifting_object: 140.8228
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.89s
                      Time elapsed: 00:06:30
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 108826 steps/s (collection: 0.773s, learning 0.131s)
             Mean action noise std: 1.84
          Mean value_function loss: 187.2364
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.0466
                       Mean reward: 706.02
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.6399
     Episode_Reward/lifting_object: 135.5085
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.90s
                      Time elapsed: 00:06:31
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 111169 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 152.8889
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0463
                       Mean reward: 775.05
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.6848
     Episode_Reward/lifting_object: 147.3556
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.88s
                      Time elapsed: 00:06:32
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 110549 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 172.2582
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0465
                       Mean reward: 717.59
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.6780
     Episode_Reward/lifting_object: 145.3733
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.89s
                      Time elapsed: 00:06:33
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 107682 steps/s (collection: 0.779s, learning 0.134s)
             Mean action noise std: 1.84
          Mean value_function loss: 170.9644
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0502
                       Mean reward: 734.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 142.2248
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.91s
                      Time elapsed: 00:06:33
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 112350 steps/s (collection: 0.787s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 159.3518
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.0541
                       Mean reward: 719.47
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 142.8706
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.87s
                      Time elapsed: 00:06:34
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 112169 steps/s (collection: 0.791s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 162.5179
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0561
                       Mean reward: 706.75
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.6782
     Episode_Reward/lifting_object: 143.7302
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.88s
                      Time elapsed: 00:06:35
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 113362 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 151.8058
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 16.0568
                       Mean reward: 736.62
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.6924
     Episode_Reward/lifting_object: 148.9775
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.87s
                      Time elapsed: 00:06:36
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 110911 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 150.2403
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0581
                       Mean reward: 713.92
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.6909
     Episode_Reward/lifting_object: 148.5227
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.89s
                      Time elapsed: 00:06:37
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 109578 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 163.9492
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0597
                       Mean reward: 760.54
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.6891
     Episode_Reward/lifting_object: 146.6421
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.90s
                      Time elapsed: 00:06:38
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 107990 steps/s (collection: 0.824s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 176.8146
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.0613
                       Mean reward: 743.14
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.6851
     Episode_Reward/lifting_object: 148.8663
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.91s
                      Time elapsed: 00:06:39
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 105631 steps/s (collection: 0.788s, learning 0.143s)
             Mean action noise std: 1.85
          Mean value_function loss: 171.7655
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 16.0673
                       Mean reward: 752.44
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.6922
     Episode_Reward/lifting_object: 149.2358
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.93s
                      Time elapsed: 00:06:40
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 115072 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 160.0191
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 16.0704
                       Mean reward: 787.02
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 152.7882
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.85s
                      Time elapsed: 00:06:41
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 109225 steps/s (collection: 0.815s, learning 0.085s)
             Mean action noise std: 1.85
          Mean value_function loss: 161.3380
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0712
                       Mean reward: 719.04
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.6872
     Episode_Reward/lifting_object: 147.8686
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.90s
                      Time elapsed: 00:06:41
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 115363 steps/s (collection: 0.767s, learning 0.085s)
             Mean action noise std: 1.85
          Mean value_function loss: 153.0790
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.0749
                       Mean reward: 727.41
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7042
     Episode_Reward/lifting_object: 152.0830
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.85s
                      Time elapsed: 00:06:42
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 112107 steps/s (collection: 0.791s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 130.2104
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 16.0795
                       Mean reward: 715.05
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 145.4389
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.88s
                      Time elapsed: 00:06:43
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 111971 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 161.5332
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.0815
                       Mean reward: 752.35
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.6899
     Episode_Reward/lifting_object: 147.9941
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.88s
                      Time elapsed: 00:06:44
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 113956 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 161.3426
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0828
                       Mean reward: 759.34
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.6928
     Episode_Reward/lifting_object: 149.4621
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.86s
                      Time elapsed: 00:06:45
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 101861 steps/s (collection: 0.821s, learning 0.144s)
             Mean action noise std: 1.85
          Mean value_function loss: 141.7871
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0837
                       Mean reward: 762.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: 147.8337
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.97s
                      Time elapsed: 00:06:46
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 108053 steps/s (collection: 0.820s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 142.3783
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 16.0842
                       Mean reward: 734.91
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.6797
     Episode_Reward/lifting_object: 146.7885
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.91s
                      Time elapsed: 00:06:47
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 112133 steps/s (collection: 0.774s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 156.1232
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0838
                       Mean reward: 708.61
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 150.1600
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.88s
                      Time elapsed: 00:06:48
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 101765 steps/s (collection: 0.809s, learning 0.157s)
             Mean action noise std: 1.85
          Mean value_function loss: 140.9957
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0841
                       Mean reward: 750.13
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.6931
     Episode_Reward/lifting_object: 152.3716
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.97s
                      Time elapsed: 00:06:49
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 115529 steps/s (collection: 0.767s, learning 0.084s)
             Mean action noise std: 1.85
          Mean value_function loss: 149.6693
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.0870
                       Mean reward: 737.63
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.6873
     Episode_Reward/lifting_object: 150.3429
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.85s
                      Time elapsed: 00:06:49
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 115628 steps/s (collection: 0.751s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 141.1430
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0907
                       Mean reward: 722.55
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.6904
     Episode_Reward/lifting_object: 149.3121
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.85s
                      Time elapsed: 00:06:50
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 114665 steps/s (collection: 0.766s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 142.8580
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.0934
                       Mean reward: 799.95
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.6981
     Episode_Reward/lifting_object: 151.7340
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.86s
                      Time elapsed: 00:06:51
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 117694 steps/s (collection: 0.753s, learning 0.083s)
             Mean action noise std: 1.85
          Mean value_function loss: 158.0888
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0944
                       Mean reward: 739.14
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 151.0827
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.84s
                      Time elapsed: 00:06:52
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 110053 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 146.5468
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.0953
                       Mean reward: 765.03
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.6894
     Episode_Reward/lifting_object: 149.4604
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.89s
                      Time elapsed: 00:06:53
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 117819 steps/s (collection: 0.747s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 151.2204
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0961
                       Mean reward: 779.44
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 155.9738
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.83s
                      Time elapsed: 00:06:54
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 108598 steps/s (collection: 0.781s, learning 0.124s)
             Mean action noise std: 1.86
          Mean value_function loss: 139.0945
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0979
                       Mean reward: 784.92
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7026
     Episode_Reward/lifting_object: 152.8900
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.91s
                      Time elapsed: 00:06:55
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 112491 steps/s (collection: 0.770s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 157.5748
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1009
                       Mean reward: 710.79
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.6859
     Episode_Reward/lifting_object: 147.1578
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.87s
                      Time elapsed: 00:06:56
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 103800 steps/s (collection: 0.836s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 141.7124
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.1060
                       Mean reward: 786.08
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 152.2406
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.95s
                      Time elapsed: 00:06:56
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 113583 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 149.7736
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.1108
                       Mean reward: 785.05
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 157.4955
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.87s
                      Time elapsed: 00:06:57
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 113732 steps/s (collection: 0.780s, learning 0.084s)
             Mean action noise std: 1.86
          Mean value_function loss: 127.7542
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.1127
                       Mean reward: 759.59
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.6923
     Episode_Reward/lifting_object: 150.3322
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.86s
                      Time elapsed: 00:06:58
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 110822 steps/s (collection: 0.793s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 142.3299
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1122
                       Mean reward: 784.58
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.6930
     Episode_Reward/lifting_object: 150.6029
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.89s
                      Time elapsed: 00:06:59
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 109712 steps/s (collection: 0.783s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 135.7617
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.1129
                       Mean reward: 702.86
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 150.1815
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.90s
                      Time elapsed: 00:07:00
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 107273 steps/s (collection: 0.790s, learning 0.126s)
             Mean action noise std: 1.86
          Mean value_function loss: 132.5408
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 16.1145
                       Mean reward: 763.84
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 151.2930
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.92s
                      Time elapsed: 00:07:01
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 105475 steps/s (collection: 0.830s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 143.5931
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.1203
                       Mean reward: 786.01
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7230
     Episode_Reward/lifting_object: 155.9267
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.93s
                      Time elapsed: 00:07:02
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 98524 steps/s (collection: 0.826s, learning 0.172s)
             Mean action noise std: 1.87
          Mean value_function loss: 126.5308
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.1310
                       Mean reward: 787.33
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7017
     Episode_Reward/lifting_object: 152.0077
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 1.00s
                      Time elapsed: 00:07:03
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 109824 steps/s (collection: 0.794s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 128.4704
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.1391
                       Mean reward: 800.64
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7254
     Episode_Reward/lifting_object: 157.8494
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.90s
                      Time elapsed: 00:07:04
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 112980 steps/s (collection: 0.761s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 164.7121
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1447
                       Mean reward: 787.35
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7126
     Episode_Reward/lifting_object: 157.6384
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.87s
                      Time elapsed: 00:07:05
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 110637 steps/s (collection: 0.761s, learning 0.127s)
             Mean action noise std: 1.87
          Mean value_function loss: 159.7723
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.1502
                       Mean reward: 763.06
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7117
     Episode_Reward/lifting_object: 155.7487
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.89s
                      Time elapsed: 00:07:05
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 111443 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 161.7506
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.1578
                       Mean reward: 803.57
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.6944
     Episode_Reward/lifting_object: 152.5721
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.88s
                      Time elapsed: 00:07:06
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 112115 steps/s (collection: 0.788s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 158.3716
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.1612
                       Mean reward: 787.78
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 152.6531
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.88s
                      Time elapsed: 00:07:07
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 110160 steps/s (collection: 0.801s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 160.2447
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.1633
                       Mean reward: 739.45
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 0.6812
     Episode_Reward/lifting_object: 148.6656
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.89s
                      Time elapsed: 00:07:08
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 116786 steps/s (collection: 0.754s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 132.7999
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1639
                       Mean reward: 812.68
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.6920
     Episode_Reward/lifting_object: 151.2759
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.84s
                      Time elapsed: 00:07:09
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 110541 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 137.4010
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.1642
                       Mean reward: 765.52
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 0.6939
     Episode_Reward/lifting_object: 150.7531
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.89s
                      Time elapsed: 00:07:10
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 115142 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 134.2085
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.1663
                       Mean reward: 782.02
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.7075
     Episode_Reward/lifting_object: 156.0167
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.85s
                      Time elapsed: 00:07:11
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 109604 steps/s (collection: 0.786s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 114.7775
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 16.1689
                       Mean reward: 775.96
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7070
     Episode_Reward/lifting_object: 155.2954
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.90s
                      Time elapsed: 00:07:12
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 106402 steps/s (collection: 0.799s, learning 0.125s)
             Mean action noise std: 1.87
          Mean value_function loss: 133.0481
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1696
                       Mean reward: 758.54
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.6994
     Episode_Reward/lifting_object: 151.9679
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.92s
                      Time elapsed: 00:07:13
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 111080 steps/s (collection: 0.792s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 123.3908
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.1723
                       Mean reward: 761.48
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.6739
     Episode_Reward/lifting_object: 145.1744
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.88s
                      Time elapsed: 00:07:13
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 115058 steps/s (collection: 0.761s, learning 0.093s)
             Mean action noise std: 1.88
          Mean value_function loss: 96.6943
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 16.1763
                       Mean reward: 766.19
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.6853
     Episode_Reward/lifting_object: 149.5403
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.85s
                      Time elapsed: 00:07:14
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 111605 steps/s (collection: 0.781s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 118.6548
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.1800
                       Mean reward: 731.06
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.6764
     Episode_Reward/lifting_object: 147.4736
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.88s
                      Time elapsed: 00:07:15
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 101576 steps/s (collection: 0.797s, learning 0.170s)
             Mean action noise std: 1.88
          Mean value_function loss: 116.7886
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1827
                       Mean reward: 768.92
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.6891
     Episode_Reward/lifting_object: 148.8716
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.97s
                      Time elapsed: 00:07:16
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 99095 steps/s (collection: 0.876s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 116.7332
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.1834
                       Mean reward: 751.54
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: 156.3367
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.99s
                      Time elapsed: 00:07:17
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 112263 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 107.6072
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1832
                       Mean reward: 786.24
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.6987
     Episode_Reward/lifting_object: 152.2648
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.88s
                      Time elapsed: 00:07:18
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 106830 steps/s (collection: 0.815s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 117.4289
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.1830
                       Mean reward: 770.37
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7030
     Episode_Reward/lifting_object: 153.6157
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.92s
                      Time elapsed: 00:07:19
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 109352 steps/s (collection: 0.775s, learning 0.124s)
             Mean action noise std: 1.88
          Mean value_function loss: 113.5957
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.1867
                       Mean reward: 824.72
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.7140
     Episode_Reward/lifting_object: 156.9472
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.90s
                      Time elapsed: 00:07:20
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 110531 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 1.88
          Mean value_function loss: 106.8735
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 16.1889
                       Mean reward: 799.74
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7166
     Episode_Reward/lifting_object: 158.0655
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.89s
                      Time elapsed: 00:07:21
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 104558 steps/s (collection: 0.798s, learning 0.142s)
             Mean action noise std: 1.88
          Mean value_function loss: 127.1396
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1913
                       Mean reward: 798.64
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: 155.0781
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.94s
                      Time elapsed: 00:07:22
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 106480 steps/s (collection: 0.782s, learning 0.141s)
             Mean action noise std: 1.88
          Mean value_function loss: 118.7482
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1929
                       Mean reward: 804.68
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 159.9162
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.92s
                      Time elapsed: 00:07:23
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 107919 steps/s (collection: 0.780s, learning 0.131s)
             Mean action noise std: 1.88
          Mean value_function loss: 115.0921
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 16.1954
                       Mean reward: 793.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 157.5867
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.91s
                      Time elapsed: 00:07:23
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 109251 steps/s (collection: 0.778s, learning 0.122s)
             Mean action noise std: 1.88
          Mean value_function loss: 109.4347
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 16.2032
                       Mean reward: 802.35
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7193
     Episode_Reward/lifting_object: 157.8287
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.90s
                      Time elapsed: 00:07:24
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 109513 steps/s (collection: 0.798s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 101.0656
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 16.2085
                       Mean reward: 785.37
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7187
     Episode_Reward/lifting_object: 158.1355
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.90s
                      Time elapsed: 00:07:25
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 112001 steps/s (collection: 0.791s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 109.1789
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.2101
                       Mean reward: 819.11
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7266
     Episode_Reward/lifting_object: 159.5873
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.88s
                      Time elapsed: 00:07:26
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 99310 steps/s (collection: 0.873s, learning 0.117s)
             Mean action noise std: 1.88
          Mean value_function loss: 121.6705
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.2118
                       Mean reward: 804.05
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 157.5399
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.99s
                      Time elapsed: 00:07:27
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 97038 steps/s (collection: 0.887s, learning 0.126s)
             Mean action noise std: 1.89
          Mean value_function loss: 109.4144
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.2126
                       Mean reward: 751.81
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.7166
     Episode_Reward/lifting_object: 156.0016
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 1.01s
                      Time elapsed: 00:07:28
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 111333 steps/s (collection: 0.778s, learning 0.105s)
             Mean action noise std: 1.89
          Mean value_function loss: 117.1741
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 16.2163
                       Mean reward: 835.71
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 161.3728
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.88s
                      Time elapsed: 00:07:29
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 105307 steps/s (collection: 0.802s, learning 0.132s)
             Mean action noise std: 1.89
          Mean value_function loss: 106.6909
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2178
                       Mean reward: 768.78
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 157.2041
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.93s
                      Time elapsed: 00:07:30
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 111524 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 120.4596
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2198
                       Mean reward: 784.01
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: 154.3880
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.88s
                      Time elapsed: 00:07:31
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 110032 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 121.4522
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.2221
                       Mean reward: 819.87
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7198
     Episode_Reward/lifting_object: 158.3907
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.89s
                      Time elapsed: 00:07:32
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 110345 steps/s (collection: 0.785s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 130.6917
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.2250
                       Mean reward: 786.73
               Mean episode length: 245.02
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 155.6749
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.89s
                      Time elapsed: 00:07:33
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 110352 steps/s (collection: 0.766s, learning 0.125s)
             Mean action noise std: 1.89
          Mean value_function loss: 122.9939
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2294
                       Mean reward: 749.45
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 152.5204
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.89s
                      Time elapsed: 00:07:34
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 110821 steps/s (collection: 0.773s, learning 0.114s)
             Mean action noise std: 1.89
          Mean value_function loss: 120.9262
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.2325
                       Mean reward: 773.16
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7023
     Episode_Reward/lifting_object: 153.7396
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.89s
                      Time elapsed: 00:07:34
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 108833 steps/s (collection: 0.781s, learning 0.122s)
             Mean action noise std: 1.89
          Mean value_function loss: 108.2812
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 16.2358
                       Mean reward: 776.81
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.6967
     Episode_Reward/lifting_object: 152.5567
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.90s
                      Time elapsed: 00:07:35
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 111576 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 113.4494
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.2413
                       Mean reward: 774.80
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.7098
     Episode_Reward/lifting_object: 155.9597
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.88s
                      Time elapsed: 00:07:36
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 89129 steps/s (collection: 1.000s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 97.6432
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2481
                       Mean reward: 807.26
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7167
     Episode_Reward/lifting_object: 156.5506
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 1.10s
                      Time elapsed: 00:07:37
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 113596 steps/s (collection: 0.761s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 118.2568
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 16.2564
                       Mean reward: 816.88
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 161.5008
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.87s
                      Time elapsed: 00:07:38
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 111704 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 119.7863
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.2637
                       Mean reward: 807.86
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7112
     Episode_Reward/lifting_object: 156.8052
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.88s
                      Time elapsed: 00:07:39
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 115249 steps/s (collection: 0.759s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 106.1716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2693
                       Mean reward: 798.16
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 160.6668
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.85s
                      Time elapsed: 00:07:40
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 111972 steps/s (collection: 0.777s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 106.0858
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.2738
                       Mean reward: 789.63
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7211
     Episode_Reward/lifting_object: 159.1173
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.88s
                      Time elapsed: 00:07:41
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 99460 steps/s (collection: 0.825s, learning 0.164s)
             Mean action noise std: 1.90
          Mean value_function loss: 98.0075
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.2764
                       Mean reward: 820.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7143
     Episode_Reward/lifting_object: 156.7858
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.99s
                      Time elapsed: 00:07:42
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 105154 steps/s (collection: 0.820s, learning 0.115s)
             Mean action noise std: 1.90
          Mean value_function loss: 84.9809
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 16.2791
                       Mean reward: 798.64
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 159.4783
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.93s
                      Time elapsed: 00:07:43
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 114069 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 1.90
          Mean value_function loss: 90.0353
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 16.2802
                       Mean reward: 784.19
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 159.9314
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.86s
                      Time elapsed: 00:07:44
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 113687 steps/s (collection: 0.756s, learning 0.109s)
             Mean action noise std: 1.90
          Mean value_function loss: 84.0999
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.2813
                       Mean reward: 794.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 156.3690
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.86s
                      Time elapsed: 00:07:44
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 105784 steps/s (collection: 0.788s, learning 0.141s)
             Mean action noise std: 1.90
          Mean value_function loss: 98.1930
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.2877
                       Mean reward: 829.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 160.2145
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.93s
                      Time elapsed: 00:07:45
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 114499 steps/s (collection: 0.761s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 96.5322
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.2946
                       Mean reward: 807.31
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7273
     Episode_Reward/lifting_object: 160.5479
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.86s
                      Time elapsed: 00:07:46
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 106741 steps/s (collection: 0.836s, learning 0.085s)
             Mean action noise std: 1.91
          Mean value_function loss: 78.5774
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.2968
                       Mean reward: 811.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7119
     Episode_Reward/lifting_object: 157.7999
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.92s
                      Time elapsed: 00:07:47
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 114621 steps/s (collection: 0.767s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 92.7023
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.3008
                       Mean reward: 808.69
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: 160.4608
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.86s
                      Time elapsed: 00:07:48
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 109433 steps/s (collection: 0.808s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 91.1671
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3047
                       Mean reward: 815.44
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7285
     Episode_Reward/lifting_object: 159.4973
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.90s
                      Time elapsed: 00:07:49
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 108802 steps/s (collection: 0.820s, learning 0.084s)
             Mean action noise std: 1.91
          Mean value_function loss: 104.1437
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.3074
                       Mean reward: 823.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 161.4729
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.90s
                      Time elapsed: 00:07:50
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 116435 steps/s (collection: 0.752s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 101.5150
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3112
                       Mean reward: 799.67
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7311
     Episode_Reward/lifting_object: 160.6013
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.84s
                      Time elapsed: 00:07:51
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 109450 steps/s (collection: 0.766s, learning 0.132s)
             Mean action noise std: 1.91
          Mean value_function loss: 98.7240
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 16.3152
                       Mean reward: 827.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 164.2974
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.90s
                      Time elapsed: 00:07:52
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 104140 steps/s (collection: 0.835s, learning 0.109s)
             Mean action noise std: 1.91
          Mean value_function loss: 94.0074
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 16.3185
                       Mean reward: 784.23
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7195
     Episode_Reward/lifting_object: 157.4456
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.94s
                      Time elapsed: 00:07:52
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 116680 steps/s (collection: 0.753s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 100.1757
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 16.3233
                       Mean reward: 783.66
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7214
     Episode_Reward/lifting_object: 157.3203
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.84s
                      Time elapsed: 00:07:53
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 110522 steps/s (collection: 0.791s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 99.1602
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 16.3319
                       Mean reward: 791.65
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.7310
     Episode_Reward/lifting_object: 161.8472
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.89s
                      Time elapsed: 00:07:54
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 112860 steps/s (collection: 0.776s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 93.5065
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.3358
                       Mean reward: 848.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 163.1760
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.87s
                      Time elapsed: 00:07:55
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 114266 steps/s (collection: 0.774s, learning 0.086s)
             Mean action noise std: 1.92
          Mean value_function loss: 77.5000
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3386
                       Mean reward: 816.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7274
     Episode_Reward/lifting_object: 159.1080
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.86s
                      Time elapsed: 00:07:56
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 113193 steps/s (collection: 0.784s, learning 0.084s)
             Mean action noise std: 1.92
          Mean value_function loss: 90.2129
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3412
                       Mean reward: 787.71
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: 161.5899
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.87s
                      Time elapsed: 00:07:57
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 114712 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 86.4663
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.3463
                       Mean reward: 810.24
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7245
     Episode_Reward/lifting_object: 158.7013
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.86s
                      Time elapsed: 00:07:58
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 108558 steps/s (collection: 0.795s, learning 0.111s)
             Mean action noise std: 1.92
          Mean value_function loss: 72.6591
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.3511
                       Mean reward: 803.78
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 159.9261
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.91s
                      Time elapsed: 00:07:59
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 108702 steps/s (collection: 0.764s, learning 0.141s)
             Mean action noise std: 1.92
          Mean value_function loss: 69.9778
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3544
                       Mean reward: 809.67
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7292
     Episode_Reward/lifting_object: 160.3679
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.90s
                      Time elapsed: 00:07:59
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 110889 steps/s (collection: 0.755s, learning 0.132s)
             Mean action noise std: 1.92
          Mean value_function loss: 74.9977
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 16.3584
                       Mean reward: 787.32
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7316
     Episode_Reward/lifting_object: 160.9597
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.89s
                      Time elapsed: 00:08:00
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 109566 steps/s (collection: 0.791s, learning 0.107s)
             Mean action noise std: 1.92
          Mean value_function loss: 68.2943
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.3599
                       Mean reward: 820.89
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 162.9269
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.90s
                      Time elapsed: 00:08:01
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 94674 steps/s (collection: 0.846s, learning 0.192s)
             Mean action noise std: 1.92
          Mean value_function loss: 60.6805
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.3618
                       Mean reward: 812.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7301
     Episode_Reward/lifting_object: 159.4323
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 1.04s
                      Time elapsed: 00:08:02
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 109346 steps/s (collection: 0.799s, learning 0.100s)
             Mean action noise std: 1.92
          Mean value_function loss: 67.1035
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.3638
                       Mean reward: 842.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7394
     Episode_Reward/lifting_object: 161.0402
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.90s
                      Time elapsed: 00:08:03
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 107451 steps/s (collection: 0.805s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 55.5947
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.3674
                       Mean reward: 842.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 164.1011
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.91s
                      Time elapsed: 00:08:04
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 110959 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 1.92
          Mean value_function loss: 74.5653
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.3717
                       Mean reward: 822.30
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 164.5652
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.89s
                      Time elapsed: 00:08:05
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 112432 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 72.0122
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3740
                       Mean reward: 790.94
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.7351
     Episode_Reward/lifting_object: 162.7237
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.87s
                      Time elapsed: 00:08:06
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 112875 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 69.2309
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.3749
                       Mean reward: 823.35
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 164.7715
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.87s
                      Time elapsed: 00:08:07
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 103546 steps/s (collection: 0.833s, learning 0.117s)
             Mean action noise std: 1.93
          Mean value_function loss: 68.0460
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 16.3758
                       Mean reward: 840.66
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 164.6843
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.95s
                      Time elapsed: 00:08:08
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 103791 steps/s (collection: 0.794s, learning 0.153s)
             Mean action noise std: 1.93
          Mean value_function loss: 70.6047
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3779
                       Mean reward: 830.98
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 159.5555
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.95s
                      Time elapsed: 00:08:09
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 100361 steps/s (collection: 0.879s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 69.6762
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.3843
                       Mean reward: 844.96
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 165.4252
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.98s
                      Time elapsed: 00:08:10
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 105159 steps/s (collection: 0.800s, learning 0.135s)
             Mean action noise std: 1.93
          Mean value_function loss: 77.2984
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3900
                       Mean reward: 793.89
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 162.6597
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.93s
                      Time elapsed: 00:08:11
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 97813 steps/s (collection: 0.886s, learning 0.119s)
             Mean action noise std: 1.93
          Mean value_function loss: 59.5704
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3942
                       Mean reward: 808.03
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7394
     Episode_Reward/lifting_object: 162.8420
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 1.01s
                      Time elapsed: 00:08:12
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 102316 steps/s (collection: 0.851s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 66.4211
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.3984
                       Mean reward: 816.93
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 167.8615
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.96s
                      Time elapsed: 00:08:13
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 101579 steps/s (collection: 0.845s, learning 0.123s)
             Mean action noise std: 1.93
          Mean value_function loss: 63.8030
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.4044
                       Mean reward: 836.81
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 164.5084
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.97s
                      Time elapsed: 00:08:14
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 107330 steps/s (collection: 0.818s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 64.9930
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.4125
                       Mean reward: 815.97
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 166.8075
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.92s
                      Time elapsed: 00:08:14
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 108095 steps/s (collection: 0.760s, learning 0.150s)
             Mean action noise std: 1.94
          Mean value_function loss: 59.0448
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 16.4171
                       Mean reward: 832.75
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 165.7310
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.91s
                      Time elapsed: 00:08:15
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 110595 steps/s (collection: 0.796s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 76.7380
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4200
                       Mean reward: 819.14
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 164.2182
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.89s
                      Time elapsed: 00:08:16
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 105780 steps/s (collection: 0.785s, learning 0.144s)
             Mean action noise std: 1.94
          Mean value_function loss: 65.3280
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.4248
                       Mean reward: 837.03
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 165.6682
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.93s
                      Time elapsed: 00:08:17
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 104536 steps/s (collection: 0.841s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 74.3844
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.4298
                       Mean reward: 838.27
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 167.1069
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.94s
                      Time elapsed: 00:08:18
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 112276 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 70.8794
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.4374
                       Mean reward: 815.16
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7415
     Episode_Reward/lifting_object: 163.7880
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.88s
                      Time elapsed: 00:08:19
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 113831 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 59.5166
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4432
                       Mean reward: 827.80
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 163.5501
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.86s
                      Time elapsed: 00:08:20
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 113065 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 1.94
          Mean value_function loss: 67.4473
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.4479
                       Mean reward: 823.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 163.9373
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.87s
                      Time elapsed: 00:08:21
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 112815 steps/s (collection: 0.747s, learning 0.125s)
             Mean action noise std: 1.95
          Mean value_function loss: 59.6874
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.4530
                       Mean reward: 856.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 164.7582
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.87s
                      Time elapsed: 00:08:22
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 115464 steps/s (collection: 0.761s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 67.4352
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.4577
                       Mean reward: 836.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 165.7516
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.85s
                      Time elapsed: 00:08:22
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 106641 steps/s (collection: 0.768s, learning 0.154s)
             Mean action noise std: 1.95
          Mean value_function loss: 57.0644
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4629
                       Mean reward: 856.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 167.1045
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.92s
                      Time elapsed: 00:08:23
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 114219 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 63.4489
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.4695
                       Mean reward: 825.43
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 166.3807
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.86s
                      Time elapsed: 00:08:24
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 111820 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 54.9229
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4753
                       Mean reward: 848.70
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 164.8890
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.88s
                      Time elapsed: 00:08:25
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 116687 steps/s (collection: 0.758s, learning 0.084s)
             Mean action noise std: 1.95
          Mean value_function loss: 53.5036
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.4808
                       Mean reward: 835.45
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 165.8910
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.84s
                      Time elapsed: 00:08:26
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 109815 steps/s (collection: 0.794s, learning 0.101s)
             Mean action noise std: 1.95
          Mean value_function loss: 47.8217
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4835
                       Mean reward: 840.64
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 167.3099
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.90s
                      Time elapsed: 00:08:27
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 112057 steps/s (collection: 0.759s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 52.3773
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4835
                       Mean reward: 850.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 167.7137
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.88s
                      Time elapsed: 00:08:28
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 113207 steps/s (collection: 0.769s, learning 0.100s)
             Mean action noise std: 1.96
          Mean value_function loss: 67.0188
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4874
                       Mean reward: 823.21
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 165.5906
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.87s
                      Time elapsed: 00:08:29
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 113394 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 62.5518
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.4945
                       Mean reward: 851.57
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 166.6124
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.87s
                      Time elapsed: 00:08:29
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 109741 steps/s (collection: 0.747s, learning 0.149s)
             Mean action noise std: 1.96
          Mean value_function loss: 57.1273
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.4989
                       Mean reward: 838.25
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 163.0660
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.90s
                      Time elapsed: 00:08:30
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 111939 steps/s (collection: 0.787s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 47.4958
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5047
                       Mean reward: 806.39
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 164.8737
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.88s
                      Time elapsed: 00:08:31
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 108121 steps/s (collection: 0.769s, learning 0.141s)
             Mean action noise std: 1.96
          Mean value_function loss: 49.1881
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.5106
                       Mean reward: 828.73
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 166.0328
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.91s
                      Time elapsed: 00:08:32
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 105525 steps/s (collection: 0.837s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 58.2033
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5148
                       Mean reward: 833.43
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 165.3629
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.93s
                      Time elapsed: 00:08:33
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 111782 steps/s (collection: 0.793s, learning 0.087s)
             Mean action noise std: 1.97
          Mean value_function loss: 60.0598
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.5204
                       Mean reward: 839.21
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 166.4466
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.88s
                      Time elapsed: 00:08:34
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 113072 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 55.0851
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.5255
                       Mean reward: 838.64
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 166.7586
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.87s
                      Time elapsed: 00:08:35
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 115259 steps/s (collection: 0.764s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 49.9849
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5325
                       Mean reward: 846.93
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 165.3360
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.85s
                      Time elapsed: 00:08:36
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 109108 steps/s (collection: 0.793s, learning 0.108s)
             Mean action noise std: 1.97
          Mean value_function loss: 57.0989
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.5363
                       Mean reward: 825.30
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 165.0741
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.90s
                      Time elapsed: 00:08:37
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 109703 steps/s (collection: 0.764s, learning 0.132s)
             Mean action noise std: 1.97
          Mean value_function loss: 60.0525
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.5380
                       Mean reward: 830.26
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 167.7465
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.90s
                      Time elapsed: 00:08:37
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 109213 steps/s (collection: 0.773s, learning 0.127s)
             Mean action noise std: 1.97
          Mean value_function loss: 62.6533
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.5402
                       Mean reward: 829.11
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 168.4716
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.90s
                      Time elapsed: 00:08:38
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 117964 steps/s (collection: 0.748s, learning 0.086s)
             Mean action noise std: 1.97
          Mean value_function loss: 79.7406
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.5471
                       Mean reward: 795.40
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 165.9454
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.83s
                      Time elapsed: 00:08:39
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 112592 steps/s (collection: 0.789s, learning 0.084s)
             Mean action noise std: 1.98
          Mean value_function loss: 66.2062
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.5591
                       Mean reward: 828.22
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 165.7899
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.87s
                      Time elapsed: 00:08:40
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 116660 steps/s (collection: 0.752s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 82.7759
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.5692
                       Mean reward: 833.98
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 167.0204
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.84s
                      Time elapsed: 00:08:41
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 113872 steps/s (collection: 0.776s, learning 0.087s)
             Mean action noise std: 1.98
          Mean value_function loss: 72.9094
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.5729
                       Mean reward: 821.61
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 163.7003
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.86s
                      Time elapsed: 00:08:42
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 114005 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 65.6374
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.5801
                       Mean reward: 827.56
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 164.9622
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.86s
                      Time elapsed: 00:08:43
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 110536 steps/s (collection: 0.762s, learning 0.127s)
             Mean action noise std: 1.98
          Mean value_function loss: 68.2814
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5908
                       Mean reward: 834.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 167.3217
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.89s
                      Time elapsed: 00:08:44
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 110450 steps/s (collection: 0.769s, learning 0.121s)
             Mean action noise std: 1.98
          Mean value_function loss: 60.6515
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.5997
                       Mean reward: 833.00
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 165.7653
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.89s
                      Time elapsed: 00:08:44
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 117405 steps/s (collection: 0.744s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 63.9244
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.6074
                       Mean reward: 831.68
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 166.2110
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.84s
                      Time elapsed: 00:08:45
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 117406 steps/s (collection: 0.753s, learning 0.084s)
             Mean action noise std: 1.99
          Mean value_function loss: 54.7225
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.6163
                       Mean reward: 850.25
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 166.3138
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.84s
                      Time elapsed: 00:08:46
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 114128 steps/s (collection: 0.774s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 50.2606
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 16.6265
                       Mean reward: 837.03
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 163.4197
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.86s
                      Time elapsed: 00:08:47
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 117469 steps/s (collection: 0.749s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 59.6864
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 16.6314
                       Mean reward: 846.90
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 166.1587
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.84s
                      Time elapsed: 00:08:48
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 112216 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 54.2019
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.6363
                       Mean reward: 830.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7353
     Episode_Reward/lifting_object: 162.3944
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.88s
                      Time elapsed: 00:08:49
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 116551 steps/s (collection: 0.742s, learning 0.102s)
             Mean action noise std: 2.00
          Mean value_function loss: 41.8743
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.6436
                       Mean reward: 852.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 165.4891
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.84s
                      Time elapsed: 00:08:49
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 108754 steps/s (collection: 0.751s, learning 0.153s)
             Mean action noise std: 2.00
          Mean value_function loss: 42.5524
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.6501
                       Mean reward: 846.40
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 167.7143
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.90s
                      Time elapsed: 00:08:50
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 109612 steps/s (collection: 0.773s, learning 0.124s)
             Mean action noise std: 2.00
          Mean value_function loss: 38.3217
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.6579
                       Mean reward: 811.37
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 166.2098
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.90s
                      Time elapsed: 00:08:51
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 114388 steps/s (collection: 0.763s, learning 0.097s)
             Mean action noise std: 2.00
          Mean value_function loss: 54.0138
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.6665
                       Mean reward: 851.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 169.0359
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.86s
                      Time elapsed: 00:08:52
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 113065 steps/s (collection: 0.778s, learning 0.091s)
             Mean action noise std: 2.00
          Mean value_function loss: 63.6369
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6766
                       Mean reward: 840.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 167.9672
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.87s
                      Time elapsed: 00:08:53
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 112258 steps/s (collection: 0.781s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 61.0360
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6813
                       Mean reward: 855.27
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 168.9508
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.88s
                      Time elapsed: 00:08:54
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 105225 steps/s (collection: 0.818s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 50.5896
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.6847
                       Mean reward: 834.20
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 167.3710
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.93s
                      Time elapsed: 00:08:55
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 112303 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 53.2433
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 16.6915
                       Mean reward: 837.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 166.9386
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.88s
                      Time elapsed: 00:08:56
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 111218 steps/s (collection: 0.761s, learning 0.123s)
             Mean action noise std: 2.01
          Mean value_function loss: 46.0764
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6955
                       Mean reward: 842.34
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 167.4982
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.88s
                      Time elapsed: 00:08:57
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 112430 steps/s (collection: 0.747s, learning 0.128s)
             Mean action noise std: 2.01
          Mean value_function loss: 51.8482
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6983
                       Mean reward: 851.53
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 167.4260
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.87s
                      Time elapsed: 00:08:57
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 115454 steps/s (collection: 0.765s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 47.3450
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.7069
                       Mean reward: 851.31
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 165.9783
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.85s
                      Time elapsed: 00:08:58
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 115409 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 2.02
          Mean value_function loss: 52.8274
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 16.7168
                       Mean reward: 844.24
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 167.3754
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.85s
                      Time elapsed: 00:08:59
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 118032 steps/s (collection: 0.748s, learning 0.085s)
             Mean action noise std: 2.02
          Mean value_function loss: 44.6043
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.7284
                       Mean reward: 822.46
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 165.4635
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.83s
                      Time elapsed: 00:09:00
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 113578 steps/s (collection: 0.768s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 40.3733
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 16.7454
                       Mean reward: 856.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 167.4183
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.87s
                      Time elapsed: 00:09:01
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 107628 steps/s (collection: 0.826s, learning 0.088s)
             Mean action noise std: 2.03
          Mean value_function loss: 44.0695
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.7581
                       Mean reward: 838.89
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 168.5505
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.91s
                      Time elapsed: 00:09:02
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 112272 steps/s (collection: 0.768s, learning 0.108s)
             Mean action noise std: 2.03
          Mean value_function loss: 51.7532
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7697
                       Mean reward: 853.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 168.4398
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.88s
                      Time elapsed: 00:09:03
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 108024 steps/s (collection: 0.808s, learning 0.102s)
             Mean action noise std: 2.04
          Mean value_function loss: 61.5015
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7870
                       Mean reward: 856.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 169.2329
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.91s
                      Time elapsed: 00:09:04
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 113542 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 60.3348
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8090
                       Mean reward: 851.01
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 169.5033
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.87s
                      Time elapsed: 00:09:04
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 110735 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 2.04
          Mean value_function loss: 54.3315
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.8153
                       Mean reward: 837.92
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 166.8833
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.89s
                      Time elapsed: 00:09:05
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 108275 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 62.6354
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.8198
                       Mean reward: 801.26
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 166.6969
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.91s
                      Time elapsed: 00:09:06
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 112948 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 57.7296
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.8235
                       Mean reward: 826.88
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 166.5532
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.87s
                      Time elapsed: 00:09:07
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 109452 steps/s (collection: 0.784s, learning 0.115s)
             Mean action noise std: 2.04
          Mean value_function loss: 57.8939
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8262
                       Mean reward: 837.93
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 165.1348
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.90s
                      Time elapsed: 00:09:08
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 113924 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 56.1219
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.8362
                       Mean reward: 845.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 166.6882
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.86s
                      Time elapsed: 00:09:09
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 112752 steps/s (collection: 0.762s, learning 0.110s)
             Mean action noise std: 2.05
          Mean value_function loss: 58.0205
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.8552
                       Mean reward: 826.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 168.0189
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.87s
                      Time elapsed: 00:09:10
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 112030 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 2.05
          Mean value_function loss: 63.3827
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.8641
                       Mean reward: 855.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 167.6767
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.88s
                      Time elapsed: 00:09:11
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 115668 steps/s (collection: 0.759s, learning 0.091s)
             Mean action noise std: 2.06
          Mean value_function loss: 49.5949
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8738
                       Mean reward: 822.55
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 165.9671
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.85s
                      Time elapsed: 00:09:11
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 114933 steps/s (collection: 0.742s, learning 0.113s)
             Mean action noise std: 2.06
          Mean value_function loss: 73.6548
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 16.8868
                       Mean reward: 837.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 164.5035
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.86s
                      Time elapsed: 00:09:12
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 114068 steps/s (collection: 0.769s, learning 0.093s)
             Mean action noise std: 2.06
          Mean value_function loss: 68.6345
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.8962
                       Mean reward: 841.92
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 167.7264
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.86s
                      Time elapsed: 00:09:13
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 116347 steps/s (collection: 0.749s, learning 0.096s)
             Mean action noise std: 2.06
          Mean value_function loss: 47.6354
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.9036
                       Mean reward: 817.77
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 166.5020
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.84s
                      Time elapsed: 00:09:14
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 107690 steps/s (collection: 0.795s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 61.7753
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.9102
                       Mean reward: 857.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 168.2790
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.91s
                      Time elapsed: 00:09:15
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 112141 steps/s (collection: 0.762s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 68.2329
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9219
                       Mean reward: 825.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 167.8329
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.88s
                      Time elapsed: 00:09:16
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 105237 steps/s (collection: 0.804s, learning 0.131s)
             Mean action noise std: 2.07
          Mean value_function loss: 64.6745
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9361
                       Mean reward: 838.09
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 167.2877
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.93s
                      Time elapsed: 00:09:17
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 110011 steps/s (collection: 0.808s, learning 0.086s)
             Mean action noise std: 2.08
          Mean value_function loss: 48.8737
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9551
                       Mean reward: 839.99
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 168.5192
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.89s
                      Time elapsed: 00:09:18
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 113173 steps/s (collection: 0.771s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 69.1634
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.9721
                       Mean reward: 849.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 166.8708
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.87s
                      Time elapsed: 00:09:19
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 113461 steps/s (collection: 0.751s, learning 0.116s)
             Mean action noise std: 2.08
          Mean value_function loss: 69.6848
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 16.9856
                       Mean reward: 861.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 167.2516
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.87s
                      Time elapsed: 00:09:19
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 111928 steps/s (collection: 0.759s, learning 0.120s)
             Mean action noise std: 2.09
          Mean value_function loss: 57.7790
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.9919
                       Mean reward: 812.33
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 164.5941
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.88s
                      Time elapsed: 00:09:20
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 112614 steps/s (collection: 0.766s, learning 0.107s)
             Mean action noise std: 2.09
          Mean value_function loss: 69.2891
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.0037
                       Mean reward: 834.01
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 163.1301
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.87s
                      Time elapsed: 00:09:21
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 100220 steps/s (collection: 0.872s, learning 0.109s)
             Mean action noise std: 2.09
          Mean value_function loss: 70.0336
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0141
                       Mean reward: 823.36
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 164.1293
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.98s
                      Time elapsed: 00:09:22
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 108982 steps/s (collection: 0.807s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 55.4490
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 17.0281
                       Mean reward: 845.27
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 164.1833
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.90s
                      Time elapsed: 00:09:23
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 112871 steps/s (collection: 0.781s, learning 0.090s)
             Mean action noise std: 2.10
          Mean value_function loss: 67.4989
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0347
                       Mean reward: 842.72
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 167.1222
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.87s
                      Time elapsed: 00:09:24
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 115381 steps/s (collection: 0.764s, learning 0.088s)
             Mean action noise std: 2.10
          Mean value_function loss: 68.2121
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0478
                       Mean reward: 843.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 166.3761
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.85s
                      Time elapsed: 00:09:25
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 111470 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 66.5452
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.0771
                       Mean reward: 852.78
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 167.5796
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.88s
                      Time elapsed: 00:09:26
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 112501 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 76.8991
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0927
                       Mean reward: 839.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 164.6090
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.87s
                      Time elapsed: 00:09:26
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 107316 steps/s (collection: 0.779s, learning 0.137s)
             Mean action noise std: 2.11
          Mean value_function loss: 80.1229
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1021
                       Mean reward: 766.38
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.7258
     Episode_Reward/lifting_object: 158.6816
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.92s
                      Time elapsed: 00:09:27
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 108622 steps/s (collection: 0.794s, learning 0.111s)
             Mean action noise std: 2.12
          Mean value_function loss: 81.6621
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.1196
                       Mean reward: 825.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 166.7079
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.91s
                      Time elapsed: 00:09:28
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 104506 steps/s (collection: 0.769s, learning 0.172s)
             Mean action noise std: 2.12
          Mean value_function loss: 82.3245
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1342
                       Mean reward: 839.80
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 164.5208
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.94s
                      Time elapsed: 00:09:29
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 107432 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 73.8023
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1506
                       Mean reward: 828.21
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 164.1134
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.92s
                      Time elapsed: 00:09:30
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 114600 steps/s (collection: 0.770s, learning 0.088s)
             Mean action noise std: 2.13
          Mean value_function loss: 70.9095
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.1626
                       Mean reward: 837.06
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 165.4081
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.86s
                      Time elapsed: 00:09:31
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 108534 steps/s (collection: 0.808s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 53.1507
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.1707
                       Mean reward: 850.15
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 166.7150
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.91s
                      Time elapsed: 00:09:32
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 110370 steps/s (collection: 0.804s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 65.0464
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.1828
                       Mean reward: 819.90
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 166.3270
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.89s
                      Time elapsed: 00:09:33
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 110157 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 65.6991
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1901
                       Mean reward: 846.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 168.6945
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.89s
                      Time elapsed: 00:09:34
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 107451 steps/s (collection: 0.814s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 82.6219
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.2031
                       Mean reward: 802.88
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 164.5761
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.91s
                      Time elapsed: 00:09:35
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 112588 steps/s (collection: 0.770s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 66.3584
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.2131
                       Mean reward: 826.84
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7384
     Episode_Reward/lifting_object: 163.9930
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.87s
                      Time elapsed: 00:09:35
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 104395 steps/s (collection: 0.795s, learning 0.147s)
             Mean action noise std: 2.15
          Mean value_function loss: 53.4668
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.2220
                       Mean reward: 816.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 159.9993
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.94s
                      Time elapsed: 00:09:36
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 110933 steps/s (collection: 0.772s, learning 0.114s)
             Mean action noise std: 2.15
          Mean value_function loss: 62.1848
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.2330
                       Mean reward: 846.77
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 165.6019
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.89s
                      Time elapsed: 00:09:37
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 107092 steps/s (collection: 0.775s, learning 0.143s)
             Mean action noise std: 2.15
          Mean value_function loss: 76.3056
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.2467
                       Mean reward: 805.27
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: 162.3710
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.92s
                      Time elapsed: 00:09:38
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 109425 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 64.3327
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.2570
                       Mean reward: 852.41
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 164.9149
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.90s
                      Time elapsed: 00:09:39
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 110346 steps/s (collection: 0.784s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 66.3791
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.2626
                       Mean reward: 835.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 166.8053
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.89s
                      Time elapsed: 00:09:40
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 104908 steps/s (collection: 0.820s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 79.0410
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.2748
                       Mean reward: 790.85
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7448
     Episode_Reward/lifting_object: 164.4998
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.94s
                      Time elapsed: 00:09:41
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 107217 steps/s (collection: 0.809s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 59.2745
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 17.2906
                       Mean reward: 840.53
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 166.5590
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.92s
                      Time elapsed: 00:09:42
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 111191 steps/s (collection: 0.796s, learning 0.088s)
             Mean action noise std: 2.17
          Mean value_function loss: 64.7867
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.3030
                       Mean reward: 805.18
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 161.6938
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.88s
                      Time elapsed: 00:09:43
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 111065 steps/s (collection: 0.786s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 65.2885
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.3126
                       Mean reward: 831.60
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7333
     Episode_Reward/lifting_object: 162.5552
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.89s
                      Time elapsed: 00:09:44
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 109404 steps/s (collection: 0.802s, learning 0.097s)
             Mean action noise std: 2.17
          Mean value_function loss: 74.5220
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.3229
                       Mean reward: 830.17
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 164.1753
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.90s
                      Time elapsed: 00:09:45
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 105303 steps/s (collection: 0.833s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 57.8646
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.3270
                       Mean reward: 820.62
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 164.3420
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.93s
                      Time elapsed: 00:09:45
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 110440 steps/s (collection: 0.771s, learning 0.119s)
             Mean action noise std: 2.17
          Mean value_function loss: 71.4496
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3328
                       Mean reward: 826.31
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 164.8259
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.89s
                      Time elapsed: 00:09:46
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 110142 steps/s (collection: 0.766s, learning 0.126s)
             Mean action noise std: 2.18
          Mean value_function loss: 55.8751
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3427
                       Mean reward: 838.97
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 166.4901
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.89s
                      Time elapsed: 00:09:47
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 106751 steps/s (collection: 0.802s, learning 0.119s)
             Mean action noise std: 2.18
          Mean value_function loss: 52.8137
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.3583
                       Mean reward: 835.11
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 165.1979
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.92s
                      Time elapsed: 00:09:48
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 111937 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 66.7917
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 17.3669
                       Mean reward: 840.09
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 166.7612
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.88s
                      Time elapsed: 00:09:49
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 108454 steps/s (collection: 0.813s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 62.1638
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3760
                       Mean reward: 831.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 166.3937
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.91s
                      Time elapsed: 00:09:50
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 110548 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 2.19
          Mean value_function loss: 50.3783
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.3875
                       Mean reward: 847.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7409
     Episode_Reward/lifting_object: 164.5844
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.89s
                      Time elapsed: 00:09:51
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 107137 steps/s (collection: 0.809s, learning 0.109s)
             Mean action noise std: 2.20
          Mean value_function loss: 60.9419
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4070
                       Mean reward: 843.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 166.0811
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.92s
                      Time elapsed: 00:09:52
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 105890 steps/s (collection: 0.801s, learning 0.128s)
             Mean action noise std: 2.20
          Mean value_function loss: 49.4002
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.4146
                       Mean reward: 844.86
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 166.0751
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.93s
                      Time elapsed: 00:09:53
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 109478 steps/s (collection: 0.796s, learning 0.102s)
             Mean action noise std: 2.20
          Mean value_function loss: 46.9276
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 17.4240
                       Mean reward: 844.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 167.7607
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.90s
                      Time elapsed: 00:09:54
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 104671 steps/s (collection: 0.817s, learning 0.123s)
             Mean action noise std: 2.21
          Mean value_function loss: 59.8799
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.4426
                       Mean reward: 810.49
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 166.4817
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.94s
                      Time elapsed: 00:09:55
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 108981 steps/s (collection: 0.794s, learning 0.108s)
             Mean action noise std: 2.21
          Mean value_function loss: 62.5294
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4593
                       Mean reward: 818.72
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 165.5672
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.90s
                      Time elapsed: 00:09:55
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 106589 steps/s (collection: 0.811s, learning 0.111s)
             Mean action noise std: 2.22
          Mean value_function loss: 62.6030
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.4755
                       Mean reward: 816.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 166.0569
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.92s
                      Time elapsed: 00:09:56
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 106763 steps/s (collection: 0.780s, learning 0.141s)
             Mean action noise std: 2.22
          Mean value_function loss: 62.7411
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4957
                       Mean reward: 820.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 164.4121
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.92s
                      Time elapsed: 00:09:57
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 113736 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 2.22
          Mean value_function loss: 53.7455
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.5088
                       Mean reward: 840.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 165.9401
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.86s
                      Time elapsed: 00:09:58
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 113121 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 2.23
          Mean value_function loss: 58.9650
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5160
                       Mean reward: 867.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 169.7585
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.87s
                      Time elapsed: 00:09:59
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 107603 steps/s (collection: 0.794s, learning 0.120s)
             Mean action noise std: 2.23
          Mean value_function loss: 67.0143
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.5215
                       Mean reward: 816.58
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 164.7596
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.91s
                      Time elapsed: 00:10:00
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 97523 steps/s (collection: 0.852s, learning 0.156s)
             Mean action noise std: 2.24
          Mean value_function loss: 69.4788
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.5416
                       Mean reward: 857.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 167.8774
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 1.01s
                      Time elapsed: 00:10:01
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 107517 steps/s (collection: 0.815s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 59.7445
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5646
                       Mean reward: 840.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 165.5891
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.91s
                      Time elapsed: 00:10:02
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 110071 steps/s (collection: 0.792s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 53.9382
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 17.5820
                       Mean reward: 832.54
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 165.5991
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.89s
                      Time elapsed: 00:10:03
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 108650 steps/s (collection: 0.799s, learning 0.106s)
             Mean action noise std: 2.25
          Mean value_function loss: 69.4740
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5943
                       Mean reward: 797.48
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7426
     Episode_Reward/lifting_object: 164.3154
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.90s
                      Time elapsed: 00:10:04
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 108622 steps/s (collection: 0.804s, learning 0.101s)
             Mean action noise std: 2.25
          Mean value_function loss: 61.7090
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.6151
                       Mean reward: 850.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 164.2520
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.91s
                      Time elapsed: 00:10:05
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 105259 steps/s (collection: 0.840s, learning 0.094s)
             Mean action noise std: 2.26
          Mean value_function loss: 54.8472
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6225
                       Mean reward: 815.03
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 166.1208
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.93s
                      Time elapsed: 00:10:06
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 107764 steps/s (collection: 0.791s, learning 0.122s)
             Mean action noise std: 2.26
          Mean value_function loss: 75.0062
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6348
                       Mean reward: 856.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 167.0324
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.91s
                      Time elapsed: 00:10:06
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 102648 steps/s (collection: 0.839s, learning 0.119s)
             Mean action noise std: 2.26
          Mean value_function loss: 54.6655
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6484
                       Mean reward: 821.43
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 164.9648
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.96s
                      Time elapsed: 00:10:07
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 102465 steps/s (collection: 0.831s, learning 0.129s)
             Mean action noise std: 2.27
          Mean value_function loss: 63.7936
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6546
                       Mean reward: 843.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 169.0689
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.96s
                      Time elapsed: 00:10:08
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 106388 steps/s (collection: 0.798s, learning 0.126s)
             Mean action noise std: 2.27
          Mean value_function loss: 63.4143
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.6614
                       Mean reward: 848.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 165.7936
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.92s
                      Time elapsed: 00:10:09
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 111661 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 60.9787
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6781
                       Mean reward: 873.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 166.6571
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.88s
                      Time elapsed: 00:10:10
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 109151 steps/s (collection: 0.791s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 61.6758
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.7096
                       Mean reward: 798.73
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 160.6230
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.90s
                      Time elapsed: 00:10:11
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 107544 steps/s (collection: 0.821s, learning 0.093s)
             Mean action noise std: 2.29
          Mean value_function loss: 55.9796
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7321
                       Mean reward: 849.57
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 166.7093
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.91s
                      Time elapsed: 00:10:12
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 106071 steps/s (collection: 0.828s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 61.3087
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7439
                       Mean reward: 807.53
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 163.8302
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.93s
                      Time elapsed: 00:10:13
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 105110 steps/s (collection: 0.823s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 61.5321
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.7622
                       Mean reward: 812.83
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 167.1093
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.94s
                      Time elapsed: 00:10:14
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 108090 steps/s (collection: 0.811s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 54.6724
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7759
                       Mean reward: 819.86
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 165.8475
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.91s
                      Time elapsed: 00:10:15
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 107109 steps/s (collection: 0.821s, learning 0.097s)
             Mean action noise std: 2.30
          Mean value_function loss: 57.1694
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7876
                       Mean reward: 841.57
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 165.8511
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.92s
                      Time elapsed: 00:10:16
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 108133 steps/s (collection: 0.805s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 47.9890
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7944
                       Mean reward: 836.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 167.6789
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.91s
                      Time elapsed: 00:10:17
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 108575 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 57.4948
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.8106
                       Mean reward: 833.89
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 166.8154
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.91s
                      Time elapsed: 00:10:17
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 104247 steps/s (collection: 0.827s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 46.6232
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 17.8310
                       Mean reward: 840.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 166.8352
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.94s
                      Time elapsed: 00:10:18
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 101301 steps/s (collection: 0.814s, learning 0.157s)
             Mean action noise std: 2.32
          Mean value_function loss: 60.1758
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 17.8414
                       Mean reward: 835.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 164.8360
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.97s
                      Time elapsed: 00:10:19
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 108401 steps/s (collection: 0.793s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 61.3285
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8530
                       Mean reward: 843.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 167.1607
      Episode_Reward/object_height: 0.0448
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.91s
                      Time elapsed: 00:10:20
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 111340 steps/s (collection: 0.791s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 62.6201
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.8671
                       Mean reward: 809.83
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 167.4305
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.88s
                      Time elapsed: 00:10:21
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 110976 steps/s (collection: 0.784s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 50.6339
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8775
                       Mean reward: 837.75
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 166.5553
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.89s
                      Time elapsed: 00:10:22
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 107158 steps/s (collection: 0.827s, learning 0.091s)
             Mean action noise std: 2.34
          Mean value_function loss: 48.6391
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8964
                       Mean reward: 858.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 167.1152
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.92s
                      Time elapsed: 00:10:23
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 112178 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 2.34
          Mean value_function loss: 50.0898
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.9111
                       Mean reward: 843.51
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 165.6939
      Episode_Reward/object_height: 0.0448
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.88s
                      Time elapsed: 00:10:24
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 106575 steps/s (collection: 0.820s, learning 0.103s)
             Mean action noise std: 2.35
          Mean value_function loss: 54.8005
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.9341
                       Mean reward: 843.03
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 167.2347
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.92s
                      Time elapsed: 00:10:25
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 110632 steps/s (collection: 0.788s, learning 0.100s)
             Mean action noise std: 2.36
          Mean value_function loss: 63.4698
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.9581
                       Mean reward: 821.61
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 167.1275
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.89s
                      Time elapsed: 00:10:26
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 106538 steps/s (collection: 0.829s, learning 0.094s)
             Mean action noise std: 2.36
          Mean value_function loss: 69.6763
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.9795
                       Mean reward: 861.52
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.0756
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.92s
                      Time elapsed: 00:10:27
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 108846 steps/s (collection: 0.810s, learning 0.093s)
             Mean action noise std: 2.37
          Mean value_function loss: 52.2118
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0010
                       Mean reward: 808.89
               Mean episode length: 245.02
    Episode_Reward/reaching_object: 0.7357
     Episode_Reward/lifting_object: 164.2503
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.90s
                      Time elapsed: 00:10:27
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 105193 steps/s (collection: 0.803s, learning 0.131s)
             Mean action noise std: 2.37
          Mean value_function loss: 60.1697
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0213
                       Mean reward: 816.65
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 164.5124
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.93s
                      Time elapsed: 00:10:28
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 108121 steps/s (collection: 0.813s, learning 0.096s)
             Mean action noise std: 2.38
          Mean value_function loss: 61.1279
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.0379
                       Mean reward: 843.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 167.1890
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.91s
                      Time elapsed: 00:10:29
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 109034 steps/s (collection: 0.777s, learning 0.124s)
             Mean action noise std: 2.38
          Mean value_function loss: 73.8230
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.0544
                       Mean reward: 852.99
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.1817
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.90s
                      Time elapsed: 00:10:30
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 106801 steps/s (collection: 0.791s, learning 0.129s)
             Mean action noise std: 2.39
          Mean value_function loss: 61.9662
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.0674
                       Mean reward: 859.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 169.9377
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.92s
                      Time elapsed: 00:10:31
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 109068 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 64.9812
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.0853
                       Mean reward: 832.13
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 166.8284
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.90s
                      Time elapsed: 00:10:32
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 105207 steps/s (collection: 0.842s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 78.7165
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.1032
                       Mean reward: 812.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 165.2624
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.93s
                      Time elapsed: 00:10:33
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 104593 steps/s (collection: 0.839s, learning 0.101s)
             Mean action noise std: 2.40
          Mean value_function loss: 80.7606
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 18.1218
                       Mean reward: 834.53
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 163.3105
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.94s
                      Time elapsed: 00:10:34
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 109077 steps/s (collection: 0.812s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 73.5558
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1275
                       Mean reward: 834.01
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7420
     Episode_Reward/lifting_object: 166.7630
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.90s
                      Time elapsed: 00:10:35
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 106269 steps/s (collection: 0.827s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 73.9321
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 18.1457
                       Mean reward: 834.96
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 167.3098
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.93s
                      Time elapsed: 00:10:36
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 106923 steps/s (collection: 0.810s, learning 0.109s)
             Mean action noise std: 2.42
          Mean value_function loss: 61.9379
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1566
                       Mean reward: 868.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 169.9199
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.92s
                      Time elapsed: 00:10:37
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 106534 steps/s (collection: 0.824s, learning 0.099s)
             Mean action noise std: 2.42
          Mean value_function loss: 70.0148
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1729
                       Mean reward: 845.84
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 166.7748
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.92s
                      Time elapsed: 00:10:38
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 112076 steps/s (collection: 0.780s, learning 0.097s)
             Mean action noise std: 2.43
          Mean value_function loss: 71.7595
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.2008
                       Mean reward: 814.68
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7331
     Episode_Reward/lifting_object: 163.9665
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.88s
                      Time elapsed: 00:10:38
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 109728 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 2.44
          Mean value_function loss: 76.3892
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.2239
                       Mean reward: 817.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 163.0930
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.90s
                      Time elapsed: 00:10:39
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 104332 steps/s (collection: 0.805s, learning 0.138s)
             Mean action noise std: 2.44
          Mean value_function loss: 69.1864
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.2432
                       Mean reward: 844.04
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7364
     Episode_Reward/lifting_object: 164.1664
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.94s
                      Time elapsed: 00:10:40
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 108953 steps/s (collection: 0.774s, learning 0.129s)
             Mean action noise std: 2.44
          Mean value_function loss: 57.8667
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.2496
                       Mean reward: 808.29
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 164.8374
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.90s
                      Time elapsed: 00:10:41
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 102197 steps/s (collection: 0.809s, learning 0.153s)
             Mean action noise std: 2.45
          Mean value_function loss: 60.9641
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.2599
                       Mean reward: 829.98
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7373
     Episode_Reward/lifting_object: 164.6050
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.96s
                      Time elapsed: 00:10:42
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 109976 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 65.8872
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2773
                       Mean reward: 845.23
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 167.9739
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.89s
                      Time elapsed: 00:10:43
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 110150 steps/s (collection: 0.801s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 57.7503
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.3067
                       Mean reward: 837.43
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 167.0480
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.89s
                      Time elapsed: 00:10:44
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 63372 steps/s (collection: 1.456s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 65.9407
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.3269
                       Mean reward: 821.05
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 165.3683
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.55s
                      Time elapsed: 00:10:45
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 32512 steps/s (collection: 2.901s, learning 0.123s)
             Mean action noise std: 2.47
          Mean value_function loss: 70.4620
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.3398
                       Mean reward: 848.45
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 167.7252
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.02s
                      Time elapsed: 00:10:49
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 30661 steps/s (collection: 3.087s, learning 0.120s)
             Mean action noise std: 2.47
          Mean value_function loss: 70.5311
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.3557
                       Mean reward: 849.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 167.8541
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.21s
                      Time elapsed: 00:10:52
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 30412 steps/s (collection: 3.082s, learning 0.150s)
             Mean action noise std: 2.48
          Mean value_function loss: 74.4777
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3742
                       Mean reward: 848.69
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 165.0501
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.23s
                      Time elapsed: 00:10:55
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 29743 steps/s (collection: 3.149s, learning 0.156s)
             Mean action noise std: 2.49
          Mean value_function loss: 64.7300
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.4038
                       Mean reward: 837.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 167.7000
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.31s
                      Time elapsed: 00:10:58
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 28937 steps/s (collection: 3.286s, learning 0.112s)
             Mean action noise std: 2.50
          Mean value_function loss: 75.7689
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 18.4283
                       Mean reward: 789.70
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 164.8877
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.40s
                      Time elapsed: 00:11:02
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 29655 steps/s (collection: 3.178s, learning 0.137s)
             Mean action noise std: 2.50
          Mean value_function loss: 79.0198
               Mean surrogate loss: 0.0205
                 Mean entropy loss: 18.4481
                       Mean reward: 836.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 167.7234
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.31s
                      Time elapsed: 00:11:05
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 30533 steps/s (collection: 3.066s, learning 0.154s)
             Mean action noise std: 2.50
          Mean value_function loss: 86.6338
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 18.4504
                       Mean reward: 820.63
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 165.5832
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.22s
                      Time elapsed: 00:11:08
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 29023 steps/s (collection: 3.241s, learning 0.146s)
             Mean action noise std: 2.50
          Mean value_function loss: 94.2038
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.4555
                       Mean reward: 807.93
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 162.1924
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.39s
                      Time elapsed: 00:11:12
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 24915 steps/s (collection: 3.827s, learning 0.119s)
             Mean action noise std: 2.51
          Mean value_function loss: 93.2506
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4675
                       Mean reward: 850.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 165.3946
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.95s
                      Time elapsed: 00:11:16
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 69960 steps/s (collection: 1.271s, learning 0.134s)
             Mean action noise std: 2.52
          Mean value_function loss: 89.3243
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4901
                       Mean reward: 782.71
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 160.4932
      Episode_Reward/object_height: 0.0439
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 1.41s
                      Time elapsed: 00:11:17
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 96195 steps/s (collection: 0.911s, learning 0.111s)
             Mean action noise std: 2.52
          Mean value_function loss: 99.6841
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5078
                       Mean reward: 831.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 166.3615
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.02s
                      Time elapsed: 00:11:18
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 102290 steps/s (collection: 0.840s, learning 0.121s)
             Mean action noise std: 2.52
          Mean value_function loss: 92.9285
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.5151
                       Mean reward: 800.06
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 159.0936
      Episode_Reward/object_height: 0.0440
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.96s
                      Time elapsed: 00:11:19
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 96576 steps/s (collection: 0.910s, learning 0.108s)
             Mean action noise std: 2.53
          Mean value_function loss: 95.8562
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 18.5288
                       Mean reward: 813.04
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 160.1652
      Episode_Reward/object_height: 0.0443
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 1.02s
                      Time elapsed: 00:11:20
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 106970 steps/s (collection: 0.817s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 106.9350
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 18.5416
                       Mean reward: 818.24
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7346
     Episode_Reward/lifting_object: 163.2165
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.92s
                      Time elapsed: 00:11:21
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 89692 steps/s (collection: 0.919s, learning 0.177s)
             Mean action noise std: 2.54
          Mean value_function loss: 116.2641
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.5705
                       Mean reward: 775.83
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.7145
     Episode_Reward/lifting_object: 158.7532
      Episode_Reward/object_height: 0.0445
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 1.10s
                      Time elapsed: 00:11:22
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 99864 steps/s (collection: 0.890s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 104.9532
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.5947
                       Mean reward: 796.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 159.5196
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.98s
                      Time elapsed: 00:11:23
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 104257 steps/s (collection: 0.849s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 105.4686
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.6186
                       Mean reward: 816.78
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.6994
     Episode_Reward/lifting_object: 156.4394
      Episode_Reward/object_height: 0.0442
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.94s
                      Time elapsed: 00:11:24
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 107453 steps/s (collection: 0.810s, learning 0.105s)
             Mean action noise std: 2.56
          Mean value_function loss: 104.0712
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.6386
                       Mean reward: 776.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 160.0916
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.91s
                      Time elapsed: 00:11:25
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 101574 steps/s (collection: 0.856s, learning 0.112s)
             Mean action noise std: 2.56
          Mean value_function loss: 89.8423
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.6448
                       Mean reward: 767.81
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.6963
     Episode_Reward/lifting_object: 154.7123
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.97s
                      Time elapsed: 00:11:26
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 109371 steps/s (collection: 0.808s, learning 0.091s)
             Mean action noise std: 2.57
          Mean value_function loss: 100.9159
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.6638
                       Mean reward: 811.23
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7220
     Episode_Reward/lifting_object: 163.4027
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.90s
                      Time elapsed: 00:11:27
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 101355 steps/s (collection: 0.850s, learning 0.120s)
             Mean action noise std: 2.58
          Mean value_function loss: 88.2951
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.6858
                       Mean reward: 817.75
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 161.8467
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.97s
                      Time elapsed: 00:11:28
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 101446 steps/s (collection: 0.853s, learning 0.116s)
             Mean action noise std: 2.58
          Mean value_function loss: 92.8960
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6998
                       Mean reward: 817.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7196
     Episode_Reward/lifting_object: 161.4075
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.97s
                      Time elapsed: 00:11:29
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 106093 steps/s (collection: 0.828s, learning 0.099s)
             Mean action noise std: 2.58
          Mean value_function loss: 79.8432
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 18.7148
                       Mean reward: 809.20
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.6936
     Episode_Reward/lifting_object: 155.9645
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.93s
                      Time elapsed: 00:11:30
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 103414 steps/s (collection: 0.846s, learning 0.105s)
             Mean action noise std: 2.59
          Mean value_function loss: 62.6459
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7167
                       Mean reward: 788.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6958
     Episode_Reward/lifting_object: 153.8049
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.95s
                      Time elapsed: 00:11:30
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 92070 steps/s (collection: 0.947s, learning 0.121s)
             Mean action noise std: 2.59
          Mean value_function loss: 61.8076
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.7218
                       Mean reward: 801.83
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 159.0533
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 1.07s
                      Time elapsed: 00:11:32
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 87338 steps/s (collection: 0.999s, learning 0.127s)
             Mean action noise std: 2.60
          Mean value_function loss: 70.5388
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7361
                       Mean reward: 832.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7132
     Episode_Reward/lifting_object: 159.9894
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 1.13s
                      Time elapsed: 00:11:33
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 112038 steps/s (collection: 0.758s, learning 0.120s)
             Mean action noise std: 2.60
          Mean value_function loss: 63.0546
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 18.7554
                       Mean reward: 845.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7270
     Episode_Reward/lifting_object: 162.7085
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.88s
                      Time elapsed: 00:11:34
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 100884 steps/s (collection: 0.848s, learning 0.126s)
             Mean action noise std: 2.60
          Mean value_function loss: 64.5356
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.7725
                       Mean reward: 813.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7341
     Episode_Reward/lifting_object: 164.7816
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.97s
                      Time elapsed: 00:11:35
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 110328 steps/s (collection: 0.791s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 62.4511
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.7806
                       Mean reward: 799.81
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 159.1927
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.89s
                      Time elapsed: 00:11:35
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 106511 steps/s (collection: 0.760s, learning 0.163s)
             Mean action noise std: 2.61
          Mean value_function loss: 62.5614
               Mean surrogate loss: 0.0245
                 Mean entropy loss: 18.7951
                       Mean reward: 781.89
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7385
     Episode_Reward/lifting_object: 163.6444
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.92s
                      Time elapsed: 00:11:36
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 114536 steps/s (collection: 0.766s, learning 0.092s)
             Mean action noise std: 2.61
          Mean value_function loss: 93.0516
               Mean surrogate loss: 0.0601
                 Mean entropy loss: 18.8038
                       Mean reward: 827.60
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7316
     Episode_Reward/lifting_object: 164.4599
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.86s
                      Time elapsed: 00:11:37
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 111059 steps/s (collection: 0.792s, learning 0.093s)
             Mean action noise std: 2.61
          Mean value_function loss: 141.6468
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 18.8047
                       Mean reward: 829.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 163.3690
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.89s
                      Time elapsed: 00:11:38
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 110932 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 178.2418
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 18.8086
                       Mean reward: 838.58
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 168.0005
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.89s
                      Time elapsed: 00:11:39
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 114259 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 195.1065
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 18.8165
                       Mean reward: 824.15
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 163.8395
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.86s
                      Time elapsed: 00:11:40
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 113021 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 196.7719
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.8407
                       Mean reward: 817.59
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.7341
     Episode_Reward/lifting_object: 164.8886
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.87s
                      Time elapsed: 00:11:41
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 111458 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 222.1115
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 18.8718
                       Mean reward: 840.86
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 164.8451
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.88s
                      Time elapsed: 00:11:42
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 114068 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 232.4146
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 18.8976
                       Mean reward: 819.88
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.7208
     Episode_Reward/lifting_object: 161.0147
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.86s
                      Time elapsed: 00:11:42
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 108556 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 209.8798
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 18.9200
                       Mean reward: 828.69
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.7190
     Episode_Reward/lifting_object: 160.4379
      Episode_Reward/object_height: 0.0445
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.91s
                      Time elapsed: 00:11:43
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 110471 steps/s (collection: 0.778s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 212.8777
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.9318
                       Mean reward: 830.66
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 163.6385
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.89s
                      Time elapsed: 00:11:44
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 113070 steps/s (collection: 0.767s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 220.4990
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.9561
                       Mean reward: 735.15
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 153.5320
      Episode_Reward/object_height: 0.0429
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.87s
                      Time elapsed: 00:11:45
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 111228 steps/s (collection: 0.798s, learning 0.086s)
             Mean action noise std: 2.67
          Mean value_function loss: 211.9967
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.9780
                       Mean reward: 648.19
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.6183
     Episode_Reward/lifting_object: 136.4496
      Episode_Reward/object_height: 0.0383
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.88s
                      Time elapsed: 00:11:46
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 112740 steps/s (collection: 0.764s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 216.3060
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 18.9952
                       Mean reward: 607.36
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 134.0979
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.87s
                      Time elapsed: 00:11:47
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 110717 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 2.68
          Mean value_function loss: 189.6082
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 19.0079
                       Mean reward: 689.55
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.6047
     Episode_Reward/lifting_object: 131.5219
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.89s
                      Time elapsed: 00:11:48
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 112168 steps/s (collection: 0.772s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 190.1015
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.0163
                       Mean reward: 682.52
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 139.5048
      Episode_Reward/object_height: 0.0387
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.88s
                      Time elapsed: 00:11:49
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 114305 steps/s (collection: 0.764s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 166.9740
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 19.0178
                       Mean reward: 710.66
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: 142.4851
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.86s
                      Time elapsed: 00:11:49
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 109910 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 165.0719
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 19.0224
                       Mean reward: 691.77
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 137.4575
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.89s
                      Time elapsed: 00:11:50
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 113993 steps/s (collection: 0.763s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 160.9228
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.0306
                       Mean reward: 710.49
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 138.7910
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.86s
                      Time elapsed: 00:11:51
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 115525 steps/s (collection: 0.758s, learning 0.093s)
             Mean action noise std: 2.70
          Mean value_function loss: 135.5792
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.0497
                       Mean reward: 667.85
               Mean episode length: 242.60
    Episode_Reward/reaching_object: 0.6222
     Episode_Reward/lifting_object: 137.3539
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.85s
                      Time elapsed: 00:11:52
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 113181 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 137.9763
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 19.0802
                       Mean reward: 720.65
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.6285
     Episode_Reward/lifting_object: 138.2637
      Episode_Reward/object_height: 0.0383
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.87s
                      Time elapsed: 00:11:53
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 112051 steps/s (collection: 0.776s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 125.4375
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0949
                       Mean reward: 715.47
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.6533
     Episode_Reward/lifting_object: 145.1983
      Episode_Reward/object_height: 0.0398
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.88s
                      Time elapsed: 00:11:54
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 112679 steps/s (collection: 0.782s, learning 0.091s)
             Mean action noise std: 2.72
          Mean value_function loss: 150.8914
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.1109
                       Mean reward: 728.27
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.6541
     Episode_Reward/lifting_object: 144.4084
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.87s
                      Time elapsed: 00:11:55
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 115204 steps/s (collection: 0.756s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 130.6488
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.1213
                       Mean reward: 727.33
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 147.4191
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.85s
                      Time elapsed: 00:11:56
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 114736 steps/s (collection: 0.763s, learning 0.094s)
             Mean action noise std: 2.73
          Mean value_function loss: 136.7006
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.1365
                       Mean reward: 791.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 152.5004
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.86s
                      Time elapsed: 00:11:56
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 115583 steps/s (collection: 0.756s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 134.9318
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 19.1615
                       Mean reward: 737.13
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.6534
     Episode_Reward/lifting_object: 145.9466
      Episode_Reward/object_height: 0.0402
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.85s
                      Time elapsed: 00:11:57
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 114804 steps/s (collection: 0.758s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 153.1364
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.1749
                       Mean reward: 688.41
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.6704
     Episode_Reward/lifting_object: 149.9795
      Episode_Reward/object_height: 0.0411
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.86s
                      Time elapsed: 00:11:58
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 110969 steps/s (collection: 0.795s, learning 0.091s)
             Mean action noise std: 2.74
          Mean value_function loss: 139.4097
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.1820
                       Mean reward: 774.33
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.6838
     Episode_Reward/lifting_object: 153.8367
      Episode_Reward/object_height: 0.0422
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.89s
                      Time elapsed: 00:11:59
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 114670 steps/s (collection: 0.770s, learning 0.087s)
             Mean action noise std: 2.75
          Mean value_function loss: 142.0991
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.2033
                       Mean reward: 803.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 156.4203
      Episode_Reward/object_height: 0.0426
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.86s
                      Time elapsed: 00:12:00
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 109534 steps/s (collection: 0.810s, learning 0.087s)
             Mean action noise std: 2.75
          Mean value_function loss: 136.7425
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.2165
                       Mean reward: 786.04
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 150.8982
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.90s
                      Time elapsed: 00:12:01
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 115344 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 2.76
          Mean value_function loss: 125.9029
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 19.2249
                       Mean reward: 767.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6834
     Episode_Reward/lifting_object: 153.6100
      Episode_Reward/object_height: 0.0412
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.85s
                      Time elapsed: 00:12:02
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 114099 steps/s (collection: 0.768s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 116.7074
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.2318
                       Mean reward: 744.80
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.6811
     Episode_Reward/lifting_object: 152.7848
      Episode_Reward/object_height: 0.0406
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.86s
                      Time elapsed: 00:12:02
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 114321 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 2.76
          Mean value_function loss: 113.5385
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.2425
                       Mean reward: 798.77
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.7036
     Episode_Reward/lifting_object: 158.1379
      Episode_Reward/object_height: 0.0420
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.86s
                      Time elapsed: 00:12:03
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 114458 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 117.3707
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 19.2505
                       Mean reward: 763.71
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.6968
     Episode_Reward/lifting_object: 155.4668
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.86s
                      Time elapsed: 00:12:04
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 114556 steps/s (collection: 0.756s, learning 0.102s)
             Mean action noise std: 2.77
          Mean value_function loss: 130.1949
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.2585
                       Mean reward: 777.59
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.6816
     Episode_Reward/lifting_object: 153.7307
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.86s
                      Time elapsed: 00:12:05
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 111794 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 2.77
          Mean value_function loss: 112.6576
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 19.2698
                       Mean reward: 730.53
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 157.0650
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.88s
                      Time elapsed: 00:12:06
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 113734 steps/s (collection: 0.779s, learning 0.085s)
             Mean action noise std: 2.78
          Mean value_function loss: 116.8390
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.2777
                       Mean reward: 750.19
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 155.4670
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.86s
                      Time elapsed: 00:12:07
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 111996 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 2.78
          Mean value_function loss: 108.0930
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.2868
                       Mean reward: 765.04
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.6918
     Episode_Reward/lifting_object: 155.3831
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.88s
                      Time elapsed: 00:12:08
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 118671 steps/s (collection: 0.742s, learning 0.087s)
             Mean action noise std: 2.78
          Mean value_function loss: 116.7072
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.2900
                       Mean reward: 787.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 158.6094
      Episode_Reward/object_height: 0.0413
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.83s
                      Time elapsed: 00:12:09
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 113279 steps/s (collection: 0.747s, learning 0.121s)
             Mean action noise std: 2.78
          Mean value_function loss: 109.4817
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 19.2895
                       Mean reward: 807.89
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7104
     Episode_Reward/lifting_object: 159.3892
      Episode_Reward/object_height: 0.0411
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.87s
                      Time elapsed: 00:12:09
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 116232 steps/s (collection: 0.748s, learning 0.098s)
             Mean action noise std: 2.78
          Mean value_function loss: 116.4826
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 19.2912
                       Mean reward: 813.97
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7206
     Episode_Reward/lifting_object: 161.5936
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.85s
                      Time elapsed: 00:12:10
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 119334 steps/s (collection: 0.741s, learning 0.083s)
             Mean action noise std: 2.78
          Mean value_function loss: 107.6002
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2936
                       Mean reward: 795.05
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.6931
     Episode_Reward/lifting_object: 156.5611
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.82s
                      Time elapsed: 00:12:11
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 111950 steps/s (collection: 0.793s, learning 0.086s)
             Mean action noise std: 2.79
          Mean value_function loss: 104.4234
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 19.3104
                       Mean reward: 774.82
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 158.3049
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.88s
                      Time elapsed: 00:12:12
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 117929 steps/s (collection: 0.752s, learning 0.082s)
             Mean action noise std: 2.79
          Mean value_function loss: 107.3686
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 19.3302
                       Mean reward: 796.25
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7028
     Episode_Reward/lifting_object: 156.6911
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.83s
                      Time elapsed: 00:12:13
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 114878 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 102.3552
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.3358
                       Mean reward: 743.96
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.7008
     Episode_Reward/lifting_object: 156.1644
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.86s
                      Time elapsed: 00:12:14
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 113640 steps/s (collection: 0.746s, learning 0.119s)
             Mean action noise std: 2.80
          Mean value_function loss: 102.6354
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.3434
                       Mean reward: 795.65
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.6993
     Episode_Reward/lifting_object: 157.5621
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.87s
                      Time elapsed: 00:12:14
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 118391 steps/s (collection: 0.737s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 99.7409
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.3484
                       Mean reward: 820.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7158
     Episode_Reward/lifting_object: 160.4980
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.83s
                      Time elapsed: 00:12:15
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 113858 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 2.81
          Mean value_function loss: 98.0638
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.3649
                       Mean reward: 836.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7193
     Episode_Reward/lifting_object: 161.2107
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.86s
                      Time elapsed: 00:12:16
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 118452 steps/s (collection: 0.744s, learning 0.086s)
             Mean action noise std: 2.81
          Mean value_function loss: 98.7538
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.3795
                       Mean reward: 818.79
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: 162.6781
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.83s
                      Time elapsed: 00:12:17
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 114234 steps/s (collection: 0.770s, learning 0.091s)
             Mean action noise std: 2.82
          Mean value_function loss: 96.5962
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.3879
                       Mean reward: 778.70
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.7190
     Episode_Reward/lifting_object: 161.0746
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.86s
                      Time elapsed: 00:12:18
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 115522 steps/s (collection: 0.759s, learning 0.092s)
             Mean action noise std: 2.82
          Mean value_function loss: 88.7249
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.3933
                       Mean reward: 814.58
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7111
     Episode_Reward/lifting_object: 160.3452
      Episode_Reward/object_height: 0.0411
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.85s
                      Time elapsed: 00:12:19
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 110660 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 2.82
          Mean value_function loss: 93.7861
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.4024
                       Mean reward: 838.78
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7298
     Episode_Reward/lifting_object: 162.8972
      Episode_Reward/object_height: 0.0413
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.89s
                      Time elapsed: 00:12:20
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 116394 steps/s (collection: 0.746s, learning 0.099s)
             Mean action noise std: 2.83
          Mean value_function loss: 82.6861
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.4118
                       Mean reward: 757.91
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7033
     Episode_Reward/lifting_object: 158.4094
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.84s
                      Time elapsed: 00:12:20
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 115691 steps/s (collection: 0.759s, learning 0.091s)
             Mean action noise std: 2.83
          Mean value_function loss: 85.8267
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.4267
                       Mean reward: 808.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: 159.6151
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.85s
                      Time elapsed: 00:12:21
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 113087 steps/s (collection: 0.784s, learning 0.086s)
             Mean action noise std: 2.83
          Mean value_function loss: 99.0328
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.4353
                       Mean reward: 831.22
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: 163.9067
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.87s
                      Time elapsed: 00:12:22
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 114468 steps/s (collection: 0.771s, learning 0.088s)
             Mean action noise std: 2.85
          Mean value_function loss: 92.2123
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.4577
                       Mean reward: 827.69
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7196
     Episode_Reward/lifting_object: 161.8813
      Episode_Reward/object_height: 0.0402
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.86s
                      Time elapsed: 00:12:23
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 115953 steps/s (collection: 0.747s, learning 0.101s)
             Mean action noise std: 2.85
          Mean value_function loss: 100.1472
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 19.4761
                       Mean reward: 835.54
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7260
     Episode_Reward/lifting_object: 163.9755
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.85s
                      Time elapsed: 00:12:24
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 111094 steps/s (collection: 0.778s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 85.9332
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.4773
                       Mean reward: 804.41
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 162.1809
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.88s
                      Time elapsed: 00:12:25
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 117049 steps/s (collection: 0.736s, learning 0.104s)
             Mean action noise std: 2.85
          Mean value_function loss: 102.9084
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.4834
                       Mean reward: 790.60
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 164.9942
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.84s
                      Time elapsed: 00:12:26
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 115163 steps/s (collection: 0.754s, learning 0.100s)
             Mean action noise std: 2.86
          Mean value_function loss: 115.0470
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.5003
                       Mean reward: 831.30
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7220
     Episode_Reward/lifting_object: 163.1730
      Episode_Reward/object_height: 0.0398
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.85s
                      Time elapsed: 00:12:26
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 114066 steps/s (collection: 0.770s, learning 0.092s)
             Mean action noise std: 2.86
          Mean value_function loss: 102.1335
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.5131
                       Mean reward: 825.28
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7250
     Episode_Reward/lifting_object: 161.8698
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.86s
                      Time elapsed: 00:12:27
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 116561 steps/s (collection: 0.759s, learning 0.085s)
             Mean action noise std: 2.87
          Mean value_function loss: 97.4487
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 19.5272
                       Mean reward: 806.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7278
     Episode_Reward/lifting_object: 162.5165
      Episode_Reward/object_height: 0.0390
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.84s
                      Time elapsed: 00:12:28
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 116692 steps/s (collection: 0.759s, learning 0.084s)
             Mean action noise std: 2.87
          Mean value_function loss: 93.0585
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.5370
                       Mean reward: 840.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7263
     Episode_Reward/lifting_object: 162.5272
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.84s
                      Time elapsed: 00:12:29
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 115392 steps/s (collection: 0.758s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 91.3061
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.5496
                       Mean reward: 811.71
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 161.3531
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.85s
                      Time elapsed: 00:12:30
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 116587 steps/s (collection: 0.750s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 79.0592
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.5635
                       Mean reward: 790.78
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.7212
     Episode_Reward/lifting_object: 160.2576
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.84s
                      Time elapsed: 00:12:31
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 115285 steps/s (collection: 0.751s, learning 0.102s)
             Mean action noise std: 2.89
          Mean value_function loss: 85.3703
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.5808
                       Mean reward: 822.26
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: 159.6306
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.85s
                      Time elapsed: 00:12:32
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 116280 steps/s (collection: 0.750s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 80.4465
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5988
                       Mean reward: 824.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7242
     Episode_Reward/lifting_object: 163.4004
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.85s
                      Time elapsed: 00:12:32
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 113164 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 88.0881
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 19.6143
                       Mean reward: 806.18
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7262
     Episode_Reward/lifting_object: 161.7201
      Episode_Reward/object_height: 0.0393
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.87s
                      Time elapsed: 00:12:33
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 119129 steps/s (collection: 0.739s, learning 0.086s)
             Mean action noise std: 2.91
          Mean value_function loss: 87.1307
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.6276
                       Mean reward: 817.70
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7267
     Episode_Reward/lifting_object: 163.2373
      Episode_Reward/object_height: 0.0402
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.83s
                      Time elapsed: 00:12:34
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 115632 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 2.92
          Mean value_function loss: 79.6100
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.6523
                       Mean reward: 808.97
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 162.7829
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.85s
                      Time elapsed: 00:12:35
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 119574 steps/s (collection: 0.735s, learning 0.087s)
             Mean action noise std: 2.92
          Mean value_function loss: 81.2384
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.6734
                       Mean reward: 777.30
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.7299
     Episode_Reward/lifting_object: 164.1864
      Episode_Reward/object_height: 0.0412
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.82s
                      Time elapsed: 00:12:36
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 119267 steps/s (collection: 0.735s, learning 0.089s)
             Mean action noise std: 2.92
          Mean value_function loss: 84.1115
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 19.6774
                       Mean reward: 813.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7235
     Episode_Reward/lifting_object: 163.2887
      Episode_Reward/object_height: 0.0412
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.82s
                      Time elapsed: 00:12:37
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 119202 steps/s (collection: 0.735s, learning 0.090s)
             Mean action noise std: 2.93
          Mean value_function loss: 76.4398
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.6881
                       Mean reward: 840.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7163
     Episode_Reward/lifting_object: 160.1530
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.82s
                      Time elapsed: 00:12:37
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 115869 steps/s (collection: 0.762s, learning 0.087s)
             Mean action noise std: 2.93
          Mean value_function loss: 72.0930
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.7000
                       Mean reward: 826.87
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 165.8889
      Episode_Reward/object_height: 0.0420
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.85s
                      Time elapsed: 00:12:38
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 117110 steps/s (collection: 0.752s, learning 0.087s)
             Mean action noise std: 2.94
          Mean value_function loss: 69.7669
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.7115
                       Mean reward: 811.86
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7242
     Episode_Reward/lifting_object: 161.0980
      Episode_Reward/object_height: 0.0408
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.84s
                      Time elapsed: 00:12:39
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 115736 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 70.1337
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.7288
                       Mean reward: 822.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 166.3681
      Episode_Reward/object_height: 0.0422
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.85s
                      Time elapsed: 00:12:40
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 116007 steps/s (collection: 0.760s, learning 0.088s)
             Mean action noise std: 2.95
          Mean value_function loss: 71.6630
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.7474
                       Mean reward: 816.19
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 165.1786
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.85s
                      Time elapsed: 00:12:41
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 114060 steps/s (collection: 0.764s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 62.6465
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 19.7640
                       Mean reward: 826.37
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 166.2625
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.86s
                      Time elapsed: 00:12:42
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 117099 steps/s (collection: 0.750s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 77.8476
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7798
                       Mean reward: 831.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 166.7758
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.84s
                      Time elapsed: 00:12:42
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 113960 steps/s (collection: 0.765s, learning 0.098s)
             Mean action noise std: 2.97
          Mean value_function loss: 70.9712
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 19.7991
                       Mean reward: 853.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 165.8285
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.86s
                      Time elapsed: 00:12:43
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 116628 steps/s (collection: 0.756s, learning 0.087s)
             Mean action noise std: 2.98
          Mean value_function loss: 67.9814
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.8153
                       Mean reward: 832.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 166.8605
      Episode_Reward/object_height: 0.0420
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.84s
                      Time elapsed: 00:12:44
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 116747 steps/s (collection: 0.747s, learning 0.095s)
             Mean action noise std: 2.98
          Mean value_function loss: 60.8804
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.8331
                       Mean reward: 846.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 166.9203
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.84s
                      Time elapsed: 00:12:45
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 113640 steps/s (collection: 0.760s, learning 0.105s)
             Mean action noise std: 2.99
          Mean value_function loss: 79.2376
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8487
                       Mean reward: 830.20
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 167.0600
      Episode_Reward/object_height: 0.0422
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.87s
                      Time elapsed: 00:12:46
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 113143 steps/s (collection: 0.772s, learning 0.097s)
             Mean action noise std: 3.00
          Mean value_function loss: 53.4041
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.8610
                       Mean reward: 808.73
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.7385
     Episode_Reward/lifting_object: 166.0786
      Episode_Reward/object_height: 0.0420
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.87s
                      Time elapsed: 00:12:47
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 113743 steps/s (collection: 0.755s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 81.2229
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.8793
                       Mean reward: 843.34
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 167.5578
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.86s
                      Time elapsed: 00:12:48
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 115139 steps/s (collection: 0.756s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 56.2425
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 19.9028
                       Mean reward: 849.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 167.0660
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.85s
                      Time elapsed: 00:12:48
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 114533 steps/s (collection: 0.745s, learning 0.114s)
             Mean action noise std: 3.02
          Mean value_function loss: 64.1847
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.9211
                       Mean reward: 836.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 167.5012
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.86s
                      Time elapsed: 00:12:49
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 113470 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 3.03
          Mean value_function loss: 65.0449
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.9465
                       Mean reward: 851.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 167.0727
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.87s
                      Time elapsed: 00:12:50
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 113575 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 3.04
          Mean value_function loss: 52.6919
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.9700
                       Mean reward: 819.49
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 165.5688
      Episode_Reward/object_height: 0.0404
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.87s
                      Time elapsed: 00:12:51
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 118380 steps/s (collection: 0.739s, learning 0.091s)
             Mean action noise std: 3.05
          Mean value_function loss: 57.0820
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.9958
                       Mean reward: 830.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 168.0262
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.83s
                      Time elapsed: 00:12:52
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 116188 steps/s (collection: 0.759s, learning 0.088s)
             Mean action noise std: 3.05
          Mean value_function loss: 50.7273
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.0080
                       Mean reward: 837.82
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7284
     Episode_Reward/lifting_object: 164.5745
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.85s
                      Time elapsed: 00:12:53
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 116904 steps/s (collection: 0.743s, learning 0.098s)
             Mean action noise std: 3.05
          Mean value_function loss: 58.8247
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.0155
                       Mean reward: 840.16
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 167.5207
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.84s
                      Time elapsed: 00:12:54
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 113733 steps/s (collection: 0.776s, learning 0.088s)
             Mean action noise std: 3.06
          Mean value_function loss: 54.3128
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.0247
                       Mean reward: 857.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 171.7336
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.86s
                      Time elapsed: 00:12:54
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 114840 steps/s (collection: 0.757s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 57.4818
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.0458
                       Mean reward: 837.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 166.9318
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.86s
                      Time elapsed: 00:12:55
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 118828 steps/s (collection: 0.740s, learning 0.087s)
             Mean action noise std: 3.08
          Mean value_function loss: 61.1446
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.0706
                       Mean reward: 864.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.1029
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.83s
                      Time elapsed: 00:12:56
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 117045 steps/s (collection: 0.752s, learning 0.088s)
             Mean action noise std: 3.08
          Mean value_function loss: 60.2969
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.0914
                       Mean reward: 813.70
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 166.2662
      Episode_Reward/object_height: 0.0390
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.84s
                      Time elapsed: 00:12:57
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 114654 steps/s (collection: 0.771s, learning 0.086s)
             Mean action noise std: 3.09
          Mean value_function loss: 53.5296
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 20.1036
                       Mean reward: 840.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 167.6850
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.86s
                      Time elapsed: 00:12:58
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 117515 steps/s (collection: 0.747s, learning 0.089s)
             Mean action noise std: 3.09
          Mean value_function loss: 54.0382
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1113
                       Mean reward: 855.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 168.4324
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.84s
                      Time elapsed: 00:12:59
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 116434 steps/s (collection: 0.757s, learning 0.087s)
             Mean action noise std: 3.09
          Mean value_function loss: 47.1406
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.1197
                       Mean reward: 828.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 168.3152
      Episode_Reward/object_height: 0.0381
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.84s
                      Time elapsed: 00:13:00
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 116795 steps/s (collection: 0.753s, learning 0.089s)
             Mean action noise std: 3.10
          Mean value_function loss: 49.5643
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 20.1331
                       Mean reward: 861.26
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 167.7804
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.84s
                      Time elapsed: 00:13:00
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 119486 steps/s (collection: 0.738s, learning 0.085s)
             Mean action noise std: 3.10
          Mean value_function loss: 52.6897
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.1363
                       Mean reward: 840.47
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 168.4479
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.82s
                      Time elapsed: 00:13:01
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 116961 steps/s (collection: 0.746s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 47.6491
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1427
                       Mean reward: 833.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 166.3042
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.84s
                      Time elapsed: 00:13:02
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 116269 steps/s (collection: 0.745s, learning 0.100s)
             Mean action noise std: 3.11
          Mean value_function loss: 55.7841
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.1525
                       Mean reward: 849.71
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 169.9812
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.85s
                      Time elapsed: 00:13:03
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 115468 steps/s (collection: 0.755s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 46.8597
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.1686
                       Mean reward: 837.72
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 167.2203
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.85s
                      Time elapsed: 00:13:04
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 118616 steps/s (collection: 0.734s, learning 0.095s)
             Mean action noise std: 3.13
          Mean value_function loss: 49.7684
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.1966
                       Mean reward: 843.23
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 168.2416
      Episode_Reward/object_height: 0.0384
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.83s
                      Time elapsed: 00:13:05
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 120783 steps/s (collection: 0.720s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 47.5817
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 20.2152
                       Mean reward: 865.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.5880
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.81s
                      Time elapsed: 00:13:05
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 115118 steps/s (collection: 0.749s, learning 0.105s)
             Mean action noise std: 3.13
          Mean value_function loss: 62.2049
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2243
                       Mean reward: 856.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 168.3624
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.85s
                      Time elapsed: 00:13:06
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 116361 steps/s (collection: 0.750s, learning 0.094s)
             Mean action noise std: 3.14
          Mean value_function loss: 60.7855
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.2368
                       Mean reward: 861.82
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 168.9441
      Episode_Reward/object_height: 0.0393
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.84s
                      Time elapsed: 00:13:07
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 116208 steps/s (collection: 0.760s, learning 0.086s)
             Mean action noise std: 3.15
          Mean value_function loss: 43.7194
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.2567
                       Mean reward: 828.28
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.1719
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.85s
                      Time elapsed: 00:13:08
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 110777 steps/s (collection: 0.787s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 51.8844
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.2689
                       Mean reward: 846.24
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 167.9875
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.89s
                      Time elapsed: 00:13:09
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 116580 steps/s (collection: 0.755s, learning 0.089s)
             Mean action noise std: 3.15
          Mean value_function loss: 53.0591
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.2744
                       Mean reward: 843.18
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 168.7195
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.84s
                      Time elapsed: 00:13:10
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 117369 steps/s (collection: 0.735s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 55.7174
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.2801
                       Mean reward: 849.85
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 168.4572
      Episode_Reward/object_height: 0.0398
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.84s
                      Time elapsed: 00:13:10
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 116559 steps/s (collection: 0.755s, learning 0.088s)
             Mean action noise std: 3.16
          Mean value_function loss: 57.0214
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2913
                       Mean reward: 861.33
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 169.8903
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.84s
                      Time elapsed: 00:13:11
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 117402 steps/s (collection: 0.745s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 49.0016
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.3065
                       Mean reward: 827.46
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 168.2021
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.84s
                      Time elapsed: 00:13:12
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 117434 steps/s (collection: 0.749s, learning 0.088s)
             Mean action noise std: 3.18
          Mean value_function loss: 57.9565
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.3256
                       Mean reward: 826.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.1552
      Episode_Reward/object_height: 0.0393
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.84s
                      Time elapsed: 00:13:13
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 118151 steps/s (collection: 0.744s, learning 0.088s)
             Mean action noise std: 3.18
          Mean value_function loss: 57.4441
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.3439
                       Mean reward: 832.07
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 168.7741
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.83s
                      Time elapsed: 00:13:14
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 115957 steps/s (collection: 0.757s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 66.7383
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.3644
                       Mean reward: 802.64
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 167.6921
      Episode_Reward/object_height: 0.0387
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.85s
                      Time elapsed: 00:13:15
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 117265 steps/s (collection: 0.742s, learning 0.097s)
             Mean action noise std: 3.19
          Mean value_function loss: 65.6694
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.3795
                       Mean reward: 836.91
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 168.4274
      Episode_Reward/object_height: 0.0388
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.84s
                      Time elapsed: 00:13:16
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 117012 steps/s (collection: 0.743s, learning 0.097s)
             Mean action noise std: 3.20
          Mean value_function loss: 69.5828
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.3877
                       Mean reward: 855.84
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 167.5372
      Episode_Reward/object_height: 0.0383
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.84s
                      Time elapsed: 00:13:16
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 111031 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 3.20
          Mean value_function loss: 52.1341
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 20.4015
                       Mean reward: 843.98
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 166.8882
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.89s
                      Time elapsed: 00:13:17
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 113029 steps/s (collection: 0.760s, learning 0.110s)
             Mean action noise std: 3.21
          Mean value_function loss: 49.7722
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 20.4169
                       Mean reward: 839.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 166.4656
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.87s
                      Time elapsed: 00:13:18
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 112508 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 3.22
          Mean value_function loss: 77.5689
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.4403
                       Mean reward: 847.60
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 168.5993
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.87s
                      Time elapsed: 00:13:19
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 115676 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 3.23
          Mean value_function loss: 66.1416
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.4648
                       Mean reward: 843.39
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 166.8435
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.85s
                      Time elapsed: 00:13:20
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 112948 steps/s (collection: 0.749s, learning 0.121s)
             Mean action noise std: 3.23
          Mean value_function loss: 64.4640
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 20.4796
                       Mean reward: 865.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 169.9760
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.87s
                      Time elapsed: 00:13:21
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 114757 steps/s (collection: 0.746s, learning 0.111s)
             Mean action noise std: 3.23
          Mean value_function loss: 51.3259
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.4866
                       Mean reward: 858.22
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7426
     Episode_Reward/lifting_object: 167.7416
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.86s
                      Time elapsed: 00:13:22
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 113325 steps/s (collection: 0.756s, learning 0.112s)
             Mean action noise std: 3.24
          Mean value_function loss: 63.5737
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.4863
                       Mean reward: 838.24
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: 165.4662
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.87s
                      Time elapsed: 00:13:22
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 116050 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 3.24
          Mean value_function loss: 69.2334
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.4926
                       Mean reward: 843.07
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 168.2712
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.85s
                      Time elapsed: 00:13:23
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 116651 steps/s (collection: 0.747s, learning 0.096s)
             Mean action noise std: 3.24
          Mean value_function loss: 77.3794
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.5030
                       Mean reward: 851.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 168.3874
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.84s
                      Time elapsed: 00:13:24
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 114865 steps/s (collection: 0.746s, learning 0.110s)
             Mean action noise std: 3.25
          Mean value_function loss: 53.7914
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.5112
                       Mean reward: 819.77
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 168.8188
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.86s
                      Time elapsed: 00:13:25
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 115674 steps/s (collection: 0.752s, learning 0.098s)
             Mean action noise std: 3.26
          Mean value_function loss: 56.6519
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.5243
                       Mean reward: 832.88
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 169.5808
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.85s
                      Time elapsed: 00:13:26
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 118036 steps/s (collection: 0.733s, learning 0.100s)
             Mean action noise std: 3.26
          Mean value_function loss: 43.4141
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 20.5385
                       Mean reward: 847.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 167.6705
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.83s
                      Time elapsed: 00:13:27
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 117661 steps/s (collection: 0.744s, learning 0.091s)
             Mean action noise std: 3.26
          Mean value_function loss: 58.5341
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.5470
                       Mean reward: 828.30
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 163.7027
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.84s
                      Time elapsed: 00:13:27
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 117466 steps/s (collection: 0.751s, learning 0.086s)
             Mean action noise std: 3.27
          Mean value_function loss: 61.6559
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.5680
                       Mean reward: 834.58
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 165.9560
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.84s
                      Time elapsed: 00:13:28
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 116866 steps/s (collection: 0.754s, learning 0.088s)
             Mean action noise std: 3.28
          Mean value_function loss: 52.1976
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 20.5811
                       Mean reward: 843.29
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7449
     Episode_Reward/lifting_object: 166.6346
      Episode_Reward/object_height: 0.0354
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.84s
                      Time elapsed: 00:13:29
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 117478 steps/s (collection: 0.749s, learning 0.088s)
             Mean action noise std: 3.28
          Mean value_function loss: 51.9276
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.5857
                       Mean reward: 849.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 165.8421
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.84s
                      Time elapsed: 00:13:30
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 117477 steps/s (collection: 0.745s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 46.7939
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.5988
                       Mean reward: 825.68
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7356
     Episode_Reward/lifting_object: 164.2874
      Episode_Reward/object_height: 0.0345
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.84s
                      Time elapsed: 00:13:31
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 116807 steps/s (collection: 0.750s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 49.1803
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.6182
                       Mean reward: 837.68
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 165.2635
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.84s
                      Time elapsed: 00:13:32
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 116190 steps/s (collection: 0.744s, learning 0.102s)
             Mean action noise std: 3.30
          Mean value_function loss: 53.6164
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.6367
                       Mean reward: 854.36
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.9756
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.85s
                      Time elapsed: 00:13:33
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 113944 steps/s (collection: 0.764s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 63.5190
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.6451
                       Mean reward: 841.17
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7299
     Episode_Reward/lifting_object: 164.9800
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.86s
                      Time elapsed: 00:13:33
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 113274 steps/s (collection: 0.761s, learning 0.107s)
             Mean action noise std: 3.31
          Mean value_function loss: 54.4557
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.6568
                       Mean reward: 848.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.4671
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.87s
                      Time elapsed: 00:13:34
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 112336 steps/s (collection: 0.773s, learning 0.103s)
             Mean action noise std: 3.31
          Mean value_function loss: 56.4069
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.6698
                       Mean reward: 828.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 169.1554
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.88s
                      Time elapsed: 00:13:35
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 116290 steps/s (collection: 0.746s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 58.0580
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.6879
                       Mean reward: 839.21
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 167.2439
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.85s
                      Time elapsed: 00:13:36
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 118036 steps/s (collection: 0.746s, learning 0.087s)
             Mean action noise std: 3.33
          Mean value_function loss: 63.4849
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.7083
                       Mean reward: 843.24
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.1848
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.83s
                      Time elapsed: 00:13:37
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 116098 steps/s (collection: 0.759s, learning 0.088s)
             Mean action noise std: 3.33
          Mean value_function loss: 50.6010
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.7215
                       Mean reward: 837.51
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 167.8290
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.85s
                      Time elapsed: 00:13:38
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 118101 steps/s (collection: 0.739s, learning 0.094s)
             Mean action noise std: 3.33
          Mean value_function loss: 55.9226
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.7298
                       Mean reward: 814.89
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7280
     Episode_Reward/lifting_object: 163.4021
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.83s
                      Time elapsed: 00:13:38
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 117571 steps/s (collection: 0.742s, learning 0.094s)
             Mean action noise std: 3.34
          Mean value_function loss: 55.0129
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.7419
                       Mean reward: 845.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.1292
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.84s
                      Time elapsed: 00:13:39
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 115683 steps/s (collection: 0.761s, learning 0.089s)
             Mean action noise std: 3.35
          Mean value_function loss: 56.1933
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.7607
                       Mean reward: 848.17
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 167.4396
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.85s
                      Time elapsed: 00:13:40
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 117193 steps/s (collection: 0.752s, learning 0.087s)
             Mean action noise std: 3.36
          Mean value_function loss: 47.9558
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.7782
                       Mean reward: 838.27
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 166.6249
      Episode_Reward/object_height: 0.0372
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.84s
                      Time elapsed: 00:13:41
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 118246 steps/s (collection: 0.742s, learning 0.089s)
             Mean action noise std: 3.36
          Mean value_function loss: 32.3459
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.7975
                       Mean reward: 852.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 167.1540
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.83s
                      Time elapsed: 00:13:42
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 113804 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 3.37
          Mean value_function loss: 47.2336
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 20.8160
                       Mean reward: 850.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 167.7020
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.86s
                      Time elapsed: 00:13:43
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 118601 steps/s (collection: 0.743s, learning 0.086s)
             Mean action noise std: 3.38
          Mean value_function loss: 50.9449
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 20.8363
                       Mean reward: 846.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 167.4798
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.83s
                      Time elapsed: 00:13:44
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 119136 steps/s (collection: 0.736s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 60.6118
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.8512
                       Mean reward: 844.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 168.9867
      Episode_Reward/object_height: 0.0375
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.83s
                      Time elapsed: 00:13:44
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 114800 steps/s (collection: 0.773s, learning 0.084s)
             Mean action noise std: 3.39
          Mean value_function loss: 45.6766
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.8694
                       Mean reward: 849.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 165.1650
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.86s
                      Time elapsed: 00:13:45
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 118036 steps/s (collection: 0.748s, learning 0.085s)
             Mean action noise std: 3.40
          Mean value_function loss: 48.3057
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 20.8862
                       Mean reward: 848.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.6693
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.83s
                      Time elapsed: 00:13:46
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 114138 steps/s (collection: 0.769s, learning 0.092s)
             Mean action noise std: 3.41
          Mean value_function loss: 68.1784
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.9011
                       Mean reward: 843.11
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7420
     Episode_Reward/lifting_object: 167.4341
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.86s
                      Time elapsed: 00:13:47
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 116272 steps/s (collection: 0.741s, learning 0.104s)
             Mean action noise std: 3.41
          Mean value_function loss: 54.7848
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.9157
                       Mean reward: 843.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 168.9063
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.85s
                      Time elapsed: 00:13:48
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 118694 steps/s (collection: 0.744s, learning 0.084s)
             Mean action noise std: 3.42
          Mean value_function loss: 45.6749
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.9255
                       Mean reward: 854.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.9128
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.83s
                      Time elapsed: 00:13:49
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 115558 steps/s (collection: 0.756s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 45.0402
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.9420
                       Mean reward: 865.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.2661
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.85s
                      Time elapsed: 00:13:49
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 112471 steps/s (collection: 0.770s, learning 0.104s)
             Mean action noise std: 3.43
          Mean value_function loss: 50.9021
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.9570
                       Mean reward: 850.33
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.3869
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.87s
                      Time elapsed: 00:13:50
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 114611 steps/s (collection: 0.752s, learning 0.106s)
             Mean action noise std: 3.44
          Mean value_function loss: 57.1427
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 20.9795
                       Mean reward: 834.06
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 169.4608
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.86s
                      Time elapsed: 00:13:51
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 113692 steps/s (collection: 0.764s, learning 0.101s)
             Mean action noise std: 3.46
          Mean value_function loss: 55.1138
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.0037
                       Mean reward: 844.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 166.6393
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.86s
                      Time elapsed: 00:13:52
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 115065 steps/s (collection: 0.749s, learning 0.105s)
             Mean action noise std: 3.46
          Mean value_function loss: 55.8057
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 21.0287
                       Mean reward: 860.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.0501
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.85s
                      Time elapsed: 00:13:53
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 112699 steps/s (collection: 0.770s, learning 0.102s)
             Mean action noise std: 3.47
          Mean value_function loss: 40.8759
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.0439
                       Mean reward: 860.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 168.4473
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.87s
                      Time elapsed: 00:13:54
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 115404 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 3.48
          Mean value_function loss: 47.6292
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.0610
                       Mean reward: 858.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 166.7634
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.85s
                      Time elapsed: 00:13:55
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 117469 steps/s (collection: 0.750s, learning 0.087s)
             Mean action noise std: 3.48
          Mean value_function loss: 59.0821
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.0767
                       Mean reward: 831.47
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 168.0143
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.84s
                      Time elapsed: 00:13:55
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 114979 steps/s (collection: 0.754s, learning 0.101s)
             Mean action noise std: 3.49
          Mean value_function loss: 46.5635
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 21.0881
                       Mean reward: 856.65
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 168.4430
      Episode_Reward/object_height: 0.0375
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.85s
                      Time elapsed: 00:13:56
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 116469 steps/s (collection: 0.757s, learning 0.087s)
             Mean action noise std: 3.50
          Mean value_function loss: 67.9511
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.1020
                       Mean reward: 848.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 169.9365
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.84s
                      Time elapsed: 00:13:57
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 111714 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 3.50
          Mean value_function loss: 76.5333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.1181
                       Mean reward: 870.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.0703
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.88s
                      Time elapsed: 00:13:58
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 116085 steps/s (collection: 0.753s, learning 0.094s)
             Mean action noise std: 3.51
          Mean value_function loss: 75.4801
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.1363
                       Mean reward: 824.02
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 166.6364
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.85s
                      Time elapsed: 00:13:59
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 113508 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 3.52
          Mean value_function loss: 77.5297
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.1517
                       Mean reward: 841.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 167.8578
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.87s
                      Time elapsed: 00:14:00
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 114075 steps/s (collection: 0.752s, learning 0.110s)
             Mean action noise std: 3.52
          Mean value_function loss: 64.5868
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.1601
                       Mean reward: 848.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 167.9741
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.86s
                      Time elapsed: 00:14:01
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 118760 steps/s (collection: 0.731s, learning 0.097s)
             Mean action noise std: 3.53
          Mean value_function loss: 74.0373
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.1719
                       Mean reward: 873.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.8747
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.83s
                      Time elapsed: 00:14:01
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 114475 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 3.54
          Mean value_function loss: 76.4589
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.1864
                       Mean reward: 845.47
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 168.6469
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.86s
                      Time elapsed: 00:14:02
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 115165 steps/s (collection: 0.772s, learning 0.082s)
             Mean action noise std: 3.54
          Mean value_function loss: 65.7684
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 21.2021
                       Mean reward: 822.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 165.7646
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.85s
                      Time elapsed: 00:14:03
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 115966 steps/s (collection: 0.761s, learning 0.087s)
             Mean action noise std: 3.55
          Mean value_function loss: 61.3980
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.2093
                       Mean reward: 860.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 169.6272
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.85s
                      Time elapsed: 00:14:04
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 116498 steps/s (collection: 0.757s, learning 0.087s)
             Mean action noise std: 3.55
          Mean value_function loss: 65.5333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.2210
                       Mean reward: 817.99
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7344
     Episode_Reward/lifting_object: 164.6621
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.84s
                      Time elapsed: 00:14:05
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 114148 steps/s (collection: 0.750s, learning 0.111s)
             Mean action noise std: 3.56
          Mean value_function loss: 66.7570
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 21.2345
                       Mean reward: 835.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7259
     Episode_Reward/lifting_object: 165.1241
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.86s
                      Time elapsed: 00:14:06
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 115466 steps/s (collection: 0.740s, learning 0.112s)
             Mean action noise std: 3.56
          Mean value_function loss: 61.8619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.2441
                       Mean reward: 839.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7172
     Episode_Reward/lifting_object: 164.4194
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.85s
                      Time elapsed: 00:14:07
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 117977 steps/s (collection: 0.750s, learning 0.084s)
             Mean action noise std: 3.56
          Mean value_function loss: 64.8713
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 21.2539
                       Mean reward: 824.74
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7168
     Episode_Reward/lifting_object: 162.5433
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.83s
                      Time elapsed: 00:14:07
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 117144 steps/s (collection: 0.755s, learning 0.085s)
             Mean action noise std: 3.57
          Mean value_function loss: 60.2233
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.2574
                       Mean reward: 849.61
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 163.1744
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.84s
                      Time elapsed: 00:14:08
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 112157 steps/s (collection: 0.789s, learning 0.088s)
             Mean action noise std: 3.57
          Mean value_function loss: 46.8992
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.2688
                       Mean reward: 847.22
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 165.8755
      Episode_Reward/object_height: 0.0354
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.88s
                      Time elapsed: 00:14:09
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 113816 steps/s (collection: 0.778s, learning 0.086s)
             Mean action noise std: 3.58
          Mean value_function loss: 58.4538
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.2918
                       Mean reward: 832.64
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 166.6736
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.86s
                      Time elapsed: 00:14:10
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 112952 steps/s (collection: 0.763s, learning 0.108s)
             Mean action noise std: 3.59
          Mean value_function loss: 48.7106
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.3023
                       Mean reward: 850.43
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 167.8510
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.87s
                      Time elapsed: 00:14:11
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 115796 steps/s (collection: 0.747s, learning 0.102s)
             Mean action noise std: 3.59
          Mean value_function loss: 52.8293
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.3175
                       Mean reward: 868.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 169.2387
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.85s
                      Time elapsed: 00:14:12
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 115061 steps/s (collection: 0.769s, learning 0.085s)
             Mean action noise std: 3.60
          Mean value_function loss: 45.1068
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 21.3294
                       Mean reward: 854.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.4983
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.85s
                      Time elapsed: 00:14:13
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 114436 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 3.60
          Mean value_function loss: 42.4643
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 21.3390
                       Mean reward: 838.65
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 164.5077
      Episode_Reward/object_height: 0.0340
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.86s
                      Time elapsed: 00:14:13
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 117270 steps/s (collection: 0.750s, learning 0.088s)
             Mean action noise std: 3.61
          Mean value_function loss: 48.3463
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.3538
                       Mean reward: 853.39
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 168.4885
      Episode_Reward/object_height: 0.0345
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.84s
                      Time elapsed: 00:14:14
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 115492 steps/s (collection: 0.759s, learning 0.092s)
             Mean action noise std: 3.62
          Mean value_function loss: 53.0426
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.3772
                       Mean reward: 825.15
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 165.2118
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.85s
                      Time elapsed: 00:14:15
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 110098 steps/s (collection: 0.797s, learning 0.096s)
             Mean action noise std: 3.63
          Mean value_function loss: 54.5088
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 21.3936
                       Mean reward: 806.45
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 165.7327
      Episode_Reward/object_height: 0.0334
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.89s
                      Time elapsed: 00:14:16
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 114893 steps/s (collection: 0.756s, learning 0.100s)
             Mean action noise std: 3.63
          Mean value_function loss: 32.7202
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.3960
                       Mean reward: 856.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.3349
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.86s
                      Time elapsed: 00:14:17
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 114725 steps/s (collection: 0.764s, learning 0.093s)
             Mean action noise std: 3.63
          Mean value_function loss: 56.9558
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.4006
                       Mean reward: 862.80
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 169.1904
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.86s
                      Time elapsed: 00:14:18
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 112480 steps/s (collection: 0.766s, learning 0.108s)
             Mean action noise std: 3.64
          Mean value_function loss: 49.7503
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.4163
                       Mean reward: 815.15
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 167.0750
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.87s
                      Time elapsed: 00:14:19
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 114292 steps/s (collection: 0.750s, learning 0.111s)
             Mean action noise std: 3.65
          Mean value_function loss: 34.8094
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.4385
                       Mean reward: 850.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.7185
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.86s
                      Time elapsed: 00:14:19
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 109489 steps/s (collection: 0.790s, learning 0.108s)
             Mean action noise std: 3.66
          Mean value_function loss: 56.6998
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.4506
                       Mean reward: 862.50
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.3651
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.90s
                      Time elapsed: 00:14:20
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 110967 steps/s (collection: 0.773s, learning 0.113s)
             Mean action noise std: 3.67
          Mean value_function loss: 51.2598
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 21.4715
                       Mean reward: 837.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 168.2094
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.89s
                      Time elapsed: 00:14:21
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 111918 steps/s (collection: 0.769s, learning 0.109s)
             Mean action noise std: 3.67
          Mean value_function loss: 63.0729
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.4841
                       Mean reward: 836.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 168.9743
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.88s
                      Time elapsed: 00:14:22
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 113104 steps/s (collection: 0.752s, learning 0.117s)
             Mean action noise std: 3.68
          Mean value_function loss: 42.6333
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.4949
                       Mean reward: 854.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.0492
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.87s
                      Time elapsed: 00:14:23
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 112638 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 3.69
          Mean value_function loss: 47.7670
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.5150
                       Mean reward: 858.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 168.9533
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.87s
                      Time elapsed: 00:14:24
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 109849 steps/s (collection: 0.790s, learning 0.105s)
             Mean action noise std: 3.69
          Mean value_function loss: 51.5682
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 21.5299
                       Mean reward: 852.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 168.3749
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.89s
                      Time elapsed: 00:14:25
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 117480 steps/s (collection: 0.750s, learning 0.087s)
             Mean action noise std: 3.70
          Mean value_function loss: 48.2035
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.5401
                       Mean reward: 818.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 167.8414
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.84s
                      Time elapsed: 00:14:26
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 115605 steps/s (collection: 0.761s, learning 0.089s)
             Mean action noise std: 3.70
          Mean value_function loss: 54.5889
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.5528
                       Mean reward: 856.86
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 168.6809
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.85s
                      Time elapsed: 00:14:26
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 116983 steps/s (collection: 0.753s, learning 0.088s)
             Mean action noise std: 3.71
          Mean value_function loss: 43.9006
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.5600
                       Mean reward: 839.73
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 166.3872
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.84s
                      Time elapsed: 00:14:27
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 111836 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 3.71
          Mean value_function loss: 65.6750
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.5756
                       Mean reward: 814.72
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 165.0652
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.88s
                      Time elapsed: 00:14:28
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 110262 steps/s (collection: 0.794s, learning 0.097s)
             Mean action noise std: 3.71
          Mean value_function loss: 50.4927
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.5808
                       Mean reward: 852.07
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.4692
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.89s
                      Time elapsed: 00:14:29
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 118316 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 3.72
          Mean value_function loss: 61.4744
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.5913
                       Mean reward: 862.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 169.8107
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.83s
                      Time elapsed: 00:14:30
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 115078 steps/s (collection: 0.765s, learning 0.089s)
             Mean action noise std: 3.73
          Mean value_function loss: 52.1594
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.6088
                       Mean reward: 858.05
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.1089
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.85s
                      Time elapsed: 00:14:31
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 117385 steps/s (collection: 0.749s, learning 0.088s)
             Mean action noise std: 3.74
          Mean value_function loss: 61.1512
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.6248
                       Mean reward: 867.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 169.3812
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.84s
                      Time elapsed: 00:14:32
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 117900 steps/s (collection: 0.745s, learning 0.089s)
             Mean action noise std: 3.75
          Mean value_function loss: 45.6605
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.6451
                       Mean reward: 842.41
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 168.0485
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.83s
                      Time elapsed: 00:14:32
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 111994 steps/s (collection: 0.778s, learning 0.100s)
             Mean action noise std: 3.75
          Mean value_function loss: 48.5455
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 21.6696
                       Mean reward: 840.22
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 167.1759
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.88s
                      Time elapsed: 00:14:33
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 113448 steps/s (collection: 0.767s, learning 0.099s)
             Mean action noise std: 3.76
          Mean value_function loss: 48.6635
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.6777
                       Mean reward: 857.09
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 168.8397
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.87s
                      Time elapsed: 00:14:34
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 114301 steps/s (collection: 0.770s, learning 0.090s)
             Mean action noise std: 3.76
          Mean value_function loss: 40.9104
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.6858
                       Mean reward: 847.78
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 169.6369
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.86s
                      Time elapsed: 00:14:35
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 117820 steps/s (collection: 0.744s, learning 0.090s)
             Mean action noise std: 3.77
          Mean value_function loss: 48.7293
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.6955
                       Mean reward: 852.17
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.3083
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.83s
                      Time elapsed: 00:14:36
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 113231 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 3.78
          Mean value_function loss: 40.6285
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.7117
                       Mean reward: 855.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 169.9055
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.87s
                      Time elapsed: 00:14:37
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 105756 steps/s (collection: 0.804s, learning 0.125s)
             Mean action noise std: 3.79
          Mean value_function loss: 61.8596
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.7349
                       Mean reward: 856.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.2380
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.93s
                      Time elapsed: 00:14:38
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 112818 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 3.79
          Mean value_function loss: 40.8930
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 21.7499
                       Mean reward: 844.70
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 170.4868
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.87s
                      Time elapsed: 00:14:38
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 113504 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 3.79
          Mean value_function loss: 51.3858
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.7540
                       Mean reward: 838.29
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 166.8671
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.87s
                      Time elapsed: 00:14:39
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 105051 steps/s (collection: 0.840s, learning 0.095s)
             Mean action noise std: 3.80
          Mean value_function loss: 58.0496
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.7600
                       Mean reward: 836.54
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 168.4852
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.94s
                      Time elapsed: 00:14:40
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 104988 steps/s (collection: 0.845s, learning 0.091s)
             Mean action noise std: 3.81
          Mean value_function loss: 47.5504
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 21.7745
                       Mean reward: 858.77
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.0868
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.94s
                      Time elapsed: 00:14:41
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 110042 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 3.81
          Mean value_function loss: 49.6918
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 21.7914
                       Mean reward: 842.16
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.0746
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.89s
                      Time elapsed: 00:14:42
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 104381 steps/s (collection: 0.832s, learning 0.110s)
             Mean action noise std: 3.82
          Mean value_function loss: 44.3025
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.8022
                       Mean reward: 874.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 170.8405
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.94s
                      Time elapsed: 00:14:43
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 108521 steps/s (collection: 0.802s, learning 0.103s)
             Mean action noise std: 3.83
          Mean value_function loss: 57.7555
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.8218
                       Mean reward: 858.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 169.9210
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.91s
                      Time elapsed: 00:14:44
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 102774 steps/s (collection: 0.829s, learning 0.128s)
             Mean action noise std: 3.84
          Mean value_function loss: 47.7881
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.8369
                       Mean reward: 855.59
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 170.7306
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.96s
                      Time elapsed: 00:14:45
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 102467 steps/s (collection: 0.859s, learning 0.100s)
             Mean action noise std: 3.84
          Mean value_function loss: 45.6131
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.8477
                       Mean reward: 835.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.0175
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.96s
                      Time elapsed: 00:14:46
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 100885 steps/s (collection: 0.862s, learning 0.113s)
             Mean action noise std: 3.85
          Mean value_function loss: 48.6760
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.8572
                       Mean reward: 848.38
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 166.5342
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.97s
                      Time elapsed: 00:14:47
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 104871 steps/s (collection: 0.829s, learning 0.109s)
             Mean action noise std: 3.86
          Mean value_function loss: 45.7276
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.8754
                       Mean reward: 826.70
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 166.3858
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.94s
                      Time elapsed: 00:14:48
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 98876 steps/s (collection: 0.889s, learning 0.105s)
             Mean action noise std: 3.87
          Mean value_function loss: 52.6736
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.8909
                       Mean reward: 815.65
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 166.5149
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.99s
                      Time elapsed: 00:14:49
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 101199 steps/s (collection: 0.867s, learning 0.104s)
             Mean action noise std: 3.87
          Mean value_function loss: 59.1061
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.9103
                       Mean reward: 848.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 168.9562
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.97s
                      Time elapsed: 00:14:50
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 107006 steps/s (collection: 0.821s, learning 0.098s)
             Mean action noise std: 3.88
          Mean value_function loss: 41.4809
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.9228
                       Mean reward: 862.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 167.2663
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.92s
                      Time elapsed: 00:14:51
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 108031 steps/s (collection: 0.798s, learning 0.112s)
             Mean action noise std: 3.89
          Mean value_function loss: 40.3995
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.9399
                       Mean reward: 854.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 169.5036
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.91s
                      Time elapsed: 00:14:52
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 106521 steps/s (collection: 0.805s, learning 0.118s)
             Mean action noise std: 3.89
          Mean value_function loss: 43.5750
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.9467
                       Mean reward: 821.72
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 166.6661
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.92s
                      Time elapsed: 00:14:53
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 103249 steps/s (collection: 0.828s, learning 0.124s)
             Mean action noise std: 3.90
          Mean value_function loss: 38.0603
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.9518
                       Mean reward: 855.78
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 169.7705
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.95s
                      Time elapsed: 00:14:53
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 108875 steps/s (collection: 0.793s, learning 0.110s)
             Mean action noise std: 3.91
          Mean value_function loss: 44.0191
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.9686
                       Mean reward: 840.16
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 169.6989
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.90s
                      Time elapsed: 00:14:54
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 106653 steps/s (collection: 0.812s, learning 0.110s)
             Mean action noise std: 3.92
          Mean value_function loss: 52.3873
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.9891
                       Mean reward: 859.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 170.8009
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.92s
                      Time elapsed: 00:14:55
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 114319 steps/s (collection: 0.773s, learning 0.087s)
             Mean action noise std: 3.93
          Mean value_function loss: 51.6247
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.0143
                       Mean reward: 852.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.8467
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.86s
                      Time elapsed: 00:14:56
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 107021 steps/s (collection: 0.823s, learning 0.096s)
             Mean action noise std: 3.93
          Mean value_function loss: 55.1185
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 22.0301
                       Mean reward: 857.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 170.4109
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.92s
                      Time elapsed: 00:14:57
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 115395 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 3.94
          Mean value_function loss: 51.8175
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.0375
                       Mean reward: 844.05
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 169.9376
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.85s
                      Time elapsed: 00:14:58
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 109848 steps/s (collection: 0.790s, learning 0.105s)
             Mean action noise std: 3.95
          Mean value_function loss: 52.7638
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.0526
                       Mean reward: 857.23
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 171.6481
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.89s
                      Time elapsed: 00:14:59
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 94261 steps/s (collection: 0.939s, learning 0.104s)
             Mean action noise std: 3.96
          Mean value_function loss: 68.6829
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.0787
                       Mean reward: 842.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 170.1019
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 1.04s
                      Time elapsed: 00:15:00
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 113157 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 3.96
          Mean value_function loss: 53.2765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.0876
                       Mean reward: 831.05
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 168.0292
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.87s
                      Time elapsed: 00:15:01
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 114322 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 3.97
          Mean value_function loss: 38.4024
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.0995
                       Mean reward: 839.98
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 169.0771
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.86s
                      Time elapsed: 00:15:02
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 114897 steps/s (collection: 0.761s, learning 0.095s)
             Mean action noise std: 3.97
          Mean value_function loss: 48.3703
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.1136
                       Mean reward: 852.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.3127
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.86s
                      Time elapsed: 00:15:02
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 113741 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 3.98
          Mean value_function loss: 53.7899
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.1262
                       Mean reward: 843.13
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 169.7238
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.86s
                      Time elapsed: 00:15:03
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 115518 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 3.99
          Mean value_function loss: 50.9872
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.1431
                       Mean reward: 858.26
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 169.0453
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.85s
                      Time elapsed: 00:15:04
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 110731 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 4.00
          Mean value_function loss: 39.0602
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 22.1596
                       Mean reward: 857.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 169.5573
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.89s
                      Time elapsed: 00:15:05
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 104416 steps/s (collection: 0.809s, learning 0.133s)
             Mean action noise std: 4.00
          Mean value_function loss: 32.7756
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.1651
                       Mean reward: 866.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 169.0947
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.94s
                      Time elapsed: 00:15:06
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 108417 steps/s (collection: 0.813s, learning 0.094s)
             Mean action noise std: 4.01
          Mean value_function loss: 49.2853
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.1809
                       Mean reward: 862.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 167.5363
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.91s
                      Time elapsed: 00:15:07
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 108812 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 4.02
          Mean value_function loss: 36.2631
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.1991
                       Mean reward: 841.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 168.3496
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.90s
                      Time elapsed: 00:15:08
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 97024 steps/s (collection: 0.880s, learning 0.134s)
             Mean action noise std: 4.03
          Mean value_function loss: 39.3475
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2167
                       Mean reward: 864.83
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.7926
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 1.01s
                      Time elapsed: 00:15:09
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 96114 steps/s (collection: 0.894s, learning 0.129s)
             Mean action noise std: 4.05
          Mean value_function loss: 48.3440
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.2411
                       Mean reward: 858.78
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 171.2262
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 1.02s
                      Time elapsed: 00:15:10
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 100615 steps/s (collection: 0.879s, learning 0.098s)
             Mean action noise std: 4.06
          Mean value_function loss: 36.7618
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.2768
                       Mean reward: 849.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 167.5794
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.98s
                      Time elapsed: 00:15:11
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 109739 steps/s (collection: 0.790s, learning 0.106s)
             Mean action noise std: 4.07
          Mean value_function loss: 49.4258
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.2968
                       Mean reward: 852.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 169.5948
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.90s
                      Time elapsed: 00:15:12
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 109353 steps/s (collection: 0.806s, learning 0.093s)
             Mean action noise std: 4.07
          Mean value_function loss: 38.6076
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.3065
                       Mean reward: 830.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 167.7924
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.90s
                      Time elapsed: 00:15:13
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 109346 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 4.08
          Mean value_function loss: 36.5728
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.3163
                       Mean reward: 875.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.6051
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.90s
                      Time elapsed: 00:15:14
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 110761 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 4.09
          Mean value_function loss: 51.3635
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.3278
                       Mean reward: 869.45
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 170.7182
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.89s
                      Time elapsed: 00:15:14
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 116152 steps/s (collection: 0.760s, learning 0.087s)
             Mean action noise std: 4.09
          Mean value_function loss: 46.2759
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.3407
                       Mean reward: 847.01
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 168.4779
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.85s
                      Time elapsed: 00:15:15
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 110856 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 4.10
          Mean value_function loss: 56.6536
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 22.3570
                       Mean reward: 848.74
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 169.1857
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.89s
                      Time elapsed: 00:15:16
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 114228 steps/s (collection: 0.762s, learning 0.099s)
             Mean action noise std: 4.11
          Mean value_function loss: 42.6922
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 22.3688
                       Mean reward: 840.09
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 168.6407
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.86s
                      Time elapsed: 00:15:17
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 115122 steps/s (collection: 0.766s, learning 0.088s)
             Mean action noise std: 4.11
          Mean value_function loss: 53.1360
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.3825
                       Mean reward: 861.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.1144
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.85s
                      Time elapsed: 00:15:18
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 112164 steps/s (collection: 0.785s, learning 0.092s)
             Mean action noise std: 4.12
          Mean value_function loss: 44.0377
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4002
                       Mean reward: 863.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 170.7750
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.88s
                      Time elapsed: 00:15:19
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 114068 steps/s (collection: 0.773s, learning 0.089s)
             Mean action noise std: 4.13
          Mean value_function loss: 37.0401
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 22.4197
                       Mean reward: 875.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.6929
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.86s
                      Time elapsed: 00:15:20
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 111367 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 4.14
          Mean value_function loss: 45.5410
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.4332
                       Mean reward: 859.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 171.6249
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.88s
                      Time elapsed: 00:15:20
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 110115 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 4.14
          Mean value_function loss: 51.5449
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.4407
                       Mean reward: 854.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 169.0150
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.89s
                      Time elapsed: 00:15:21
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 112587 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 4.15
          Mean value_function loss: 51.1095
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.4523
                       Mean reward: 847.42
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 169.2174
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.87s
                      Time elapsed: 00:15:22
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 108223 steps/s (collection: 0.801s, learning 0.107s)
             Mean action noise std: 4.15
          Mean value_function loss: 41.8799
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.4643
                       Mean reward: 851.70
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 167.9720
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.91s
                      Time elapsed: 00:15:23
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 112749 steps/s (collection: 0.773s, learning 0.099s)
             Mean action noise std: 4.16
          Mean value_function loss: 57.9967
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.4726
                       Mean reward: 840.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 168.1843
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.87s
                      Time elapsed: 00:15:24
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 110224 steps/s (collection: 0.788s, learning 0.104s)
             Mean action noise std: 4.17
          Mean value_function loss: 44.7729
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.4884
                       Mean reward: 858.72
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 169.9477
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.89s
                      Time elapsed: 00:15:25
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 110439 steps/s (collection: 0.787s, learning 0.103s)
             Mean action noise std: 4.17
          Mean value_function loss: 52.8069
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5005
                       Mean reward: 858.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 168.3829
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.89s
                      Time elapsed: 00:15:26
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 110053 steps/s (collection: 0.784s, learning 0.109s)
             Mean action noise std: 4.18
          Mean value_function loss: 52.3066
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.5057
                       Mean reward: 830.78
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 166.6054
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.89s
                      Time elapsed: 00:15:27
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 113276 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 4.19
          Mean value_function loss: 52.6801
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.5204
                       Mean reward: 852.51
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 169.5070
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.87s
                      Time elapsed: 00:15:28
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 113198 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 4.19
          Mean value_function loss: 46.2670
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.5334
                       Mean reward: 850.19
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 168.0119
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.87s
                      Time elapsed: 00:15:28
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 110035 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 4.20
          Mean value_function loss: 45.1396
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5451
                       Mean reward: 840.59
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.7192
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.89s
                      Time elapsed: 00:15:29
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 115506 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 4.20
          Mean value_function loss: 46.0265
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5565
                       Mean reward: 842.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 165.6478
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.85s
                      Time elapsed: 00:15:30
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 111468 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 4.21
          Mean value_function loss: 41.5479
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.5674
                       Mean reward: 865.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.7022
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.88s
                      Time elapsed: 00:15:31
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 112753 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 4.22
          Mean value_function loss: 42.9315
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.5827
                       Mean reward: 839.57
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 168.3146
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.87s
                      Time elapsed: 00:15:32
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 113224 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 4.23
          Mean value_function loss: 39.5885
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.5959
                       Mean reward: 874.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.6030
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.87s
                      Time elapsed: 00:15:33
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 109319 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 4.23
          Mean value_function loss: 50.2039
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.6128
                       Mean reward: 851.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.9879
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.90s
                      Time elapsed: 00:15:34
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 112149 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 4.23
          Mean value_function loss: 45.8199
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.6200
                       Mean reward: 861.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 168.9962
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.88s
                      Time elapsed: 00:15:35
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 113137 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 4.24
          Mean value_function loss: 46.6879
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.6277
                       Mean reward: 839.28
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 168.7537
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.87s
                      Time elapsed: 00:15:35
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 111973 steps/s (collection: 0.786s, learning 0.092s)
             Mean action noise std: 4.25
          Mean value_function loss: 40.2102
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.6507
                       Mean reward: 846.53
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 170.1060
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.88s
                      Time elapsed: 00:15:36
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 114105 steps/s (collection: 0.769s, learning 0.093s)
             Mean action noise std: 4.26
          Mean value_function loss: 40.4800
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.6675
                       Mean reward: 865.89
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 170.8951
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.86s
                      Time elapsed: 00:15:37
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 110232 steps/s (collection: 0.800s, learning 0.092s)
             Mean action noise std: 4.26
          Mean value_function loss: 43.0359
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.6773
                       Mean reward: 856.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 168.6043
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.89s
                      Time elapsed: 00:15:38
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 111284 steps/s (collection: 0.778s, learning 0.106s)
             Mean action noise std: 4.27
          Mean value_function loss: 33.1343
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 22.6891
                       Mean reward: 841.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 169.1634
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.88s
                      Time elapsed: 00:15:39
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 110931 steps/s (collection: 0.796s, learning 0.090s)
             Mean action noise std: 4.28
          Mean value_function loss: 44.0077
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.7046
                       Mean reward: 852.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.0773
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.89s
                      Time elapsed: 00:15:40
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 110579 steps/s (collection: 0.796s, learning 0.093s)
             Mean action noise std: 4.29
          Mean value_function loss: 48.4265
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.7215
                       Mean reward: 849.31
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.2258
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.89s
                      Time elapsed: 00:15:41
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 110559 steps/s (collection: 0.781s, learning 0.109s)
             Mean action noise std: 4.30
          Mean value_function loss: 44.3235
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.7350
                       Mean reward: 869.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.3504
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.89s
                      Time elapsed: 00:15:42
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 113747 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 4.31
          Mean value_function loss: 43.9125
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.7539
                       Mean reward: 865.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.7752
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.86s
                      Time elapsed: 00:15:42
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 115798 steps/s (collection: 0.761s, learning 0.088s)
             Mean action noise std: 4.31
          Mean value_function loss: 42.9674
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.7695
                       Mean reward: 823.67
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 168.8808
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.85s
                      Time elapsed: 00:15:43
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 111786 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 4.32
          Mean value_function loss: 35.3146
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.7745
                       Mean reward: 848.05
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.1859
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.88s
                      Time elapsed: 00:15:44
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 112933 steps/s (collection: 0.783s, learning 0.087s)
             Mean action noise std: 4.32
          Mean value_function loss: 40.8969
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 22.7852
                       Mean reward: 858.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.0905
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.87s
                      Time elapsed: 00:15:45
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 111517 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 4.33
          Mean value_function loss: 48.5851
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.7977
                       Mean reward: 852.20
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 168.8601
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.88s
                      Time elapsed: 00:15:46
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 111745 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 4.34
          Mean value_function loss: 50.1712
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.8068
                       Mean reward: 852.25
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.5240
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.88s
                      Time elapsed: 00:15:47
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 112860 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 4.35
          Mean value_function loss: 58.3835
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.8202
                       Mean reward: 859.54
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.2387
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.87s
                      Time elapsed: 00:15:48
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 112049 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 4.35
          Mean value_function loss: 53.8101
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.8327
                       Mean reward: 848.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.6808
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.88s
                      Time elapsed: 00:15:49
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 110391 steps/s (collection: 0.798s, learning 0.092s)
             Mean action noise std: 4.36
          Mean value_function loss: 54.4354
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.8465
                       Mean reward: 856.88
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.0147
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.89s
                      Time elapsed: 00:15:49
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 113456 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 4.36
          Mean value_function loss: 37.3677
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.8593
                       Mean reward: 836.43
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 166.8678
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.87s
                      Time elapsed: 00:15:50
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 110656 steps/s (collection: 0.798s, learning 0.090s)
             Mean action noise std: 4.37
          Mean value_function loss: 60.9008
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.8660
                       Mean reward: 855.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 170.8285
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.89s
                      Time elapsed: 00:15:51
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 108883 steps/s (collection: 0.796s, learning 0.107s)
             Mean action noise std: 4.38
          Mean value_function loss: 44.7826
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 22.8789
                       Mean reward: 823.55
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 170.2819
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.90s
                      Time elapsed: 00:15:52
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 110172 steps/s (collection: 0.795s, learning 0.097s)
             Mean action noise std: 4.38
          Mean value_function loss: 51.8175
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.8955
                       Mean reward: 840.82
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 167.9259
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.89s
                      Time elapsed: 00:15:53
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 106346 steps/s (collection: 0.832s, learning 0.092s)
             Mean action noise std: 4.39
          Mean value_function loss: 51.9064
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9064
                       Mean reward: 822.33
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 166.7157
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.92s
                      Time elapsed: 00:15:54
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 109461 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 4.41
          Mean value_function loss: 65.7509
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.9260
                       Mean reward: 849.82
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 166.7527
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.90s
                      Time elapsed: 00:15:55
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 109786 steps/s (collection: 0.782s, learning 0.113s)
             Mean action noise std: 4.42
          Mean value_function loss: 62.8773
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 22.9531
                       Mean reward: 830.02
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 167.2788
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.90s
                      Time elapsed: 00:15:56
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 112885 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 4.43
          Mean value_function loss: 48.4501
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.9714
                       Mean reward: 835.21
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 167.0175
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.87s
                      Time elapsed: 00:15:57
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 107615 steps/s (collection: 0.804s, learning 0.109s)
             Mean action noise std: 4.44
          Mean value_function loss: 52.0447
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.9908
                       Mean reward: 846.72
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 168.1709
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.91s
                      Time elapsed: 00:15:58
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 105894 steps/s (collection: 0.826s, learning 0.102s)
             Mean action noise std: 4.45
          Mean value_function loss: 53.4708
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.0125
                       Mean reward: 864.69
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.9097
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.93s
                      Time elapsed: 00:15:58
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 110477 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 4.46
          Mean value_function loss: 42.7013
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.0336
                       Mean reward: 850.36
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 168.8153
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.89s
                      Time elapsed: 00:15:59
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 33081 steps/s (collection: 2.864s, learning 0.107s)
             Mean action noise std: 4.47
          Mean value_function loss: 47.9337
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 23.0491
                       Mean reward: 860.27
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 169.6588
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.97s
                      Time elapsed: 00:16:02
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 32548 steps/s (collection: 2.904s, learning 0.117s)
             Mean action noise std: 4.47
          Mean value_function loss: 47.8168
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.0530
                       Mean reward: 843.61
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 167.9574
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.02s
                      Time elapsed: 00:16:05
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 32275 steps/s (collection: 2.931s, learning 0.115s)
             Mean action noise std: 4.48
          Mean value_function loss: 51.6533
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.0648
                       Mean reward: 866.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 167.8102
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.05s
                      Time elapsed: 00:16:08
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 30762 steps/s (collection: 3.061s, learning 0.135s)
             Mean action noise std: 4.49
          Mean value_function loss: 46.9453
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.0829
                       Mean reward: 858.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 166.4373
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.20s
                      Time elapsed: 00:16:12
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 30655 steps/s (collection: 3.073s, learning 0.134s)
             Mean action noise std: 4.49
          Mean value_function loss: 50.7386
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.0927
                       Mean reward: 849.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 168.7676
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.21s
                      Time elapsed: 00:16:15
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 28667 steps/s (collection: 3.302s, learning 0.127s)
             Mean action noise std: 4.50
          Mean value_function loss: 50.5291
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 23.1003
                       Mean reward: 835.65
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 168.1838
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.43s
                      Time elapsed: 00:16:18
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 31623 steps/s (collection: 3.000s, learning 0.109s)
             Mean action noise std: 4.50
          Mean value_function loss: 54.5781
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.1086
                       Mean reward: 864.99
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 168.5940
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.11s
                      Time elapsed: 00:16:21
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 31650 steps/s (collection: 2.982s, learning 0.124s)
             Mean action noise std: 4.51
          Mean value_function loss: 56.0546
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.1230
                       Mean reward: 858.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 167.1443
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.11s
                      Time elapsed: 00:16:24
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 27025 steps/s (collection: 3.515s, learning 0.123s)
             Mean action noise std: 4.51
          Mean value_function loss: 64.5242
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.1328
                       Mean reward: 857.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 167.2749
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.64s
                      Time elapsed: 00:16:28
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 110579 steps/s (collection: 0.792s, learning 0.097s)
             Mean action noise std: 4.52
          Mean value_function loss: 54.6696
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.1392
                       Mean reward: 829.48
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 166.4920
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.89s
                      Time elapsed: 00:16:29
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 116065 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 4.52
          Mean value_function loss: 53.9802
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 23.1452
                       Mean reward: 855.35
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.5237
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.85s
                      Time elapsed: 00:16:30
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 116624 steps/s (collection: 0.731s, learning 0.112s)
             Mean action noise std: 4.53
          Mean value_function loss: 55.0965
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.1590
                       Mean reward: 843.05
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.5665
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.84s
                      Time elapsed: 00:16:31
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 112258 steps/s (collection: 0.762s, learning 0.114s)
             Mean action noise std: 4.54
          Mean value_function loss: 46.5111
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 23.1730
                       Mean reward: 863.55
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.3765
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.88s
                      Time elapsed: 00:16:32
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 114163 steps/s (collection: 0.747s, learning 0.115s)
             Mean action noise std: 4.55
          Mean value_function loss: 62.9621
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.1882
                       Mean reward: 856.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.8627
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.86s
                      Time elapsed: 00:16:32
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 108052 steps/s (collection: 0.796s, learning 0.114s)
             Mean action noise std: 4.56
          Mean value_function loss: 60.6633
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 23.2070
                       Mean reward: 857.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 169.2486
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.91s
                      Time elapsed: 00:16:33
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 111913 steps/s (collection: 0.784s, learning 0.094s)
             Mean action noise std: 4.56
          Mean value_function loss: 64.6303
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.2140
                       Mean reward: 832.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 167.7998
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.88s
                      Time elapsed: 00:16:34
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 110420 steps/s (collection: 0.800s, learning 0.090s)
             Mean action noise std: 4.56
          Mean value_function loss: 56.7175
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.2230
                       Mean reward: 846.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 166.9890
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.89s
                      Time elapsed: 00:16:35
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 111643 steps/s (collection: 0.793s, learning 0.088s)
             Mean action noise std: 4.57
          Mean value_function loss: 46.8263
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.2292
                       Mean reward: 807.31
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 167.5898
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.88s
                      Time elapsed: 00:16:36
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 113523 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 4.58
          Mean value_function loss: 58.7434
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 23.2390
                       Mean reward: 827.22
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 169.6348
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.87s
                      Time elapsed: 00:16:37
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 114076 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 4.58
          Mean value_function loss: 49.2409
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 23.2447
                       Mean reward: 849.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 166.4410
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.86s
                      Time elapsed: 00:16:38
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 112172 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 4.59
          Mean value_function loss: 58.8070
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.2550
                       Mean reward: 863.21
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.8482
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.88s
                      Time elapsed: 00:16:39
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 110899 steps/s (collection: 0.797s, learning 0.089s)
             Mean action noise std: 4.60
          Mean value_function loss: 39.0860
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.2728
                       Mean reward: 850.11
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 169.9215
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.89s
                      Time elapsed: 00:16:39
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 112512 steps/s (collection: 0.786s, learning 0.088s)
             Mean action noise std: 4.60
          Mean value_function loss: 40.2826
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.2803
                       Mean reward: 851.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 169.4230
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.87s
                      Time elapsed: 00:16:40
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 110060 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 4.61
          Mean value_function loss: 54.6625
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.2911
                       Mean reward: 861.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 167.8371
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.89s
                      Time elapsed: 00:16:41
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 107386 steps/s (collection: 0.810s, learning 0.105s)
             Mean action noise std: 4.62
          Mean value_function loss: 41.8023
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.3076
                       Mean reward: 857.38
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.3315
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.92s
                      Time elapsed: 00:16:42
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 111223 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 4.62
          Mean value_function loss: 50.1322
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.3164
                       Mean reward: 849.00
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 166.9931
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.88s
                      Time elapsed: 00:16:43
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 114165 steps/s (collection: 0.765s, learning 0.096s)
             Mean action noise std: 4.63
          Mean value_function loss: 45.5537
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 23.3286
                       Mean reward: 850.22
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.1104
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.86s
                      Time elapsed: 00:16:44
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 107346 steps/s (collection: 0.821s, learning 0.095s)
             Mean action noise std: 4.64
          Mean value_function loss: 42.4647
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.3445
                       Mean reward: 849.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 167.7326
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.92s
                      Time elapsed: 00:16:45
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 109761 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 4.66
          Mean value_function loss: 46.8629
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.3687
                       Mean reward: 873.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 167.6052
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.90s
                      Time elapsed: 00:16:46
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 108429 steps/s (collection: 0.808s, learning 0.098s)
             Mean action noise std: 4.67
          Mean value_function loss: 41.0698
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.3906
                       Mean reward: 862.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 170.1332
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.91s
                      Time elapsed: 00:16:47
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 112184 steps/s (collection: 0.787s, learning 0.089s)
             Mean action noise std: 4.67
          Mean value_function loss: 44.3415
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.4049
                       Mean reward: 849.19
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.3254
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.88s
                      Time elapsed: 00:16:47
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 108829 steps/s (collection: 0.816s, learning 0.088s)
             Mean action noise std: 4.68
          Mean value_function loss: 38.8305
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.4104
                       Mean reward: 849.37
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.2524
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.90s
                      Time elapsed: 00:16:48
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 112747 steps/s (collection: 0.783s, learning 0.089s)
             Mean action noise std: 4.68
          Mean value_function loss: 39.4352
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.4216
                       Mean reward: 856.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 168.8690
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.87s
                      Time elapsed: 00:16:49
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 108923 steps/s (collection: 0.806s, learning 0.097s)
             Mean action noise std: 4.69
          Mean value_function loss: 40.4273
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.4342
                       Mean reward: 870.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 171.1946
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.90s
                      Time elapsed: 00:16:50
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 113006 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 4.70
          Mean value_function loss: 39.7728
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.4430
                       Mean reward: 867.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.0344
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.87s
                      Time elapsed: 00:16:51
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 108196 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 4.70
          Mean value_function loss: 44.3937
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.4578
                       Mean reward: 851.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.2408
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.91s
                      Time elapsed: 00:16:52
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 110468 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 4.71
          Mean value_function loss: 43.8795
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.4731
                       Mean reward: 850.64
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 168.0967
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.89s
                      Time elapsed: 00:16:53
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 106434 steps/s (collection: 0.826s, learning 0.098s)
             Mean action noise std: 4.72
          Mean value_function loss: 47.6944
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.4838
                       Mean reward: 844.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 168.9214
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.92s
                      Time elapsed: 00:16:54
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 109299 steps/s (collection: 0.789s, learning 0.110s)
             Mean action noise std: 4.73
          Mean value_function loss: 49.5841
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.4992
                       Mean reward: 845.16
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.1505
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.90s
                      Time elapsed: 00:16:55
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 111907 steps/s (collection: 0.786s, learning 0.092s)
             Mean action noise std: 4.73
          Mean value_function loss: 44.4548
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.5172
                       Mean reward: 831.83
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 167.7168
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.88s
                      Time elapsed: 00:16:55
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 108227 steps/s (collection: 0.817s, learning 0.091s)
             Mean action noise std: 4.74
          Mean value_function loss: 39.1754
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 23.5208
                       Mean reward: 859.82
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.0671
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.91s
                      Time elapsed: 00:16:56
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 107502 steps/s (collection: 0.819s, learning 0.096s)
             Mean action noise std: 4.75
          Mean value_function loss: 46.7347
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.5292
                       Mean reward: 859.75
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.2013
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.91s
                      Time elapsed: 00:16:57
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 110287 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 4.76
          Mean value_function loss: 42.0976
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 23.5452
                       Mean reward: 837.04
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.7271
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.89s
                      Time elapsed: 00:16:58
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 108875 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 4.76
          Mean value_function loss: 49.1092
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.5590
                       Mean reward: 871.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 170.9157
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.90s
                      Time elapsed: 00:16:59
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 112756 steps/s (collection: 0.770s, learning 0.102s)
             Mean action noise std: 4.78
          Mean value_function loss: 43.8721
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.5766
                       Mean reward: 820.94
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 168.8304
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.87s
                      Time elapsed: 00:17:00
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 110148 steps/s (collection: 0.782s, learning 0.110s)
             Mean action noise std: 4.78
          Mean value_function loss: 42.0444
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.5932
                       Mean reward: 859.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.5924
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.89s
                      Time elapsed: 00:17:01
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 109721 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 4.79
          Mean value_function loss: 43.4871
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.6049
                       Mean reward: 854.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 168.4892
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.90s
                      Time elapsed: 00:17:02
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 111334 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 4.80
          Mean value_function loss: 39.7325
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.6217
                       Mean reward: 856.81
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 169.5849
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.88s
                      Time elapsed: 00:17:03
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 111215 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 4.81
          Mean value_function loss: 50.4096
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 23.6420
                       Mean reward: 855.57
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 168.9662
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.88s
                      Time elapsed: 00:17:04
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 112818 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 4.82
          Mean value_function loss: 42.9942
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.6526
                       Mean reward: 858.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 169.8566
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.87s
                      Time elapsed: 00:17:04
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 112999 steps/s (collection: 0.783s, learning 0.087s)
             Mean action noise std: 4.82
          Mean value_function loss: 58.4970
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.6622
                       Mean reward: 865.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 169.8745
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.87s
                      Time elapsed: 00:17:05
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 111830 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 4.84
          Mean value_function loss: 45.1015
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.6804
                       Mean reward: 826.14
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 168.5797
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.88s
                      Time elapsed: 00:17:06
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 107781 steps/s (collection: 0.811s, learning 0.101s)
             Mean action noise std: 4.85
          Mean value_function loss: 55.2429
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.7039
                       Mean reward: 857.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 171.4365
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.91s
                      Time elapsed: 00:17:07
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 106667 steps/s (collection: 0.828s, learning 0.094s)
             Mean action noise std: 4.86
          Mean value_function loss: 51.9552
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.7240
                       Mean reward: 861.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 168.9388
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.92s
                      Time elapsed: 00:17:08
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 111594 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 4.87
          Mean value_function loss: 42.5971
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.7362
                       Mean reward: 841.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 168.2574
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.88s
                      Time elapsed: 00:17:09
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 111662 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 4.87
          Mean value_function loss: 45.6251
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 23.7439
                       Mean reward: 868.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.6092
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.88s
                      Time elapsed: 00:17:10
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 109585 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 4.87
          Mean value_function loss: 41.7189
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.7467
                       Mean reward: 846.31
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 170.2738
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.90s
                      Time elapsed: 00:17:11
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 111219 steps/s (collection: 0.787s, learning 0.097s)
             Mean action noise std: 4.88
          Mean value_function loss: 52.3205
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.7543
                       Mean reward: 821.88
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 169.0097
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.88s
                      Time elapsed: 00:17:12
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 113200 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 4.89
          Mean value_function loss: 36.4445
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.7675
                       Mean reward: 844.99
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 166.6797
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.87s
                      Time elapsed: 00:17:12
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 113343 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 4.90
          Mean value_function loss: 52.3308
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.7829
                       Mean reward: 854.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.7839
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.87s
                      Time elapsed: 00:17:13
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 108573 steps/s (collection: 0.812s, learning 0.094s)
             Mean action noise std: 4.91
          Mean value_function loss: 49.4720
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.7944
                       Mean reward: 847.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 166.7899
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.91s
                      Time elapsed: 00:17:14
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 109898 steps/s (collection: 0.803s, learning 0.091s)
             Mean action noise std: 4.91
          Mean value_function loss: 56.6993
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.8069
                       Mean reward: 849.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 168.9433
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.89s
                      Time elapsed: 00:17:15
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 108043 steps/s (collection: 0.817s, learning 0.093s)
             Mean action noise std: 4.92
          Mean value_function loss: 66.6715
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.8138
                       Mean reward: 833.45
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 166.3745
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.91s
                      Time elapsed: 00:17:16
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 106746 steps/s (collection: 0.814s, learning 0.107s)
             Mean action noise std: 4.93
          Mean value_function loss: 59.1413
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.8232
                       Mean reward: 861.46
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 170.7916
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.92s
                      Time elapsed: 00:17:17
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 112679 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 4.93
          Mean value_function loss: 59.7548
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 23.8348
                       Mean reward: 857.97
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 170.0029
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.87s
                      Time elapsed: 00:17:18
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 106869 steps/s (collection: 0.818s, learning 0.102s)
             Mean action noise std: 4.94
          Mean value_function loss: 51.4631
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.8445
                       Mean reward: 852.73
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 167.7097
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.92s
                      Time elapsed: 00:17:19
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 111982 steps/s (collection: 0.776s, learning 0.102s)
             Mean action noise std: 4.95
          Mean value_function loss: 61.1119
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.8583
                       Mean reward: 863.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 170.9108
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.88s
                      Time elapsed: 00:17:20
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 113275 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 4.96
          Mean value_function loss: 48.7505
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 23.8674
                       Mean reward: 847.59
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 168.0790
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.87s
                      Time elapsed: 00:17:20
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 110272 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 4.96
          Mean value_function loss: 51.8307
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.8810
                       Mean reward: 831.28
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.2573
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.89s
                      Time elapsed: 00:17:21
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 104874 steps/s (collection: 0.836s, learning 0.101s)
             Mean action noise std: 4.97
          Mean value_function loss: 54.6396
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.8878
                       Mean reward: 832.95
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 167.1875
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.94s
                      Time elapsed: 00:17:22
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 104318 steps/s (collection: 0.841s, learning 0.102s)
             Mean action noise std: 4.97
          Mean value_function loss: 55.1997
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.8962
                       Mean reward: 818.75
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.7316
     Episode_Reward/lifting_object: 164.8126
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.94s
                      Time elapsed: 00:17:23
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 108227 steps/s (collection: 0.800s, learning 0.108s)
             Mean action noise std: 4.98
          Mean value_function loss: 48.5406
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.9053
                       Mean reward: 788.35
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 162.7978
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.91s
                      Time elapsed: 00:17:24
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 99362 steps/s (collection: 0.893s, learning 0.096s)
             Mean action noise std: 4.98
          Mean value_function loss: 46.1914
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.9144
                       Mean reward: 827.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 167.0441
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.99s
                      Time elapsed: 00:17:25
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 104148 steps/s (collection: 0.842s, learning 0.102s)
             Mean action noise std: 4.99
          Mean value_function loss: 58.1704
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 23.9191
                       Mean reward: 854.18
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 166.7183
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.94s
                      Time elapsed: 00:17:26
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 103178 steps/s (collection: 0.855s, learning 0.098s)
             Mean action noise std: 5.00
          Mean value_function loss: 48.1758
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.9307
                       Mean reward: 865.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.1041
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.95s
                      Time elapsed: 00:17:27
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 106406 steps/s (collection: 0.826s, learning 0.098s)
             Mean action noise std: 5.01
          Mean value_function loss: 49.4282
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 23.9471
                       Mean reward: 841.90
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.5543
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.92s
                      Time elapsed: 00:17:28
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 104098 steps/s (collection: 0.816s, learning 0.128s)
             Mean action noise std: 5.01
          Mean value_function loss: 56.2996
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.9576
                       Mean reward: 860.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 171.0921
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.94s
                      Time elapsed: 00:17:29
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 106958 steps/s (collection: 0.818s, learning 0.101s)
             Mean action noise std: 5.02
          Mean value_function loss: 57.9516
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.9703
                       Mean reward: 862.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.1812
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.92s
                      Time elapsed: 00:17:30
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 97323 steps/s (collection: 0.880s, learning 0.131s)
             Mean action noise std: 5.03
          Mean value_function loss: 50.8137
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 23.9801
                       Mean reward: 837.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 168.5697
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.01s
                      Time elapsed: 00:17:31
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 100978 steps/s (collection: 0.863s, learning 0.110s)
             Mean action noise std: 5.03
          Mean value_function loss: 44.9054
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.9917
                       Mean reward: 853.34
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.2823
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.97s
                      Time elapsed: 00:17:32
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 103988 steps/s (collection: 0.843s, learning 0.103s)
             Mean action noise std: 5.04
          Mean value_function loss: 60.4478
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.0011
                       Mean reward: 855.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 167.4092
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.95s
                      Time elapsed: 00:17:33
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 109479 steps/s (collection: 0.785s, learning 0.113s)
             Mean action noise std: 5.05
          Mean value_function loss: 54.7057
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.0134
                       Mean reward: 829.60
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 167.4137
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.90s
                      Time elapsed: 00:17:34
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 106561 steps/s (collection: 0.813s, learning 0.110s)
             Mean action noise std: 5.06
          Mean value_function loss: 51.9595
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.0238
                       Mean reward: 833.06
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 167.5784
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.92s
                      Time elapsed: 00:17:35
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 111285 steps/s (collection: 0.786s, learning 0.098s)
             Mean action noise std: 5.07
          Mean value_function loss: 55.1290
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.0389
                       Mean reward: 847.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 166.5284
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.88s
                      Time elapsed: 00:17:35
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 109671 steps/s (collection: 0.804s, learning 0.093s)
             Mean action noise std: 5.08
          Mean value_function loss: 55.9229
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.0587
                       Mean reward: 823.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 167.1726
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.90s
                      Time elapsed: 00:17:36
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 110939 steps/s (collection: 0.795s, learning 0.091s)
             Mean action noise std: 5.08
          Mean value_function loss: 49.0138
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.0680
                       Mean reward: 845.80
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 166.8532
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.89s
                      Time elapsed: 00:17:37
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 111657 steps/s (collection: 0.776s, learning 0.105s)
             Mean action noise std: 5.10
          Mean value_function loss: 56.0264
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.0807
                       Mean reward: 842.44
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 167.1547
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.88s
                      Time elapsed: 00:17:38
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 113730 steps/s (collection: 0.773s, learning 0.091s)
             Mean action noise std: 5.11
          Mean value_function loss: 53.4537
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 24.1003
                       Mean reward: 857.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 167.9482
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.86s
                      Time elapsed: 00:17:39
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 111035 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 5.11
          Mean value_function loss: 62.7113
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.1154
                       Mean reward: 864.33
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 168.1689
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.89s
                      Time elapsed: 00:17:40
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 108233 steps/s (collection: 0.810s, learning 0.098s)
             Mean action noise std: 5.12
          Mean value_function loss: 68.9503
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 24.1230
                       Mean reward: 825.12
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 167.2763
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.91s
                      Time elapsed: 00:17:41
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 99470 steps/s (collection: 0.886s, learning 0.103s)
             Mean action noise std: 5.13
          Mean value_function loss: 69.5784
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 24.1322
                       Mean reward: 849.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 167.5368
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.99s
                      Time elapsed: 00:17:42
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 102687 steps/s (collection: 0.855s, learning 0.103s)
             Mean action noise std: 5.13
          Mean value_function loss: 60.5947
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 24.1438
                       Mean reward: 842.55
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 168.6857
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.96s
                      Time elapsed: 00:17:43
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 99909 steps/s (collection: 0.873s, learning 0.111s)
             Mean action noise std: 5.13
          Mean value_function loss: 63.3384
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.1460
                       Mean reward: 840.86
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 168.5550
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.98s
                      Time elapsed: 00:17:44
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 105244 steps/s (collection: 0.827s, learning 0.107s)
             Mean action noise std: 5.14
          Mean value_function loss: 64.6428
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 24.1562
                       Mean reward: 855.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 169.7977
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.93s
                      Time elapsed: 00:17:45
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 106011 steps/s (collection: 0.819s, learning 0.108s)
             Mean action noise std: 5.15
          Mean value_function loss: 69.3746
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.1683
                       Mean reward: 866.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.8972
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.93s
                      Time elapsed: 00:17:46
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 108730 steps/s (collection: 0.807s, learning 0.098s)
             Mean action noise std: 5.15
          Mean value_function loss: 54.6735
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 24.1771
                       Mean reward: 862.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 168.8049
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.90s
                      Time elapsed: 00:17:46
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 103927 steps/s (collection: 0.841s, learning 0.105s)
             Mean action noise std: 5.15
          Mean value_function loss: 74.5955
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.1812
                       Mean reward: 848.53
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 167.4908
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.95s
                      Time elapsed: 00:17:47
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 107509 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 5.16
          Mean value_function loss: 71.4215
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.1907
                       Mean reward: 850.17
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 167.7335
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.91s
                      Time elapsed: 00:17:48
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 106379 steps/s (collection: 0.830s, learning 0.095s)
             Mean action noise std: 5.17
          Mean value_function loss: 73.5172
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 24.2108
                       Mean reward: 803.55
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 163.9871
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.92s
                      Time elapsed: 00:17:49
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 109020 steps/s (collection: 0.807s, learning 0.095s)
             Mean action noise std: 5.19
          Mean value_function loss: 56.3253
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.2305
                       Mean reward: 858.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 168.6221
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.90s
                      Time elapsed: 00:17:50
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 112404 steps/s (collection: 0.787s, learning 0.088s)
             Mean action noise std: 5.20
          Mean value_function loss: 69.6926
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.2514
                       Mean reward: 851.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 164.6350
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.87s
                      Time elapsed: 00:17:51
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 112453 steps/s (collection: 0.787s, learning 0.087s)
             Mean action noise std: 5.21
          Mean value_function loss: 75.8383
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.2727
                       Mean reward: 791.40
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 163.9050
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.87s
                      Time elapsed: 00:17:52
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 113378 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 5.23
          Mean value_function loss: 62.5081
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 24.3004
                       Mean reward: 805.21
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 164.1438
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.87s
                      Time elapsed: 00:17:53
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 111210 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 5.24
          Mean value_function loss: 86.6052
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.3236
                       Mean reward: 854.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 166.2962
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.88s
                      Time elapsed: 00:17:54
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 113060 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 5.25
          Mean value_function loss: 100.4370
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 24.3427
                       Mean reward: 837.06
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 168.8513
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.87s
                      Time elapsed: 00:17:54
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 110946 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 5.26
          Mean value_function loss: 89.1508
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 24.3523
                       Mean reward: 799.72
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 164.1938
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.89s
                      Time elapsed: 00:17:55
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 112487 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 5.26
          Mean value_function loss: 62.6480
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.3557
                       Mean reward: 837.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 165.7640
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.87s
                      Time elapsed: 00:17:56
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 109248 steps/s (collection: 0.806s, learning 0.094s)
             Mean action noise std: 5.26
          Mean value_function loss: 74.9195
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.3598
                       Mean reward: 822.29
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 166.9033
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.90s
                      Time elapsed: 00:17:57
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 109792 steps/s (collection: 0.805s, learning 0.091s)
             Mean action noise std: 5.27
          Mean value_function loss: 61.5091
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 24.3682
                       Mean reward: 835.89
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 168.8387
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.90s
                      Time elapsed: 00:17:58
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 104695 steps/s (collection: 0.825s, learning 0.114s)
             Mean action noise std: 5.27
          Mean value_function loss: 62.4120
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.3800
                       Mean reward: 852.99
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7307
     Episode_Reward/lifting_object: 164.0185
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.94s
                      Time elapsed: 00:17:59
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 104900 steps/s (collection: 0.844s, learning 0.093s)
             Mean action noise std: 5.28
          Mean value_function loss: 61.4204
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 24.3845
                       Mean reward: 839.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7359
     Episode_Reward/lifting_object: 165.8858
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.94s
                      Time elapsed: 00:18:00
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 108198 steps/s (collection: 0.802s, learning 0.107s)
             Mean action noise std: 5.29
          Mean value_function loss: 67.3771
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 24.3950
                       Mean reward: 839.41
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 165.1482
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.91s
                      Time elapsed: 00:18:01
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 109995 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 5.29
          Mean value_function loss: 60.9122
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 24.4069
                       Mean reward: 788.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 162.4101
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.89s
                      Time elapsed: 00:18:02
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 107027 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 5.30
          Mean value_function loss: 48.1903
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.4149
                       Mean reward: 794.63
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7167
     Episode_Reward/lifting_object: 160.7222
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.92s
                      Time elapsed: 00:18:03
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 110544 steps/s (collection: 0.772s, learning 0.118s)
             Mean action noise std: 5.30
          Mean value_function loss: 51.4327
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.4240
                       Mean reward: 837.48
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7258
     Episode_Reward/lifting_object: 161.8221
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.89s
                      Time elapsed: 00:18:04
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 110980 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 5.31
          Mean value_function loss: 48.6299
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.4333
                       Mean reward: 836.04
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 166.5218
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.89s
                      Time elapsed: 00:18:04
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 106054 steps/s (collection: 0.819s, learning 0.108s)
             Mean action noise std: 5.32
          Mean value_function loss: 51.4598
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.4491
                       Mean reward: 827.95
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7400
     Episode_Reward/lifting_object: 164.8153
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.93s
                      Time elapsed: 00:18:05
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 107276 steps/s (collection: 0.814s, learning 0.103s)
             Mean action noise std: 5.33
          Mean value_function loss: 53.7405
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.4577
                       Mean reward: 847.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 167.1848
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.92s
                      Time elapsed: 00:18:06
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 110151 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 5.34
          Mean value_function loss: 56.6380
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 24.4706
                       Mean reward: 854.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 167.7820
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.89s
                      Time elapsed: 00:18:07
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 105703 steps/s (collection: 0.839s, learning 0.091s)
             Mean action noise std: 5.35
          Mean value_function loss: 56.0410
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.4850
                       Mean reward: 854.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 168.6072
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.93s
                      Time elapsed: 00:18:08
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 107691 steps/s (collection: 0.821s, learning 0.092s)
             Mean action noise std: 5.35
          Mean value_function loss: 51.3018
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.4911
                       Mean reward: 825.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 169.4284
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.91s
                      Time elapsed: 00:18:09
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 109907 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 5.36
          Mean value_function loss: 52.0081
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 24.4975
                       Mean reward: 829.70
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 166.1135
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.89s
                      Time elapsed: 00:18:10
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 110297 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 5.36
          Mean value_function loss: 55.2388
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 24.5036
                       Mean reward: 859.92
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 169.8200
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.89s
                      Time elapsed: 00:18:11
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 106799 steps/s (collection: 0.799s, learning 0.122s)
             Mean action noise std: 5.36
          Mean value_function loss: 46.5650
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 24.5095
                       Mean reward: 850.47
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 168.4087
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.92s
                      Time elapsed: 00:18:12
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 108443 steps/s (collection: 0.796s, learning 0.110s)
             Mean action noise std: 5.38
          Mean value_function loss: 44.4302
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.5262
                       Mean reward: 844.68
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.1171
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.91s
                      Time elapsed: 00:18:13
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 106062 steps/s (collection: 0.810s, learning 0.117s)
             Mean action noise std: 5.39
          Mean value_function loss: 42.6202
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 24.5482
                       Mean reward: 839.02
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 170.4108
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.93s
                      Time elapsed: 00:18:14
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 107202 steps/s (collection: 0.800s, learning 0.117s)
             Mean action noise std: 5.40
          Mean value_function loss: 56.0340
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 24.5549
                       Mean reward: 844.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 169.0318
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.92s
                      Time elapsed: 00:18:14
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 102831 steps/s (collection: 0.846s, learning 0.110s)
             Mean action noise std: 5.40
          Mean value_function loss: 75.8850
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.5638
                       Mean reward: 866.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.2345
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.96s
                      Time elapsed: 00:18:15
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 103815 steps/s (collection: 0.836s, learning 0.111s)
             Mean action noise std: 5.41
          Mean value_function loss: 68.8890
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.5731
                       Mean reward: 818.26
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 166.4088
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.95s
                      Time elapsed: 00:18:16
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 98852 steps/s (collection: 0.884s, learning 0.111s)
             Mean action noise std: 5.41
          Mean value_function loss: 65.8841
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.5820
                       Mean reward: 861.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 167.8933
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.99s
                      Time elapsed: 00:18:17
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 103899 steps/s (collection: 0.833s, learning 0.113s)
             Mean action noise std: 5.42
          Mean value_function loss: 61.3221
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.5900
                       Mean reward: 861.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 169.8985
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.95s
                      Time elapsed: 00:18:18
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 99519 steps/s (collection: 0.872s, learning 0.116s)
             Mean action noise std: 5.42
          Mean value_function loss: 53.3623
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.5955
                       Mean reward: 848.52
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 167.9939
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.99s
                      Time elapsed: 00:18:19
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 110581 steps/s (collection: 0.793s, learning 0.096s)
             Mean action noise std: 5.43
          Mean value_function loss: 34.4730
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.6075
                       Mean reward: 852.65
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 168.9260
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.89s
                      Time elapsed: 00:18:20
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 106819 steps/s (collection: 0.822s, learning 0.099s)
             Mean action noise std: 5.43
          Mean value_function loss: 48.4386
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.6147
                       Mean reward: 833.69
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.2835
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.92s
                      Time elapsed: 00:18:21
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 109478 steps/s (collection: 0.799s, learning 0.099s)
             Mean action noise std: 5.44
          Mean value_function loss: 46.1567
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.6297
                       Mean reward: 845.77
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 167.9067
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.90s
                      Time elapsed: 00:18:22
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 107916 steps/s (collection: 0.820s, learning 0.091s)
             Mean action noise std: 5.45
          Mean value_function loss: 37.6946
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.6445
                       Mean reward: 788.92
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7256
     Episode_Reward/lifting_object: 162.7661
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.91s
                      Time elapsed: 00:18:23
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 111210 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 5.47
          Mean value_function loss: 42.0568
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.6615
                       Mean reward: 840.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 164.1845
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.88s
                      Time elapsed: 00:18:24
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 109345 steps/s (collection: 0.794s, learning 0.105s)
             Mean action noise std: 5.48
          Mean value_function loss: 47.4466
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.6860
                       Mean reward: 837.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 164.3392
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.90s
                      Time elapsed: 00:18:25
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 111822 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 5.49
          Mean value_function loss: 47.9111
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 24.7013
                       Mean reward: 864.15
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 169.9145
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.88s
                      Time elapsed: 00:18:26
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 109961 steps/s (collection: 0.792s, learning 0.102s)
             Mean action noise std: 5.51
          Mean value_function loss: 33.8082
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.7171
                       Mean reward: 842.09
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 164.4902
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.89s
                      Time elapsed: 00:18:26
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 113156 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 5.51
          Mean value_function loss: 38.1461
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.7302
                       Mean reward: 849.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 169.5901
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.87s
                      Time elapsed: 00:18:27
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 100507 steps/s (collection: 0.872s, learning 0.106s)
             Mean action noise std: 5.51
          Mean value_function loss: 32.3017
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.7349
                       Mean reward: 875.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.3363
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.98s
                      Time elapsed: 00:18:28
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 103167 steps/s (collection: 0.852s, learning 0.101s)
             Mean action noise std: 5.52
          Mean value_function loss: 41.6167
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.7425
                       Mean reward: 838.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 169.9182
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.95s
                      Time elapsed: 00:18:29
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 100003 steps/s (collection: 0.894s, learning 0.089s)
             Mean action noise std: 5.53
          Mean value_function loss: 40.0900
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.7560
                       Mean reward: 828.22
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 168.1371
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.98s
                      Time elapsed: 00:18:30
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 106939 steps/s (collection: 0.825s, learning 0.094s)
             Mean action noise std: 5.54
          Mean value_function loss: 36.2284
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 24.7718
                       Mean reward: 865.31
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 169.0595
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.92s
                      Time elapsed: 00:18:31
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 100391 steps/s (collection: 0.865s, learning 0.114s)
             Mean action noise std: 5.56
          Mean value_function loss: 36.0304
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.7942
                       Mean reward: 855.23
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.1740
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.98s
                      Time elapsed: 00:18:32
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 93046 steps/s (collection: 0.913s, learning 0.143s)
             Mean action noise std: 5.57
          Mean value_function loss: 43.7935
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.8095
                       Mean reward: 860.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.0308
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.06s
                      Time elapsed: 00:18:33
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 88695 steps/s (collection: 0.956s, learning 0.152s)
             Mean action noise std: 5.57
          Mean value_function loss: 43.2376
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.8209
                       Mean reward: 870.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 170.9721
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 1.11s
                      Time elapsed: 00:18:34
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 82138 steps/s (collection: 0.970s, learning 0.227s)
             Mean action noise std: 5.59
          Mean value_function loss: 45.4331
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.8363
                       Mean reward: 855.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.1186
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.20s
                      Time elapsed: 00:18:36
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 69795 steps/s (collection: 1.190s, learning 0.218s)
             Mean action noise std: 5.59
          Mean value_function loss: 39.3936
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.8516
                       Mean reward: 861.08
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.3804
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.41s
                      Time elapsed: 00:18:37
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 79298 steps/s (collection: 1.005s, learning 0.234s)
             Mean action noise std: 5.60
          Mean value_function loss: 42.6979
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.8618
                       Mean reward: 852.54
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.2386
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.24s
                      Time elapsed: 00:18:38
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 67316 steps/s (collection: 1.203s, learning 0.258s)
             Mean action noise std: 5.61
          Mean value_function loss: 40.7118
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.8716
                       Mean reward: 860.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.2704
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 1.46s
                      Time elapsed: 00:18:40
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 84350 steps/s (collection: 0.990s, learning 0.175s)
             Mean action noise std: 5.61
          Mean value_function loss: 38.6808
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.8833
                       Mean reward: 863.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.7562
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 1.17s
                      Time elapsed: 00:18:41
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 86490 steps/s (collection: 0.990s, learning 0.147s)
             Mean action noise std: 5.62
          Mean value_function loss: 42.7728
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.8905
                       Mean reward: 862.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 167.5788
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 1.14s
                      Time elapsed: 00:18:42
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 82627 steps/s (collection: 1.063s, learning 0.127s)
             Mean action noise std: 5.63
          Mean value_function loss: 39.5000
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.9033
                       Mean reward: 855.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 169.6582
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.19s
                      Time elapsed: 00:18:43
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 100374 steps/s (collection: 0.872s, learning 0.107s)
             Mean action noise std: 5.64
          Mean value_function loss: 52.9549
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.9165
                       Mean reward: 844.21
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 169.5368
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.98s
                      Time elapsed: 00:18:44
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 104398 steps/s (collection: 0.841s, learning 0.101s)
             Mean action noise std: 5.64
          Mean value_function loss: 43.2895
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.9271
                       Mean reward: 832.58
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 168.8918
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.94s
                      Time elapsed: 00:18:45
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 101496 steps/s (collection: 0.873s, learning 0.095s)
             Mean action noise std: 5.65
          Mean value_function loss: 50.5877
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 24.9359
                       Mean reward: 853.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.1802
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.97s
                      Time elapsed: 00:18:46
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 106204 steps/s (collection: 0.833s, learning 0.093s)
             Mean action noise std: 5.66
          Mean value_function loss: 40.5736
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.9493
                       Mean reward: 846.59
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 170.7334
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.93s
                      Time elapsed: 00:18:47
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 102511 steps/s (collection: 0.864s, learning 0.095s)
             Mean action noise std: 5.67
          Mean value_function loss: 31.5151
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 24.9599
                       Mean reward: 861.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.1327
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.96s
                      Time elapsed: 00:18:48
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 103476 steps/s (collection: 0.845s, learning 0.105s)
             Mean action noise std: 5.68
          Mean value_function loss: 46.8477
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.9697
                       Mean reward: 860.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.5676
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.95s
                      Time elapsed: 00:18:49
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 102456 steps/s (collection: 0.854s, learning 0.106s)
             Mean action noise std: 5.68
          Mean value_function loss: 58.4018
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.9836
                       Mean reward: 856.04
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 168.6424
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.96s
                      Time elapsed: 00:18:50
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 102651 steps/s (collection: 0.863s, learning 0.094s)
             Mean action noise std: 5.69
          Mean value_function loss: 45.4884
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.9954
                       Mean reward: 872.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 166.4205
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.96s
                      Time elapsed: 00:18:51
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 101307 steps/s (collection: 0.826s, learning 0.145s)
             Mean action noise std: 5.71
          Mean value_function loss: 46.1588
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.0108
                       Mean reward: 845.51
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 167.1255
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.97s
                      Time elapsed: 00:18:52
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 101856 steps/s (collection: 0.817s, learning 0.149s)
             Mean action noise std: 5.72
          Mean value_function loss: 39.2560
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.0289
                       Mean reward: 813.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 168.3809
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.97s
                      Time elapsed: 00:18:53
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 111413 steps/s (collection: 0.794s, learning 0.088s)
             Mean action noise std: 5.73
          Mean value_function loss: 44.9276
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 25.0456
                       Mean reward: 868.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 169.7820
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.88s
                      Time elapsed: 00:18:54
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 101890 steps/s (collection: 0.784s, learning 0.180s)
             Mean action noise std: 5.74
          Mean value_function loss: 41.0716
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.0669
                       Mean reward: 858.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 169.2545
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.96s
                      Time elapsed: 00:18:55
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 105750 steps/s (collection: 0.814s, learning 0.116s)
             Mean action noise std: 5.75
          Mean value_function loss: 34.7231
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.0800
                       Mean reward: 868.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 171.8941
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.93s
                      Time elapsed: 00:18:55
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 89204 steps/s (collection: 0.862s, learning 0.240s)
             Mean action noise std: 5.76
          Mean value_function loss: 39.9275
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.0943
                       Mean reward: 856.99
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 169.5990
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.10s
                      Time elapsed: 00:18:57
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 92791 steps/s (collection: 0.932s, learning 0.127s)
             Mean action noise std: 5.76
          Mean value_function loss: 49.5409
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.1020
                       Mean reward: 841.93
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 171.4941
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.06s
                      Time elapsed: 00:18:58
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 92773 steps/s (collection: 0.891s, learning 0.169s)
             Mean action noise std: 5.77
          Mean value_function loss: 41.6406
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.1103
                       Mean reward: 851.04
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 170.5131
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.06s
                      Time elapsed: 00:18:59
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 87259 steps/s (collection: 0.931s, learning 0.195s)
             Mean action noise std: 5.78
          Mean value_function loss: 47.5119
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.1217
                       Mean reward: 848.46
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 169.7239
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.13s
                      Time elapsed: 00:19:00
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 71346 steps/s (collection: 1.194s, learning 0.184s)
             Mean action noise std: 5.79
          Mean value_function loss: 48.1072
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.1371
                       Mean reward: 861.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 169.7105
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.38s
                      Time elapsed: 00:19:01
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 80829 steps/s (collection: 1.072s, learning 0.144s)
             Mean action noise std: 5.80
          Mean value_function loss: 44.2164
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.1540
                       Mean reward: 844.13
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 169.4169
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.22s
                      Time elapsed: 00:19:02
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 83680 steps/s (collection: 1.009s, learning 0.166s)
             Mean action noise std: 5.82
          Mean value_function loss: 43.9049
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.1708
                       Mean reward: 840.02
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 168.0827
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.17s
                      Time elapsed: 00:19:04
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 85619 steps/s (collection: 0.959s, learning 0.190s)
             Mean action noise std: 5.82
          Mean value_function loss: 48.0849
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.1826
                       Mean reward: 866.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 170.6030
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 1.15s
                      Time elapsed: 00:19:05
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 112442 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 5.82
          Mean value_function loss: 47.3730
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.1865
                       Mean reward: 863.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.0169
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.87s
                      Time elapsed: 00:19:06
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 111996 steps/s (collection: 0.772s, learning 0.106s)
             Mean action noise std: 5.83
          Mean value_function loss: 45.2832
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.1943
                       Mean reward: 851.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 170.5075
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.88s
                      Time elapsed: 00:19:06
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 111921 steps/s (collection: 0.765s, learning 0.114s)
             Mean action noise std: 5.84
          Mean value_function loss: 41.0096
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 25.2070
                       Mean reward: 845.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.2176
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.88s
                      Time elapsed: 00:19:07
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 110956 steps/s (collection: 0.771s, learning 0.115s)
             Mean action noise std: 5.84
          Mean value_function loss: 53.3616
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.2149
                       Mean reward: 857.60
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 167.4733
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.89s
                      Time elapsed: 00:19:08
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 109133 steps/s (collection: 0.792s, learning 0.109s)
             Mean action noise std: 5.86
          Mean value_function loss: 53.7230
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.2243
                       Mean reward: 842.73
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 169.6784
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.90s
                      Time elapsed: 00:19:09
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 110720 steps/s (collection: 0.790s, learning 0.098s)
             Mean action noise std: 5.87
          Mean value_function loss: 51.3813
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 25.2428
                       Mean reward: 829.72
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 168.4238
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.89s
                      Time elapsed: 00:19:10
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 112296 steps/s (collection: 0.786s, learning 0.089s)
             Mean action noise std: 5.88
          Mean value_function loss: 56.8166
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.2574
                       Mean reward: 858.24
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 168.5454
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.88s
                      Time elapsed: 00:19:11
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 108450 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 5.88
          Mean value_function loss: 61.4664
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.2715
                       Mean reward: 841.57
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 166.3229
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.91s
                      Time elapsed: 00:19:12
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 109657 steps/s (collection: 0.788s, learning 0.109s)
             Mean action noise std: 5.90
          Mean value_function loss: 61.6129
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.2849
                       Mean reward: 849.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 167.3860
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.90s
                      Time elapsed: 00:19:13
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 113781 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 5.91
          Mean value_function loss: 54.9340
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.3004
                       Mean reward: 859.54
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 169.6394
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.86s
                      Time elapsed: 00:19:14
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 111718 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 5.92
          Mean value_function loss: 47.9037
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 25.3131
                       Mean reward: 849.60
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 166.8959
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.88s
                      Time elapsed: 00:19:14
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 113690 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 5.92
          Mean value_function loss: 53.5032
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 25.3191
                       Mean reward: 823.78
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 165.4162
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.86s
                      Time elapsed: 00:19:15
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 111992 steps/s (collection: 0.790s, learning 0.088s)
             Mean action noise std: 5.93
          Mean value_function loss: 49.6718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.3258
                       Mean reward: 838.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 167.6356
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.88s
                      Time elapsed: 00:19:16
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 111797 steps/s (collection: 0.791s, learning 0.089s)
             Mean action noise std: 5.93
          Mean value_function loss: 41.8412
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.3381
                       Mean reward: 845.30
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 169.2065
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.88s
                      Time elapsed: 00:19:17
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 113118 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 5.94
          Mean value_function loss: 48.7230
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.3462
                       Mean reward: 852.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.9354
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.87s
                      Time elapsed: 00:19:18
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 111587 steps/s (collection: 0.778s, learning 0.103s)
             Mean action noise std: 5.95
          Mean value_function loss: 50.7150
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.3562
                       Mean reward: 859.96
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 168.6546
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.88s
                      Time elapsed: 00:19:19
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 112373 steps/s (collection: 0.782s, learning 0.093s)
             Mean action noise std: 5.96
          Mean value_function loss: 57.3654
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 25.3714
                       Mean reward: 827.36
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7380
     Episode_Reward/lifting_object: 165.3492
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.87s
                      Time elapsed: 00:19:20
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 111863 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 5.97
          Mean value_function loss: 57.9193
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 25.3916
                       Mean reward: 845.64
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 165.3754
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.88s
                      Time elapsed: 00:19:21
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 113526 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 5.98
          Mean value_function loss: 54.1743
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 25.4010
                       Mean reward: 850.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 167.5927
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.87s
                      Time elapsed: 00:19:21
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 112952 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 5.99
          Mean value_function loss: 51.5775
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.4110
                       Mean reward: 858.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 168.3485
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.87s
                      Time elapsed: 00:19:22
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 111530 steps/s (collection: 0.785s, learning 0.097s)
             Mean action noise std: 5.99
          Mean value_function loss: 61.5337
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 25.4211
                       Mean reward: 843.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.5524
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.88s
                      Time elapsed: 00:19:23
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 112181 steps/s (collection: 0.778s, learning 0.098s)
             Mean action noise std: 5.99
          Mean value_function loss: 58.3863
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.4221
                       Mean reward: 852.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 169.0270
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.88s
                      Time elapsed: 00:19:24
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 109815 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 6.00
          Mean value_function loss: 60.0715
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 25.4260
                       Mean reward: 846.17
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 167.1783
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.90s
                      Time elapsed: 00:19:25
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 110605 steps/s (collection: 0.785s, learning 0.104s)
             Mean action noise std: 6.01
          Mean value_function loss: 61.0266
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.4357
                       Mean reward: 840.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 168.7812
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.89s
                      Time elapsed: 00:19:26
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 107660 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 6.01
          Mean value_function loss: 68.4965
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 25.4463
                       Mean reward: 853.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 169.3762
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.91s
                      Time elapsed: 00:19:27
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 110053 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 6.02
          Mean value_function loss: 60.0061
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.4510
                       Mean reward: 846.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 166.0397
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.89s
                      Time elapsed: 00:19:28
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 109843 steps/s (collection: 0.792s, learning 0.103s)
             Mean action noise std: 6.02
          Mean value_function loss: 47.6996
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.4574
                       Mean reward: 848.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 168.8335
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.89s
                      Time elapsed: 00:19:29
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 113152 steps/s (collection: 0.777s, learning 0.092s)
             Mean action noise std: 6.03
          Mean value_function loss: 68.6708
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.4638
                       Mean reward: 834.17
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 167.0494
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.87s
                      Time elapsed: 00:19:29
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 113969 steps/s (collection: 0.762s, learning 0.101s)
             Mean action noise std: 6.03
          Mean value_function loss: 54.8901
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.4716
                       Mean reward: 850.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 166.8437
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.86s
                      Time elapsed: 00:19:30
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 110883 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 6.04
          Mean value_function loss: 59.2091
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.4810
                       Mean reward: 850.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 167.2196
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.89s
                      Time elapsed: 00:19:31
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 114731 steps/s (collection: 0.761s, learning 0.096s)
             Mean action noise std: 6.06
          Mean value_function loss: 63.6569
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.5010
                       Mean reward: 839.91
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 167.9714
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.86s
                      Time elapsed: 00:19:32
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 113290 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 6.06
          Mean value_function loss: 53.9065
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.5164
                       Mean reward: 821.98
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 166.2009
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.87s
                      Time elapsed: 00:19:33
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 111532 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 6.08
          Mean value_function loss: 57.8854
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.5299
                       Mean reward: 818.28
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 165.7167
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.88s
                      Time elapsed: 00:19:34
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 115552 steps/s (collection: 0.763s, learning 0.088s)
             Mean action noise std: 6.09
          Mean value_function loss: 46.6287
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.5510
                       Mean reward: 832.50
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 165.4064
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.85s
                      Time elapsed: 00:19:35
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 110721 steps/s (collection: 0.770s, learning 0.118s)
             Mean action noise std: 6.11
          Mean value_function loss: 48.8073
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.5696
                       Mean reward: 838.76
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 167.7046
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.89s
                      Time elapsed: 00:19:36
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 113681 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 6.12
          Mean value_function loss: 52.9457
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 25.5893
                       Mean reward: 850.85
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.8750
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.86s
                      Time elapsed: 00:19:36
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 114789 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 6.13
          Mean value_function loss: 73.1290
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 25.6075
                       Mean reward: 827.76
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 167.6102
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.86s
                      Time elapsed: 00:19:37
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 109390 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 6.14
          Mean value_function loss: 74.7712
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.6144
                       Mean reward: 861.54
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 168.8772
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.90s
                      Time elapsed: 00:19:38
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 112925 steps/s (collection: 0.767s, learning 0.104s)
             Mean action noise std: 6.14
          Mean value_function loss: 71.9649
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.6244
                       Mean reward: 820.01
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 167.0829
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.87s
                      Time elapsed: 00:19:39
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 110907 steps/s (collection: 0.774s, learning 0.113s)
             Mean action noise std: 6.15
          Mean value_function loss: 79.4742
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.6338
                       Mean reward: 854.41
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 167.6170
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.89s
                      Time elapsed: 00:19:40
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 108329 steps/s (collection: 0.795s, learning 0.113s)
             Mean action noise std: 6.16
          Mean value_function loss: 65.6022
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.6439
                       Mean reward: 857.03
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 169.8689
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.91s
                      Time elapsed: 00:19:41
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 112181 steps/s (collection: 0.786s, learning 0.090s)
             Mean action noise std: 6.17
          Mean value_function loss: 66.8494
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.6572
                       Mean reward: 845.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 166.7734
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.88s
                      Time elapsed: 00:19:42
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 111530 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 6.18
          Mean value_function loss: 51.2462
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.6708
                       Mean reward: 823.52
               Mean episode length: 245.02
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 166.6517
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.88s
                      Time elapsed: 00:19:43
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 113184 steps/s (collection: 0.777s, learning 0.092s)
             Mean action noise std: 6.19
          Mean value_function loss: 59.1470
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.6821
                       Mean reward: 831.50
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 168.8139
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.87s
                      Time elapsed: 00:19:43
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 110832 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 6.19
          Mean value_function loss: 48.9104
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.6886
                       Mean reward: 834.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7448
     Episode_Reward/lifting_object: 166.2740
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.89s
                      Time elapsed: 00:19:44
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 111048 steps/s (collection: 0.784s, learning 0.102s)
             Mean action noise std: 6.20
          Mean value_function loss: 54.1902
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 25.6922
                       Mean reward: 819.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7274
     Episode_Reward/lifting_object: 161.5749
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.89s
                      Time elapsed: 00:19:45
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 113916 steps/s (collection: 0.772s, learning 0.091s)
             Mean action noise std: 6.21
          Mean value_function loss: 50.9901
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.6998
                       Mean reward: 832.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 166.8908
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.86s
                      Time elapsed: 00:19:46
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 105258 steps/s (collection: 0.835s, learning 0.099s)
             Mean action noise std: 6.21
          Mean value_function loss: 58.7598
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 25.7084
                       Mean reward: 851.70
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7282
     Episode_Reward/lifting_object: 162.5617
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.93s
                      Time elapsed: 00:19:47
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 112837 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 6.22
          Mean value_function loss: 58.2523
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.7131
                       Mean reward: 775.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7276
     Episode_Reward/lifting_object: 161.1630
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.87s
                      Time elapsed: 00:19:48
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 104572 steps/s (collection: 0.830s, learning 0.110s)
             Mean action noise std: 6.22
          Mean value_function loss: 62.1153
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.7212
                       Mean reward: 854.67
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 166.6483
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.94s
                      Time elapsed: 00:19:49
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 105892 steps/s (collection: 0.823s, learning 0.106s)
             Mean action noise std: 6.23
          Mean value_function loss: 49.8714
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.7297
                       Mean reward: 810.97
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7462
     Episode_Reward/lifting_object: 166.9015
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.93s
                      Time elapsed: 00:19:50
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 99180 steps/s (collection: 0.866s, learning 0.126s)
             Mean action noise std: 6.25
          Mean value_function loss: 51.1061
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.7450
                       Mean reward: 854.35
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 167.6792
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.99s
                      Time elapsed: 00:19:51
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 96260 steps/s (collection: 0.920s, learning 0.102s)
             Mean action noise std: 6.26
          Mean value_function loss: 57.8743
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.7639
                       Mean reward: 852.62
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.4168
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 1.02s
                      Time elapsed: 00:19:52
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 101563 steps/s (collection: 0.850s, learning 0.118s)
             Mean action noise std: 6.27
          Mean value_function loss: 59.9945
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.7778
                       Mean reward: 857.38
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 167.3926
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.97s
                      Time elapsed: 00:19:53
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 98130 steps/s (collection: 0.890s, learning 0.112s)
             Mean action noise std: 6.28
          Mean value_function loss: 60.5126
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 25.7917
                       Mean reward: 856.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 166.4878
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 1.00s
                      Time elapsed: 00:19:54
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 101045 steps/s (collection: 0.848s, learning 0.125s)
             Mean action noise std: 6.29
          Mean value_function loss: 53.0912
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.8047
                       Mean reward: 835.90
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 168.6647
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.97s
                      Time elapsed: 00:19:55
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 104419 steps/s (collection: 0.829s, learning 0.112s)
             Mean action noise std: 6.30
          Mean value_function loss: 50.0409
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 25.8234
                       Mean reward: 829.01
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 166.9014
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.94s
                      Time elapsed: 00:19:56
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 106575 steps/s (collection: 0.806s, learning 0.117s)
             Mean action noise std: 6.30
          Mean value_function loss: 53.5717
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.8246
                       Mean reward: 830.75
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 165.2896
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.92s
                      Time elapsed: 00:19:57
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 103926 steps/s (collection: 0.837s, learning 0.109s)
             Mean action noise std: 6.31
          Mean value_function loss: 50.6420
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.8284
                       Mean reward: 820.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 165.3199
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.95s
                      Time elapsed: 00:19:58
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 84703 steps/s (collection: 1.025s, learning 0.135s)
             Mean action noise std: 6.32
          Mean value_function loss: 38.1458
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.8406
                       Mean reward: 836.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 169.7346
      Episode_Reward/object_height: 0.0334
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 1.16s
                      Time elapsed: 00:19:59
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 93111 steps/s (collection: 0.916s, learning 0.140s)
             Mean action noise std: 6.33
          Mean value_function loss: 41.4006
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.8511
                       Mean reward: 861.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.0947
      Episode_Reward/object_height: 0.0336
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 1.06s
                      Time elapsed: 00:20:00
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 100742 steps/s (collection: 0.835s, learning 0.141s)
             Mean action noise std: 6.33
          Mean value_function loss: 50.0712
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 25.8628
                       Mean reward: 845.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 165.4432
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.98s
                      Time elapsed: 00:20:01
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 103090 steps/s (collection: 0.821s, learning 0.133s)
             Mean action noise std: 6.34
          Mean value_function loss: 53.8040
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.8721
                       Mean reward: 832.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 167.5651
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.95s
                      Time elapsed: 00:20:02
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 104997 steps/s (collection: 0.795s, learning 0.142s)
             Mean action noise std: 6.35
          Mean value_function loss: 51.2092
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.8840
                       Mean reward: 846.96
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 165.5817
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.94s
                      Time elapsed: 00:20:03
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 96001 steps/s (collection: 0.913s, learning 0.111s)
             Mean action noise std: 6.37
          Mean value_function loss: 52.8152
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.9012
                       Mean reward: 833.54
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 168.7192
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 1.02s
                      Time elapsed: 00:20:04
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 101578 steps/s (collection: 0.815s, learning 0.153s)
             Mean action noise std: 6.38
          Mean value_function loss: 46.8953
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 25.9167
                       Mean reward: 842.34
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.9770
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.97s
                      Time elapsed: 00:20:05
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 100271 steps/s (collection: 0.846s, learning 0.135s)
             Mean action noise std: 6.39
          Mean value_function loss: 70.3708
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.9307
                       Mean reward: 853.09
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 167.5961
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.98s
                      Time elapsed: 00:20:06
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 102410 steps/s (collection: 0.817s, learning 0.143s)
             Mean action noise std: 6.40
          Mean value_function loss: 94.4992
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.9489
                       Mean reward: 830.35
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 165.5284
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.96s
                      Time elapsed: 00:20:07
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 100038 steps/s (collection: 0.827s, learning 0.156s)
             Mean action noise std: 6.41
          Mean value_function loss: 60.6580
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 25.9608
                       Mean reward: 857.27
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 170.0487
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.98s
                      Time elapsed: 00:20:07
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 107259 steps/s (collection: 0.797s, learning 0.120s)
             Mean action noise std: 6.42
          Mean value_function loss: 68.7634
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.9686
                       Mean reward: 825.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 168.9298
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.92s
                      Time elapsed: 00:20:08
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 101876 steps/s (collection: 0.836s, learning 0.129s)
             Mean action noise std: 6.43
          Mean value_function loss: 53.9996
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.9846
                       Mean reward: 836.52
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 167.5226
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.96s
                      Time elapsed: 00:20:09
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 107547 steps/s (collection: 0.804s, learning 0.110s)
             Mean action noise std: 6.44
          Mean value_function loss: 68.2586
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 25.9935
                       Mean reward: 875.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 168.4119
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.91s
                      Time elapsed: 00:20:10
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 108365 steps/s (collection: 0.797s, learning 0.111s)
             Mean action noise std: 6.44
          Mean value_function loss: 57.2227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.9991
                       Mean reward: 859.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 167.4523
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.91s
                      Time elapsed: 00:20:11
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 105498 steps/s (collection: 0.812s, learning 0.120s)
             Mean action noise std: 6.45
          Mean value_function loss: 52.4180
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 26.0076
                       Mean reward: 825.98
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 168.6832
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.93s
                      Time elapsed: 00:20:12
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 107212 steps/s (collection: 0.817s, learning 0.100s)
             Mean action noise std: 6.46
          Mean value_function loss: 47.6594
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.0159
                       Mean reward: 857.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 167.4677
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.92s
                      Time elapsed: 00:20:13
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 110200 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 6.46
          Mean value_function loss: 53.4641
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.0263
                       Mean reward: 831.94
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: 164.5979
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.89s
                      Time elapsed: 00:20:14
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 87317 steps/s (collection: 0.900s, learning 0.226s)
             Mean action noise std: 6.48
          Mean value_function loss: 57.9894
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0392
                       Mean reward: 811.85
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7211
     Episode_Reward/lifting_object: 163.0865
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 1.13s
                      Time elapsed: 00:20:15
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 78463 steps/s (collection: 1.041s, learning 0.212s)
             Mean action noise std: 6.48
          Mean value_function loss: 52.9215
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.0520
                       Mean reward: 858.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 168.3808
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 1.25s
                      Time elapsed: 00:20:16
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 85313 steps/s (collection: 0.917s, learning 0.235s)
             Mean action noise std: 6.50
          Mean value_function loss: 52.3591
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.0628
                       Mean reward: 843.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7368
     Episode_Reward/lifting_object: 167.2474
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 1.15s
                      Time elapsed: 00:20:17
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 93365 steps/s (collection: 0.927s, learning 0.126s)
             Mean action noise std: 6.51
          Mean value_function loss: 56.6520
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.0787
                       Mean reward: 826.36
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 165.8134
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 1.05s
                      Time elapsed: 00:20:19
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 98924 steps/s (collection: 0.858s, learning 0.136s)
             Mean action noise std: 6.52
          Mean value_function loss: 39.3109
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.0870
                       Mean reward: 852.03
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.0928
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.99s
                      Time elapsed: 00:20:20
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 97980 steps/s (collection: 0.902s, learning 0.102s)
             Mean action noise std: 6.53
          Mean value_function loss: 51.5486
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 26.1037
                       Mean reward: 859.03
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 168.8487
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 1.00s
                      Time elapsed: 00:20:21
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 97815 steps/s (collection: 0.837s, learning 0.168s)
             Mean action noise std: 6.54
          Mean value_function loss: 48.3906
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.1194
                       Mean reward: 860.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 168.6141
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 1.00s
                      Time elapsed: 00:20:22
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 101563 steps/s (collection: 0.852s, learning 0.116s)
             Mean action noise std: 6.55
          Mean value_function loss: 42.0102
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.1315
                       Mean reward: 866.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.9298
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.97s
                      Time elapsed: 00:20:22
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 99157 steps/s (collection: 0.786s, learning 0.205s)
             Mean action noise std: 6.56
          Mean value_function loss: 54.2839
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.1420
                       Mean reward: 869.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 169.7551
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.99s
                      Time elapsed: 00:20:23
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 114270 steps/s (collection: 0.763s, learning 0.097s)
             Mean action noise std: 6.56
          Mean value_function loss: 47.0932
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.1496
                       Mean reward: 830.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 168.5706
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.86s
                      Time elapsed: 00:20:24
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 111374 steps/s (collection: 0.772s, learning 0.111s)
             Mean action noise std: 6.57
          Mean value_function loss: 55.5280
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.1599
                       Mean reward: 853.79
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 169.1244
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.88s
                      Time elapsed: 00:20:25
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 112303 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 6.58
          Mean value_function loss: 46.9657
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.1680
                       Mean reward: 831.68
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 169.1507
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.88s
                      Time elapsed: 00:20:26
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 110732 steps/s (collection: 0.785s, learning 0.103s)
             Mean action noise std: 6.59
          Mean value_function loss: 50.7448
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.1808
                       Mean reward: 840.18
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 168.8995
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.89s
                      Time elapsed: 00:20:27
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 103824 steps/s (collection: 0.846s, learning 0.101s)
             Mean action noise std: 6.60
          Mean value_function loss: 44.5912
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.1921
                       Mean reward: 819.62
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 167.6946
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.95s
                      Time elapsed: 00:20:28
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 103279 steps/s (collection: 0.827s, learning 0.125s)
             Mean action noise std: 6.61
          Mean value_function loss: 43.2994
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.2071
                       Mean reward: 857.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 168.7486
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.95s
                      Time elapsed: 00:20:29
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 101449 steps/s (collection: 0.788s, learning 0.181s)
             Mean action noise std: 6.61
          Mean value_function loss: 38.4716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.2137
                       Mean reward: 853.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 167.4497
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.97s
                      Time elapsed: 00:20:30
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 102371 steps/s (collection: 0.858s, learning 0.103s)
             Mean action noise std: 6.62
          Mean value_function loss: 47.6385
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.2177
                       Mean reward: 852.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.6905
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.96s
                      Time elapsed: 00:20:31
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 101832 steps/s (collection: 0.864s, learning 0.102s)
             Mean action noise std: 6.62
          Mean value_function loss: 41.7129
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.2238
                       Mean reward: 858.03
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 168.7801
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.97s
                      Time elapsed: 00:20:32
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 107842 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 6.63
          Mean value_function loss: 35.2610
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.2307
                       Mean reward: 862.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.8938
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.91s
                      Time elapsed: 00:20:33
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 105906 steps/s (collection: 0.838s, learning 0.090s)
             Mean action noise std: 6.63
          Mean value_function loss: 40.0311
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 26.2382
                       Mean reward: 834.72
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.4081
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.93s
                      Time elapsed: 00:20:34
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 104442 steps/s (collection: 0.822s, learning 0.119s)
             Mean action noise std: 6.64
          Mean value_function loss: 43.5599
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.2433
                       Mean reward: 843.03
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.5056
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.94s
                      Time elapsed: 00:20:35
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 106514 steps/s (collection: 0.823s, learning 0.100s)
             Mean action noise std: 6.65
          Mean value_function loss: 37.3771
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.2512
                       Mean reward: 832.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 166.9504
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.92s
                      Time elapsed: 00:20:35
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 104224 steps/s (collection: 0.786s, learning 0.157s)
             Mean action noise std: 6.65
          Mean value_function loss: 43.3298
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.2618
                       Mean reward: 860.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 168.6487
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.94s
                      Time elapsed: 00:20:36
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 107103 steps/s (collection: 0.786s, learning 0.132s)
             Mean action noise std: 6.66
          Mean value_function loss: 52.0046
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.2691
                       Mean reward: 855.46
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 171.1403
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.92s
                      Time elapsed: 00:20:37
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 104314 steps/s (collection: 0.789s, learning 0.154s)
             Mean action noise std: 6.67
          Mean value_function loss: 37.9346
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 26.2825
                       Mean reward: 856.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.3818
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.94s
                      Time elapsed: 00:20:38
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 100275 steps/s (collection: 0.827s, learning 0.153s)
             Mean action noise std: 6.69
          Mean value_function loss: 47.0135
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.2963
                       Mean reward: 842.69
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.3330
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.98s
                      Time elapsed: 00:20:39
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 106216 steps/s (collection: 0.826s, learning 0.099s)
             Mean action noise std: 6.70
          Mean value_function loss: 39.4924
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.3127
                       Mean reward: 818.99
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 168.8054
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.93s
                      Time elapsed: 00:20:40
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 109774 steps/s (collection: 0.806s, learning 0.090s)
             Mean action noise std: 6.71
          Mean value_function loss: 43.9132
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.3290
                       Mean reward: 873.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.5333
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.90s
                      Time elapsed: 00:20:41
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 107453 steps/s (collection: 0.811s, learning 0.104s)
             Mean action noise std: 6.73
          Mean value_function loss: 43.2709
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3441
                       Mean reward: 865.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.2060
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.91s
                      Time elapsed: 00:20:42
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 111888 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 6.74
          Mean value_function loss: 53.2383
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.3632
                       Mean reward: 848.29
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 168.4939
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.88s
                      Time elapsed: 00:20:43
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 109312 steps/s (collection: 0.805s, learning 0.094s)
             Mean action noise std: 6.75
          Mean value_function loss: 46.8504
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 26.3757
                       Mean reward: 849.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.7363
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.90s
                      Time elapsed: 00:20:44
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 110529 steps/s (collection: 0.803s, learning 0.087s)
             Mean action noise std: 6.76
          Mean value_function loss: 50.7981
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.3891
                       Mean reward: 858.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.5111
      Episode_Reward/object_height: 0.0334
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.89s
                      Time elapsed: 00:20:45
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 111759 steps/s (collection: 0.790s, learning 0.090s)
             Mean action noise std: 6.78
          Mean value_function loss: 46.2594
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.4044
                       Mean reward: 831.71
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 169.4979
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.88s
                      Time elapsed: 00:20:46
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 106750 steps/s (collection: 0.778s, learning 0.143s)
             Mean action noise std: 6.78
          Mean value_function loss: 54.3222
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 26.4187
                       Mean reward: 867.92
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 169.9045
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.92s
                      Time elapsed: 00:20:46
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 109251 steps/s (collection: 0.784s, learning 0.116s)
             Mean action noise std: 6.79
          Mean value_function loss: 41.9048
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.4278
                       Mean reward: 854.44
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 168.2714
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.90s
                      Time elapsed: 00:20:47
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 103764 steps/s (collection: 0.793s, learning 0.155s)
             Mean action noise std: 6.80
          Mean value_function loss: 42.7421
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 26.4407
                       Mean reward: 856.01
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.4414
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.95s
                      Time elapsed: 00:20:48
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 113309 steps/s (collection: 0.761s, learning 0.107s)
             Mean action noise std: 6.81
          Mean value_function loss: 51.8333
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 26.4487
                       Mean reward: 853.96
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 169.8288
      Episode_Reward/object_height: 0.0338
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.87s
                      Time elapsed: 00:20:49
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 109974 steps/s (collection: 0.799s, learning 0.095s)
             Mean action noise std: 6.82
          Mean value_function loss: 48.2461
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 26.4589
                       Mean reward: 847.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.5963
      Episode_Reward/object_height: 0.0341
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.89s
                      Time elapsed: 00:20:50
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 111522 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 6.83
          Mean value_function loss: 36.9479
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 26.4718
                       Mean reward: 838.37
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 168.7943
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.88s
                      Time elapsed: 00:20:51
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 102555 steps/s (collection: 0.840s, learning 0.118s)
             Mean action noise std: 6.84
          Mean value_function loss: 42.7989
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.4873
                       Mean reward: 847.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 167.5842
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.96s
                      Time elapsed: 00:20:52
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 108110 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 6.87
          Mean value_function loss: 53.5180
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.5118
                       Mean reward: 851.32
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 169.8351
      Episode_Reward/object_height: 0.0343
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.91s
                      Time elapsed: 00:20:53
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 107035 steps/s (collection: 0.811s, learning 0.107s)
             Mean action noise std: 6.88
          Mean value_function loss: 59.2342
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 26.5329
                       Mean reward: 838.31
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 167.9456
      Episode_Reward/object_height: 0.0342
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.92s
                      Time elapsed: 00:20:54
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 108050 steps/s (collection: 0.815s, learning 0.095s)
             Mean action noise std: 6.89
          Mean value_function loss: 47.1904
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.5425
                       Mean reward: 851.46
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 164.1977
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.91s
                      Time elapsed: 00:20:55
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 104369 steps/s (collection: 0.829s, learning 0.113s)
             Mean action noise std: 6.89
          Mean value_function loss: 46.2227
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 26.5540
                       Mean reward: 869.48
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.6649
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.94s
                      Time elapsed: 00:20:56
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 104998 steps/s (collection: 0.814s, learning 0.122s)
             Mean action noise std: 6.90
          Mean value_function loss: 54.3656
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.5622
                       Mean reward: 856.81
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 168.1167
      Episode_Reward/object_height: 0.0346
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.94s
                      Time elapsed: 00:20:57
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 99718 steps/s (collection: 0.872s, learning 0.114s)
             Mean action noise std: 6.91
          Mean value_function loss: 49.3445
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 26.5700
                       Mean reward: 847.50
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 170.1774
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.99s
                      Time elapsed: 00:20:58
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 109435 steps/s (collection: 0.810s, learning 0.088s)
             Mean action noise std: 6.92
          Mean value_function loss: 44.0798
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 26.5757
                       Mean reward: 858.47
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 168.9975
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.90s
                      Time elapsed: 00:20:58
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 107906 steps/s (collection: 0.779s, learning 0.132s)
             Mean action noise std: 6.93
          Mean value_function loss: 49.8073
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.5889
                       Mean reward: 861.72
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.4426
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.91s
                      Time elapsed: 00:20:59
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 106829 steps/s (collection: 0.786s, learning 0.134s)
             Mean action noise std: 6.94
          Mean value_function loss: 44.4540
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.6018
                       Mean reward: 834.99
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 168.0254
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.92s
                      Time elapsed: 00:21:00
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 110171 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 6.96
          Mean value_function loss: 45.5813
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.6170
                       Mean reward: 864.17
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 169.1627
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.89s
                      Time elapsed: 00:21:01
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 108060 steps/s (collection: 0.792s, learning 0.118s)
             Mean action noise std: 6.96
          Mean value_function loss: 55.6306
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 26.6315
                       Mean reward: 863.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 170.2821
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1010
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.91s
                      Time elapsed: 00:21:02
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 107038 steps/s (collection: 0.813s, learning 0.105s)
             Mean action noise std: 6.97
          Mean value_function loss: 47.5270
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.6362
                       Mean reward: 858.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.3045
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.92s
                      Time elapsed: 00:21:03
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 110648 steps/s (collection: 0.792s, learning 0.097s)
             Mean action noise std: 6.97
          Mean value_function loss: 69.7117
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.6446
                       Mean reward: 858.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 168.9480
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.89s
                      Time elapsed: 00:21:04
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 111569 steps/s (collection: 0.785s, learning 0.097s)
             Mean action noise std: 6.99
          Mean value_function loss: 45.2705
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.6575
                       Mean reward: 831.88
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 168.2784
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.88s
                      Time elapsed: 00:21:05
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 110064 steps/s (collection: 0.794s, learning 0.100s)
             Mean action noise std: 7.00
          Mean value_function loss: 50.6594
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.6732
                       Mean reward: 865.75
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.5966
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.89s
                      Time elapsed: 00:21:06
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 107343 steps/s (collection: 0.803s, learning 0.113s)
             Mean action noise std: 7.00
          Mean value_function loss: 48.0157
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.6813
                       Mean reward: 854.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 168.9472
      Episode_Reward/object_height: 0.0354
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.92s
                      Time elapsed: 00:21:07
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 109206 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 7.01
          Mean value_function loss: 59.2037
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.6907
                       Mean reward: 855.59
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 169.5549
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.90s
                      Time elapsed: 00:21:07
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 107286 steps/s (collection: 0.777s, learning 0.140s)
             Mean action noise std: 7.02
          Mean value_function loss: 44.8185
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.7004
                       Mean reward: 867.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 171.6406
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.92s
                      Time elapsed: 00:21:08
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 106909 steps/s (collection: 0.767s, learning 0.152s)
             Mean action noise std: 7.03
          Mean value_function loss: 71.4707
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.7124
                       Mean reward: 816.34
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 168.4603
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.92s
                      Time elapsed: 00:21:09
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 112480 steps/s (collection: 0.757s, learning 0.117s)
             Mean action noise std: 7.05
          Mean value_function loss: 58.3276
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.7334
                       Mean reward: 847.25
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 165.5291
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.87s
                      Time elapsed: 00:21:10
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 106700 steps/s (collection: 0.817s, learning 0.104s)
             Mean action noise std: 7.06
          Mean value_function loss: 62.3318
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.7532
                       Mean reward: 847.52
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.7885
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.1027
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.92s
                      Time elapsed: 00:21:11
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 109389 steps/s (collection: 0.806s, learning 0.093s)
             Mean action noise std: 7.07
          Mean value_function loss: 63.8555
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.7660
                       Mean reward: 837.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7322
     Episode_Reward/lifting_object: 164.1033
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.1034
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.90s
                      Time elapsed: 00:21:12
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 113729 steps/s (collection: 0.778s, learning 0.087s)
             Mean action noise std: 7.08
          Mean value_function loss: 54.8229
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.7789
                       Mean reward: 840.23
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 167.0692
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.86s
                      Time elapsed: 00:21:13
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 110602 steps/s (collection: 0.796s, learning 0.093s)
             Mean action noise std: 7.09
          Mean value_function loss: 58.9989
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.7913
                       Mean reward: 846.08
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 167.8711
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.89s
                      Time elapsed: 00:21:14
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 113799 steps/s (collection: 0.771s, learning 0.093s)
             Mean action noise std: 7.10
          Mean value_function loss: 49.8550
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.8035
                       Mean reward: 857.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.4296
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.86s
                      Time elapsed: 00:21:15
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 105656 steps/s (collection: 0.815s, learning 0.115s)
             Mean action noise std: 7.11
          Mean value_function loss: 63.5645
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.8122
                       Mean reward: 842.67
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.3474
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.93s
                      Time elapsed: 00:21:16
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 112121 steps/s (collection: 0.773s, learning 0.104s)
             Mean action noise std: 7.12
          Mean value_function loss: 68.9575
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 26.8197
                       Mean reward: 823.63
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 166.3385
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.88s
                      Time elapsed: 00:21:16
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 103259 steps/s (collection: 0.800s, learning 0.152s)
             Mean action noise std: 7.13
          Mean value_function loss: 58.8031
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.8279
                       Mean reward: 827.53
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7359
     Episode_Reward/lifting_object: 164.4451
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.95s
                      Time elapsed: 00:21:17
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 106112 steps/s (collection: 0.770s, learning 0.156s)
             Mean action noise std: 7.13
          Mean value_function loss: 53.5161
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.8362
                       Mean reward: 873.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 169.0206
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.93s
                      Time elapsed: 00:21:18
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 106577 steps/s (collection: 0.774s, learning 0.149s)
             Mean action noise std: 7.14
          Mean value_function loss: 62.2790
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.8455
                       Mean reward: 856.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7388
     Episode_Reward/lifting_object: 165.1517
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.92s
                      Time elapsed: 00:21:19
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 109310 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 7.15
          Mean value_function loss: 74.0791
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.8541
                       Mean reward: 830.72
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 167.1982
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.90s
                      Time elapsed: 00:21:20
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 112288 steps/s (collection: 0.780s, learning 0.095s)
             Mean action noise std: 7.16
          Mean value_function loss: 83.1867
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 26.8614
                       Mean reward: 832.77
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 166.5478
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.88s
                      Time elapsed: 00:21:21
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 108444 steps/s (collection: 0.802s, learning 0.105s)
             Mean action noise std: 7.16
          Mean value_function loss: 54.5427
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.8710
                       Mean reward: 841.01
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 166.9790
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.91s
                      Time elapsed: 00:21:22
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 111655 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 7.17
          Mean value_function loss: 53.2765
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.8756
                       Mean reward: 865.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 168.4399
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.88s
                      Time elapsed: 00:21:23
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 114059 steps/s (collection: 0.770s, learning 0.092s)
             Mean action noise std: 7.18
          Mean value_function loss: 62.1715
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.8836
                       Mean reward: 819.24
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 168.7453
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.86s
                      Time elapsed: 00:21:24
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 112165 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 7.19
          Mean value_function loss: 52.3974
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 26.8952
                       Mean reward: 821.92
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7344
     Episode_Reward/lifting_object: 163.9082
      Episode_Reward/object_height: 0.0354
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.88s
                      Time elapsed: 00:21:25
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 113379 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 7.21
          Mean value_function loss: 57.4442
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.9133
                       Mean reward: 858.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 167.6028
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.87s
                      Time elapsed: 00:21:25
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 106149 steps/s (collection: 0.762s, learning 0.165s)
             Mean action noise std: 7.22
          Mean value_function loss: 64.5805
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.9316
                       Mean reward: 847.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 169.2908
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1082
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.93s
                      Time elapsed: 00:21:26
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 110771 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 7.23
          Mean value_function loss: 59.7598
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.9457
                       Mean reward: 844.64
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 166.2698
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.89s
                      Time elapsed: 00:21:27
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 113485 steps/s (collection: 0.766s, learning 0.101s)
             Mean action noise std: 7.24
          Mean value_function loss: 64.5459
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 26.9577
                       Mean reward: 823.04
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.7383
     Episode_Reward/lifting_object: 164.7619
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.87s
                      Time elapsed: 00:21:28
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 106825 steps/s (collection: 0.802s, learning 0.119s)
             Mean action noise std: 7.25
          Mean value_function loss: 55.7791
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.9710
                       Mean reward: 840.14
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 166.6849
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.92s
                      Time elapsed: 00:21:29
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 41896 steps/s (collection: 2.229s, learning 0.118s)
             Mean action noise std: 7.25
          Mean value_function loss: 59.9525
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.9737
                       Mean reward: 837.21
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 168.8170
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.35s
                      Time elapsed: 00:21:31
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 31658 steps/s (collection: 2.994s, learning 0.112s)
             Mean action noise std: 7.26
          Mean value_function loss: 51.4176
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.9764
                       Mean reward: 847.48
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 167.9158
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.11s
                      Time elapsed: 00:21:34
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 29417 steps/s (collection: 3.191s, learning 0.151s)
             Mean action noise std: 7.26
          Mean value_function loss: 48.5054
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.9842
                       Mean reward: 863.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 170.6032
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.34s
                      Time elapsed: 00:21:38
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 26706 steps/s (collection: 3.550s, learning 0.131s)
             Mean action noise std: 7.27
          Mean value_function loss: 49.1765
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.9928
                       Mean reward: 855.04
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 168.2999
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.68s
                      Time elapsed: 00:21:41
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 26783 steps/s (collection: 3.469s, learning 0.201s)
             Mean action noise std: 7.28
          Mean value_function loss: 51.3664
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.0041
                       Mean reward: 851.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 166.1484
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1103
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.67s
                      Time elapsed: 00:21:45
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 30233 steps/s (collection: 3.127s, learning 0.124s)
             Mean action noise std: 7.29
          Mean value_function loss: 52.4532
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.0133
                       Mean reward: 823.23
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 166.8600
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.25s
                      Time elapsed: 00:21:48
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 30135 steps/s (collection: 3.108s, learning 0.154s)
             Mean action noise std: 7.30
          Mean value_function loss: 51.5991
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.0256
                       Mean reward: 847.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 169.4673
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.26s
                      Time elapsed: 00:21:52
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 30878 steps/s (collection: 3.054s, learning 0.130s)
             Mean action noise std: 7.31
          Mean value_function loss: 54.9620
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.0342
                       Mean reward: 857.20
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 168.7504
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.1114
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.18s
                      Time elapsed: 00:21:55
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 22844 steps/s (collection: 4.173s, learning 0.130s)
             Mean action noise std: 7.31
          Mean value_function loss: 66.9120
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.0421
                       Mean reward: 847.20
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 168.2546
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.30s
                      Time elapsed: 00:21:59
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 108826 steps/s (collection: 0.798s, learning 0.105s)
             Mean action noise std: 7.33
          Mean value_function loss: 46.0783
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.0526
                       Mean reward: 840.20
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 165.4105
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.90s
                      Time elapsed: 00:22:00
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 116987 steps/s (collection: 0.738s, learning 0.103s)
             Mean action noise std: 7.33
          Mean value_function loss: 45.4965
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.0654
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.2923
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.84s
                      Time elapsed: 00:22:01
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 111788 steps/s (collection: 0.778s, learning 0.101s)
             Mean action noise std: 7.34
          Mean value_function loss: 55.8383
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 27.0727
                       Mean reward: 852.10
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 167.5752
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.88s
                      Time elapsed: 00:22:02
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 108998 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 7.35
          Mean value_function loss: 49.3243
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.0811
                       Mean reward: 855.20
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.1063
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.90s
                      Time elapsed: 00:22:03
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 104389 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 7.36
          Mean value_function loss: 45.8503
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.0972
                       Mean reward: 850.59
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.2968
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.94s
                      Time elapsed: 00:22:04
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 109637 steps/s (collection: 0.800s, learning 0.097s)
             Mean action noise std: 7.37
          Mean value_function loss: 49.3685
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.1076
                       Mean reward: 831.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.3637
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.90s
                      Time elapsed: 00:22:04
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 116609 steps/s (collection: 0.739s, learning 0.104s)
             Mean action noise std: 7.39
          Mean value_function loss: 55.7604
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.1234
                       Mean reward: 842.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 167.8337
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.84s
                      Time elapsed: 00:22:05
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 116184 steps/s (collection: 0.748s, learning 0.098s)
             Mean action noise std: 7.40
          Mean value_function loss: 40.5126
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.1373
                       Mean reward: 862.24
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 168.6353
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.85s
                      Time elapsed: 00:22:06
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 118232 steps/s (collection: 0.740s, learning 0.092s)
             Mean action noise std: 7.40
          Mean value_function loss: 58.9468
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.1444
                       Mean reward: 840.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 165.6343
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.1163
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.83s
                      Time elapsed: 00:22:07
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 115510 steps/s (collection: 0.763s, learning 0.088s)
             Mean action noise std: 7.41
          Mean value_function loss: 40.6432
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.1528
                       Mean reward: 841.10
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.3595
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1155
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.85s
                      Time elapsed: 00:22:08
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 117298 steps/s (collection: 0.751s, learning 0.088s)
             Mean action noise std: 7.42
          Mean value_function loss: 51.4550
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.1586
                       Mean reward: 857.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.9774
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.84s
                      Time elapsed: 00:22:09
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 116002 steps/s (collection: 0.757s, learning 0.091s)
             Mean action noise std: 7.42
          Mean value_function loss: 50.6351
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.1657
                       Mean reward: 868.75
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 169.7648
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.1159
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.85s
                      Time elapsed: 00:22:10
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 111294 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 7.43
          Mean value_function loss: 45.1321
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.1690
                       Mean reward: 819.51
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 169.0233
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1155
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.88s
                      Time elapsed: 00:22:10
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 114920 steps/s (collection: 0.766s, learning 0.090s)
             Mean action noise std: 7.43
          Mean value_function loss: 43.8231
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.1759
                       Mean reward: 824.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 168.4665
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.86s
                      Time elapsed: 00:22:11
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 119369 steps/s (collection: 0.730s, learning 0.094s)
             Mean action noise std: 7.44
          Mean value_function loss: 38.5012
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.1800
                       Mean reward: 861.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 170.1623
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.82s
                      Time elapsed: 00:22:12
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 115450 steps/s (collection: 0.752s, learning 0.099s)
             Mean action noise std: 7.44
          Mean value_function loss: 44.7167
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 27.1895
                       Mean reward: 846.64
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 167.0219
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.85s
                      Time elapsed: 00:22:13
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 115606 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 7.45
          Mean value_function loss: 36.2258
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.1927
                       Mean reward: 856.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 169.9820
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.85s
                      Time elapsed: 00:22:14
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 115084 steps/s (collection: 0.768s, learning 0.087s)
             Mean action noise std: 7.45
          Mean value_function loss: 46.8186
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.1962
                       Mean reward: 838.59
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 167.6003
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.85s
                      Time elapsed: 00:22:15
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 117756 steps/s (collection: 0.745s, learning 0.090s)
             Mean action noise std: 7.46
          Mean value_function loss: 37.0530
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.2018
                       Mean reward: 859.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 170.8375
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.83s
                      Time elapsed: 00:22:15
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 116293 steps/s (collection: 0.725s, learning 0.120s)
             Mean action noise std: 7.46
          Mean value_function loss: 38.4187
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.2050
                       Mean reward: 849.29
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 169.9760
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.85s
                      Time elapsed: 00:22:16
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 114077 steps/s (collection: 0.746s, learning 0.116s)
             Mean action noise std: 7.47
          Mean value_function loss: 49.3820
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.2090
                       Mean reward: 866.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 169.0627
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.86s
                      Time elapsed: 00:22:17
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 114873 steps/s (collection: 0.750s, learning 0.106s)
             Mean action noise std: 7.48
          Mean value_function loss: 35.4674
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.2205
                       Mean reward: 875.63
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.0579
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.1167
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.86s
                      Time elapsed: 00:22:18
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 112381 steps/s (collection: 0.772s, learning 0.103s)
             Mean action noise std: 7.48
          Mean value_function loss: 49.0193
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.2291
                       Mean reward: 866.68
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.4492
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.1167
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.87s
                      Time elapsed: 00:22:19
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 107605 steps/s (collection: 0.816s, learning 0.098s)
             Mean action noise std: 7.50
          Mean value_function loss: 48.4405
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.2390
                       Mean reward: 860.30
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 170.1805
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.91s
                      Time elapsed: 00:22:20
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 102000 steps/s (collection: 0.852s, learning 0.112s)
             Mean action noise std: 7.50
          Mean value_function loss: 40.4544
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 27.2471
                       Mean reward: 862.14
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 168.8987
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.96s
                      Time elapsed: 00:22:21
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 109320 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 7.50
          Mean value_function loss: 37.7378
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2481
                       Mean reward: 854.18
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 167.3791
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.90s
                      Time elapsed: 00:22:22
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 119187 steps/s (collection: 0.727s, learning 0.098s)
             Mean action noise std: 7.51
          Mean value_function loss: 33.3013
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.2524
                       Mean reward: 825.25
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 168.4984
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.82s
                      Time elapsed: 00:22:23
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 114451 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 7.52
          Mean value_function loss: 39.9538
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.2628
                       Mean reward: 859.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.7353
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.1183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.86s
                      Time elapsed: 00:22:23
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 105660 steps/s (collection: 0.828s, learning 0.102s)
             Mean action noise std: 7.53
          Mean value_function loss: 37.9621
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.2781
                       Mean reward: 874.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 172.3300
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.93s
                      Time elapsed: 00:22:24
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 106822 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 7.54
          Mean value_function loss: 43.3408
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.2913
                       Mean reward: 858.43
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.4721
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.92s
                      Time elapsed: 00:22:25
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 110781 steps/s (collection: 0.792s, learning 0.096s)
             Mean action noise std: 7.56
          Mean value_function loss: 31.9123
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.3038
                       Mean reward: 868.12
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.2981
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.89s
                      Time elapsed: 00:22:26
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 109803 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 7.56
          Mean value_function loss: 26.6087
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.3152
                       Mean reward: 856.79
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 169.9529
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.90s
                      Time elapsed: 00:22:27
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 111679 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 7.57
          Mean value_function loss: 40.1166
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.3236
                       Mean reward: 850.98
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 170.4936
      Episode_Reward/object_height: 0.0369
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.88s
                      Time elapsed: 00:22:28
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 113289 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 7.58
          Mean value_function loss: 41.4486
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.3302
                       Mean reward: 839.46
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.2591
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.1183
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.87s
                      Time elapsed: 00:22:29
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 95571 steps/s (collection: 0.875s, learning 0.154s)
             Mean action noise std: 7.58
          Mean value_function loss: 44.8590
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.3325
                       Mean reward: 875.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 172.8886
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.03s
                      Time elapsed: 00:22:30
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 95445 steps/s (collection: 0.863s, learning 0.167s)
             Mean action noise std: 7.59
          Mean value_function loss: 51.7211
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.3404
                       Mean reward: 869.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.3995
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.1205
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.03s
                      Time elapsed: 00:22:31
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 99575 steps/s (collection: 0.869s, learning 0.118s)
             Mean action noise std: 7.60
          Mean value_function loss: 55.2943
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.3502
                       Mean reward: 847.93
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 169.7522
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.99s
                      Time elapsed: 00:22:32
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 105437 steps/s (collection: 0.841s, learning 0.091s)
             Mean action noise std: 7.61
          Mean value_function loss: 56.0313
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.3605
                       Mean reward: 846.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 171.4830
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.1206
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.93s
                      Time elapsed: 00:22:33
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 92955 steps/s (collection: 0.855s, learning 0.203s)
             Mean action noise std: 7.63
          Mean value_function loss: 47.2519
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.3746
                       Mean reward: 851.70
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 169.6548
      Episode_Reward/object_height: 0.0354
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.06s
                      Time elapsed: 00:22:34
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 90573 steps/s (collection: 0.975s, learning 0.111s)
             Mean action noise std: 7.64
          Mean value_function loss: 50.1551
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.3912
                       Mean reward: 867.70
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 172.0548
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.09s
                      Time elapsed: 00:22:35
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 94904 steps/s (collection: 0.930s, learning 0.106s)
             Mean action noise std: 7.66
          Mean value_function loss: 65.7851
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.4087
                       Mean reward: 850.70
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 170.9554
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.04s
                      Time elapsed: 00:22:36
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 105248 steps/s (collection: 0.824s, learning 0.110s)
             Mean action noise std: 7.67
          Mean value_function loss: 70.6103
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 27.4242
                       Mean reward: 841.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.2410
      Episode_Reward/object_height: 0.0344
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.93s
                      Time elapsed: 00:22:37
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 104753 steps/s (collection: 0.817s, learning 0.122s)
             Mean action noise std: 7.67
          Mean value_function loss: 55.6522
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.4266
                       Mean reward: 836.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 169.9874
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.1228
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.94s
                      Time elapsed: 00:22:38
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 109866 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 7.68
          Mean value_function loss: 57.1389
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.4362
                       Mean reward: 838.15
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.7390
     Episode_Reward/lifting_object: 166.6407
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.89s
                      Time elapsed: 00:22:39
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 99365 steps/s (collection: 0.821s, learning 0.169s)
             Mean action noise std: 7.69
          Mean value_function loss: 51.2263
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.4483
                       Mean reward: 833.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 166.3831
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.99s
                      Time elapsed: 00:22:40
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 103408 steps/s (collection: 0.856s, learning 0.095s)
             Mean action noise std: 7.70
          Mean value_function loss: 41.1980
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.4616
                       Mean reward: 851.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7291
     Episode_Reward/lifting_object: 165.3777
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.95s
                      Time elapsed: 00:22:41
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 110022 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 7.71
          Mean value_function loss: 48.7677
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.4753
                       Mean reward: 865.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 168.3700
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.1247
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.89s
                      Time elapsed: 00:22:42
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 108468 steps/s (collection: 0.794s, learning 0.113s)
             Mean action noise std: 7.73
          Mean value_function loss: 50.9019
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.4882
                       Mean reward: 862.60
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 170.0322
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.1243
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.91s
                      Time elapsed: 00:22:42
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 113331 steps/s (collection: 0.780s, learning 0.087s)
             Mean action noise std: 7.74
          Mean value_function loss: 43.0647
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.5004
                       Mean reward: 866.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.1746
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.1242
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.87s
                      Time elapsed: 00:22:43
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 118162 steps/s (collection: 0.739s, learning 0.093s)
             Mean action noise std: 7.75
          Mean value_function loss: 50.6885
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.5100
                       Mean reward: 851.39
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 168.5221
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.83s
                      Time elapsed: 00:22:44
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 115497 steps/s (collection: 0.734s, learning 0.117s)
             Mean action noise std: 7.76
          Mean value_function loss: 46.9882
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.5261
                       Mean reward: 861.62
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.9044
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.85s
                      Time elapsed: 00:22:45
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 118193 steps/s (collection: 0.742s, learning 0.090s)
             Mean action noise std: 7.77
          Mean value_function loss: 55.8493
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.5385
                       Mean reward: 859.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.8072
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.83s
                      Time elapsed: 00:22:46
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 114139 steps/s (collection: 0.773s, learning 0.089s)
             Mean action noise std: 7.78
          Mean value_function loss: 39.1233
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.5503
                       Mean reward: 855.91
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 168.6100
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1248
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.86s
                      Time elapsed: 00:22:47
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 112436 steps/s (collection: 0.776s, learning 0.099s)
             Mean action noise std: 7.79
          Mean value_function loss: 47.0741
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.5597
                       Mean reward: 850.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 169.3683
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.87s
                      Time elapsed: 00:22:48
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 114192 steps/s (collection: 0.751s, learning 0.110s)
             Mean action noise std: 7.81
          Mean value_function loss: 51.8760
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.5716
                       Mean reward: 836.59
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 169.7833
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.86s
                      Time elapsed: 00:22:48
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 113484 steps/s (collection: 0.757s, learning 0.110s)
             Mean action noise std: 7.82
          Mean value_function loss: 37.8993
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 27.5857
                       Mean reward: 862.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.7204
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.87s
                      Time elapsed: 00:22:49
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 117041 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 7.82
          Mean value_function loss: 38.0897
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.5931
                       Mean reward: 862.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.2589
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1257
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.84s
                      Time elapsed: 00:22:50
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 115607 steps/s (collection: 0.735s, learning 0.116s)
             Mean action noise std: 7.84
          Mean value_function loss: 37.6514
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6022
                       Mean reward: 835.14
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 169.3960
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.85s
                      Time elapsed: 00:22:51
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 114331 steps/s (collection: 0.747s, learning 0.113s)
             Mean action noise std: 7.85
          Mean value_function loss: 37.1799
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6160
                       Mean reward: 866.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 168.4187
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.86s
                      Time elapsed: 00:22:52
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 114480 steps/s (collection: 0.750s, learning 0.109s)
             Mean action noise std: 7.86
          Mean value_function loss: 37.9369
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.6254
                       Mean reward: 872.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.2015
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.86s
                      Time elapsed: 00:22:53
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 112630 steps/s (collection: 0.765s, learning 0.108s)
             Mean action noise std: 7.87
          Mean value_function loss: 35.9396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6360
                       Mean reward: 858.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 169.8679
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.87s
                      Time elapsed: 00:22:54
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 111977 steps/s (collection: 0.757s, learning 0.121s)
             Mean action noise std: 7.88
          Mean value_function loss: 32.2279
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.6453
                       Mean reward: 870.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 169.8972
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.88s
                      Time elapsed: 00:22:54
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 110893 steps/s (collection: 0.793s, learning 0.093s)
             Mean action noise std: 7.89
          Mean value_function loss: 44.0794
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.6592
                       Mean reward: 866.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 170.9910
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.1275
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.89s
                      Time elapsed: 00:22:55
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 112803 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 7.90
          Mean value_function loss: 46.3682
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.6697
                       Mean reward: 859.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.0156
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1277
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.87s
                      Time elapsed: 00:22:56
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 119267 steps/s (collection: 0.730s, learning 0.094s)
             Mean action noise std: 7.91
          Mean value_function loss: 46.9658
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 27.6793
                       Mean reward: 865.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 170.3723
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.1268
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.82s
                      Time elapsed: 00:22:57
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 118399 steps/s (collection: 0.738s, learning 0.092s)
             Mean action noise std: 7.91
          Mean value_function loss: 46.5199
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 27.6857
                       Mean reward: 840.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.4099
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.83s
                      Time elapsed: 00:22:58
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 118326 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 7.92
          Mean value_function loss: 46.6964
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.6908
                       Mean reward: 840.33
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.4474
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.83s
                      Time elapsed: 00:22:59
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 119618 steps/s (collection: 0.734s, learning 0.088s)
             Mean action noise std: 7.93
          Mean value_function loss: 44.5777
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 27.7005
                       Mean reward: 856.70
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.0742
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.82s
                      Time elapsed: 00:23:00
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 113398 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 7.94
          Mean value_function loss: 42.2784
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 27.7087
                       Mean reward: 825.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 171.0060
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.87s
                      Time elapsed: 00:23:00
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 112813 steps/s (collection: 0.768s, learning 0.103s)
             Mean action noise std: 7.95
          Mean value_function loss: 43.2004
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.7169
                       Mean reward: 873.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 171.5618
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.87s
                      Time elapsed: 00:23:01
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 121980 steps/s (collection: 0.717s, learning 0.089s)
             Mean action noise std: 7.96
          Mean value_function loss: 48.5321
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.7282
                       Mean reward: 845.22
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 168.5714
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1277
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.81s
                      Time elapsed: 00:23:02
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 116989 steps/s (collection: 0.751s, learning 0.089s)
             Mean action noise std: 7.97
          Mean value_function loss: 49.1877
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.7377
                       Mean reward: 855.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 167.7563
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.84s
                      Time elapsed: 00:23:03
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 120716 steps/s (collection: 0.722s, learning 0.092s)
             Mean action noise std: 7.99
          Mean value_function loss: 59.6786
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.7535
                       Mean reward: 845.28
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.5552
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.81s
                      Time elapsed: 00:23:04
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 117328 steps/s (collection: 0.745s, learning 0.093s)
             Mean action noise std: 7.99
          Mean value_function loss: 56.9575
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.7675
                       Mean reward: 817.17
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7448
     Episode_Reward/lifting_object: 165.9358
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1296
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.84s
                      Time elapsed: 00:23:05
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 115525 steps/s (collection: 0.752s, learning 0.099s)
             Mean action noise std: 8.00
          Mean value_function loss: 50.3146
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 27.7747
                       Mean reward: 844.95
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.8655
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.85s
                      Time elapsed: 00:23:05
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 119607 steps/s (collection: 0.727s, learning 0.095s)
             Mean action noise std: 8.01
          Mean value_function loss: 41.2679
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 27.7809
                       Mean reward: 857.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 169.5796
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.82s
                      Time elapsed: 00:23:06
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 118541 steps/s (collection: 0.733s, learning 0.096s)
             Mean action noise std: 8.01
          Mean value_function loss: 40.6792
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 27.7881
                       Mean reward: 845.36
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 169.2578
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1308
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.83s
                      Time elapsed: 00:23:07
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 119469 steps/s (collection: 0.737s, learning 0.086s)
             Mean action noise std: 8.02
          Mean value_function loss: 39.9664
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.7957
                       Mean reward: 842.79
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 168.2354
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.82s
                      Time elapsed: 00:23:08
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 118322 steps/s (collection: 0.745s, learning 0.086s)
             Mean action noise std: 8.03
          Mean value_function loss: 34.4378
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.8060
                       Mean reward: 854.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.3101
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1305
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.83s
                      Time elapsed: 00:23:09
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 121530 steps/s (collection: 0.727s, learning 0.082s)
             Mean action noise std: 8.04
          Mean value_function loss: 41.9018
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 27.8133
                       Mean reward: 842.55
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 169.5000
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1317
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.81s
                      Time elapsed: 00:23:10
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 113877 steps/s (collection: 0.764s, learning 0.100s)
             Mean action noise std: 8.04
          Mean value_function loss: 51.6998
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.8178
                       Mean reward: 869.18
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 169.2344
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1317
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.86s
                      Time elapsed: 00:23:10
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 114391 steps/s (collection: 0.755s, learning 0.104s)
             Mean action noise std: 8.04
          Mean value_function loss: 46.9539
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.8221
                       Mean reward: 820.99
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 167.1227
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1327
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.86s
                      Time elapsed: 00:23:11
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 111907 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 8.05
          Mean value_function loss: 39.0158
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.8258
                       Mean reward: 846.12
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.5050
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1327
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.88s
                      Time elapsed: 00:23:12
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 116156 steps/s (collection: 0.752s, learning 0.095s)
             Mean action noise std: 8.06
          Mean value_function loss: 31.9077
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.8323
                       Mean reward: 848.75
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 168.6864
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1322
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.85s
                      Time elapsed: 00:23:13
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 113015 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 8.06
          Mean value_function loss: 43.6319
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.8368
                       Mean reward: 857.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.0924
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1342
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.87s
                      Time elapsed: 00:23:14
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 119236 steps/s (collection: 0.739s, learning 0.086s)
             Mean action noise std: 8.06
          Mean value_function loss: 31.0230
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.8396
                       Mean reward: 870.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.7969
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.82s
                      Time elapsed: 00:23:15
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 114606 steps/s (collection: 0.758s, learning 0.100s)
             Mean action noise std: 8.07
          Mean value_function loss: 26.5700
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 27.8456
                       Mean reward: 862.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 168.8912
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1354
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.86s
                      Time elapsed: 00:23:16
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 114911 steps/s (collection: 0.760s, learning 0.095s)
             Mean action noise std: 8.08
          Mean value_function loss: 44.6671
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.8547
                       Mean reward: 852.64
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 170.9169
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1346
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.86s
                      Time elapsed: 00:23:16
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 117619 steps/s (collection: 0.728s, learning 0.108s)
             Mean action noise std: 8.09
          Mean value_function loss: 36.5331
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.8670
                       Mean reward: 825.01
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 168.3461
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1350
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.84s
                      Time elapsed: 00:23:17
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 119836 steps/s (collection: 0.729s, learning 0.091s)
             Mean action noise std: 8.10
          Mean value_function loss: 41.7385
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 27.8775
                       Mean reward: 852.54
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.9100
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.82s
                      Time elapsed: 00:23:18
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 112949 steps/s (collection: 0.771s, learning 0.100s)
             Mean action noise std: 8.11
          Mean value_function loss: 41.7052
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 27.8863
                       Mean reward: 851.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.1315
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.1365
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.87s
                      Time elapsed: 00:23:19
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 112754 steps/s (collection: 0.760s, learning 0.112s)
             Mean action noise std: 8.12
          Mean value_function loss: 36.4099
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 27.8962
                       Mean reward: 855.64
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.4328
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.1366
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.87s
                      Time elapsed: 00:23:20
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 114642 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 8.13
          Mean value_function loss: 38.7551
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 27.9027
                       Mean reward: 857.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.9607
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.1364
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.86s
                      Time elapsed: 00:23:21
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 117250 steps/s (collection: 0.738s, learning 0.101s)
             Mean action noise std: 8.13
          Mean value_function loss: 41.5252
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.9107
                       Mean reward: 855.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.7186
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.84s
                      Time elapsed: 00:23:21
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 112431 steps/s (collection: 0.767s, learning 0.107s)
             Mean action noise std: 8.14
          Mean value_function loss: 36.1618
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 27.9164
                       Mean reward: 880.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 171.6207
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.87s
                      Time elapsed: 00:23:22
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 117647 steps/s (collection: 0.731s, learning 0.105s)
             Mean action noise std: 8.15
          Mean value_function loss: 38.7855
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.9278
                       Mean reward: 847.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.8125
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.84s
                      Time elapsed: 00:23:23
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 119485 steps/s (collection: 0.723s, learning 0.100s)
             Mean action noise std: 8.15
          Mean value_function loss: 42.3460
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 27.9340
                       Mean reward: 853.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 170.6281
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.1387
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.82s
                      Time elapsed: 00:23:24
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 118698 steps/s (collection: 0.730s, learning 0.098s)
             Mean action noise std: 8.16
          Mean value_function loss: 41.9474
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.9390
                       Mean reward: 856.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.1400
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.1382
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.83s
                      Time elapsed: 00:23:25
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 116296 steps/s (collection: 0.747s, learning 0.098s)
             Mean action noise std: 8.17
          Mean value_function loss: 49.2648
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.9492
                       Mean reward: 848.40
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.4030
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.1385
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.85s
                      Time elapsed: 00:23:26
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 117992 steps/s (collection: 0.743s, learning 0.090s)
             Mean action noise std: 8.19
          Mean value_function loss: 39.2782
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.9647
                       Mean reward: 848.07
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 170.6982
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.1389
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.83s
                      Time elapsed: 00:23:26
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 118907 steps/s (collection: 0.740s, learning 0.087s)
             Mean action noise std: 8.20
          Mean value_function loss: 47.3404
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.9786
                       Mean reward: 837.64
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 168.1382
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.1386
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.83s
                      Time elapsed: 00:23:27
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 116040 steps/s (collection: 0.742s, learning 0.105s)
             Mean action noise std: 8.21
          Mean value_function loss: 37.8877
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.9882
                       Mean reward: 851.63
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.9156
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.1395
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.85s
                      Time elapsed: 00:23:28
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 114582 steps/s (collection: 0.768s, learning 0.090s)
             Mean action noise std: 8.22
          Mean value_function loss: 42.3979
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.9945
                       Mean reward: 839.13
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.0827
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.1399
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.86s
                      Time elapsed: 00:23:29
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 119706 steps/s (collection: 0.734s, learning 0.088s)
             Mean action noise std: 8.22
          Mean value_function loss: 48.5775
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.9957
                       Mean reward: 858.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.0921
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.1405
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.82s
                      Time elapsed: 00:23:30
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 119140 steps/s (collection: 0.734s, learning 0.092s)
             Mean action noise std: 8.23
          Mean value_function loss: 42.6390
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.0009
                       Mean reward: 854.80
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 169.7502
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.1404
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.83s
                      Time elapsed: 00:23:31
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 118089 steps/s (collection: 0.746s, learning 0.086s)
             Mean action noise std: 8.23
          Mean value_function loss: 46.1256
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 28.0079
                       Mean reward: 850.49
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.3845
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.1410
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.83s
                      Time elapsed: 00:23:32
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 117053 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 8.24
          Mean value_function loss: 35.6412
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.0123
                       Mean reward: 855.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.6123
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1420
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.84s
                      Time elapsed: 00:23:32
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 119115 steps/s (collection: 0.730s, learning 0.095s)
             Mean action noise std: 8.25
          Mean value_function loss: 46.5225
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.0193
                       Mean reward: 844.85
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.2387
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.1412
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.83s
                      Time elapsed: 00:23:33
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 118386 steps/s (collection: 0.736s, learning 0.095s)
             Mean action noise std: 8.26
          Mean value_function loss: 30.6072
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.0288
                       Mean reward: 869.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.8121
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.1421
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.83s
                      Time elapsed: 00:23:34
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 113400 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 8.26
          Mean value_function loss: 34.1015
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.0342
                       Mean reward: 860.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 169.8801
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1427
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.87s
                      Time elapsed: 00:23:35
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 119989 steps/s (collection: 0.733s, learning 0.086s)
             Mean action noise std: 8.27
          Mean value_function loss: 39.1446
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.0388
                       Mean reward: 875.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.6421
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.1429
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.82s
                      Time elapsed: 00:23:36
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 116821 steps/s (collection: 0.754s, learning 0.087s)
             Mean action noise std: 8.27
          Mean value_function loss: 33.0284
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.0421
                       Mean reward: 853.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.1373
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.1426
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.84s
                      Time elapsed: 00:23:37
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 117486 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 8.27
          Mean value_function loss: 41.0272
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.0465
                       Mean reward: 861.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.7411
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1433
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.84s
                      Time elapsed: 00:23:37
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 110538 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 8.28
          Mean value_function loss: 38.4094
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.0519
                       Mean reward: 868.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 168.6666
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1443
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.89s
                      Time elapsed: 00:23:38
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 114999 steps/s (collection: 0.754s, learning 0.101s)
             Mean action noise std: 8.29
          Mean value_function loss: 38.1288
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.0591
                       Mean reward: 847.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 169.7934
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.85s
                      Time elapsed: 00:23:39
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 116903 steps/s (collection: 0.752s, learning 0.089s)
             Mean action noise std: 8.29
          Mean value_function loss: 39.4610
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.0601
                       Mean reward: 874.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.2256
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.1447
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.84s
                      Time elapsed: 00:23:40
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 120418 steps/s (collection: 0.732s, learning 0.084s)
             Mean action noise std: 8.29
          Mean value_function loss: 36.0705
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.0616
                       Mean reward: 851.49
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.9756
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.1443
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.82s
                      Time elapsed: 00:23:41
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 114821 steps/s (collection: 0.768s, learning 0.089s)
             Mean action noise std: 8.29
          Mean value_function loss: 41.4232
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0637
                       Mean reward: 862.31
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.9412
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.1452
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.86s
                      Time elapsed: 00:23:42
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 118739 steps/s (collection: 0.737s, learning 0.091s)
             Mean action noise std: 8.29
          Mean value_function loss: 30.7435
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.0664
                       Mean reward: 859.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.5597
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.1453
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.83s
                      Time elapsed: 00:23:42
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 115473 steps/s (collection: 0.750s, learning 0.101s)
             Mean action noise std: 8.30
          Mean value_function loss: 35.1968
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.0698
                       Mean reward: 866.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.7134
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.1442
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.85s
                      Time elapsed: 00:23:43
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 115601 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 8.31
          Mean value_function loss: 30.8736
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.0754
                       Mean reward: 860.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.0897
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.1454
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.85s
                      Time elapsed: 00:23:44
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 116722 steps/s (collection: 0.743s, learning 0.099s)
             Mean action noise std: 8.31
          Mean value_function loss: 41.1341
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.0814
                       Mean reward: 826.20
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 167.9096
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.1441
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.84s
                      Time elapsed: 00:23:45
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 114474 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 8.31
          Mean value_function loss: 33.8954
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.0844
                       Mean reward: 830.04
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.8248
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1454
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.86s
                      Time elapsed: 00:23:46
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 111207 steps/s (collection: 0.767s, learning 0.117s)
             Mean action noise std: 8.32
          Mean value_function loss: 30.3386
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.0861
                       Mean reward: 866.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.5557
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.1447
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.88s
                      Time elapsed: 00:23:47
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 113568 steps/s (collection: 0.779s, learning 0.087s)
             Mean action noise std: 8.33
          Mean value_function loss: 32.4621
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.0969
                       Mean reward: 852.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 170.6775
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1456
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.87s
                      Time elapsed: 00:23:48
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 112607 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 8.34
          Mean value_function loss: 35.4973
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.1044
                       Mean reward: 862.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.6911
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1460
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.87s
                      Time elapsed: 00:23:48
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 112429 steps/s (collection: 0.771s, learning 0.103s)
             Mean action noise std: 8.35
          Mean value_function loss: 36.1803
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.1142
                       Mean reward: 856.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.5502
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.1466
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.87s
                      Time elapsed: 00:23:49
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 117335 steps/s (collection: 0.739s, learning 0.099s)
             Mean action noise std: 8.36
          Mean value_function loss: 32.2241
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.1235
                       Mean reward: 864.11
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.4732
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1469
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.84s
                      Time elapsed: 00:23:50
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 113635 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 8.37
          Mean value_function loss: 37.9991
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1305
                       Mean reward: 867.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 171.8878
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1461
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.87s
                      Time elapsed: 00:23:51
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 116751 steps/s (collection: 0.739s, learning 0.103s)
             Mean action noise std: 8.37
          Mean value_function loss: 36.2405
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 28.1369
                       Mean reward: 876.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 173.8207
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1470
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.84s
                      Time elapsed: 00:23:52
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 112333 steps/s (collection: 0.778s, learning 0.097s)
             Mean action noise std: 8.37
          Mean value_function loss: 31.8673
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.1384
                       Mean reward: 860.33
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.0679
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1473
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.88s
                      Time elapsed: 00:23:53
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 114345 steps/s (collection: 0.746s, learning 0.114s)
             Mean action noise std: 8.38
          Mean value_function loss: 33.9741
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.1429
                       Mean reward: 881.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.2350
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1471
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.86s
                      Time elapsed: 00:23:54
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 110167 steps/s (collection: 0.783s, learning 0.109s)
             Mean action noise std: 8.38
          Mean value_function loss: 42.7471
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.1481
                       Mean reward: 869.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.9270
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1469
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.89s
                      Time elapsed: 00:23:55
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 113773 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 8.39
          Mean value_function loss: 33.3998
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.1520
                       Mean reward: 869.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 170.9983
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.1475
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.86s
                      Time elapsed: 00:23:55
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 115290 steps/s (collection: 0.739s, learning 0.114s)
             Mean action noise std: 8.40
          Mean value_function loss: 44.0079
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.1610
                       Mean reward: 868.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.1166
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.1490
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.85s
                      Time elapsed: 00:23:56
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 118228 steps/s (collection: 0.742s, learning 0.090s)
             Mean action noise std: 8.41
          Mean value_function loss: 45.8948
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.1690
                       Mean reward: 835.49
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.2254
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1488
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.83s
                      Time elapsed: 00:23:57
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 115705 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 8.41
          Mean value_function loss: 44.0871
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.1770
                       Mean reward: 854.35
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 169.6386
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1485
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.85s
                      Time elapsed: 00:23:58
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 112442 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 8.42
          Mean value_function loss: 42.0065
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.1839
                       Mean reward: 863.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.1126
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1497
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.87s
                      Time elapsed: 00:23:59
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 108974 steps/s (collection: 0.809s, learning 0.094s)
             Mean action noise std: 8.43
          Mean value_function loss: 37.9002
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.1940
                       Mean reward: 849.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 170.2375
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1497
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.90s
                      Time elapsed: 00:24:00
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 112632 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 8.44
          Mean value_function loss: 33.6313
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.1981
                       Mean reward: 861.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.3651
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1500
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.87s
                      Time elapsed: 00:24:01
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 112873 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 8.44
          Mean value_function loss: 39.6945
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.2073
                       Mean reward: 859.32
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.3187
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1506
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.87s
                      Time elapsed: 00:24:01
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 117631 steps/s (collection: 0.749s, learning 0.087s)
             Mean action noise std: 8.45
          Mean value_function loss: 35.7609
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.2132
                       Mean reward: 860.71
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.4276
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1498
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.84s
                      Time elapsed: 00:24:02
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 109825 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 8.46
          Mean value_function loss: 34.9289
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 28.2222
                       Mean reward: 858.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.5332
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1509
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.90s
                      Time elapsed: 00:24:03
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 113069 steps/s (collection: 0.768s, learning 0.102s)
             Mean action noise std: 8.47
          Mean value_function loss: 25.8414
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 28.2288
                       Mean reward: 860.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 169.4865
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.1511
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.87s
                      Time elapsed: 00:24:04
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 114597 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 8.47
          Mean value_function loss: 26.9537
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 28.2363
                       Mean reward: 848.95
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.0343
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1511
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.86s
                      Time elapsed: 00:24:05
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 113397 steps/s (collection: 0.766s, learning 0.101s)
             Mean action noise std: 8.48
          Mean value_function loss: 40.6760
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.2419
                       Mean reward: 860.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.1709
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1520
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.87s
                      Time elapsed: 00:24:06
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 114462 steps/s (collection: 0.752s, learning 0.107s)
             Mean action noise std: 8.50
          Mean value_function loss: 42.9633
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.2530
                       Mean reward: 871.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.5214
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1513
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.86s
                      Time elapsed: 00:24:07
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 116004 steps/s (collection: 0.745s, learning 0.102s)
             Mean action noise std: 8.51
          Mean value_function loss: 43.5662
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.2664
                       Mean reward: 843.93
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 168.9496
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1534
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.85s
                      Time elapsed: 00:24:07
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 119674 steps/s (collection: 0.732s, learning 0.090s)
             Mean action noise std: 8.52
          Mean value_function loss: 42.5370
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.2774
                       Mean reward: 867.79
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.0573
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1522
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.82s
                      Time elapsed: 00:24:08
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 116543 steps/s (collection: 0.750s, learning 0.093s)
             Mean action noise std: 8.53
          Mean value_function loss: 35.1568
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.2867
                       Mean reward: 846.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 171.6201
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.1537
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.84s
                      Time elapsed: 00:24:09
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 119267 steps/s (collection: 0.736s, learning 0.088s)
             Mean action noise std: 8.54
          Mean value_function loss: 39.7624
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.2975
                       Mean reward: 857.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.1249
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1545
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.82s
                      Time elapsed: 00:24:10
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 118442 steps/s (collection: 0.740s, learning 0.090s)
             Mean action noise std: 8.55
          Mean value_function loss: 33.0049
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 28.3073
                       Mean reward: 861.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.0518
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1544
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.83s
                      Time elapsed: 00:24:11
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 118395 steps/s (collection: 0.742s, learning 0.088s)
             Mean action noise std: 8.55
          Mean value_function loss: 46.1509
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.3107
                       Mean reward: 848.56
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.2860
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1543
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.83s
                      Time elapsed: 00:24:12
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 120656 steps/s (collection: 0.731s, learning 0.084s)
             Mean action noise std: 8.56
          Mean value_function loss: 36.0040
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.3160
                       Mean reward: 857.00
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.5715
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1557
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.81s
                      Time elapsed: 00:24:12
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 116330 steps/s (collection: 0.760s, learning 0.085s)
             Mean action noise std: 8.58
          Mean value_function loss: 38.1405
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.3277
                       Mean reward: 861.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.5405
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1545
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.85s
                      Time elapsed: 00:24:13
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 116176 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 8.59
          Mean value_function loss: 56.5774
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.3419
                       Mean reward: 849.49
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 166.5753
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1570
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.85s
                      Time elapsed: 00:24:14
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 118122 steps/s (collection: 0.746s, learning 0.086s)
             Mean action noise std: 8.60
          Mean value_function loss: 51.6852
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.3520
                       Mean reward: 866.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 170.1285
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1570
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.83s
                      Time elapsed: 00:24:15
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 119627 steps/s (collection: 0.733s, learning 0.089s)
             Mean action noise std: 8.61
          Mean value_function loss: 48.2928
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.3622
                       Mean reward: 878.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.6266
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1578
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.82s
                      Time elapsed: 00:24:16
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 116570 steps/s (collection: 0.753s, learning 0.090s)
             Mean action noise std: 8.62
          Mean value_function loss: 48.5144
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.3764
                       Mean reward: 868.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.9062
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.1581
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.84s
                      Time elapsed: 00:24:17
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 118757 steps/s (collection: 0.738s, learning 0.090s)
             Mean action noise std: 8.63
          Mean value_function loss: 45.9441
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.3882
                       Mean reward: 856.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.2702
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.1572
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.83s
                      Time elapsed: 00:24:17
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 118249 steps/s (collection: 0.746s, learning 0.086s)
             Mean action noise std: 8.65
          Mean value_function loss: 34.1955
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.4002
                       Mean reward: 844.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 169.8701
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.83s
                      Time elapsed: 00:24:18
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 119113 steps/s (collection: 0.738s, learning 0.088s)
             Mean action noise std: 8.66
          Mean value_function loss: 45.4087
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4119
                       Mean reward: 840.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.7210
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.1608
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.83s
                      Time elapsed: 00:24:19
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 119605 steps/s (collection: 0.733s, learning 0.089s)
             Mean action noise std: 8.67
          Mean value_function loss: 28.2457
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.4254
                       Mean reward: 859.10
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.9155
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.1608
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.82s
                      Time elapsed: 00:24:20
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 118824 steps/s (collection: 0.742s, learning 0.086s)
             Mean action noise std: 8.68
          Mean value_function loss: 47.4345
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.4358
                       Mean reward: 855.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.3874
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1616
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.83s
                      Time elapsed: 00:24:21
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 111854 steps/s (collection: 0.772s, learning 0.107s)
             Mean action noise std: 8.69
          Mean value_function loss: 31.4844
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 28.4433
                       Mean reward: 844.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.1014
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1632
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.88s
                      Time elapsed: 00:24:22
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 113255 steps/s (collection: 0.762s, learning 0.106s)
             Mean action noise std: 8.70
          Mean value_function loss: 30.2460
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.4483
                       Mean reward: 870.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.1506
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1625
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.87s
                      Time elapsed: 00:24:23
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 112260 steps/s (collection: 0.777s, learning 0.099s)
             Mean action noise std: 8.70
          Mean value_function loss: 27.8489
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.4575
                       Mean reward: 855.08
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.5288
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.1636
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.88s
                      Time elapsed: 00:24:23
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 115535 steps/s (collection: 0.746s, learning 0.105s)
             Mean action noise std: 8.71
          Mean value_function loss: 30.7842
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.4632
                       Mean reward: 877.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.0907
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.1645
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.85s
                      Time elapsed: 00:24:24
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 114035 steps/s (collection: 0.747s, learning 0.115s)
             Mean action noise std: 8.72
          Mean value_function loss: 40.3991
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.4696
                       Mean reward: 840.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 168.4681
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1657
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.86s
                      Time elapsed: 00:24:25
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 118004 steps/s (collection: 0.736s, learning 0.097s)
             Mean action noise std: 8.74
          Mean value_function loss: 43.8196
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.4870
                       Mean reward: 864.70
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 170.4330
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1648
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.83s
                      Time elapsed: 00:24:26
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 114902 steps/s (collection: 0.732s, learning 0.123s)
             Mean action noise std: 8.75
          Mean value_function loss: 41.9266
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.5037
                       Mean reward: 857.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 171.3662
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.1641
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.86s
                      Time elapsed: 00:24:27
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 116035 steps/s (collection: 0.752s, learning 0.095s)
             Mean action noise std: 8.76
          Mean value_function loss: 33.4738
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.5122
                       Mean reward: 860.69
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.4691
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1659
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.85s
                      Time elapsed: 00:24:28
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 115696 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 8.77
          Mean value_function loss: 43.2831
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5195
                       Mean reward: 861.28
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.7619
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1653
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.85s
                      Time elapsed: 00:24:28
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 115337 steps/s (collection: 0.754s, learning 0.099s)
             Mean action noise std: 8.78
          Mean value_function loss: 42.5471
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5276
                       Mean reward: 874.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.6588
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.1660
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.85s
                      Time elapsed: 00:24:29
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 115312 steps/s (collection: 0.768s, learning 0.085s)
             Mean action noise std: 8.79
          Mean value_function loss: 49.5231
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.5345
                       Mean reward: 875.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.8059
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1655
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.85s
                      Time elapsed: 00:24:30
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 118037 steps/s (collection: 0.746s, learning 0.087s)
             Mean action noise std: 8.80
          Mean value_function loss: 46.4276
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.5435
                       Mean reward: 849.92
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.2167
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1668
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.83s
                      Time elapsed: 00:24:31
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 113978 steps/s (collection: 0.756s, learning 0.107s)
             Mean action noise std: 8.80
          Mean value_function loss: 44.2484
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 28.5527
                       Mean reward: 859.36
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.6537
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.1677
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.86s
                      Time elapsed: 00:24:32
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 115663 steps/s (collection: 0.759s, learning 0.091s)
             Mean action noise std: 8.81
          Mean value_function loss: 43.0704
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5570
                       Mean reward: 832.90
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.4087
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.1676
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.85s
                      Time elapsed: 00:24:33
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 111791 steps/s (collection: 0.777s, learning 0.102s)
             Mean action noise std: 8.82
          Mean value_function loss: 49.8263
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.5657
                       Mean reward: 842.69
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 169.9444
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.1680
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.88s
                      Time elapsed: 00:24:34
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 118219 steps/s (collection: 0.746s, learning 0.085s)
             Mean action noise std: 8.83
          Mean value_function loss: 39.6896
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.5743
                       Mean reward: 867.05
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.4708
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.1698
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.83s
                      Time elapsed: 00:24:34
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 115598 steps/s (collection: 0.764s, learning 0.086s)
             Mean action noise std: 8.83
          Mean value_function loss: 41.8383
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5769
                       Mean reward: 855.76
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 169.5259
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1695
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.85s
                      Time elapsed: 00:24:35
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 119827 steps/s (collection: 0.723s, learning 0.097s)
             Mean action noise std: 8.84
          Mean value_function loss: 32.9001
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.5826
                       Mean reward: 849.03
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 170.6705
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1699
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.82s
                      Time elapsed: 00:24:36
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 122007 steps/s (collection: 0.718s, learning 0.088s)
             Mean action noise std: 8.84
          Mean value_function loss: 36.6861
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.5890
                       Mean reward: 849.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 169.8216
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1710
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.81s
                      Time elapsed: 00:24:37
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 120224 steps/s (collection: 0.734s, learning 0.084s)
             Mean action noise std: 8.85
          Mean value_function loss: 40.7067
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.5966
                       Mean reward: 852.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 169.3012
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.1701
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.82s
                      Time elapsed: 00:24:38
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 117099 steps/s (collection: 0.754s, learning 0.085s)
             Mean action noise std: 8.86
          Mean value_function loss: 24.1994
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.6058
                       Mean reward: 849.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 169.2371
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1714
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.84s
                      Time elapsed: 00:24:39
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 118439 steps/s (collection: 0.746s, learning 0.084s)
             Mean action noise std: 8.87
          Mean value_function loss: 41.4378
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.6092
                       Mean reward: 865.79
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 172.5413
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1703
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.83s
                      Time elapsed: 00:24:39
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 114494 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 8.87
          Mean value_function loss: 39.1884
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 28.6142
                       Mean reward: 844.18
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 170.9246
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1718
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.86s
                      Time elapsed: 00:24:40
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 115071 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 8.88
          Mean value_function loss: 37.2121
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 28.6215
                       Mean reward: 836.72
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.5580
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1716
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.85s
                      Time elapsed: 00:24:41
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 113806 steps/s (collection: 0.778s, learning 0.086s)
             Mean action noise std: 8.90
          Mean value_function loss: 26.5799
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.6317
                       Mean reward: 864.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 170.2610
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.1726
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.86s
                      Time elapsed: 00:24:42
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 113657 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 8.91
          Mean value_function loss: 38.9393
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.6462
                       Mean reward: 823.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 169.2118
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.1736
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.86s
                      Time elapsed: 00:24:43
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 115868 steps/s (collection: 0.757s, learning 0.092s)
             Mean action noise std: 8.92
          Mean value_function loss: 39.5388
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 28.6586
                       Mean reward: 861.13
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 171.4391
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1722
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.85s
                      Time elapsed: 00:24:44
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 114533 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 8.92
          Mean value_function loss: 27.6659
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.6611
                       Mean reward: 858.64
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.0979
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1728
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.86s
                      Time elapsed: 00:24:45
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 113256 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 8.93
          Mean value_function loss: 33.2346
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 28.6686
                       Mean reward: 843.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.9525
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.1736
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.87s
                      Time elapsed: 00:24:45
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 117876 steps/s (collection: 0.746s, learning 0.088s)
             Mean action noise std: 8.95
          Mean value_function loss: 30.0024
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.6806
                       Mean reward: 847.61
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 169.7270
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.1745
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.83s
                      Time elapsed: 00:24:46
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 118166 steps/s (collection: 0.743s, learning 0.089s)
             Mean action noise std: 8.96
          Mean value_function loss: 25.1110
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.6903
                       Mean reward: 849.64
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 170.4041
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1750
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.83s
                      Time elapsed: 00:24:47
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 121601 steps/s (collection: 0.722s, learning 0.087s)
             Mean action noise std: 8.96
          Mean value_function loss: 32.3741
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.6971
                       Mean reward: 838.23
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 171.4165
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1747
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.81s
                      Time elapsed: 00:24:48
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 118416 steps/s (collection: 0.744s, learning 0.086s)
             Mean action noise std: 8.97
          Mean value_function loss: 27.4652
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.7035
                       Mean reward: 854.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.7072
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.1748
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.83s
                      Time elapsed: 00:24:49
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 119798 steps/s (collection: 0.735s, learning 0.086s)
             Mean action noise std: 8.98
          Mean value_function loss: 45.5546
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.7127
                       Mean reward: 858.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.7876
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.1762
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.82s
                      Time elapsed: 00:24:50
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 117006 steps/s (collection: 0.755s, learning 0.085s)
             Mean action noise std: 8.99
          Mean value_function loss: 41.3238
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.7205
                       Mean reward: 860.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.6364
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1753
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.84s
                      Time elapsed: 00:24:50
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 119675 steps/s (collection: 0.736s, learning 0.086s)
             Mean action noise std: 9.00
          Mean value_function loss: 41.4198
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 28.7245
                       Mean reward: 851.23
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.4132
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.1751
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.82s
                      Time elapsed: 00:24:51
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 117060 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 9.00
          Mean value_function loss: 38.9773
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 28.7323
                       Mean reward: 871.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.9995
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.1754
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.84s
                      Time elapsed: 00:24:52
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 116172 steps/s (collection: 0.733s, learning 0.113s)
             Mean action noise std: 9.01
          Mean value_function loss: 35.3555
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.7374
                       Mean reward: 862.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.0793
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.1765
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.85s
                      Time elapsed: 00:24:53
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 109916 steps/s (collection: 0.777s, learning 0.117s)
             Mean action noise std: 9.01
          Mean value_function loss: 30.5757
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.7430
                       Mean reward: 878.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 173.0323
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1773
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.89s
                      Time elapsed: 00:24:54
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 114421 steps/s (collection: 0.752s, learning 0.107s)
             Mean action noise std: 9.02
          Mean value_function loss: 39.2773
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.7489
                       Mean reward: 878.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 172.5381
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1772
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.86s
                      Time elapsed: 00:24:55
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 114143 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 9.04
          Mean value_function loss: 36.4824
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.7580
                       Mean reward: 834.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.0947
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.1772
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.86s
                      Time elapsed: 00:24:56
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 114046 steps/s (collection: 0.755s, learning 0.107s)
             Mean action noise std: 9.04
          Mean value_function loss: 38.9109
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.7681
                       Mean reward: 854.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 170.2719
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1776
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.86s
                      Time elapsed: 00:24:56
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 114375 steps/s (collection: 0.748s, learning 0.112s)
             Mean action noise std: 9.05
          Mean value_function loss: 38.2582
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.7743
                       Mean reward: 867.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.0351
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.1786
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.86s
                      Time elapsed: 00:24:57
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 111450 steps/s (collection: 0.759s, learning 0.123s)
             Mean action noise std: 9.06
          Mean value_function loss: 39.2510
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.7814
                       Mean reward: 861.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.8010
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1789
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.88s
                      Time elapsed: 00:24:58
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 115852 steps/s (collection: 0.752s, learning 0.097s)
             Mean action noise std: 9.07
          Mean value_function loss: 39.5556
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 28.7910
                       Mean reward: 874.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.1881
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1779
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.85s
                      Time elapsed: 00:24:59
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 112288 steps/s (collection: 0.770s, learning 0.106s)
             Mean action noise std: 9.09
          Mean value_function loss: 35.3972
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 28.8030
                       Mean reward: 872.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.0955
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1768
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.88s
                      Time elapsed: 00:25:00
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 115561 steps/s (collection: 0.749s, learning 0.102s)
             Mean action noise std: 9.10
          Mean value_function loss: 30.7188
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.8147
                       Mean reward: 844.05
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 170.1935
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.1784
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.85s
                      Time elapsed: 00:25:01
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 118912 steps/s (collection: 0.732s, learning 0.095s)
             Mean action noise std: 9.11
          Mean value_function loss: 41.1395
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.8254
                       Mean reward: 856.99
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 170.7987
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.83s
                      Time elapsed: 00:25:02
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 114502 steps/s (collection: 0.766s, learning 0.092s)
             Mean action noise std: 9.12
          Mean value_function loss: 39.5390
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.8373
                       Mean reward: 852.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.4557
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1792
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.86s
                      Time elapsed: 00:25:02
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 115873 steps/s (collection: 0.754s, learning 0.094s)
             Mean action noise std: 9.13
          Mean value_function loss: 33.4015
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.8410
                       Mean reward: 850.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 170.6477
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1813
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.85s
                      Time elapsed: 00:25:03
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 117475 steps/s (collection: 0.742s, learning 0.095s)
             Mean action noise std: 9.13
          Mean value_function loss: 33.0612
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.8458
                       Mean reward: 859.58
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.7093
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1813
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.84s
                      Time elapsed: 00:25:04
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 117382 steps/s (collection: 0.749s, learning 0.089s)
             Mean action noise std: 9.14
          Mean value_function loss: 38.2442
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.8513
                       Mean reward: 856.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 169.8956
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1812
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.84s
                      Time elapsed: 00:25:05
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 118378 steps/s (collection: 0.740s, learning 0.091s)
             Mean action noise std: 9.15
          Mean value_function loss: 42.2230
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.8611
                       Mean reward: 867.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 172.4232
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.1823
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.83s
                      Time elapsed: 00:25:06
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 115433 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 9.16
          Mean value_function loss: 41.5948
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.8714
                       Mean reward: 840.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 169.2357
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1829
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.85s
                      Time elapsed: 00:25:07
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 115850 steps/s (collection: 0.760s, learning 0.089s)
             Mean action noise std: 9.17
          Mean value_function loss: 29.8381
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.8756
                       Mean reward: 874.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.3877
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1824
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.85s
                      Time elapsed: 00:25:07
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 116757 steps/s (collection: 0.757s, learning 0.085s)
             Mean action noise std: 9.18
          Mean value_function loss: 36.0117
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.8828
                       Mean reward: 854.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.3618
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1829
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.84s
                      Time elapsed: 00:25:08
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 114003 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 9.18
          Mean value_function loss: 44.4001
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.8909
                       Mean reward: 852.57
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.2680
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1825
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.86s
                      Time elapsed: 00:25:09
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 114581 steps/s (collection: 0.763s, learning 0.095s)
             Mean action noise std: 9.19
          Mean value_function loss: 37.5108
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.8967
                       Mean reward: 864.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.5760
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.1843
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.86s
                      Time elapsed: 00:25:10
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 116348 steps/s (collection: 0.758s, learning 0.087s)
             Mean action noise std: 9.20
          Mean value_function loss: 31.8395
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.9010
                       Mean reward: 860.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.1364
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1848
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.84s
                      Time elapsed: 00:25:11
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 120468 steps/s (collection: 0.725s, learning 0.091s)
             Mean action noise std: 9.21
          Mean value_function loss: 36.8276
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 28.9083
                       Mean reward: 848.29
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 171.3452
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.1839
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.82s
                      Time elapsed: 00:25:12
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 118100 steps/s (collection: 0.749s, learning 0.084s)
             Mean action noise std: 9.21
          Mean value_function loss: 38.9259
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.9156
                       Mean reward: 847.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 170.6106
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1861
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.83s
                      Time elapsed: 00:25:12
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 116865 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 9.21
          Mean value_function loss: 32.2814
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.9183
                       Mean reward: 844.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 168.1919
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.84s
                      Time elapsed: 00:25:13
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 117489 steps/s (collection: 0.751s, learning 0.086s)
             Mean action noise std: 9.22
          Mean value_function loss: 32.7328
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.9201
                       Mean reward: 852.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 171.5822
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.1858
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.84s
                      Time elapsed: 00:25:14
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 116577 steps/s (collection: 0.741s, learning 0.102s)
             Mean action noise std: 9.23
          Mean value_function loss: 42.2869
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 28.9258
                       Mean reward: 860.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.5023
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.84s
                      Time elapsed: 00:25:15
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 117202 steps/s (collection: 0.747s, learning 0.092s)
             Mean action noise std: 9.23
          Mean value_function loss: 29.7527
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 28.9320
                       Mean reward: 851.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 169.0835
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.84s
                      Time elapsed: 00:25:16
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 118060 steps/s (collection: 0.743s, learning 0.090s)
             Mean action noise std: 9.24
          Mean value_function loss: 30.9637
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9381
                       Mean reward: 856.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 169.8879
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.1877
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.83s
                      Time elapsed: 00:25:17
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 112385 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 9.25
          Mean value_function loss: 40.3906
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.9500
                       Mean reward: 876.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.6173
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.87s
                      Time elapsed: 00:25:18
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 114597 steps/s (collection: 0.754s, learning 0.104s)
             Mean action noise std: 9.27
          Mean value_function loss: 42.3542
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.9611
                       Mean reward: 877.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.7061
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.86s
                      Time elapsed: 00:25:18
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 115676 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 9.27
          Mean value_function loss: 37.5864
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 28.9682
                       Mean reward: 862.68
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.5146
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.85s
                      Time elapsed: 00:25:19
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 116105 steps/s (collection: 0.759s, learning 0.088s)
             Mean action noise std: 9.27
          Mean value_function loss: 41.2531
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.9700
                       Mean reward: 841.86
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.2429
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.85s
                      Time elapsed: 00:25:20
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 113873 steps/s (collection: 0.761s, learning 0.103s)
             Mean action noise std: 9.28
          Mean value_function loss: 52.5045
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 28.9786
                       Mean reward: 858.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 170.4569
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1890
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.86s
                      Time elapsed: 00:25:21
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 111833 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 9.31
          Mean value_function loss: 76.0007
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.9915
                       Mean reward: 862.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.8112
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.88s
                      Time elapsed: 00:25:22
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 112904 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 9.32
          Mean value_function loss: 68.2936
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 29.0123
                       Mean reward: 845.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.5631
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1896
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.87s
                      Time elapsed: 00:25:23
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 109394 steps/s (collection: 0.788s, learning 0.110s)
             Mean action noise std: 9.33
          Mean value_function loss: 70.7357
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.0224
                       Mean reward: 843.48
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.3115
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.1904
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.90s
                      Time elapsed: 00:25:24
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 109873 steps/s (collection: 0.791s, learning 0.104s)
             Mean action noise std: 9.34
          Mean value_function loss: 73.6862
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 29.0325
                       Mean reward: 858.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.8683
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1898
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.89s
                      Time elapsed: 00:25:25
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 114028 steps/s (collection: 0.764s, learning 0.099s)
             Mean action noise std: 9.36
          Mean value_function loss: 81.2327
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 29.0452
                       Mean reward: 861.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.2974
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.1909
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.86s
                      Time elapsed: 00:25:25
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 114028 steps/s (collection: 0.757s, learning 0.106s)
             Mean action noise std: 9.37
          Mean value_function loss: 80.1660
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 29.0603
                       Mean reward: 855.06
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 168.8807
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.86s
                      Time elapsed: 00:25:26
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 115339 steps/s (collection: 0.751s, learning 0.102s)
             Mean action noise std: 9.38
          Mean value_function loss: 96.2824
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.0679
                       Mean reward: 847.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.8077
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1906
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.85s
                      Time elapsed: 00:25:27
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 114981 steps/s (collection: 0.752s, learning 0.103s)
             Mean action noise std: 9.40
          Mean value_function loss: 86.6146
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.0807
                       Mean reward: 844.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.3026
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1911
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.85s
                      Time elapsed: 00:25:28
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 112225 steps/s (collection: 0.768s, learning 0.108s)
             Mean action noise std: 9.42
          Mean value_function loss: 85.5111
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 29.0969
                       Mean reward: 824.05
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 164.4606
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1935
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.88s
                      Time elapsed: 00:25:29
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 118047 steps/s (collection: 0.747s, learning 0.086s)
             Mean action noise std: 9.43
          Mean value_function loss: 75.9566
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 29.1098
                       Mean reward: 800.39
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 165.7924
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1932
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 0.83s
                      Time elapsed: 00:25:30
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 112171 steps/s (collection: 0.790s, learning 0.087s)
             Mean action noise std: 9.44
          Mean value_function loss: 75.8512
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.1176
                       Mean reward: 824.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 163.2248
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1949
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 0.88s
                      Time elapsed: 00:25:31
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 113179 steps/s (collection: 0.780s, learning 0.089s)
             Mean action noise std: 9.45
          Mean value_function loss: 76.0531
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.1266
                       Mean reward: 826.71
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 165.6424
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1950
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 0.87s
                      Time elapsed: 00:25:31
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 111257 steps/s (collection: 0.789s, learning 0.095s)
             Mean action noise std: 9.47
          Mean value_function loss: 61.5454
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.1410
                       Mean reward: 848.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 165.8157
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1945
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 0.88s
                      Time elapsed: 00:25:32
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 110038 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 9.48
          Mean value_function loss: 62.8986
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 29.1517
                       Mean reward: 823.11
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 162.8768
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.1967
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.89s
                      Time elapsed: 00:25:33
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 115267 steps/s (collection: 0.756s, learning 0.097s)
             Mean action noise std: 9.49
          Mean value_function loss: 58.8220
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 29.1600
                       Mean reward: 782.82
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7335
     Episode_Reward/lifting_object: 163.3016
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 0.85s
                      Time elapsed: 00:25:34
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 113083 steps/s (collection: 0.772s, learning 0.097s)
             Mean action noise std: 9.50
          Mean value_function loss: 60.4253
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.1724
                       Mean reward: 848.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7304
     Episode_Reward/lifting_object: 163.1911
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.87s
                      Time elapsed: 00:25:35
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 117559 steps/s (collection: 0.747s, learning 0.090s)
             Mean action noise std: 9.51
          Mean value_function loss: 55.5250
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.1806
                       Mean reward: 809.92
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 164.5514
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.1951
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.84s
                      Time elapsed: 00:25:36
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 113527 steps/s (collection: 0.778s, learning 0.088s)
             Mean action noise std: 9.52
          Mean value_function loss: 52.1815
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.1857
                       Mean reward: 822.46
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 165.4882
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 0.87s
                      Time elapsed: 00:25:37
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 112974 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 9.53
          Mean value_function loss: 47.9616
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.1897
                       Mean reward: 833.61
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 166.2139
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.1964
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 0.87s
                      Time elapsed: 00:25:37
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 110690 steps/s (collection: 0.792s, learning 0.097s)
             Mean action noise std: 9.53
          Mean value_function loss: 46.5674
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.1944
                       Mean reward: 820.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 164.6129
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.1976
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.89s
                      Time elapsed: 00:25:38
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 112711 steps/s (collection: 0.771s, learning 0.102s)
             Mean action noise std: 9.53
          Mean value_function loss: 47.8423
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.1954
                       Mean reward: 848.03
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 165.0396
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1976
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 0.87s
                      Time elapsed: 00:25:39
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 116656 steps/s (collection: 0.751s, learning 0.092s)
             Mean action noise std: 9.54
          Mean value_function loss: 52.8841
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.1991
                       Mean reward: 843.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 167.8053
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.1963
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.84s
                      Time elapsed: 00:25:40
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 116256 steps/s (collection: 0.752s, learning 0.093s)
             Mean action noise std: 9.55
          Mean value_function loss: 47.0675
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.2038
                       Mean reward: 829.03
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 165.4635
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.1964
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 0.85s
                      Time elapsed: 00:25:41
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 111841 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 9.56
          Mean value_function loss: 49.2379
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.2154
                       Mean reward: 841.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 168.3485
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.1969
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 0.88s
                      Time elapsed: 00:25:42
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 112681 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 9.56
          Mean value_function loss: 43.0290
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 29.2226
                       Mean reward: 859.65
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 168.9079
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.87s
                      Time elapsed: 00:25:43
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 109975 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 9.57
          Mean value_function loss: 44.8822
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.2249
                       Mean reward: 841.90
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 169.0320
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1981
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.89s
                      Time elapsed: 00:25:44
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 111889 steps/s (collection: 0.781s, learning 0.098s)
             Mean action noise std: 9.57
          Mean value_function loss: 46.7923
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.2307
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.8029
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.1955
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 0.88s
                      Time elapsed: 00:25:44
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 109069 steps/s (collection: 0.782s, learning 0.120s)
             Mean action noise std: 9.59
          Mean value_function loss: 40.4836
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.2386
                       Mean reward: 840.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.2227
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1974
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.90s
                      Time elapsed: 00:25:45
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 92584 steps/s (collection: 0.942s, learning 0.120s)
             Mean action noise std: 9.59
          Mean value_function loss: 48.3460
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 29.2456
                       Mean reward: 873.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.3913
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 1.06s
                      Time elapsed: 00:25:46
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 85715 steps/s (collection: 1.033s, learning 0.114s)
             Mean action noise std: 9.59
          Mean value_function loss: 45.0571
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.2465
                       Mean reward: 847.20
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 167.8654
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.1960
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 1.15s
                      Time elapsed: 00:25:48
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 103223 steps/s (collection: 0.851s, learning 0.102s)
             Mean action noise std: 9.60
          Mean value_function loss: 46.3868
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.2485
                       Mean reward: 865.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.2245
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.1984
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.95s
                      Time elapsed: 00:25:48
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 113164 steps/s (collection: 0.781s, learning 0.088s)
             Mean action noise std: 9.60
          Mean value_function loss: 44.1642
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.2506
                       Mean reward: 864.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.0094
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.1969
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.87s
                      Time elapsed: 00:25:49
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 112490 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 9.61
          Mean value_function loss: 45.3936
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.2575
                       Mean reward: 871.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.1380
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.1964
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 0.87s
                      Time elapsed: 00:25:50
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 107989 steps/s (collection: 0.807s, learning 0.103s)
             Mean action noise std: 9.62
          Mean value_function loss: 42.4400
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.2655
                       Mean reward: 854.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.3867
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.1966
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 0.91s
                      Time elapsed: 00:25:51
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 108111 steps/s (collection: 0.804s, learning 0.106s)
             Mean action noise std: 9.63
          Mean value_function loss: 39.3642
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.2715
                       Mean reward: 860.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.4530
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 0.91s
                      Time elapsed: 00:25:52
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 106701 steps/s (collection: 0.812s, learning 0.110s)
             Mean action noise std: 9.64
          Mean value_function loss: 40.9053
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.2819
                       Mean reward: 831.16
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 167.3491
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.1978
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 0.92s
                      Time elapsed: 00:25:53
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 109120 steps/s (collection: 0.807s, learning 0.094s)
             Mean action noise std: 9.66
          Mean value_function loss: 46.2421
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.2924
                       Mean reward: 851.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.1638
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1964
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 0.90s
                      Time elapsed: 00:25:54
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 105578 steps/s (collection: 0.806s, learning 0.125s)
             Mean action noise std: 9.66
          Mean value_function loss: 53.1055
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.3013
                       Mean reward: 856.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.6598
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.1989
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.93s
                      Time elapsed: 00:25:55
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 107513 steps/s (collection: 0.798s, learning 0.117s)
             Mean action noise std: 9.68
          Mean value_function loss: 59.4356
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.3102
                       Mean reward: 863.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.0017
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.1966
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 0.91s
                      Time elapsed: 00:25:56
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 104395 steps/s (collection: 0.824s, learning 0.118s)
             Mean action noise std: 9.69
          Mean value_function loss: 45.0279
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 29.3212
                       Mean reward: 851.30
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.0477
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.1980
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.94s
                      Time elapsed: 00:25:57
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 108684 steps/s (collection: 0.781s, learning 0.124s)
             Mean action noise std: 9.70
          Mean value_function loss: 45.8331
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 29.3270
                       Mean reward: 853.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.8538
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.1981
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.90s
                      Time elapsed: 00:25:58
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 108508 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 9.70
          Mean value_function loss: 34.5339
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 29.3317
                       Mean reward: 862.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 169.1894
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.1996
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.91s
                      Time elapsed: 00:25:58
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 110191 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 9.71
          Mean value_function loss: 44.8691
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.3375
                       Mean reward: 855.27
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.6839
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.1983
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.89s
                      Time elapsed: 00:25:59
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 110317 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 9.72
          Mean value_function loss: 37.7080
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.3464
                       Mean reward: 838.06
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 167.6072
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.1999
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 0.89s
                      Time elapsed: 00:26:00
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 109372 steps/s (collection: 0.810s, learning 0.089s)
             Mean action noise std: 9.73
          Mean value_function loss: 46.2593
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.3555
                       Mean reward: 862.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.9937
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.1984
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.90s
                      Time elapsed: 00:26:01
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 105607 steps/s (collection: 0.823s, learning 0.108s)
             Mean action noise std: 9.75
          Mean value_function loss: 43.1534
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.3689
                       Mean reward: 844.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 168.2560
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.2034
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.93s
                      Time elapsed: 00:26:02
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 107784 steps/s (collection: 0.801s, learning 0.111s)
             Mean action noise std: 9.76
          Mean value_function loss: 46.2131
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 29.3789
                       Mean reward: 874.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.3987
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2026
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.91s
                      Time elapsed: 00:26:03
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 100231 steps/s (collection: 0.859s, learning 0.122s)
             Mean action noise std: 9.77
          Mean value_function loss: 41.1439
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.3891
                       Mean reward: 846.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 169.5793
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2034
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.98s
                      Time elapsed: 00:26:04
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 105821 steps/s (collection: 0.801s, learning 0.128s)
             Mean action noise std: 9.79
          Mean value_function loss: 40.0637
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.3999
                       Mean reward: 871.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.0488
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2039
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 0.93s
                      Time elapsed: 00:26:05
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 110784 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 9.80
          Mean value_function loss: 31.5624
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.4138
                       Mean reward: 852.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 166.5124
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.89s
                      Time elapsed: 00:26:06
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 104866 steps/s (collection: 0.826s, learning 0.112s)
             Mean action noise std: 9.81
          Mean value_function loss: 41.0461
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 29.4214
                       Mean reward: 862.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.7970
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2046
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 0.94s
                      Time elapsed: 00:26:07
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 108577 steps/s (collection: 0.803s, learning 0.102s)
             Mean action noise std: 9.81
          Mean value_function loss: 32.2856
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.4238
                       Mean reward: 843.11
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 168.5890
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2033
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.91s
                      Time elapsed: 00:26:08
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 102407 steps/s (collection: 0.852s, learning 0.108s)
             Mean action noise std: 9.82
          Mean value_function loss: 31.7201
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.4289
                       Mean reward: 875.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.5022
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2063
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.96s
                      Time elapsed: 00:26:09
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 96684 steps/s (collection: 0.876s, learning 0.141s)
             Mean action noise std: 9.83
          Mean value_function loss: 26.1697
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.4354
                       Mean reward: 829.56
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 168.9569
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.2076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 1.02s
                      Time elapsed: 00:26:10
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 86584 steps/s (collection: 1.004s, learning 0.131s)
             Mean action noise std: 9.84
          Mean value_function loss: 35.1007
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.4458
                       Mean reward: 838.87
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 168.9856
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 1.14s
                      Time elapsed: 00:26:11
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 89816 steps/s (collection: 0.984s, learning 0.111s)
             Mean action noise std: 9.85
          Mean value_function loss: 33.3748
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 29.4530
                       Mean reward: 850.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.1058
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2068
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 1.09s
                      Time elapsed: 00:26:12
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 107364 steps/s (collection: 0.804s, learning 0.112s)
             Mean action noise std: 9.86
          Mean value_function loss: 43.5925
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.4625
                       Mean reward: 862.61
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 169.7039
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2066
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.92s
                      Time elapsed: 00:26:13
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 110526 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 9.87
          Mean value_function loss: 59.2824
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.4715
                       Mean reward: 877.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.2328
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2071
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 0.89s
                      Time elapsed: 00:26:14
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 109838 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 9.88
          Mean value_function loss: 72.9056
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.4788
                       Mean reward: 871.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.7450
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.2062
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 0.89s
                      Time elapsed: 00:26:15
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 109909 steps/s (collection: 0.806s, learning 0.088s)
             Mean action noise std: 9.89
          Mean value_function loss: 67.5143
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.4863
                       Mean reward: 874.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.9441
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2065
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 0.89s
                      Time elapsed: 00:26:15
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 108775 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 9.90
          Mean value_function loss: 47.6629
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.4954
                       Mean reward: 874.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.2922
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2057
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 0.90s
                      Time elapsed: 00:26:16
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 109068 steps/s (collection: 0.785s, learning 0.116s)
             Mean action noise std: 9.91
          Mean value_function loss: 54.1813
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.5022
                       Mean reward: 856.14
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.0347
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.2061
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.90s
                      Time elapsed: 00:26:17
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 106219 steps/s (collection: 0.809s, learning 0.117s)
             Mean action noise std: 9.92
          Mean value_function loss: 46.4551
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.5095
                       Mean reward: 858.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 172.1556
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2067
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.93s
                      Time elapsed: 00:26:18
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 106305 steps/s (collection: 0.822s, learning 0.103s)
             Mean action noise std: 9.93
          Mean value_function loss: 36.0764
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 29.5198
                       Mean reward: 852.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 169.4963
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2094
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.92s
                      Time elapsed: 00:26:19
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 105875 steps/s (collection: 0.830s, learning 0.098s)
             Mean action noise std: 9.94
          Mean value_function loss: 49.9388
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.5283
                       Mean reward: 854.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.4642
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2089
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.93s
                      Time elapsed: 00:26:20
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 102357 steps/s (collection: 0.862s, learning 0.098s)
             Mean action noise std: 9.95
          Mean value_function loss: 49.5780
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.5375
                       Mean reward: 848.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 170.3349
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2095
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 0.96s
                      Time elapsed: 00:26:21
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 112551 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 9.96
          Mean value_function loss: 43.5395
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.5476
                       Mean reward: 846.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7307
     Episode_Reward/lifting_object: 166.4122
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.2122
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.87s
                      Time elapsed: 00:26:22
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 110511 steps/s (collection: 0.789s, learning 0.100s)
             Mean action noise std: 9.97
          Mean value_function loss: 37.7183
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 29.5562
                       Mean reward: 817.80
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 167.3853
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.2117
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.89s
                      Time elapsed: 00:26:23
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 108495 steps/s (collection: 0.810s, learning 0.096s)
             Mean action noise std: 9.98
          Mean value_function loss: 31.9317
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.5633
                       Mean reward: 842.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 167.0707
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.2132
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.91s
                      Time elapsed: 00:26:24
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 106610 steps/s (collection: 0.822s, learning 0.100s)
             Mean action noise std: 9.99
          Mean value_function loss: 28.6467
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.5705
                       Mean reward: 864.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 172.0555
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2120
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.92s
                      Time elapsed: 00:26:25
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 106458 steps/s (collection: 0.830s, learning 0.094s)
             Mean action noise std: 10.00
          Mean value_function loss: 38.4100
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.5773
                       Mean reward: 856.52
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 171.0200
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2136
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.92s
                      Time elapsed: 00:26:26
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 104101 steps/s (collection: 0.835s, learning 0.110s)
             Mean action noise std: 10.01
          Mean value_function loss: 40.6393
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.5829
                       Mean reward: 851.01
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 169.4701
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2123
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.94s
                      Time elapsed: 00:26:26
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 106054 steps/s (collection: 0.831s, learning 0.096s)
             Mean action noise std: 10.01
          Mean value_function loss: 35.5445
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.5894
                       Mean reward: 839.58
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 171.1686
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2124
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.93s
                      Time elapsed: 00:26:27
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 110643 steps/s (collection: 0.799s, learning 0.089s)
             Mean action noise std: 10.02
          Mean value_function loss: 34.7081
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.5919
                       Mean reward: 851.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 169.9024
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2146
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.89s
                      Time elapsed: 00:26:28
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 110335 steps/s (collection: 0.782s, learning 0.109s)
             Mean action noise std: 10.02
          Mean value_function loss: 42.1185
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 29.5978
                       Mean reward: 848.10
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 170.3764
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2145
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.89s
                      Time elapsed: 00:26:29
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 107042 steps/s (collection: 0.816s, learning 0.103s)
             Mean action noise std: 10.03
          Mean value_function loss: 28.3584
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.6049
                       Mean reward: 859.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.0101
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.2149
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.92s
                      Time elapsed: 00:26:30
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 107091 steps/s (collection: 0.811s, learning 0.107s)
             Mean action noise std: 10.05
          Mean value_function loss: 36.5465
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.6172
                       Mean reward: 871.98
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 173.5800
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.2141
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 0.92s
                      Time elapsed: 00:26:31
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 109598 steps/s (collection: 0.786s, learning 0.111s)
             Mean action noise std: 10.06
          Mean value_function loss: 38.6019
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.6277
                       Mean reward: 857.15
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4245
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2144
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 0.90s
                      Time elapsed: 00:26:32
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 109412 steps/s (collection: 0.789s, learning 0.109s)
             Mean action noise std: 10.08
          Mean value_function loss: 47.9192
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.6408
                       Mean reward: 854.68
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 173.3594
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2145
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 0.90s
                      Time elapsed: 00:26:33
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 104203 steps/s (collection: 0.834s, learning 0.109s)
             Mean action noise std: 10.08
          Mean value_function loss: 37.0874
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.6479
                       Mean reward: 860.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.3104
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.2163
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 0.94s
                      Time elapsed: 00:26:34
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 103896 steps/s (collection: 0.818s, learning 0.128s)
             Mean action noise std: 10.09
          Mean value_function loss: 32.4875
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.6507
                       Mean reward: 853.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.0120
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.2159
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.95s
                      Time elapsed: 00:26:35
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 84061 steps/s (collection: 1.015s, learning 0.154s)
             Mean action noise std: 10.10
          Mean value_function loss: 24.3424
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6570
                       Mean reward: 870.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 173.4791
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2154
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 1.17s
                      Time elapsed: 00:26:36
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 103291 steps/s (collection: 0.847s, learning 0.105s)
             Mean action noise std: 10.11
          Mean value_function loss: 34.3717
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.6660
                       Mean reward: 855.53
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 166.9643
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.2191
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.95s
                      Time elapsed: 00:26:37
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 108333 steps/s (collection: 0.803s, learning 0.105s)
             Mean action noise std: 10.12
          Mean value_function loss: 28.6119
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6714
                       Mean reward: 866.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.8808
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2172
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 0.91s
                      Time elapsed: 00:26:38
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 106220 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 10.12
          Mean value_function loss: 34.1224
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 29.6754
                       Mean reward: 863.08
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.6929
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2169
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 0.93s
                      Time elapsed: 00:26:39
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 107698 steps/s (collection: 0.794s, learning 0.119s)
             Mean action noise std: 10.12
          Mean value_function loss: 28.1845
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.6787
                       Mean reward: 878.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 173.1616
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.2185
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.91s
                      Time elapsed: 00:26:40
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 110557 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 10.14
          Mean value_function loss: 31.1537
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.6836
                       Mean reward: 843.30
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.2835
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.2187
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 0.89s
                      Time elapsed: 00:26:40
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 113035 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 10.15
          Mean value_function loss: 28.3770
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6955
                       Mean reward: 861.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.7717
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2189
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 0.87s
                      Time elapsed: 00:26:41
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 110125 steps/s (collection: 0.800s, learning 0.093s)
             Mean action noise std: 10.17
          Mean value_function loss: 31.3853
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.7107
                       Mean reward: 862.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.8404
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.2198
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.89s
                      Time elapsed: 00:26:42
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 55572 steps/s (collection: 1.650s, learning 0.119s)
             Mean action noise std: 10.18
          Mean value_function loss: 34.2005
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.7201
                       Mean reward: 870.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.1956
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2181
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 1.77s
                      Time elapsed: 00:26:44
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 32442 steps/s (collection: 2.926s, learning 0.104s)
             Mean action noise std: 10.20
          Mean value_function loss: 36.3220
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.7293
                       Mean reward: 874.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.0434
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.2207
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 3.03s
                      Time elapsed: 00:26:47
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 31685 steps/s (collection: 2.986s, learning 0.117s)
             Mean action noise std: 10.21
          Mean value_function loss: 34.1445
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.7427
                       Mean reward: 852.57
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.0760
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2211
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 3.10s
                      Time elapsed: 00:26:50
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 30328 steps/s (collection: 3.098s, learning 0.144s)
             Mean action noise std: 10.22
          Mean value_function loss: 41.2383
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.7519
                       Mean reward: 868.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.0513
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2200
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 3.24s
                      Time elapsed: 00:26:53
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 31055 steps/s (collection: 3.051s, learning 0.114s)
             Mean action noise std: 10.23
          Mean value_function loss: 34.2825
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.7614
                       Mean reward: 861.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 172.3204
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2230
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 3.17s
                      Time elapsed: 00:26:57
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 30412 steps/s (collection: 3.108s, learning 0.124s)
             Mean action noise std: 10.24
          Mean value_function loss: 32.4626
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.7671
                       Mean reward: 878.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.2495
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2211
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 3.23s
                      Time elapsed: 00:27:00
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 31462 steps/s (collection: 2.988s, learning 0.137s)
             Mean action noise std: 10.25
          Mean value_function loss: 36.3895
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 29.7748
                       Mean reward: 877.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.1271
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2230
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 3.12s
                      Time elapsed: 00:27:03
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 29454 steps/s (collection: 3.215s, learning 0.123s)
             Mean action noise std: 10.26
          Mean value_function loss: 30.6323
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.7841
                       Mean reward: 856.44
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.6946
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2225
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 3.34s
                      Time elapsed: 00:27:06
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 31073 steps/s (collection: 3.053s, learning 0.111s)
             Mean action noise std: 10.27
          Mean value_function loss: 25.6727
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7906
                       Mean reward: 862.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.1834
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2241
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 3.16s
                      Time elapsed: 00:27:09
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 31746 steps/s (collection: 2.986s, learning 0.111s)
             Mean action noise std: 10.27
          Mean value_function loss: 26.4863
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.7961
                       Mean reward: 857.16
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 171.0970
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2235
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 3.10s
                      Time elapsed: 00:27:12
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 93398 steps/s (collection: 0.928s, learning 0.124s)
             Mean action noise std: 10.28
          Mean value_function loss: 24.0581
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.8003
                       Mean reward: 866.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.5039
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2247
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.05s
                      Time elapsed: 00:27:14
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 99972 steps/s (collection: 0.889s, learning 0.094s)
             Mean action noise std: 10.29
          Mean value_function loss: 32.0931
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.8077
                       Mean reward: 856.14
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 173.2804
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2235
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 0.98s
                      Time elapsed: 00:27:14
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 106333 steps/s (collection: 0.821s, learning 0.103s)
             Mean action noise std: 10.30
          Mean value_function loss: 29.7106
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.8126
                       Mean reward: 869.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.0932
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2250
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 0.92s
                      Time elapsed: 00:27:15
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 108542 steps/s (collection: 0.813s, learning 0.093s)
             Mean action noise std: 10.31
          Mean value_function loss: 31.1162
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.8195
                       Mean reward: 867.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.1761
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2250
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 0.91s
                      Time elapsed: 00:27:16
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 101078 steps/s (collection: 0.847s, learning 0.126s)
             Mean action noise std: 10.31
          Mean value_function loss: 31.3300
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.8235
                       Mean reward: 870.88
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.8454
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2246
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.97s
                      Time elapsed: 00:27:17
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 104568 steps/s (collection: 0.847s, learning 0.093s)
             Mean action noise std: 10.33
          Mean value_function loss: 30.8035
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.8325
                       Mean reward: 873.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.8450
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.2238
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.94s
                      Time elapsed: 00:27:18
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 103566 steps/s (collection: 0.849s, learning 0.100s)
             Mean action noise std: 10.34
          Mean value_function loss: 27.1480
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.8434
                       Mean reward: 858.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.9296
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.2254
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.95s
                      Time elapsed: 00:27:19
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 105591 steps/s (collection: 0.829s, learning 0.102s)
             Mean action noise std: 10.35
          Mean value_function loss: 27.8274
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.8480
                       Mean reward: 869.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 174.1021
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2253
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.93s
                      Time elapsed: 00:27:20
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 101137 steps/s (collection: 0.858s, learning 0.114s)
             Mean action noise std: 10.36
          Mean value_function loss: 27.2259
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.8570
                       Mean reward: 864.40
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 174.1103
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2247
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.97s
                      Time elapsed: 00:27:21
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 107450 steps/s (collection: 0.820s, learning 0.095s)
             Mean action noise std: 10.37
          Mean value_function loss: 24.0280
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.8661
                       Mean reward: 872.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.0826
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2255
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.91s
                      Time elapsed: 00:27:22
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 104556 steps/s (collection: 0.825s, learning 0.116s)
             Mean action noise std: 10.38
          Mean value_function loss: 32.1238
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 29.8761
                       Mean reward: 864.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 171.9690
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2265
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.94s
                      Time elapsed: 00:27:23
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 104543 steps/s (collection: 0.825s, learning 0.116s)
             Mean action noise std: 10.38
          Mean value_function loss: 35.3239
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8816
                       Mean reward: 863.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 172.9584
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.2278
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.94s
                      Time elapsed: 00:27:24
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 108684 steps/s (collection: 0.804s, learning 0.101s)
             Mean action noise std: 10.39
          Mean value_function loss: 30.9964
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.8866
                       Mean reward: 859.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 171.0596
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2274
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.90s
                      Time elapsed: 00:27:25
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 106775 steps/s (collection: 0.819s, learning 0.102s)
             Mean action noise std: 10.40
          Mean value_function loss: 33.3631
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.8929
                       Mean reward: 868.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 171.7662
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2274
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.92s
                      Time elapsed: 00:27:26
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 107091 steps/s (collection: 0.819s, learning 0.099s)
             Mean action noise std: 10.41
          Mean value_function loss: 28.2906
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.9014
                       Mean reward: 853.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 171.7073
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.2272
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.92s
                      Time elapsed: 00:27:27
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 108456 steps/s (collection: 0.810s, learning 0.096s)
             Mean action noise std: 10.42
          Mean value_function loss: 30.4524
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.9076
                       Mean reward: 874.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.5020
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.2271
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.91s
                      Time elapsed: 00:27:28
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 94502 steps/s (collection: 0.906s, learning 0.134s)
             Mean action noise std: 10.43
          Mean value_function loss: 31.6276
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.9146
                       Mean reward: 859.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 173.8759
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.2282
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.04s
                      Time elapsed: 00:27:29
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 92182 steps/s (collection: 0.945s, learning 0.121s)
             Mean action noise std: 10.44
          Mean value_function loss: 31.9012
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.9211
                       Mean reward: 876.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 173.3979
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.2287
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.07s
                      Time elapsed: 00:27:30
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 98212 steps/s (collection: 0.888s, learning 0.113s)
             Mean action noise std: 10.45
          Mean value_function loss: 26.6515
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.9282
                       Mean reward: 870.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.9615
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.2294
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.00s
                      Time elapsed: 00:27:31
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 111349 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 10.46
          Mean value_function loss: 39.6126
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.9338
                       Mean reward: 864.57
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.3980
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.2304
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.88s
                      Time elapsed: 00:27:32
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 97894 steps/s (collection: 0.884s, learning 0.121s)
             Mean action noise std: 10.46
          Mean value_function loss: 30.1262
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 29.9400
                       Mean reward: 875.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 171.5609
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.2300
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.00s
                      Time elapsed: 00:27:33
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 95614 steps/s (collection: 0.917s, learning 0.111s)
             Mean action noise std: 10.47
          Mean value_function loss: 44.9399
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.9461
                       Mean reward: 848.44
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.2061
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.2301
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.03s
                      Time elapsed: 00:27:34
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 102804 steps/s (collection: 0.847s, learning 0.109s)
             Mean action noise std: 10.47
          Mean value_function loss: 35.5930
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.9507
                       Mean reward: 875.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 172.1195
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.2296
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.96s
                      Time elapsed: 00:27:35
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 105174 steps/s (collection: 0.818s, learning 0.117s)
             Mean action noise std: 10.48
          Mean value_function loss: 27.9412
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.9524
                       Mean reward: 860.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 173.0259
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.2298
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.93s
                      Time elapsed: 00:27:35
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 109114 steps/s (collection: 0.803s, learning 0.098s)
             Mean action noise std: 10.49
          Mean value_function loss: 26.9515
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.9572
                       Mean reward: 874.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.7853
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2303
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.90s
                      Time elapsed: 00:27:36
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 106080 steps/s (collection: 0.821s, learning 0.105s)
             Mean action noise std: 10.49
          Mean value_function loss: 35.7575
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.9619
                       Mean reward: 875.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 173.0575
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2322
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.93s
                      Time elapsed: 00:27:37
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 99559 steps/s (collection: 0.878s, learning 0.109s)
             Mean action noise std: 10.49
          Mean value_function loss: 29.9474
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 29.9638
                       Mean reward: 843.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 170.3759
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2325
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.99s
                      Time elapsed: 00:27:38
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 99088 steps/s (collection: 0.858s, learning 0.134s)
             Mean action noise std: 10.50
          Mean value_function loss: 26.9988
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.9695
                       Mean reward: 852.66
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.2959
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.2326
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.99s
                      Time elapsed: 00:27:39
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 85074 steps/s (collection: 1.017s, learning 0.138s)
             Mean action noise std: 10.51
          Mean value_function loss: 29.9684
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.9757
                       Mean reward: 851.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.4383
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.2340
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.16s
                      Time elapsed: 00:27:40
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 97002 steps/s (collection: 0.918s, learning 0.095s)
             Mean action noise std: 10.52
          Mean value_function loss: 32.2082
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.9836
                       Mean reward: 855.38
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 170.1803
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.2321
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.01s
                      Time elapsed: 00:27:41
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 108426 steps/s (collection: 0.813s, learning 0.094s)
             Mean action noise std: 10.54
          Mean value_function loss: 28.9119
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 29.9940
                       Mean reward: 844.97
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 170.5340
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2337
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.91s
                      Time elapsed: 00:27:42
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 107109 steps/s (collection: 0.819s, learning 0.099s)
             Mean action noise std: 10.54
          Mean value_function loss: 33.8841
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 30.0005
                       Mean reward: 863.10
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.4529
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2337
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.92s
                      Time elapsed: 00:27:43
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 107649 steps/s (collection: 0.825s, learning 0.089s)
             Mean action noise std: 10.55
          Mean value_function loss: 30.1005
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.0046
                       Mean reward: 867.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.9043
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.2335
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.91s
                      Time elapsed: 00:27:44
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 111602 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 10.56
          Mean value_function loss: 21.2962
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.0114
                       Mean reward: 862.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.8797
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2329
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.88s
                      Time elapsed: 00:27:45
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 111967 steps/s (collection: 0.781s, learning 0.097s)
             Mean action noise std: 10.57
          Mean value_function loss: 23.0938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.0193
                       Mean reward: 854.78
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.7746
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2332
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.88s
                      Time elapsed: 00:27:46
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 111168 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 10.58
          Mean value_function loss: 28.8253
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.0277
                       Mean reward: 866.87
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 171.8827
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2347
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.88s
                      Time elapsed: 00:27:47
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 110058 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 10.59
          Mean value_function loss: 29.0596
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.0337
                       Mean reward: 874.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.3675
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2322
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.89s
                      Time elapsed: 00:27:48
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 111212 steps/s (collection: 0.796s, learning 0.088s)
             Mean action noise std: 10.60
          Mean value_function loss: 29.3043
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.0408
                       Mean reward: 872.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.4035
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2352
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.88s
                      Time elapsed: 00:27:49
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 110534 steps/s (collection: 0.801s, learning 0.088s)
             Mean action noise std: 10.60
          Mean value_function loss: 25.9858
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 30.0467
                       Mean reward: 848.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.5902
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.2347
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.89s
                      Time elapsed: 00:27:49
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 111772 steps/s (collection: 0.791s, learning 0.088s)
             Mean action noise std: 10.61
          Mean value_function loss: 26.3323
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.0530
                       Mean reward: 852.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.3586
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.2365
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.88s
                      Time elapsed: 00:27:50
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 111171 steps/s (collection: 0.784s, learning 0.101s)
             Mean action noise std: 10.62
          Mean value_function loss: 24.3832
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 30.0602
                       Mean reward: 854.03
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.2013
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.2361
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.88s
                      Time elapsed: 00:27:51
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 107684 steps/s (collection: 0.788s, learning 0.125s)
             Mean action noise std: 10.63
          Mean value_function loss: 33.9885
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.0665
                       Mean reward: 867.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.5211
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.2355
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.91s
                      Time elapsed: 00:27:52
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 112041 steps/s (collection: 0.780s, learning 0.097s)
             Mean action noise std: 10.64
          Mean value_function loss: 29.4576
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.0722
                       Mean reward: 879.73
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 173.6340
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.2363
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.88s
                      Time elapsed: 00:27:53
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 111522 steps/s (collection: 0.771s, learning 0.111s)
             Mean action noise std: 10.65
          Mean value_function loss: 28.5649
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.0762
                       Mean reward: 867.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.4342
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.2355
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.88s
                      Time elapsed: 00:27:54
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 107628 steps/s (collection: 0.814s, learning 0.100s)
             Mean action noise std: 10.65
          Mean value_function loss: 27.4712
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 30.0835
                       Mean reward: 870.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.7180
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.2374
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.91s
                      Time elapsed: 00:27:55
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 111661 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 10.67
          Mean value_function loss: 24.7512
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.0911
                       Mean reward: 854.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.3035
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.2366
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.88s
                      Time elapsed: 00:27:56
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 106329 steps/s (collection: 0.822s, learning 0.102s)
             Mean action noise std: 10.68
          Mean value_function loss: 26.8280
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.0987
                       Mean reward: 837.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.4327
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.2383
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.92s
                      Time elapsed: 00:27:57
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 106100 steps/s (collection: 0.828s, learning 0.098s)
             Mean action noise std: 10.69
          Mean value_function loss: 27.2680
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.1077
                       Mean reward: 869.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 173.4363
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.2380
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.93s
                      Time elapsed: 00:27:58
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 110328 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 10.69
          Mean value_function loss: 29.9955
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.1144
                       Mean reward: 865.30
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 173.5789
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2387
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.89s
                      Time elapsed: 00:27:58
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 109236 steps/s (collection: 0.802s, learning 0.098s)
             Mean action noise std: 10.70
          Mean value_function loss: 36.4352
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.1214
                       Mean reward: 854.15
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.0543
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.2386
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.90s
                      Time elapsed: 00:27:59
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 109876 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 10.71
          Mean value_function loss: 35.5504
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 30.1289
                       Mean reward: 840.35
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 170.4572
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2401
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.89s
                      Time elapsed: 00:28:00
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 111717 steps/s (collection: 0.782s, learning 0.098s)
             Mean action noise std: 10.72
          Mean value_function loss: 38.0368
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.1365
                       Mean reward: 874.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 173.1810
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2393
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.88s
                      Time elapsed: 00:28:01
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 112534 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 10.73
          Mean value_function loss: 33.2861
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 30.1421
                       Mean reward: 878.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.0500
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2400
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.87s
                      Time elapsed: 00:28:02
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 111394 steps/s (collection: 0.785s, learning 0.097s)
             Mean action noise std: 10.75
          Mean value_function loss: 40.3086
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 30.1520
                       Mean reward: 860.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 170.2127
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2406
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.88s
                      Time elapsed: 00:28:03
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 111765 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 10.75
          Mean value_function loss: 33.4675
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 30.1595
                       Mean reward: 871.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 172.0964
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2405
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.88s
                      Time elapsed: 00:28:04
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 112125 steps/s (collection: 0.780s, learning 0.097s)
             Mean action noise std: 10.75
          Mean value_function loss: 33.7427
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 30.1617
                       Mean reward: 863.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.8939
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2406
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.88s
                      Time elapsed: 00:28:05
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 108310 steps/s (collection: 0.812s, learning 0.096s)
             Mean action noise std: 10.76
          Mean value_function loss: 36.0122
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.1645
                       Mean reward: 839.86
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 170.5666
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2411
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.91s
                      Time elapsed: 00:28:06
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 112997 steps/s (collection: 0.777s, learning 0.093s)
             Mean action noise std: 10.76
          Mean value_function loss: 35.2514
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.1689
                       Mean reward: 869.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 173.6315
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2401
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.87s
                      Time elapsed: 00:28:06
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 108792 steps/s (collection: 0.790s, learning 0.114s)
             Mean action noise std: 10.77
          Mean value_function loss: 31.6108
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.1729
                       Mean reward: 874.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 172.0914
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2420
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.90s
                      Time elapsed: 00:28:07
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 110651 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 10.78
          Mean value_function loss: 29.4753
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.1783
                       Mean reward: 871.83
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 173.3283
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2412
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.89s
                      Time elapsed: 00:28:08
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 110769 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 10.79
          Mean value_function loss: 36.9109
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.1845
                       Mean reward: 869.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 169.8389
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2431
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.89s
                      Time elapsed: 00:28:09
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 109204 steps/s (collection: 0.798s, learning 0.102s)
             Mean action noise std: 10.79
          Mean value_function loss: 37.6106
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 30.1885
                       Mean reward: 856.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 172.3785
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.2434
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.90s
                      Time elapsed: 00:28:10
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 113727 steps/s (collection: 0.765s, learning 0.099s)
             Mean action noise std: 10.80
          Mean value_function loss: 32.0251
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 30.1935
                       Mean reward: 872.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 171.1485
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2437
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.86s
                      Time elapsed: 00:28:11
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 109658 steps/s (collection: 0.803s, learning 0.094s)
             Mean action noise std: 10.80
          Mean value_function loss: 24.9347
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.1952
                       Mean reward: 849.98
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.5295
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2435
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.90s
                      Time elapsed: 00:28:12
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 109895 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 10.80
          Mean value_function loss: 31.6040
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.1972
                       Mean reward: 866.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.5758
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.2433
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.89s
                      Time elapsed: 00:28:13
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 109915 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 10.82
          Mean value_function loss: 30.1612
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.2050
                       Mean reward: 859.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.4227
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.2438
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.89s
                      Time elapsed: 00:28:14
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 109910 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 10.83
          Mean value_function loss: 27.2622
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 30.2160
                       Mean reward: 867.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.6579
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.2423
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.89s
                      Time elapsed: 00:28:14
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 110283 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 10.84
          Mean value_function loss: 25.3970
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.2227
                       Mean reward: 871.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.4473
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.2443
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.89s
                      Time elapsed: 00:28:15
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 107448 steps/s (collection: 0.811s, learning 0.104s)
             Mean action noise std: 10.85
          Mean value_function loss: 36.2662
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.2307
                       Mean reward: 876.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 171.7774
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.2462
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.91s
                      Time elapsed: 00:28:16
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 109435 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 10.86
          Mean value_function loss: 24.2207
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 30.2380
                       Mean reward: 865.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 173.0675
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.2453
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.90s
                      Time elapsed: 00:28:17
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 108039 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 10.87
          Mean value_function loss: 30.0976
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 30.2426
                       Mean reward: 859.85
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.9632
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.2455
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.91s
                      Time elapsed: 00:28:18
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 110940 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 10.88
          Mean value_function loss: 26.7680
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.2504
                       Mean reward: 882.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.0732
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.2452
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.89s
                      Time elapsed: 00:28:19
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 108112 steps/s (collection: 0.804s, learning 0.106s)
             Mean action noise std: 10.89
          Mean value_function loss: 34.4526
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.2584
                       Mean reward: 874.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 174.0822
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.2460
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.91s
                      Time elapsed: 00:28:20
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 109064 steps/s (collection: 0.801s, learning 0.100s)
             Mean action noise std: 10.90
          Mean value_function loss: 30.0687
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.2651
                       Mean reward: 841.05
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 170.5952
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.2469
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.90s
                      Time elapsed: 00:28:21
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 108292 steps/s (collection: 0.796s, learning 0.112s)
             Mean action noise std: 10.91
          Mean value_function loss: 27.5592
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 30.2744
                       Mean reward: 871.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.7930
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.2465
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.91s
                      Time elapsed: 00:28:22
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 108825 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 10.91
          Mean value_function loss: 39.2871
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 30.2787
                       Mean reward: 857.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.8353
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.2471
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.90s
                      Time elapsed: 00:28:23
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 102410 steps/s (collection: 0.858s, learning 0.102s)
             Mean action noise std: 10.92
          Mean value_function loss: 26.6965
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.2847
                       Mean reward: 858.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.1068
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.2483
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.96s
                      Time elapsed: 00:28:24
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 106210 steps/s (collection: 0.836s, learning 0.090s)
             Mean action noise std: 10.93
          Mean value_function loss: 35.4891
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.2907
                       Mean reward: 860.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.3775
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.2475
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.93s
                      Time elapsed: 00:28:24
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 100530 steps/s (collection: 0.849s, learning 0.129s)
             Mean action noise std: 10.94
          Mean value_function loss: 36.0354
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 30.2972
                       Mean reward: 868.23
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 172.0521
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.2489
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 0.98s
                      Time elapsed: 00:28:25
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 101527 steps/s (collection: 0.871s, learning 0.098s)
             Mean action noise std: 10.95
          Mean value_function loss: 26.4902
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3020
                       Mean reward: 863.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.7490
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2491
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 0.97s
                      Time elapsed: 00:28:26
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 102094 steps/s (collection: 0.860s, learning 0.103s)
             Mean action noise std: 10.95
          Mean value_function loss: 34.5695
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.3077
                       Mean reward: 861.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.2277
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.2496
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.96s
                      Time elapsed: 00:28:27
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 106145 steps/s (collection: 0.827s, learning 0.099s)
             Mean action noise std: 10.96
          Mean value_function loss: 28.2202
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.3121
                       Mean reward: 828.88
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.1174
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.2514
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.93s
                      Time elapsed: 00:28:28
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 102860 steps/s (collection: 0.856s, learning 0.100s)
             Mean action noise std: 10.97
          Mean value_function loss: 30.8945
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.3205
                       Mean reward: 867.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.8596
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.2500
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.96s
                      Time elapsed: 00:28:29
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 106216 steps/s (collection: 0.819s, learning 0.107s)
             Mean action noise std: 10.98
          Mean value_function loss: 26.9524
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3259
                       Mean reward: 866.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.9054
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2524
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.93s
                      Time elapsed: 00:28:30
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 111668 steps/s (collection: 0.780s, learning 0.100s)
             Mean action noise std: 10.99
          Mean value_function loss: 23.7329
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.3304
                       Mean reward: 867.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 172.2094
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2525
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.88s
                      Time elapsed: 00:28:31
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 107787 steps/s (collection: 0.813s, learning 0.099s)
             Mean action noise std: 10.99
          Mean value_function loss: 20.9569
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.3372
                       Mean reward: 849.70
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.3341
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.2519
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.91s
                      Time elapsed: 00:28:32
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 108568 steps/s (collection: 0.797s, learning 0.108s)
             Mean action noise std: 11.00
          Mean value_function loss: 22.3332
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.3402
                       Mean reward: 874.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 173.4663
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.2516
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.91s
                      Time elapsed: 00:28:33
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 100683 steps/s (collection: 0.880s, learning 0.096s)
             Mean action noise std: 11.00
          Mean value_function loss: 21.8721
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.3434
                       Mean reward: 879.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.3586
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.2527
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.98s
                      Time elapsed: 00:28:34
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 111028 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 11.01
          Mean value_function loss: 20.7199
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.3477
                       Mean reward: 867.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.4123
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.2519
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.89s
                      Time elapsed: 00:28:35
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 110208 steps/s (collection: 0.790s, learning 0.102s)
             Mean action noise std: 11.02
          Mean value_function loss: 26.4797
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.3547
                       Mean reward: 863.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.1991
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.2517
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 0.89s
                      Time elapsed: 00:28:36
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 106069 steps/s (collection: 0.818s, learning 0.109s)
             Mean action noise std: 11.03
          Mean value_function loss: 27.3682
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 30.3630
                       Mean reward: 847.84
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.3425
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2533
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 0.93s
                      Time elapsed: 00:28:37
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 106657 steps/s (collection: 0.817s, learning 0.104s)
             Mean action noise std: 11.04
          Mean value_function loss: 32.9136
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.3695
                       Mean reward: 870.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 174.1580
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.2541
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.92s
                      Time elapsed: 00:28:37
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 106793 steps/s (collection: 0.805s, learning 0.115s)
             Mean action noise std: 11.04
          Mean value_function loss: 29.9585
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3726
                       Mean reward: 868.33
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.9057
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2542
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.92s
                      Time elapsed: 00:28:38
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 111497 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 11.05
          Mean value_function loss: 28.9817
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.3773
                       Mean reward: 850.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.5706
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2530
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.88s
                      Time elapsed: 00:28:39
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 110768 steps/s (collection: 0.791s, learning 0.097s)
             Mean action noise std: 11.07
          Mean value_function loss: 37.9740
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.3893
                       Mean reward: 862.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.1455
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.2555
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 0.89s
                      Time elapsed: 00:28:40
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 112294 steps/s (collection: 0.767s, learning 0.109s)
             Mean action noise std: 11.08
          Mean value_function loss: 42.5624
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 30.3982
                       Mean reward: 857.43
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 173.5612
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2532
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 0.88s
                      Time elapsed: 00:28:41
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 111798 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 11.09
          Mean value_function loss: 55.8073
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.4049
                       Mean reward: 863.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.4453
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.2565
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 0.88s
                      Time elapsed: 00:28:42
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 109981 steps/s (collection: 0.794s, learning 0.100s)
             Mean action noise std: 11.10
          Mean value_function loss: 42.8517
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.4169
                       Mean reward: 881.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.4507
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.2538
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 0.89s
                      Time elapsed: 00:28:43
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 108814 steps/s (collection: 0.800s, learning 0.103s)
             Mean action noise std: 11.11
          Mean value_function loss: 37.7333
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.4234
                       Mean reward: 875.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.4930
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2555
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 0.90s
                      Time elapsed: 00:28:44
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 108948 steps/s (collection: 0.797s, learning 0.105s)
             Mean action noise std: 11.12
          Mean value_function loss: 38.3314
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.4278
                       Mean reward: 850.80
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.0280
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.2551
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.90s
                      Time elapsed: 00:28:45
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 108609 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 11.12
          Mean value_function loss: 31.9798
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.4341
                       Mean reward: 871.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 174.0857
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.2557
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.91s
                      Time elapsed: 00:28:46
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 107577 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 11.13
          Mean value_function loss: 31.1460
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 30.4371
                       Mean reward: 871.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.8028
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.2571
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.91s
                      Time elapsed: 00:28:46
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 108861 steps/s (collection: 0.811s, learning 0.092s)
             Mean action noise std: 11.14
          Mean value_function loss: 32.7050
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 30.4459
                       Mean reward: 849.83
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.8557
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.2553
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.90s
                      Time elapsed: 00:28:47
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 109715 steps/s (collection: 0.794s, learning 0.102s)
             Mean action noise std: 11.15
          Mean value_function loss: 37.0554
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 30.4518
                       Mean reward: 840.44
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.3326
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2590
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.90s
                      Time elapsed: 00:28:48
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 110023 steps/s (collection: 0.801s, learning 0.092s)
             Mean action noise std: 11.15
          Mean value_function loss: 33.3166
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.4556
                       Mean reward: 860.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 171.6952
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.2566
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 0.89s
                      Time elapsed: 00:28:49
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 112812 steps/s (collection: 0.779s, learning 0.092s)
             Mean action noise std: 11.17
          Mean value_function loss: 34.2073
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.4635
                       Mean reward: 872.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.8854
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.2570
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.87s
                      Time elapsed: 00:28:50
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 110085 steps/s (collection: 0.792s, learning 0.101s)
             Mean action noise std: 11.18
          Mean value_function loss: 27.6852
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.4712
                       Mean reward: 873.36
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 171.3513
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.2557
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.89s
                      Time elapsed: 00:28:51
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 111223 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 11.18
          Mean value_function loss: 27.2470
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.4768
                       Mean reward: 873.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 172.4023
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.2579
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.88s
                      Time elapsed: 00:28:52
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 111733 steps/s (collection: 0.791s, learning 0.089s)
             Mean action noise std: 11.19
          Mean value_function loss: 28.9185
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 30.4820
                       Mean reward: 842.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.9961
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.2588
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.88s
                      Time elapsed: 00:28:53
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 109669 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 11.20
          Mean value_function loss: 23.9560
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.4887
                       Mean reward: 872.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 172.1758
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.2580
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.90s
                      Time elapsed: 00:28:54
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 112502 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 11.21
          Mean value_function loss: 36.6408
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 30.4977
                       Mean reward: 869.21
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 174.1915
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.2584
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.87s
                      Time elapsed: 00:28:54
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 110728 steps/s (collection: 0.787s, learning 0.101s)
             Mean action noise std: 11.22
          Mean value_function loss: 37.8575
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 30.5011
                       Mean reward: 855.54
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 172.6830
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.2580
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.89s
                      Time elapsed: 00:28:55
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 111847 steps/s (collection: 0.791s, learning 0.088s)
             Mean action noise std: 11.22
          Mean value_function loss: 30.9678
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.5047
                       Mean reward: 877.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.3579
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.2614
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.88s
                      Time elapsed: 00:28:56
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 111524 steps/s (collection: 0.781s, learning 0.101s)
             Mean action noise std: 11.23
          Mean value_function loss: 34.4362
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.5091
                       Mean reward: 841.15
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.6524
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.2604
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.88s
                      Time elapsed: 00:28:57
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 110410 steps/s (collection: 0.797s, learning 0.094s)
             Mean action noise std: 11.24
          Mean value_function loss: 29.2481
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.5149
                       Mean reward: 865.71
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 172.5831
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.2609
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.89s
                      Time elapsed: 00:28:58
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 107224 steps/s (collection: 0.824s, learning 0.093s)
             Mean action noise std: 11.25
          Mean value_function loss: 22.5168
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.5234
                       Mean reward: 868.25
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 170.8958
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.2621
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.92s
                      Time elapsed: 00:28:59
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 109308 steps/s (collection: 0.806s, learning 0.094s)
             Mean action noise std: 11.26
          Mean value_function loss: 30.0842
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.5300
                       Mean reward: 858.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.5799
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.2626
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.90s
                      Time elapsed: 00:29:00
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 112112 steps/s (collection: 0.788s, learning 0.089s)
             Mean action noise std: 11.27
          Mean value_function loss: 31.0944
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 30.5379
                       Mean reward: 862.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 173.2017
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.2612
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 0.88s
                      Time elapsed: 00:29:01
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 110659 steps/s (collection: 0.795s, learning 0.093s)
             Mean action noise std: 11.27
          Mean value_function loss: 32.4308
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.5420
                       Mean reward: 878.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.9492
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.2616
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.89s
                      Time elapsed: 00:29:02
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 110481 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 11.28
          Mean value_function loss: 23.3328
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 30.5459
                       Mean reward: 876.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 173.2076
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.2622
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.89s
                      Time elapsed: 00:29:02
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 112368 steps/s (collection: 0.786s, learning 0.089s)
             Mean action noise std: 11.29
          Mean value_function loss: 32.5562
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.5514
                       Mean reward: 871.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 169.9385
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2654
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.87s
                      Time elapsed: 00:29:03
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 109922 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 11.30
          Mean value_function loss: 25.0776
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 30.5579
                       Mean reward: 869.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.3595
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.2637
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.89s
                      Time elapsed: 00:29:04
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 111609 steps/s (collection: 0.777s, learning 0.104s)
             Mean action noise std: 11.31
          Mean value_function loss: 30.5807
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.5664
                       Mean reward: 858.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 173.5843
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2638
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.88s
                      Time elapsed: 00:29:05
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 111237 steps/s (collection: 0.782s, learning 0.102s)
             Mean action noise std: 11.32
          Mean value_function loss: 31.9009
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.5769
                       Mean reward: 881.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.0232
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2647
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.88s
                      Time elapsed: 00:29:06
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 106296 steps/s (collection: 0.804s, learning 0.121s)
             Mean action noise std: 11.33
          Mean value_function loss: 35.1356
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 30.5852
                       Mean reward: 880.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.4206
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2636
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.92s
                      Time elapsed: 00:29:07
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 103769 steps/s (collection: 0.829s, learning 0.119s)
             Mean action noise std: 11.34
          Mean value_function loss: 28.6664
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.5903
                       Mean reward: 881.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.9755
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2657
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.95s
                      Time elapsed: 00:29:08
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 109787 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 11.35
          Mean value_function loss: 33.0321
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 30.5953
                       Mean reward: 844.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.9383
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2679
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.90s
                      Time elapsed: 00:29:09
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 107378 steps/s (collection: 0.810s, learning 0.106s)
             Mean action noise std: 11.36
          Mean value_function loss: 23.8029
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.6038
                       Mean reward: 857.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.3386
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2688
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.92s
                      Time elapsed: 00:29:10
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 105826 steps/s (collection: 0.803s, learning 0.126s)
             Mean action noise std: 11.37
          Mean value_function loss: 22.7499
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.6121
                       Mean reward: 863.84
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 173.6080
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2669
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.93s
                      Time elapsed: 00:29:11
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 110364 steps/s (collection: 0.772s, learning 0.119s)
             Mean action noise std: 11.38
          Mean value_function loss: 28.5609
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.6178
                       Mean reward: 876.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.4613
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2699
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.89s
                      Time elapsed: 00:29:11
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 106412 steps/s (collection: 0.835s, learning 0.089s)
             Mean action noise std: 11.40
          Mean value_function loss: 25.0388
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 30.6280
                       Mean reward: 880.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 173.6381
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2697
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.92s
                      Time elapsed: 00:29:12
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 110027 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 11.41
          Mean value_function loss: 29.7241
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 30.6420
                       Mean reward: 861.74
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.7489
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2716
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.89s
                      Time elapsed: 00:29:13
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 111418 steps/s (collection: 0.790s, learning 0.093s)
             Mean action noise std: 11.42
          Mean value_function loss: 22.0456
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.6479
                       Mean reward: 860.80
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.4844
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2714
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.88s
                      Time elapsed: 00:29:14
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 112101 steps/s (collection: 0.776s, learning 0.101s)
             Mean action noise std: 11.43
          Mean value_function loss: 25.5693
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.6540
                       Mean reward: 863.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.9218
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2737
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.88s
                      Time elapsed: 00:29:15
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 110797 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 11.44
          Mean value_function loss: 22.4278
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.6612
                       Mean reward: 871.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 173.0806
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2724
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.89s
                      Time elapsed: 00:29:16
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 107688 steps/s (collection: 0.805s, learning 0.108s)
             Mean action noise std: 11.45
          Mean value_function loss: 23.3133
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.6678
                       Mean reward: 864.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 174.0847
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2740
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.91s
                      Time elapsed: 00:29:17
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 110340 steps/s (collection: 0.785s, learning 0.106s)
             Mean action noise std: 11.46
          Mean value_function loss: 20.2437
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.6771
                       Mean reward: 874.26
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.1601
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.2736
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.89s
                      Time elapsed: 00:29:18
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 111160 steps/s (collection: 0.796s, learning 0.089s)
             Mean action noise std: 11.47
          Mean value_function loss: 22.3099
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 30.6830
                       Mean reward: 881.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 174.0454
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.2749
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.88s
                      Time elapsed: 00:29:19
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 108340 steps/s (collection: 0.810s, learning 0.098s)
             Mean action noise std: 11.47
          Mean value_function loss: 22.9921
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.6846
                       Mean reward: 877.64
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.4901
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2769
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.91s
                      Time elapsed: 00:29:20
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 112910 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 11.48
          Mean value_function loss: 23.8294
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.6897
                       Mean reward: 875.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.4101
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.2762
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.87s
                      Time elapsed: 00:29:20
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 108198 steps/s (collection: 0.815s, learning 0.094s)
             Mean action noise std: 11.49
          Mean value_function loss: 32.7413
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.6975
                       Mean reward: 873.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 173.4734
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.2767
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.91s
                      Time elapsed: 00:29:21
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 108171 steps/s (collection: 0.817s, learning 0.092s)
             Mean action noise std: 11.50
          Mean value_function loss: 25.3834
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 30.7067
                       Mean reward: 865.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.0215
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2790
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.91s
                      Time elapsed: 00:29:22
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 110778 steps/s (collection: 0.794s, learning 0.093s)
             Mean action noise std: 11.51
          Mean value_function loss: 27.4146
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.7125
                       Mean reward: 866.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 173.4710
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.2789
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.89s
                      Time elapsed: 00:29:23
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 107826 steps/s (collection: 0.796s, learning 0.116s)
             Mean action noise std: 11.52
          Mean value_function loss: 28.1643
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.7218
                       Mean reward: 875.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 174.3836
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2802
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.91s
                      Time elapsed: 00:29:24
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 110417 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 11.53
          Mean value_function loss: 27.9533
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.7267
                       Mean reward: 866.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.5159
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2806
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.89s
                      Time elapsed: 00:29:25
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 105938 steps/s (collection: 0.814s, learning 0.114s)
             Mean action noise std: 11.54
          Mean value_function loss: 25.1762
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.7308
                       Mean reward: 877.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 174.3059
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.2799
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.93s
                      Time elapsed: 00:29:26
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 110863 steps/s (collection: 0.790s, learning 0.096s)
             Mean action noise std: 11.54
          Mean value_function loss: 27.8000
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 30.7346
                       Mean reward: 865.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 173.1805
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2813
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.89s
                      Time elapsed: 00:29:27
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 109240 steps/s (collection: 0.806s, learning 0.094s)
             Mean action noise std: 11.55
          Mean value_function loss: 24.2880
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.7391
                       Mean reward: 871.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 174.4211
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2824
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.90s
                      Time elapsed: 00:29:28
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 111288 steps/s (collection: 0.792s, learning 0.091s)
             Mean action noise std: 11.56
          Mean value_function loss: 24.1180
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 30.7472
                       Mean reward: 880.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 173.8551
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2814
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.88s
                      Time elapsed: 00:29:28
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 111916 steps/s (collection: 0.789s, learning 0.089s)
             Mean action noise std: 11.57
          Mean value_function loss: 18.6261
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.7538
                       Mean reward: 868.98
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.8726
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2826
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.88s
                      Time elapsed: 00:29:29
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 109489 steps/s (collection: 0.806s, learning 0.092s)
             Mean action noise std: 11.59
          Mean value_function loss: 20.1154
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.7614
                       Mean reward: 873.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.2119
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2829
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.90s
                      Time elapsed: 00:29:30
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 110808 steps/s (collection: 0.797s, learning 0.091s)
             Mean action noise std: 11.60
          Mean value_function loss: 23.3481
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.7725
                       Mean reward: 874.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.9749
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2832
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.89s
                      Time elapsed: 00:29:31
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 110187 steps/s (collection: 0.775s, learning 0.117s)
             Mean action noise std: 11.62
          Mean value_function loss: 32.7399
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 30.7856
                       Mean reward: 855.81
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 174.2382
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2833
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.89s
                      Time elapsed: 00:29:32
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 111382 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 11.64
          Mean value_function loss: 38.5636
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 30.7985
                       Mean reward: 868.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 173.0060
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2856
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.88s
                      Time elapsed: 00:29:33
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 113350 steps/s (collection: 0.767s, learning 0.101s)
             Mean action noise std: 11.65
          Mean value_function loss: 37.4227
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.8111
                       Mean reward: 858.20
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.2336
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.2828
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 0.87s
                      Time elapsed: 00:29:34
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 106598 steps/s (collection: 0.818s, learning 0.104s)
             Mean action noise std: 11.66
          Mean value_function loss: 32.1858
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.8177
                       Mean reward: 849.90
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 173.3732
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2857
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 0.92s
                      Time elapsed: 00:29:35
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 112840 steps/s (collection: 0.775s, learning 0.096s)
             Mean action noise std: 11.67
          Mean value_function loss: 31.7880
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 30.8244
                       Mean reward: 846.58
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.2226
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2851
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 0.87s
                      Time elapsed: 00:29:36
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 107896 steps/s (collection: 0.807s, learning 0.105s)
             Mean action noise std: 11.68
          Mean value_function loss: 32.5411
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 30.8295
                       Mean reward: 858.40
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 173.0876
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2869
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 0.91s
                      Time elapsed: 00:29:36
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 101044 steps/s (collection: 0.871s, learning 0.102s)
             Mean action noise std: 11.69
          Mean value_function loss: 33.9182
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.8354
                       Mean reward: 853.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.2396
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2844
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 0.97s
                      Time elapsed: 00:29:37
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 107093 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 11.70
          Mean value_function loss: 40.7130
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.8423
                       Mean reward: 859.77
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 174.3327
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2851
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 0.92s
                      Time elapsed: 00:29:38
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 108450 steps/s (collection: 0.810s, learning 0.096s)
             Mean action noise std: 11.71
          Mean value_function loss: 36.8312
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 30.8484
                       Mean reward: 834.79
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 170.5155
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.2888
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 0.91s
                      Time elapsed: 00:29:39
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 109431 steps/s (collection: 0.789s, learning 0.110s)
             Mean action noise std: 11.71
          Mean value_function loss: 32.8050
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.8529
                       Mean reward: 836.36
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.3837
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.2875
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 0.90s
                      Time elapsed: 00:29:40
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 108095 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 11.73
          Mean value_function loss: 25.3167
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.8594
                       Mean reward: 849.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 172.5416
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2887
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 0.91s
                      Time elapsed: 00:29:41
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 102792 steps/s (collection: 0.844s, learning 0.112s)
             Mean action noise std: 11.74
          Mean value_function loss: 33.8406
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.8697
                       Mean reward: 863.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 169.3084
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.2915
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 0.96s
                      Time elapsed: 00:29:42
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 102257 steps/s (collection: 0.848s, learning 0.113s)
             Mean action noise std: 11.75
          Mean value_function loss: 33.5251
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.8762
                       Mean reward: 863.40
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 171.7227
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.2916
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 0.96s
                      Time elapsed: 00:29:43
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 106679 steps/s (collection: 0.812s, learning 0.109s)
             Mean action noise std: 11.76
          Mean value_function loss: 29.6063
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.8840
                       Mean reward: 868.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.7966
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.2916
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 0.92s
                      Time elapsed: 00:29:44
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 99544 steps/s (collection: 0.842s, learning 0.146s)
             Mean action noise std: 11.78
          Mean value_function loss: 32.3613
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.8930
                       Mean reward: 862.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 173.1208
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.2924
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 0.99s
                      Time elapsed: 00:29:45
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 97412 steps/s (collection: 0.889s, learning 0.121s)
             Mean action noise std: 11.78
          Mean value_function loss: 34.9188
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 30.9032
                       Mean reward: 863.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 169.9286
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.2939
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.01s
                      Time elapsed: 00:29:46
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 102578 steps/s (collection: 0.849s, learning 0.109s)
             Mean action noise std: 11.79
          Mean value_function loss: 34.1456
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.9076
                       Mean reward: 873.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 173.5468
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.2932
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 0.96s
                      Time elapsed: 00:29:47
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 103510 steps/s (collection: 0.849s, learning 0.101s)
             Mean action noise std: 11.80
          Mean value_function loss: 28.3031
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.9134
                       Mean reward: 855.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.5175
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2943
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 0.95s
                      Time elapsed: 00:29:48
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 105720 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 11.81
          Mean value_function loss: 31.4961
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.9200
                       Mean reward: 857.92
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.2012
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.2962
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 0.93s
                      Time elapsed: 00:29:49
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 97841 steps/s (collection: 0.890s, learning 0.115s)
             Mean action noise std: 11.83
          Mean value_function loss: 26.1189
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.9299
                       Mean reward: 882.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 174.0134
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.2950
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.00s
                      Time elapsed: 00:29:50
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 104006 steps/s (collection: 0.837s, learning 0.108s)
             Mean action noise std: 11.84
          Mean value_function loss: 39.2487
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.9399
                       Mean reward: 860.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.5954
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2964
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 0.95s
                      Time elapsed: 00:29:51
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 92907 steps/s (collection: 0.947s, learning 0.111s)
             Mean action noise std: 11.85
          Mean value_function loss: 33.9268
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.9478
                       Mean reward: 862.15
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.5824
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.2967
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.06s
                      Time elapsed: 00:29:52
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 105648 steps/s (collection: 0.819s, learning 0.111s)
             Mean action noise std: 11.86
          Mean value_function loss: 29.8732
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.9561
                       Mean reward: 861.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.6408
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.2986
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 0.93s
                      Time elapsed: 00:29:53
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 98770 steps/s (collection: 0.879s, learning 0.117s)
             Mean action noise std: 11.87
          Mean value_function loss: 31.3867
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.9619
                       Mean reward: 862.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.0594
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.3008
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 1.00s
                      Time elapsed: 00:29:54
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 98711 steps/s (collection: 0.869s, learning 0.127s)
             Mean action noise std: 11.88
          Mean value_function loss: 28.6259
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.9673
                       Mean reward: 858.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.7675
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.2998
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.00s
                      Time elapsed: 00:29:55
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 100977 steps/s (collection: 0.848s, learning 0.126s)
             Mean action noise std: 11.88
          Mean value_function loss: 23.9836
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.9727
                       Mean reward: 857.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.1574
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.3004
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 0.97s
                      Time elapsed: 00:29:56
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 91542 steps/s (collection: 0.925s, learning 0.149s)
             Mean action noise std: 11.89
          Mean value_function loss: 32.8800
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.9785
                       Mean reward: 858.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 173.1770
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.3018
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.07s
                      Time elapsed: 00:29:57
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 99301 steps/s (collection: 0.871s, learning 0.119s)
             Mean action noise std: 11.90
          Mean value_function loss: 20.2838
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.9848
                       Mean reward: 866.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.1684
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.2995
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 0.99s
                      Time elapsed: 00:29:58
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 104147 steps/s (collection: 0.849s, learning 0.095s)
             Mean action noise std: 11.91
          Mean value_function loss: 25.2430
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.9910
                       Mean reward: 851.37
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.6083
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3022
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 0.94s
                      Time elapsed: 00:29:59
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 105448 steps/s (collection: 0.834s, learning 0.098s)
             Mean action noise std: 11.92
          Mean value_function loss: 27.9834
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 30.9971
                       Mean reward: 855.62
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.5332
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3019
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 0.93s
                      Time elapsed: 00:30:00
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 106859 steps/s (collection: 0.823s, learning 0.097s)
             Mean action noise std: 11.93
          Mean value_function loss: 29.4182
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 31.0042
                       Mean reward: 851.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.2829
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3055
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 0.92s
                      Time elapsed: 00:30:01
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 103255 steps/s (collection: 0.852s, learning 0.100s)
             Mean action noise std: 11.93
          Mean value_function loss: 29.2060
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.0079
                       Mean reward: 869.01
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 173.3008
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3025
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 0.95s
                      Time elapsed: 00:30:01
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 100392 steps/s (collection: 0.879s, learning 0.100s)
             Mean action noise std: 11.94
          Mean value_function loss: 30.4500
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.0120
                       Mean reward: 880.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 173.9479
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3037
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 0.98s
                      Time elapsed: 00:30:02
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 104883 steps/s (collection: 0.830s, learning 0.107s)
             Mean action noise std: 11.94
          Mean value_function loss: 26.3794
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 31.0165
                       Mean reward: 868.73
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.9281
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3040
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 0.94s
                      Time elapsed: 00:30:03
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 102054 steps/s (collection: 0.868s, learning 0.095s)
             Mean action noise std: 11.95
          Mean value_function loss: 35.2005
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.0205
                       Mean reward: 857.72
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.4019
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3055
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 0.96s
                      Time elapsed: 00:30:04
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 104047 steps/s (collection: 0.848s, learning 0.097s)
             Mean action noise std: 11.96
          Mean value_function loss: 35.6100
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.0274
                       Mean reward: 857.44
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 173.0989
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3058
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 0.94s
                      Time elapsed: 00:30:05
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 105568 steps/s (collection: 0.823s, learning 0.109s)
             Mean action noise std: 11.97
          Mean value_function loss: 35.6597
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.0343
                       Mean reward: 872.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 174.2265
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3058
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 0.93s
                      Time elapsed: 00:30:06
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 105075 steps/s (collection: 0.816s, learning 0.120s)
             Mean action noise std: 11.98
          Mean value_function loss: 31.7419
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.0404
                       Mean reward: 871.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 173.7598
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3063
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 0.94s
                      Time elapsed: 00:30:07
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 105882 steps/s (collection: 0.825s, learning 0.104s)
             Mean action noise std: 11.99
          Mean value_function loss: 19.8330
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.0437
                       Mean reward: 874.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 173.2981
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3078
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 0.93s
                      Time elapsed: 00:30:08
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 109735 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 11.99
          Mean value_function loss: 37.3173
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 31.0485
                       Mean reward: 860.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.8105
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.3079
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 0.90s
                      Time elapsed: 00:30:09
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 104497 steps/s (collection: 0.831s, learning 0.110s)
             Mean action noise std: 11.99
          Mean value_function loss: 25.7255
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 31.0495
                       Mean reward: 874.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.0654
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3089
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 0.94s
                      Time elapsed: 00:30:10
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 110521 steps/s (collection: 0.780s, learning 0.110s)
             Mean action noise std: 12.00
          Mean value_function loss: 23.0755
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 31.0516
                       Mean reward: 869.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.0272
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3091
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 0.89s
                      Time elapsed: 00:30:11
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 94787 steps/s (collection: 0.916s, learning 0.121s)
             Mean action noise std: 12.00
          Mean value_function loss: 27.6845
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.0550
                       Mean reward: 872.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.9256
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.3078
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.04s
                      Time elapsed: 00:30:12
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 92401 steps/s (collection: 0.913s, learning 0.151s)
             Mean action noise std: 12.01
          Mean value_function loss: 28.4067
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.0588
                       Mean reward: 868.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.1888
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3076
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.06s
                      Time elapsed: 00:30:13
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 93732 steps/s (collection: 0.921s, learning 0.127s)
             Mean action noise std: 12.01
          Mean value_function loss: 21.8606
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0622
                       Mean reward: 870.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.5482
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.3100
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 1.05s
                      Time elapsed: 00:30:14
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 92031 steps/s (collection: 0.927s, learning 0.142s)
             Mean action noise std: 12.02
          Mean value_function loss: 22.4137
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.0681
                       Mean reward: 872.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 172.8836
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.3095
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.07s
                      Time elapsed: 00:30:15
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 76983 steps/s (collection: 1.129s, learning 0.148s)
             Mean action noise std: 12.03
          Mean value_function loss: 21.9668
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.0722
                       Mean reward: 864.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.5454
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.3098
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 1.28s
                      Time elapsed: 00:30:16
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 88994 steps/s (collection: 1.002s, learning 0.103s)
             Mean action noise std: 12.03
          Mean value_function loss: 25.9306
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.0744
                       Mean reward: 871.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 174.0911
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.3108
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 1.10s
                      Time elapsed: 00:30:17
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 101641 steps/s (collection: 0.878s, learning 0.090s)
             Mean action noise std: 12.03
          Mean value_function loss: 21.2010
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.0761
                       Mean reward: 857.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.3309
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3107
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 0.97s
                      Time elapsed: 00:30:18
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 99966 steps/s (collection: 0.860s, learning 0.123s)
             Mean action noise std: 12.04
          Mean value_function loss: 17.3896
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.0778
                       Mean reward: 866.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.3935
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.3144
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 0.98s
                      Time elapsed: 00:30:19
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 79347 steps/s (collection: 1.089s, learning 0.150s)
             Mean action noise std: 12.04
          Mean value_function loss: 21.0049
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.0812
                       Mean reward: 858.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.7463
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3123
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.24s
                      Time elapsed: 00:30:21
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 97135 steps/s (collection: 0.907s, learning 0.105s)
             Mean action noise std: 12.06
          Mean value_function loss: 22.6647
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0884
                       Mean reward: 867.47
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 173.4369
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3122
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.01s
                      Time elapsed: 00:30:22
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 96719 steps/s (collection: 0.910s, learning 0.106s)
             Mean action noise std: 12.07
          Mean value_function loss: 34.3240
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 31.0983
                       Mean reward: 875.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.6296
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.3140
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.02s
                      Time elapsed: 00:30:23
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 98817 steps/s (collection: 0.878s, learning 0.117s)
             Mean action noise std: 12.08
          Mean value_function loss: 32.2477
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 31.1051
                       Mean reward: 868.16
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 172.0119
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.3123
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 0.99s
                      Time elapsed: 00:30:24
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 98397 steps/s (collection: 0.899s, learning 0.100s)
             Mean action noise std: 12.09
          Mean value_function loss: 25.3385
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 31.1096
                       Mean reward: 879.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 172.8069
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3130
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.00s
                      Time elapsed: 00:30:25
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 92944 steps/s (collection: 0.926s, learning 0.132s)
             Mean action noise std: 12.10
          Mean value_function loss: 24.1312
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.1160
                       Mean reward: 883.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 174.1101
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3150
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.06s
                      Time elapsed: 00:30:26
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 86758 steps/s (collection: 1.000s, learning 0.133s)
             Mean action noise std: 12.11
          Mean value_function loss: 22.9697
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.1235
                       Mean reward: 871.20
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 173.9876
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.3142
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 1.13s
                      Time elapsed: 00:30:27
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 91070 steps/s (collection: 0.916s, learning 0.164s)
             Mean action noise std: 12.12
          Mean value_function loss: 25.4041
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 31.1286
                       Mean reward: 872.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 174.2193
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3145
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.08s
                      Time elapsed: 00:30:28
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 89534 steps/s (collection: 0.925s, learning 0.173s)
             Mean action noise std: 12.13
          Mean value_function loss: 27.6311
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 31.1358
                       Mean reward: 871.39
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 173.8992
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.3162
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 1.10s
                      Time elapsed: 00:30:29
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 96144 steps/s (collection: 0.885s, learning 0.137s)
             Mean action noise std: 12.14
          Mean value_function loss: 35.5430
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.1456
                       Mean reward: 860.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.3514
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3148
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 1.02s
                      Time elapsed: 00:30:30
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 88706 steps/s (collection: 0.954s, learning 0.154s)
             Mean action noise std: 12.15
          Mean value_function loss: 24.8908
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 31.1534
                       Mean reward: 858.63
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.5510
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.3165
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.11s
                      Time elapsed: 00:30:31
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 98403 steps/s (collection: 0.829s, learning 0.170s)
             Mean action noise std: 12.16
          Mean value_function loss: 20.5001
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.1583
                       Mean reward: 846.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.8770
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.3190
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 1.00s
                      Time elapsed: 00:30:32
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 92818 steps/s (collection: 0.940s, learning 0.120s)
             Mean action noise std: 12.16
          Mean value_function loss: 16.6526
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.1620
                       Mean reward: 871.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 172.2529
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.3201
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.06s
                      Time elapsed: 00:30:33
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 105965 steps/s (collection: 0.795s, learning 0.133s)
             Mean action noise std: 12.17
          Mean value_function loss: 25.7852
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.1635
                       Mean reward: 880.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 174.3280
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.3183
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 0.93s
                      Time elapsed: 00:30:34
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 103898 steps/s (collection: 0.835s, learning 0.111s)
             Mean action noise std: 12.18
          Mean value_function loss: 27.2072
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.1702
                       Mean reward: 878.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.8791
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3196
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 0.95s
                      Time elapsed: 00:30:35
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 100334 steps/s (collection: 0.851s, learning 0.129s)
             Mean action noise std: 12.19
          Mean value_function loss: 24.0316
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.1783
                       Mean reward: 877.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.7677
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3193
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 0.98s
                      Time elapsed: 00:30:36
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 100633 steps/s (collection: 0.850s, learning 0.127s)
             Mean action noise std: 12.20
          Mean value_function loss: 19.8953
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.1832
                       Mean reward: 860.16
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.1961
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3200
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 0.98s
                      Time elapsed: 00:30:37
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 95607 steps/s (collection: 0.867s, learning 0.161s)
             Mean action noise std: 12.20
          Mean value_function loss: 29.0841
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.1875
                       Mean reward: 869.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.6098
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.3233
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 1.03s
                      Time elapsed: 00:30:38
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 103234 steps/s (collection: 0.828s, learning 0.125s)
             Mean action noise std: 12.22
          Mean value_function loss: 19.5018
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 31.1955
                       Mean reward: 867.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 172.1331
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3242
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 0.95s
                      Time elapsed: 00:30:39
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 103252 steps/s (collection: 0.832s, learning 0.120s)
             Mean action noise std: 12.23
          Mean value_function loss: 22.4083
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.2057
                       Mean reward: 872.73
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.8067
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3235
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 0.95s
                      Time elapsed: 00:30:40
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 104436 steps/s (collection: 0.828s, learning 0.114s)
             Mean action noise std: 12.24
          Mean value_function loss: 26.7635
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 31.2120
                       Mean reward: 873.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 174.4058
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3230
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 0.94s
                      Time elapsed: 00:30:41
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 100369 steps/s (collection: 0.839s, learning 0.141s)
             Mean action noise std: 12.25
          Mean value_function loss: 28.7137
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 31.2208
                       Mean reward: 877.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 174.5321
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3214
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 0.98s
                      Time elapsed: 00:30:42
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 94134 steps/s (collection: 0.864s, learning 0.180s)
             Mean action noise std: 12.26
          Mean value_function loss: 21.1400
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.2258
                       Mean reward: 881.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.3226
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.3231
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 1.04s
                      Time elapsed: 00:30:43
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 95362 steps/s (collection: 0.872s, learning 0.159s)
             Mean action noise std: 12.27
          Mean value_function loss: 29.9299
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.2302
                       Mean reward: 868.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 173.9811
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.3231
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 1.03s
                      Time elapsed: 00:30:44
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 99148 steps/s (collection: 0.845s, learning 0.146s)
             Mean action noise std: 12.28
          Mean value_function loss: 26.7058
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.2356
                       Mean reward: 869.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 174.4926
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.3240
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 0.99s
                      Time elapsed: 00:30:45
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 67477 steps/s (collection: 1.324s, learning 0.133s)
             Mean action noise std: 12.29
          Mean value_function loss: 29.0306
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.2419
                       Mean reward: 870.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 173.6122
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3247
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 1.46s
                      Time elapsed: 00:30:46
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 98670 steps/s (collection: 0.889s, learning 0.108s)
             Mean action noise std: 12.30
          Mean value_function loss: 26.8486
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.2496
                       Mean reward: 855.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.6473
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.3229
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 1.00s
                      Time elapsed: 00:30:47
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 104669 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 12.31
          Mean value_function loss: 34.4411
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 31.2576
                       Mean reward: 881.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 174.3443
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3245
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 0.94s
                      Time elapsed: 00:30:48
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 104948 steps/s (collection: 0.846s, learning 0.091s)
             Mean action noise std: 12.31
          Mean value_function loss: 26.4549
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 31.2617
                       Mean reward: 877.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 175.7238
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.3235
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 0.94s
                      Time elapsed: 00:30:49
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 92921 steps/s (collection: 0.858s, learning 0.200s)
             Mean action noise std: 12.32
          Mean value_function loss: 34.5394
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.2669
                       Mean reward: 856.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.1485
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.3270
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 1.06s
                      Time elapsed: 00:30:50
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 99286 steps/s (collection: 0.895s, learning 0.095s)
             Mean action noise std: 12.33
          Mean value_function loss: 37.6107
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.2733
                       Mean reward: 867.63
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 173.9294
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3236
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 0.99s
                      Time elapsed: 00:30:51
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 105311 steps/s (collection: 0.842s, learning 0.092s)
             Mean action noise std: 12.33
          Mean value_function loss: 33.1445
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.2766
                       Mean reward: 873.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.5393
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.3255
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 0.93s
                      Time elapsed: 00:30:52
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 107951 steps/s (collection: 0.796s, learning 0.115s)
             Mean action noise std: 12.34
          Mean value_function loss: 29.4079
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.2794
                       Mean reward: 849.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.7469
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3263
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 0.91s
                      Time elapsed: 00:30:53
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 99093 steps/s (collection: 0.863s, learning 0.129s)
             Mean action noise std: 12.35
          Mean value_function loss: 24.8884
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.2846
                       Mean reward: 842.33
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.1600
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.3270
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 0.99s
                      Time elapsed: 00:30:54
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 99505 steps/s (collection: 0.882s, learning 0.106s)
             Mean action noise std: 12.36
          Mean value_function loss: 28.8395
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 31.2894
                       Mean reward: 859.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 173.1360
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.3287
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 0.99s
                      Time elapsed: 00:30:55
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 106002 steps/s (collection: 0.836s, learning 0.091s)
             Mean action noise std: 12.36
          Mean value_function loss: 34.5934
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.2937
                       Mean reward: 872.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.0242
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.3288
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 0.93s
                      Time elapsed: 00:30:56
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 106296 steps/s (collection: 0.818s, learning 0.107s)
             Mean action noise std: 12.38
          Mean value_function loss: 33.1202
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.3002
                       Mean reward: 860.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.1001
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.3284
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 0.92s
                      Time elapsed: 00:30:57
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 103312 steps/s (collection: 0.855s, learning 0.097s)
             Mean action noise std: 12.38
          Mean value_function loss: 28.4783
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.3056
                       Mean reward: 868.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 173.0157
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.3283
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 0.95s
                      Time elapsed: 00:30:58
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 99003 steps/s (collection: 0.887s, learning 0.106s)
             Mean action noise std: 12.40
          Mean value_function loss: 26.5285
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.3119
                       Mean reward: 880.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.1617
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.3283
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 0.99s
                      Time elapsed: 00:30:59
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 108701 steps/s (collection: 0.799s, learning 0.105s)
             Mean action noise std: 12.40
          Mean value_function loss: 31.6001
               Mean surrogate loss: 0.0246
                 Mean entropy loss: 31.3191
                       Mean reward: 854.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.2594
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.3310
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 0.90s
                      Time elapsed: 00:31:00
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 100330 steps/s (collection: 0.822s, learning 0.158s)
             Mean action noise std: 12.40
          Mean value_function loss: 48.8178
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 31.3199
                       Mean reward: 878.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.4647
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.3288
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 0.98s
                      Time elapsed: 00:31:01
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 103013 steps/s (collection: 0.845s, learning 0.110s)
             Mean action noise std: 12.41
          Mean value_function loss: 63.3411
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 31.3224
                       Mean reward: 879.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 173.1601
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.3276
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 0.95s
                      Time elapsed: 00:31:02
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 100665 steps/s (collection: 0.834s, learning 0.143s)
             Mean action noise std: 12.42
          Mean value_function loss: 68.1508
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 31.3284
                       Mean reward: 866.82
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.5726
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3255
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 0.98s
                      Time elapsed: 00:31:03
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 104073 steps/s (collection: 0.806s, learning 0.139s)
             Mean action noise std: 12.42
          Mean value_function loss: 61.6647
               Mean surrogate loss: 0.0171
                 Mean entropy loss: 31.3351
                       Mean reward: 869.57
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.1829
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3274
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 0.94s
                      Time elapsed: 00:31:04
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 95479 steps/s (collection: 0.909s, learning 0.121s)
             Mean action noise std: 12.42
          Mean value_function loss: 53.8177
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.3357
                       Mean reward: 869.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.8630
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.3293
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 1.03s
                      Time elapsed: 00:31:05
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 99853 steps/s (collection: 0.827s, learning 0.158s)
             Mean action noise std: 12.43
          Mean value_function loss: 48.8353
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.3388
                       Mean reward: 853.28
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 172.0642
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.3285
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 0.98s
                      Time elapsed: 00:31:06
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 92719 steps/s (collection: 0.937s, learning 0.123s)
             Mean action noise std: 12.45
          Mean value_function loss: 39.3238
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.3464
                       Mean reward: 873.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.0146
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.3278
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 1.06s
                      Time elapsed: 00:31:07
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 109988 steps/s (collection: 0.780s, learning 0.114s)
             Mean action noise std: 12.46
          Mean value_function loss: 40.8159
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.3530
                       Mean reward: 847.83
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 170.9610
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.3281
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 0.89s
                      Time elapsed: 00:31:08
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 83055 steps/s (collection: 0.959s, learning 0.224s)
             Mean action noise std: 12.47
          Mean value_function loss: 57.0068
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 31.3597
                       Mean reward: 862.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.8902
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.3260
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 1.18s
                      Time elapsed: 00:31:09
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 81898 steps/s (collection: 1.069s, learning 0.132s)
             Mean action noise std: 12.47
          Mean value_function loss: 48.8846
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 31.3663
                       Mean reward: 826.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 166.0999
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.3303
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 1.20s
                      Time elapsed: 00:31:10
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 92316 steps/s (collection: 0.948s, learning 0.117s)
             Mean action noise std: 12.48
          Mean value_function loss: 39.4447
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 31.3700
                       Mean reward: 819.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7211
     Episode_Reward/lifting_object: 166.5307
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.3326
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 1.06s
                      Time elapsed: 00:31:11
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 91540 steps/s (collection: 0.976s, learning 0.098s)
             Mean action noise std: 12.48
          Mean value_function loss: 37.7129
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 31.3725
                       Mean reward: 824.19
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 164.2832
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.3331
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 1.07s
                      Time elapsed: 00:31:12
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 90721 steps/s (collection: 0.955s, learning 0.129s)
             Mean action noise std: 12.49
          Mean value_function loss: 41.0807
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.3755
                       Mean reward: 847.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7357
     Episode_Reward/lifting_object: 167.7930
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3317
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 1.08s
                      Time elapsed: 00:31:13
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 83897 steps/s (collection: 1.064s, learning 0.108s)
             Mean action noise std: 12.50
          Mean value_function loss: 42.0252
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 31.3810
                       Mean reward: 880.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.7794
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3301
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 1.17s
                      Time elapsed: 00:31:14
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 81185 steps/s (collection: 1.064s, learning 0.147s)
             Mean action noise std: 12.51
          Mean value_function loss: 34.3556
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.3875
                       Mean reward: 869.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.0794
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.3285
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 1.21s
                      Time elapsed: 00:31:16
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 86688 steps/s (collection: 0.963s, learning 0.171s)
             Mean action noise std: 12.52
          Mean value_function loss: 36.4738
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.3935
                       Mean reward: 873.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.4564
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.3283
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 1.13s
                      Time elapsed: 00:31:17
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 94230 steps/s (collection: 0.893s, learning 0.151s)
             Mean action noise std: 12.52
          Mean value_function loss: 33.1425
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.3971
                       Mean reward: 859.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.6622
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.3320
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 1.04s
                      Time elapsed: 00:31:18
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 101333 steps/s (collection: 0.813s, learning 0.157s)
             Mean action noise std: 12.54
          Mean value_function loss: 36.0330
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 31.4053
                       Mean reward: 850.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.3056
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.3296
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 0.97s
                      Time elapsed: 00:31:19
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 107134 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 12.55
          Mean value_function loss: 42.5036
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 31.4145
                       Mean reward: 850.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.7325
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.3299
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 0.92s
                      Time elapsed: 00:31:20
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 106063 steps/s (collection: 0.813s, learning 0.114s)
             Mean action noise std: 12.56
          Mean value_function loss: 42.8981
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 31.4242
                       Mean reward: 863.07
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.4376
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.3286
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 0.93s
                      Time elapsed: 00:31:21
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 105385 steps/s (collection: 0.837s, learning 0.095s)
             Mean action noise std: 12.57
          Mean value_function loss: 51.1248
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 31.4298
                       Mean reward: 866.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.0531
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.3285
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 0.93s
                      Time elapsed: 00:31:22
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 102327 steps/s (collection: 0.858s, learning 0.102s)
             Mean action noise std: 12.59
          Mean value_function loss: 35.7164
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 31.4383
                       Mean reward: 856.66
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.0325
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.3300
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 0.96s
                      Time elapsed: 00:31:23
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 101459 steps/s (collection: 0.842s, learning 0.127s)
             Mean action noise std: 12.59
          Mean value_function loss: 49.0706
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 31.4453
                       Mean reward: 860.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.5044
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.3321
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 0.97s
                      Time elapsed: 00:31:24
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 99675 steps/s (collection: 0.872s, learning 0.114s)
             Mean action noise std: 12.60
          Mean value_function loss: 41.8362
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.4492
                       Mean reward: 853.58
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 171.3503
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.3311
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 0.99s
                      Time elapsed: 00:31:25
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 83865 steps/s (collection: 1.004s, learning 0.168s)
             Mean action noise std: 12.61
          Mean value_function loss: 47.0920
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.4522
                       Mean reward: 873.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 172.8634
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.3318
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 1.17s
                      Time elapsed: 00:31:26
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 76040 steps/s (collection: 1.041s, learning 0.252s)
             Mean action noise std: 12.61
          Mean value_function loss: 47.9099
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.4561
                       Mean reward: 872.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.8951
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.3310
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 1.29s
                      Time elapsed: 00:31:27
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 78229 steps/s (collection: 1.081s, learning 0.176s)
             Mean action noise std: 12.62
          Mean value_function loss: 45.8155
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 31.4603
                       Mean reward: 839.11
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 169.7213
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.3330
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 1.26s
                      Time elapsed: 00:31:28
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 98168 steps/s (collection: 0.858s, learning 0.143s)
             Mean action noise std: 12.62
          Mean value_function loss: 42.1126
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 31.4640
                       Mean reward: 841.13
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 169.4993
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.3344
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 1.00s
                      Time elapsed: 00:31:29
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 78826 steps/s (collection: 1.052s, learning 0.195s)
             Mean action noise std: 12.63
          Mean value_function loss: 40.0318
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.4690
                       Mean reward: 857.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.7224
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.3359
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 1.25s
                      Time elapsed: 00:31:30
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 69679 steps/s (collection: 1.199s, learning 0.212s)
             Mean action noise std: 12.64
          Mean value_function loss: 37.2472
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.4735
                       Mean reward: 856.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 168.0730
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.3387
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 1.41s
                      Time elapsed: 00:31:32
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 85850 steps/s (collection: 0.996s, learning 0.149s)
             Mean action noise std: 12.64
          Mean value_function loss: 36.7367
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 31.4782
                       Mean reward: 853.24
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 169.5420
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3382
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 1.15s
                      Time elapsed: 00:31:33
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 85232 steps/s (collection: 0.989s, learning 0.164s)
             Mean action noise std: 12.65
          Mean value_function loss: 39.9343
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.4804
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.6465
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3375
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 1.15s
                      Time elapsed: 00:31:34
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 89591 steps/s (collection: 0.979s, learning 0.119s)
             Mean action noise std: 12.66
          Mean value_function loss: 40.6144
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.4857
                       Mean reward: 859.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.2793
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.3372
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 1.10s
                      Time elapsed: 00:31:35
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 97417 steps/s (collection: 0.902s, learning 0.108s)
             Mean action noise std: 12.66
          Mean value_function loss: 30.5962
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 31.4901
                       Mean reward: 863.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 171.3405
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.3394
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 1.01s
                      Time elapsed: 00:31:36
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 105922 steps/s (collection: 0.829s, learning 0.099s)
             Mean action noise std: 12.67
          Mean value_function loss: 37.4513
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 31.4939
                       Mean reward: 866.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 171.1392
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.3385
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 0.93s
                      Time elapsed: 00:31:37
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 103287 steps/s (collection: 0.845s, learning 0.107s)
             Mean action noise std: 12.69
          Mean value_function loss: 36.4545
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 31.5030
                       Mean reward: 862.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.0872
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.3387
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 0.95s
                      Time elapsed: 00:31:38
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 108638 steps/s (collection: 0.815s, learning 0.090s)
             Mean action noise std: 12.70
          Mean value_function loss: 50.8245
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.5123
                       Mean reward: 876.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.4563
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3398
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 0.90s
                      Time elapsed: 00:31:39
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 106691 steps/s (collection: 0.818s, learning 0.104s)
             Mean action noise std: 12.71
          Mean value_function loss: 39.9956
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 31.5194
                       Mean reward: 852.19
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.1929
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3404
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 0.92s
                      Time elapsed: 00:31:40
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 79047 steps/s (collection: 1.108s, learning 0.136s)
             Mean action noise std: 12.72
          Mean value_function loss: 54.2610
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.5275
                       Mean reward: 856.22
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 173.0765
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3398
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 1.24s
                      Time elapsed: 00:31:41
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 82932 steps/s (collection: 1.067s, learning 0.119s)
             Mean action noise std: 12.74
          Mean value_function loss: 67.7448
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.5375
                       Mean reward: 863.62
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 170.7897
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.3399
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 1.19s
                      Time elapsed: 00:31:42
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 77195 steps/s (collection: 1.042s, learning 0.231s)
             Mean action noise std: 12.75
          Mean value_function loss: 51.8158
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 31.5450
                       Mean reward: 848.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.4848
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.3419
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 1.27s
                      Time elapsed: 00:31:44
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 90163 steps/s (collection: 0.930s, learning 0.160s)
             Mean action noise std: 12.76
          Mean value_function loss: 46.7465
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.5505
                       Mean reward: 863.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.1485
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.3427
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 1.09s
                      Time elapsed: 00:31:45
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 96707 steps/s (collection: 0.838s, learning 0.179s)
             Mean action noise std: 12.78
          Mean value_function loss: 41.8148
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 31.5591
                       Mean reward: 868.48
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.0849
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.3407
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 1.02s
                      Time elapsed: 00:31:46
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 91997 steps/s (collection: 0.898s, learning 0.170s)
             Mean action noise std: 12.79
          Mean value_function loss: 54.2916
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 31.5707
                       Mean reward: 859.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.3089
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.3452
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 1.07s
                      Time elapsed: 00:31:47
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 104485 steps/s (collection: 0.791s, learning 0.150s)
             Mean action noise std: 12.80
          Mean value_function loss: 50.0211
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.5773
                       Mean reward: 848.22
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 170.9161
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.3437
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 0.94s
                      Time elapsed: 00:31:48
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 100084 steps/s (collection: 0.848s, learning 0.135s)
             Mean action noise std: 12.81
          Mean value_function loss: 46.7259
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.5844
                       Mean reward: 842.94
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 169.1729
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.3460
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 0.98s
                      Time elapsed: 00:31:49
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 108312 steps/s (collection: 0.795s, learning 0.113s)
             Mean action noise std: 12.83
          Mean value_function loss: 50.8921
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 31.5932
                       Mean reward: 854.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7399
     Episode_Reward/lifting_object: 169.9471
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.3497
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 0.91s
                      Time elapsed: 00:31:50
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 104942 steps/s (collection: 0.832s, learning 0.105s)
             Mean action noise std: 12.85
          Mean value_function loss: 43.8191
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 31.6049
                       Mean reward: 825.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7240
     Episode_Reward/lifting_object: 166.8771
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.3502
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 0.94s
                      Time elapsed: 00:31:51
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 113262 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 12.86
          Mean value_function loss: 44.8284
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.6157
                       Mean reward: 839.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7143
     Episode_Reward/lifting_object: 165.6906
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.3532
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 0.87s
                      Time elapsed: 00:31:52
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 112484 steps/s (collection: 0.772s, learning 0.102s)
             Mean action noise std: 12.87
          Mean value_function loss: 40.5739
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 31.6245
                       Mean reward: 861.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 170.0089
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.3498
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 0.87s
                      Time elapsed: 00:31:52
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 106923 steps/s (collection: 0.830s, learning 0.089s)
             Mean action noise std: 12.88
          Mean value_function loss: 43.7769
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.6302
                       Mean reward: 845.60
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 170.1276
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.3519
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 0.92s
                      Time elapsed: 00:31:53
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 97676 steps/s (collection: 0.898s, learning 0.108s)
             Mean action noise std: 12.89
          Mean value_function loss: 44.7755
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.6342
                       Mean reward: 856.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 172.0208
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.3505
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 1.01s
                      Time elapsed: 00:31:54
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 107188 steps/s (collection: 0.797s, learning 0.121s)
             Mean action noise std: 12.90
          Mean value_function loss: 31.2767
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.6397
                       Mean reward: 862.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.8512
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3518
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 0.92s
                      Time elapsed: 00:31:55
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 105437 steps/s (collection: 0.839s, learning 0.093s)
             Mean action noise std: 12.91
          Mean value_function loss: 30.4569
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.6483
                       Mean reward: 849.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.2973
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3515
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 0.93s
                      Time elapsed: 00:31:56
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 113172 steps/s (collection: 0.777s, learning 0.092s)
             Mean action noise std: 12.92
          Mean value_function loss: 37.1554
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.6562
                       Mean reward: 851.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.9227
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.3525
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 0.87s
                      Time elapsed: 00:31:57
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 101861 steps/s (collection: 0.862s, learning 0.103s)
             Mean action noise std: 12.93
          Mean value_function loss: 33.7990
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 31.6606
                       Mean reward: 861.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.1943
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3492
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 0.97s
                      Time elapsed: 00:31:58
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 107939 steps/s (collection: 0.792s, learning 0.119s)
             Mean action noise std: 12.94
          Mean value_function loss: 26.9770
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.6661
                       Mean reward: 870.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.5131
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.3509
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 0.91s
                      Time elapsed: 00:31:59
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 104506 steps/s (collection: 0.810s, learning 0.131s)
             Mean action noise std: 12.95
          Mean value_function loss: 36.0622
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 31.6739
                       Mean reward: 852.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.6680
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.3530
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 0.94s
                      Time elapsed: 00:32:00
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 98869 steps/s (collection: 0.898s, learning 0.097s)
             Mean action noise std: 12.96
          Mean value_function loss: 31.6217
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 31.6813
                       Mean reward: 845.08
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.4882
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.3528
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 0.99s
                      Time elapsed: 00:32:01
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 106061 steps/s (collection: 0.779s, learning 0.148s)
             Mean action noise std: 12.97
          Mean value_function loss: 28.8760
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.6866
                       Mean reward: 863.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.3309
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3537
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 0.93s
                      Time elapsed: 00:32:02
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 103949 steps/s (collection: 0.780s, learning 0.165s)
             Mean action noise std: 12.98
          Mean value_function loss: 38.6266
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.6907
                       Mean reward: 839.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 171.6417
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.3542
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 0.95s
                      Time elapsed: 00:32:03
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 107584 steps/s (collection: 0.800s, learning 0.114s)
             Mean action noise std: 12.98
          Mean value_function loss: 30.4490
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 31.6952
                       Mean reward: 866.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.9406
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3530
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 0.91s
                      Time elapsed: 00:32:04
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 98159 steps/s (collection: 0.831s, learning 0.170s)
             Mean action noise std: 13.00
          Mean value_function loss: 29.0483
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 31.7019
                       Mean reward: 869.57
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 172.2650
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3532
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 1.00s
                      Time elapsed: 00:32:05
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 113119 steps/s (collection: 0.781s, learning 0.088s)
             Mean action noise std: 13.01
          Mean value_function loss: 30.8991
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 31.7079
                       Mean reward: 865.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 173.1123
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3567
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 0.87s
                      Time elapsed: 00:32:06
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 115445 steps/s (collection: 0.766s, learning 0.086s)
             Mean action noise std: 13.01
          Mean value_function loss: 22.9432
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.7130
                       Mean reward: 873.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 172.5140
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.3561
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 0.85s
                      Time elapsed: 00:32:06
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 106322 steps/s (collection: 0.839s, learning 0.086s)
             Mean action noise std: 13.02
          Mean value_function loss: 30.0942
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 31.7184
                       Mean reward: 867.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 173.9702
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.3547
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 0.92s
                      Time elapsed: 00:32:07
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 112156 steps/s (collection: 0.786s, learning 0.091s)
             Mean action noise std: 13.03
          Mean value_function loss: 33.9841
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 31.7248
                       Mean reward: 852.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 171.9777
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.3580
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 0.88s
                      Time elapsed: 00:32:08
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 103814 steps/s (collection: 0.859s, learning 0.088s)
             Mean action noise std: 13.04
          Mean value_function loss: 30.3474
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 31.7296
                       Mean reward: 867.12
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4503
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.3565
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 0.95s
                      Time elapsed: 00:32:09
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 100524 steps/s (collection: 0.858s, learning 0.120s)
             Mean action noise std: 13.05
          Mean value_function loss: 36.0069
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 31.7330
                       Mean reward: 861.30
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.7789
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.3589
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 0.98s
                      Time elapsed: 00:32:10
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 94578 steps/s (collection: 0.902s, learning 0.137s)
             Mean action noise std: 13.06
          Mean value_function loss: 34.5279
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.7395
                       Mean reward: 840.70
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 169.8668
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.3596
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 1.04s
                      Time elapsed: 00:32:11
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 95377 steps/s (collection: 0.895s, learning 0.136s)
             Mean action noise std: 13.07
          Mean value_function loss: 36.6016
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.7465
                       Mean reward: 870.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.5966
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3580
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 1.03s
                      Time elapsed: 00:32:12
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 79234 steps/s (collection: 1.093s, learning 0.148s)
             Mean action noise std: 13.08
          Mean value_function loss: 32.1199
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.7517
                       Mean reward: 876.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.2806
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3604
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 1.24s
                      Time elapsed: 00:32:13
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 108235 steps/s (collection: 0.821s, learning 0.087s)
             Mean action noise std: 13.08
          Mean value_function loss: 29.1829
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 31.7595
                       Mean reward: 878.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 173.1683
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.3587
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 0.91s
                      Time elapsed: 00:32:14
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 104462 steps/s (collection: 0.853s, learning 0.088s)
             Mean action noise std: 13.09
          Mean value_function loss: 33.1555
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.7615
                       Mean reward: 871.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.4886
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.3619
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 0.94s
                      Time elapsed: 00:32:15
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 109473 steps/s (collection: 0.806s, learning 0.092s)
             Mean action noise std: 13.10
          Mean value_function loss: 31.3257
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 31.7653
                       Mean reward: 869.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.3391
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.3625
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 0.90s
                      Time elapsed: 00:32:16
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 114562 steps/s (collection: 0.767s, learning 0.091s)
             Mean action noise std: 13.10
          Mean value_function loss: 32.8646
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.7674
                       Mean reward: 860.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.0323
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.3632
          Episode_Reward/joint_vel: -0.0028
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 0.86s
                      Time elapsed: 00:32:17
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 111806 steps/s (collection: 0.757s, learning 0.123s)
             Mean action noise std: 13.11
          Mean value_function loss: 33.8103
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.7727
                       Mean reward: 861.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.8558
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.3653
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 0.88s
                      Time elapsed: 00:32:18
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 116606 steps/s (collection: 0.755s, learning 0.088s)
             Mean action noise std: 13.13
          Mean value_function loss: 35.7294
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 31.7826
                       Mean reward: 857.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.7096
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.3681
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 0.84s
                      Time elapsed: 00:32:19
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 84558 steps/s (collection: 0.795s, learning 0.367s)
             Mean action noise std: 13.14
          Mean value_function loss: 40.3323
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 31.7914
                       Mean reward: 865.70
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.6627
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.3659
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 1.16s
                      Time elapsed: 00:32:20
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 94347 steps/s (collection: 0.929s, learning 0.113s)
             Mean action noise std: 13.15
          Mean value_function loss: 37.3450
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 31.7999
                       Mean reward: 859.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 172.1452
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.3686
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 1.04s
                      Time elapsed: 00:32:21
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 109077 steps/s (collection: 0.802s, learning 0.099s)
             Mean action noise std: 13.16
          Mean value_function loss: 34.1549
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 31.8046
                       Mean reward: 849.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 170.7048
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3697
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 0.90s
                      Time elapsed: 00:32:22
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 116587 steps/s (collection: 0.741s, learning 0.102s)
             Mean action noise std: 13.18
          Mean value_function loss: 35.7156
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 31.8128
                       Mean reward: 879.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 172.4944
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.3699
          Episode_Reward/joint_vel: -0.0029
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 0.84s
                      Time elapsed: 00:32:23
                               ETA: 00:00:00

