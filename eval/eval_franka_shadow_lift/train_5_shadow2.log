################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 11005 steps/s (collection: 8.670s, learning 0.263s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 25.5870
                       Mean reward: 0.00
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0004
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0002
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.93s
                      Time elapsed: 00:00:08
                               ETA: 04:57:44

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14821 steps/s (collection: 6.494s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 25.6865
                       Mean reward: 0.00
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0010
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.63s
                      Time elapsed: 00:00:15
                               ETA: 04:19:17

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14908 steps/s (collection: 6.462s, learning 0.132s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7144
                       Mean reward: 0.00
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0016
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.59s
                      Time elapsed: 00:00:22
                               ETA: 04:05:57

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14770 steps/s (collection: 6.523s, learning 0.133s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.7208
                       Mean reward: 0.00
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0022
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.66s
                      Time elapsed: 00:00:28
                               ETA: 03:59:45

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14752 steps/s (collection: 6.532s, learning 0.132s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.7528
                       Mean reward: 0.00
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0027
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.66s
                      Time elapsed: 00:00:35
                               ETA: 03:56:02

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 15426 steps/s (collection: 6.234s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.7732
                       Mean reward: -0.00
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0030
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.37s
                      Time elapsed: 00:00:41
                               ETA: 03:51:55

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 15029 steps/s (collection: 6.400s, learning 0.141s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 25.7951
                       Mean reward: -0.00
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0032
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.54s
                      Time elapsed: 00:00:48
                               ETA: 03:49:44

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 15128 steps/s (collection: 6.353s, learning 0.145s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 25.7897
                       Mean reward: -0.00
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0038
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0030
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.50s
                      Time elapsed: 00:00:54
                               ETA: 03:47:54

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 19328 steps/s (collection: 4.988s, learning 0.098s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 25.7537
                       Mean reward: -0.00
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0040
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0034
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.09s
                      Time elapsed: 00:00:59
                               ETA: 03:41:14

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 60964 steps/s (collection: 1.525s, learning 0.088s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 25.7154
                       Mean reward: -0.00
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.61s
                      Time elapsed: 00:01:01
                               ETA: 03:24:22

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 60700 steps/s (collection: 1.529s, learning 0.090s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 25.6685
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0043
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.62s
                      Time elapsed: 00:01:03
                               ETA: 03:10:34

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 61814 steps/s (collection: 1.504s, learning 0.086s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 25.5898
                       Mean reward: -0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.59s
                      Time elapsed: 00:01:04
                               ETA: 02:59:00

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 63221 steps/s (collection: 1.466s, learning 0.089s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 25.4958
                       Mean reward: -0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0047
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.55s
                      Time elapsed: 00:01:06
                               ETA: 02:49:06

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 62230 steps/s (collection: 1.474s, learning 0.106s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 25.4250
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.58s
                      Time elapsed: 00:01:07
                               ETA: 02:40:41

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 62588 steps/s (collection: 1.470s, learning 0.101s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 25.3403
                       Mean reward: 0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0052
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.57s
                      Time elapsed: 00:01:09
                               ETA: 02:33:22

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 61991 steps/s (collection: 1.494s, learning 0.092s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 25.2986
                       Mean reward: 0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.59s
                      Time elapsed: 00:01:11
                               ETA: 02:26:59

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 62831 steps/s (collection: 1.472s, learning 0.093s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 25.2567
                       Mean reward: 0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0065
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.56s
                      Time elapsed: 00:01:12
                               ETA: 02:21:18

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 61805 steps/s (collection: 1.500s, learning 0.091s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 25.2290
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0080
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.59s
                      Time elapsed: 00:01:14
                               ETA: 02:16:19

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 60754 steps/s (collection: 1.512s, learning 0.106s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 25.2103
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0095
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.62s
                      Time elapsed: 00:01:15
                               ETA: 02:11:53

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 61760 steps/s (collection: 1.495s, learning 0.097s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 25.2245
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0126
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.59s
                      Time elapsed: 00:01:17
                               ETA: 02:07:51

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 59850 steps/s (collection: 1.554s, learning 0.088s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 25.2216
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0162
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.64s
                      Time elapsed: 00:01:19
                               ETA: 02:04:17

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 61930 steps/s (collection: 1.493s, learning 0.094s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 25.2339
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0207
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.59s
                      Time elapsed: 00:01:20
                               ETA: 02:00:57

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 63138 steps/s (collection: 1.470s, learning 0.087s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 25.2460
                       Mean reward: 0.12
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.0283
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.56s
                      Time elapsed: 00:01:22
                               ETA: 01:57:52

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 59011 steps/s (collection: 1.569s, learning 0.097s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 25.2744
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0354
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.67s
                      Time elapsed: 00:01:23
                               ETA: 01:55:11

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 60780 steps/s (collection: 1.531s, learning 0.086s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.1412
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.3762
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0479
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.62s
                      Time elapsed: 00:01:25
                               ETA: 01:52:39

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 58431 steps/s (collection: 1.596s, learning 0.087s)
             Mean action noise std: 1.00
          Mean value_function loss: 2.6568
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.4502
                       Mean reward: -1.27
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.0652
     Episode_Reward/lifting_object: -0.2495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.68s
                      Time elapsed: 00:01:27
                               ETA: 01:50:24

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 56346 steps/s (collection: 1.645s, learning 0.100s)
             Mean action noise std: 1.01
          Mean value_function loss: 5.7970
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 25.6376
                       Mean reward: -4.91
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.0872
     Episode_Reward/lifting_object: -0.5138
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0034
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.74s
                      Time elapsed: 00:01:28
                               ETA: 01:48:23

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 58080 steps/s (collection: 1.605s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.9962
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.7395
                       Mean reward: -0.46
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.1064
     Episode_Reward/lifting_object: -0.3800
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0034
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.69s
                      Time elapsed: 00:01:30
                               ETA: 01:46:27

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 56328 steps/s (collection: 1.658s, learning 0.087s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.5059
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.8642
                       Mean reward: 0.60
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.1174
     Episode_Reward/lifting_object: -0.0751
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0034
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.75s
                      Time elapsed: 00:01:32
                               ETA: 01:44:42

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 55501 steps/s (collection: 1.680s, learning 0.091s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.9523
                       Mean reward: 0.68
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.1306
     Episode_Reward/lifting_object: -0.0698
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.77s
                      Time elapsed: 00:01:34
                               ETA: 01:43:06

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 56703 steps/s (collection: 1.637s, learning 0.097s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 26.0840
                       Mean reward: 0.73
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.1495
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.73s
                      Time elapsed: 00:01:35
                               ETA: 01:41:33

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 57598 steps/s (collection: 1.621s, learning 0.086s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.0951
                       Mean reward: 0.79
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.1580
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.71s
                      Time elapsed: 00:01:37
                               ETA: 01:40:05

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 54494 steps/s (collection: 1.691s, learning 0.113s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.1075
                       Mean reward: 0.74
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.1642
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.80s
                      Time elapsed: 00:01:39
                               ETA: 01:38:48

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 54861 steps/s (collection: 1.683s, learning 0.109s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2140
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.1450
                       Mean reward: 0.36
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.1652
     Episode_Reward/lifting_object: -0.0515
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.79s
                      Time elapsed: 00:01:41
                               ETA: 01:37:34

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 55614 steps/s (collection: 1.664s, learning 0.103s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.1717
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.1963
                       Mean reward: 0.76
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.1542
     Episode_Reward/lifting_object: -0.0333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.77s
                      Time elapsed: 00:01:42
                               ETA: 01:36:23

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 56741 steps/s (collection: 1.645s, learning 0.088s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.3594
                       Mean reward: 0.58
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.1520
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.73s
                      Time elapsed: 00:01:44
                               ETA: 01:35:14

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 56632 steps/s (collection: 1.642s, learning 0.094s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 26.4328
                       Mean reward: 0.68
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.1441
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.74s
                      Time elapsed: 00:01:46
                               ETA: 01:34:09

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 56978 steps/s (collection: 1.640s, learning 0.085s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 26.4603
                       Mean reward: 0.62
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.1387
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.73s
                      Time elapsed: 00:01:48
                               ETA: 01:33:07

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 55533 steps/s (collection: 1.679s, learning 0.092s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 26.4762
                       Mean reward: 0.69
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.1390
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.77s
                      Time elapsed: 00:01:49
                               ETA: 01:32:10

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 55388 steps/s (collection: 1.683s, learning 0.091s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.6971
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.5567
                       Mean reward: -0.34
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.1386
     Episode_Reward/lifting_object: -0.0548
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.77s
                      Time elapsed: 00:01:51
                               ETA: 01:31:16

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 52609 steps/s (collection: 1.761s, learning 0.108s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3303
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.6230
                       Mean reward: 0.56
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.1565
     Episode_Reward/lifting_object: -0.0454
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.87s
                      Time elapsed: 00:01:53
                               ETA: 01:30:29

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 51462 steps/s (collection: 1.807s, learning 0.104s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0991
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.7715
                       Mean reward: 0.38
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.1607
     Episode_Reward/lifting_object: -0.0581
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.91s
                      Time elapsed: 00:01:55
                               ETA: 01:29:46

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 54486 steps/s (collection: 1.712s, learning 0.092s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.3009
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.8664
                       Mean reward: 0.65
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.1738
     Episode_Reward/lifting_object: -0.0783
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.80s
                      Time elapsed: 00:01:57
                               ETA: 01:29:00

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 55419 steps/s (collection: 1.687s, learning 0.086s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0422
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.9799
                       Mean reward: 0.84
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.1748
     Episode_Reward/lifting_object: -0.0345
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.77s
                      Time elapsed: 00:01:59
                               ETA: 01:28:15

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 54079 steps/s (collection: 1.727s, learning 0.091s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.5002
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.1864
                       Mean reward: 0.48
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 0.1843
     Episode_Reward/lifting_object: -0.0532
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.82s
                      Time elapsed: 00:02:00
                               ETA: 01:27:34

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 54738 steps/s (collection: 1.712s, learning 0.084s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.6285
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.2385
                       Mean reward: 0.96
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.1953
     Episode_Reward/lifting_object: -0.1015
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.80s
                      Time elapsed: 00:02:02
                               ETA: 01:26:53

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 55599 steps/s (collection: 1.668s, learning 0.100s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 27.3337
                       Mean reward: 1.00
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.2054
     Episode_Reward/lifting_object: -0.0370
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.77s
                      Time elapsed: 00:02:04
                               ETA: 01:26:13

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 56038 steps/s (collection: 1.663s, learning 0.092s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0308
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.3961
                       Mean reward: 0.87
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 0.2094
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.75s
                      Time elapsed: 00:02:06
                               ETA: 01:25:34

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 54523 steps/s (collection: 1.704s, learning 0.099s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 27.4216
                       Mean reward: 1.12
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 0.2230
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.80s
                      Time elapsed: 00:02:07
                               ETA: 01:24:58

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 54601 steps/s (collection: 1.709s, learning 0.092s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.5066
                       Mean reward: 0.99
               Mean episode length: 226.39
    Episode_Reward/reaching_object: 0.2065
     Episode_Reward/lifting_object: 0.0111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.80s
                      Time elapsed: 00:02:09
                               ETA: 01:24:24

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 53924 steps/s (collection: 1.731s, learning 0.092s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.4680
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.5622
                       Mean reward: 1.03
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 0.2103
     Episode_Reward/lifting_object: -0.0734
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.82s
                      Time elapsed: 00:02:11
                               ETA: 01:23:52

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 54350 steps/s (collection: 1.702s, learning 0.107s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6025
                       Mean reward: 1.00
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 0.2045
     Episode_Reward/lifting_object: -0.0034
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.81s
                      Time elapsed: 00:02:13
                               ETA: 01:23:21

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 53949 steps/s (collection: 1.737s, learning 0.085s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 27.6567
                       Mean reward: 0.96
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.2009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.82s
                      Time elapsed: 00:02:15
                               ETA: 01:22:51

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 54676 steps/s (collection: 1.709s, learning 0.089s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 27.7164
                       Mean reward: 0.93
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 0.2040
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.80s
                      Time elapsed: 00:02:17
                               ETA: 01:22:21

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 54110 steps/s (collection: 1.718s, learning 0.099s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0403
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.7651
                       Mean reward: 0.78
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 0.2074
     Episode_Reward/lifting_object: -0.0137
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.82s
                      Time elapsed: 00:02:18
                               ETA: 01:21:53

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 54186 steps/s (collection: 1.707s, learning 0.107s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0321
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.7942
                       Mean reward: 0.97
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 0.2049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.81s
                      Time elapsed: 00:02:20
                               ETA: 01:21:26

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 53901 steps/s (collection: 1.732s, learning 0.092s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1154
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.8753
                       Mean reward: 0.99
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 0.2105
     Episode_Reward/lifting_object: -0.0067
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.82s
                      Time elapsed: 00:02:22
                               ETA: 01:21:00

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 54056 steps/s (collection: 1.716s, learning 0.103s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.9181
                       Mean reward: 0.99
               Mean episode length: 206.83
    Episode_Reward/reaching_object: 0.2046
     Episode_Reward/lifting_object: -0.0173
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.82s
                      Time elapsed: 00:02:24
                               ETA: 01:20:34

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 51427 steps/s (collection: 1.825s, learning 0.087s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2593
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.0082
                       Mean reward: 0.61
               Mean episode length: 203.87
    Episode_Reward/reaching_object: 0.2082
     Episode_Reward/lifting_object: -0.0673
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.91s
                      Time elapsed: 00:02:26
                               ETA: 01:20:13

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 53381 steps/s (collection: 1.740s, learning 0.101s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1351
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.0388
                       Mean reward: 0.95
               Mean episode length: 199.26
    Episode_Reward/reaching_object: 0.2090
     Episode_Reward/lifting_object: -0.0404
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.84s
                      Time elapsed: 00:02:28
                               ETA: 01:19:50

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52628 steps/s (collection: 1.776s, learning 0.092s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 28.0899
                       Mean reward: 1.18
               Mean episode length: 203.97
    Episode_Reward/reaching_object: 0.2296
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.87s
                      Time elapsed: 00:02:29
                               ETA: 01:19:28

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52536 steps/s (collection: 1.742s, learning 0.129s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 28.1624
                       Mean reward: 1.23
               Mean episode length: 206.04
    Episode_Reward/reaching_object: 0.2524
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.87s
                      Time elapsed: 00:02:31
                               ETA: 01:19:07

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 54487 steps/s (collection: 1.716s, learning 0.089s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.1610
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.1974
                       Mean reward: 1.11
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 0.2782
     Episode_Reward/lifting_object: -0.0340
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.80s
                      Time elapsed: 00:02:33
                               ETA: 01:18:45

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 53829 steps/s (collection: 1.733s, learning 0.094s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0815
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.2479
                       Mean reward: 1.09
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: -0.0281
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.83s
                      Time elapsed: 00:02:35
                               ETA: 01:18:24

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 52909 steps/s (collection: 1.773s, learning 0.085s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.5460
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.3265
                       Mean reward: 1.63
               Mean episode length: 221.41
    Episode_Reward/reaching_object: 0.3259
     Episode_Reward/lifting_object: -0.0593
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.86s
                      Time elapsed: 00:02:37
                               ETA: 01:18:05

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 53142 steps/s (collection: 1.743s, learning 0.107s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2712
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.3570
                       Mean reward: 1.67
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 0.3454
     Episode_Reward/lifting_object: -0.0583
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.85s
                      Time elapsed: 00:02:39
                               ETA: 01:17:46

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 53300 steps/s (collection: 1.736s, learning 0.109s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1437
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.4321
                       Mean reward: 1.74
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 0.3521
     Episode_Reward/lifting_object: -0.0284
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.84s
                      Time elapsed: 00:02:40
                               ETA: 01:17:27

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 52604 steps/s (collection: 1.770s, learning 0.099s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3802
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5128
                       Mean reward: 1.00
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 0.3580
     Episode_Reward/lifting_object: -0.0706
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.87s
                      Time elapsed: 00:02:42
                               ETA: 01:17:09

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 54043 steps/s (collection: 1.731s, learning 0.088s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0324
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.5344
                       Mean reward: 1.92
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.3747
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.82s
                      Time elapsed: 00:02:44
                               ETA: 01:16:51

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 53932 steps/s (collection: 1.737s, learning 0.086s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 28.5765
                       Mean reward: 1.97
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.4038
     Episode_Reward/lifting_object: 0.0121
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.82s
                      Time elapsed: 00:02:46
                               ETA: 01:16:33

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 55031 steps/s (collection: 1.701s, learning 0.086s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 28.5828
                       Mean reward: 1.98
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.3930
     Episode_Reward/lifting_object: 0.0119
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.79s
                      Time elapsed: 00:02:48
                               ETA: 01:16:14

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 53717 steps/s (collection: 1.742s, learning 0.089s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 28.5760
                       Mean reward: 1.93
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 0.3886
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.83s
                      Time elapsed: 00:02:50
                               ETA: 01:15:57

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 51935 steps/s (collection: 1.778s, learning 0.115s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.5612
                       Mean reward: 1.74
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 0.3973
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.89s
                      Time elapsed: 00:02:52
                               ETA: 01:15:43

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 52812 steps/s (collection: 1.755s, learning 0.107s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0898
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.5839
                       Mean reward: 1.61
               Mean episode length: 217.16
    Episode_Reward/reaching_object: 0.3844
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.86s
                      Time elapsed: 00:02:53
                               ETA: 01:15:27

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 52522 steps/s (collection: 1.783s, learning 0.089s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0596
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.6633
                       Mean reward: 1.89
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 0.3679
     Episode_Reward/lifting_object: -0.0130
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.87s
                      Time elapsed: 00:02:55
                               ETA: 01:15:13

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 52886 steps/s (collection: 1.771s, learning 0.088s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2173
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.7198
                       Mean reward: 1.64
               Mean episode length: 209.40
    Episode_Reward/reaching_object: 0.3720
     Episode_Reward/lifting_object: -0.0280
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.86s
                      Time elapsed: 00:02:57
                               ETA: 01:14:58

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 53198 steps/s (collection: 1.754s, learning 0.094s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3034
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.7458
                       Mean reward: 1.55
               Mean episode length: 221.20
    Episode_Reward/reaching_object: 0.3967
     Episode_Reward/lifting_object: -0.0707
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.85s
                      Time elapsed: 00:02:59
                               ETA: 01:14:44

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 52081 steps/s (collection: 1.797s, learning 0.091s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.8448
                       Mean reward: 1.64
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: -0.0275
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.89s
                      Time elapsed: 00:03:01
                               ETA: 01:14:30

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 53783 steps/s (collection: 1.734s, learning 0.094s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0312
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.9307
                       Mean reward: 2.08
               Mean episode length: 225.08
    Episode_Reward/reaching_object: 0.4327
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.83s
                      Time elapsed: 00:03:03
                               ETA: 01:14:16

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 53562 steps/s (collection: 1.749s, learning 0.086s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.6257
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.0002
                       Mean reward: 1.38
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 0.4454
     Episode_Reward/lifting_object: -0.0439
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.84s
                      Time elapsed: 00:03:05
                               ETA: 01:14:02

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 51759 steps/s (collection: 1.814s, learning 0.085s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1430
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.0294
                       Mean reward: 2.43
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 0.4954
     Episode_Reward/lifting_object: -0.0062
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.90s
                      Time elapsed: 00:03:06
                               ETA: 01:13:50

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 54005 steps/s (collection: 1.718s, learning 0.103s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 29.1158
                       Mean reward: 2.41
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 0.4995
     Episode_Reward/lifting_object: -0.0160
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.82s
                      Time elapsed: 00:03:08
                               ETA: 01:13:36

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 52909 steps/s (collection: 1.757s, learning 0.101s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0317
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.1821
                       Mean reward: 2.50
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 0.4890
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.86s
                      Time elapsed: 00:03:10
                               ETA: 01:13:24

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 53618 steps/s (collection: 1.734s, learning 0.099s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0548
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.2608
                       Mean reward: 2.43
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 0.4874
     Episode_Reward/lifting_object: -0.0177
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.83s
                      Time elapsed: 00:03:12
                               ETA: 01:13:11

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 53856 steps/s (collection: 1.732s, learning 0.094s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1178
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.3603
                       Mean reward: 2.12
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 0.5064
     Episode_Reward/lifting_object: -0.0295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.83s
                      Time elapsed: 00:03:14
                               ETA: 01:12:58

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 53140 steps/s (collection: 1.756s, learning 0.093s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 29.3735
                       Mean reward: 2.57
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 0.5189
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.85s
                      Time elapsed: 00:03:16
                               ETA: 01:12:46

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 52659 steps/s (collection: 1.772s, learning 0.095s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2538
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.3937
                       Mean reward: 1.83
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.5102
     Episode_Reward/lifting_object: -0.0195
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.87s
                      Time elapsed: 00:03:17
                               ETA: 01:12:35

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 52837 steps/s (collection: 1.773s, learning 0.087s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0301
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.4197
                       Mean reward: 2.45
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 0.5033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.86s
                      Time elapsed: 00:03:19
                               ETA: 01:12:23

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 51422 steps/s (collection: 1.810s, learning 0.101s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.4685
                       Mean reward: 2.60
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.5242
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.91s
                      Time elapsed: 00:03:21
                               ETA: 01:12:13

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 51163 steps/s (collection: 1.807s, learning 0.114s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.4872
                       Mean reward: 2.47
               Mean episode length: 223.54
    Episode_Reward/reaching_object: 0.5145
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.92s
                      Time elapsed: 00:03:23
                               ETA: 01:12:04

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 46926 steps/s (collection: 1.957s, learning 0.138s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.5017
                       Mean reward: 2.28
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 0.5219
     Episode_Reward/lifting_object: -0.0014
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.09s
                      Time elapsed: 00:03:25
                               ETA: 01:11:58

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 48600 steps/s (collection: 1.930s, learning 0.092s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 29.5197
                       Mean reward: 2.56
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 0.5142
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.02s
                      Time elapsed: 00:03:27
                               ETA: 01:11:51

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 52027 steps/s (collection: 1.791s, learning 0.099s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2257
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.5286
                       Mean reward: 2.76
               Mean episode length: 230.70
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.89s
                      Time elapsed: 00:03:29
                               ETA: 01:11:41

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 52080 steps/s (collection: 1.799s, learning 0.089s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 29.5381
                       Mean reward: 2.87
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 0.5634
     Episode_Reward/lifting_object: 0.0118
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.89s
                      Time elapsed: 00:03:31
                               ETA: 01:11:31

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 52544 steps/s (collection: 1.781s, learning 0.090s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1187
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.5620
                       Mean reward: 3.03
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.87s
                      Time elapsed: 00:03:33
                               ETA: 01:11:21

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 53483 steps/s (collection: 1.753s, learning 0.085s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 29.5891
                       Mean reward: 2.97
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.6186
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.84s
                      Time elapsed: 00:03:35
                               ETA: 01:11:11

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 52297 steps/s (collection: 1.738s, learning 0.142s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.6011
                       Mean reward: 2.96
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: -0.0675
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.88s
                      Time elapsed: 00:03:37
                               ETA: 01:11:02

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 52753 steps/s (collection: 1.775s, learning 0.088s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1237
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.6412
                       Mean reward: 2.87
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 0.6071
     Episode_Reward/lifting_object: 0.0117
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.86s
                      Time elapsed: 00:03:38
                               ETA: 01:10:52

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 51726 steps/s (collection: 1.804s, learning 0.096s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.6530
                       Mean reward: 2.87
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.90s
                      Time elapsed: 00:03:40
                               ETA: 01:10:43

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 51786 steps/s (collection: 1.790s, learning 0.108s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 29.6684
                       Mean reward: 3.16
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 0.6482
     Episode_Reward/lifting_object: 0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.90s
                      Time elapsed: 00:03:42
                               ETA: 01:10:35

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 52371 steps/s (collection: 1.785s, learning 0.092s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.6684
                       Mean reward: 3.33
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.88s
                      Time elapsed: 00:03:44
                               ETA: 01:10:26

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 51563 steps/s (collection: 1.820s, learning 0.087s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1137
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.6989
                       Mean reward: 2.82
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: -0.0133
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.91s
                      Time elapsed: 00:03:46
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 50760 steps/s (collection: 1.831s, learning 0.106s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.4670
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.7526
                       Mean reward: 3.20
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 0.6369
     Episode_Reward/lifting_object: -0.0162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.94s
                      Time elapsed: 00:03:48
                               ETA: 01:10:10

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 50194 steps/s (collection: 1.848s, learning 0.110s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.7682
                       Mean reward: 3.88
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 0.0046
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.96s
                      Time elapsed: 00:03:50
                               ETA: 01:10:03

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 49582 steps/s (collection: 1.874s, learning 0.109s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0945
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.8009
                       Mean reward: 3.09
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.98s
                      Time elapsed: 00:03:52
                               ETA: 01:09:57

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 50143 steps/s (collection: 1.833s, learning 0.127s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1506
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.8293
                       Mean reward: 3.47
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 0.7172
     Episode_Reward/lifting_object: 0.0070
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.96s
                      Time elapsed: 00:03:54
                               ETA: 01:09:50

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 50422 steps/s (collection: 1.800s, learning 0.150s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.8406
                       Mean reward: 3.58
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.7132
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.95s
                      Time elapsed: 00:03:56
                               ETA: 01:09:43

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 49388 steps/s (collection: 1.838s, learning 0.152s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.4098
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.8858
                       Mean reward: 3.52
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.99s
                      Time elapsed: 00:03:58
                               ETA: 01:09:37

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 48400 steps/s (collection: 1.893s, learning 0.139s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.2032
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.9744
                       Mean reward: 3.56
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: -0.0061
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.03s
                      Time elapsed: 00:04:00
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 47828 steps/s (collection: 1.941s, learning 0.114s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.7423
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.9927
                       Mean reward: 3.72
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.7298
     Episode_Reward/lifting_object: -0.0253
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.06s
                      Time elapsed: 00:04:02
                               ETA: 01:09:27

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 49826 steps/s (collection: 1.872s, learning 0.101s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1163
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.0084
                       Mean reward: 3.90
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: -0.0106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.97s
                      Time elapsed: 00:04:04
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 48429 steps/s (collection: 1.915s, learning 0.115s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 30.0432
                       Mean reward: 3.75
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 0.0316
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.03s
                      Time elapsed: 00:04:06
                               ETA: 01:09:16

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 49976 steps/s (collection: 1.877s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 30.0685
                       Mean reward: 3.28
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: -0.0087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.97s
                      Time elapsed: 00:04:08
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 50285 steps/s (collection: 1.851s, learning 0.104s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.0849
                       Mean reward: 3.83
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 0.0035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.95s
                      Time elapsed: 00:04:10
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 48187 steps/s (collection: 1.911s, learning 0.129s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.1275
                       Mean reward: 3.93
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: -0.0551
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.04s
                      Time elapsed: 00:04:12
                               ETA: 01:08:59

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 45286 steps/s (collection: 2.005s, learning 0.166s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.1808
                       Mean reward: 3.05
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: -0.0614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.17s
                      Time elapsed: 00:04:14
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 38688 steps/s (collection: 2.323s, learning 0.218s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0648
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.2035
                       Mean reward: 3.76
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: -0.0134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.54s
                      Time elapsed: 00:04:17
                               ETA: 01:09:00

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 41057 steps/s (collection: 2.201s, learning 0.194s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 30.2107
                       Mean reward: 3.80
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 0.0047
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.39s
                      Time elapsed: 00:04:19
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 36840 steps/s (collection: 2.441s, learning 0.228s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0341
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.2153
                       Mean reward: 3.77
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 0.0004
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.67s
                      Time elapsed: 00:04:22
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 38171 steps/s (collection: 2.398s, learning 0.177s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0394
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.2445
                       Mean reward: 3.42
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.58s
                      Time elapsed: 00:04:24
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 40554 steps/s (collection: 2.270s, learning 0.154s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0764
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.2986
                       Mean reward: 3.65
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: -0.0190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.42s
                      Time elapsed: 00:04:27
                               ETA: 01:09:11

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 46988 steps/s (collection: 1.956s, learning 0.136s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0738
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.3284
                       Mean reward: 3.88
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: -0.0032
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.09s
                      Time elapsed: 00:04:29
                               ETA: 01:09:07

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 41641 steps/s (collection: 2.227s, learning 0.134s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.2758
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.3717
                       Mean reward: 4.09
               Mean episode length: 239.07
    Episode_Reward/reaching_object: 0.7937
     Episode_Reward/lifting_object: 0.0118
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.36s
                      Time elapsed: 00:04:31
                               ETA: 01:09:07

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 44361 steps/s (collection: 2.070s, learning 0.146s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1618
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.3848
                       Mean reward: 3.83
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.22s
                      Time elapsed: 00:04:33
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 42485 steps/s (collection: 2.162s, learning 0.152s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 30.4136
                       Mean reward: 3.99
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 0.7904
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.31s
                      Time elapsed: 00:04:36
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 45686 steps/s (collection: 1.943s, learning 0.209s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0549
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.4298
                       Mean reward: 3.81
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 0.7971
     Episode_Reward/lifting_object: -0.0033
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.15s
                      Time elapsed: 00:04:38
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 48126 steps/s (collection: 1.907s, learning 0.136s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.4496
                       Mean reward: 3.97
               Mean episode length: 234.25
    Episode_Reward/reaching_object: 0.8031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.04s
                      Time elapsed: 00:04:40
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 43635 steps/s (collection: 2.081s, learning 0.172s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.4902
                       Mean reward: 4.12
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.8023
     Episode_Reward/lifting_object: 0.0073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.25s
                      Time elapsed: 00:04:42
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 47585 steps/s (collection: 1.943s, learning 0.123s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.5253
                       Mean reward: 3.78
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 0.8086
     Episode_Reward/lifting_object: 0.0180
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.07s
                      Time elapsed: 00:04:44
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 48990 steps/s (collection: 1.904s, learning 0.103s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0505
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.5613
                       Mean reward: 3.93
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: -0.0044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.01s
                      Time elapsed: 00:04:46
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 49682 steps/s (collection: 1.879s, learning 0.100s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1267
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.5807
                       Mean reward: 4.12
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.8375
     Episode_Reward/lifting_object: 0.0044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.98s
                      Time elapsed: 00:04:48
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 49603 steps/s (collection: 1.857s, learning 0.125s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0339
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.5919
                       Mean reward: 4.00
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 0.8084
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.98s
                      Time elapsed: 00:04:50
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 50018 steps/s (collection: 1.853s, learning 0.113s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0671
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.6150
                       Mean reward: 4.00
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 0.8153
     Episode_Reward/lifting_object: -0.0260
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.97s
                      Time elapsed: 00:04:52
                               ETA: 01:08:29

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 48813 steps/s (collection: 1.927s, learning 0.087s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0541
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.6360
                       Mean reward: 3.92
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.01s
                      Time elapsed: 00:04:54
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 50663 steps/s (collection: 1.814s, learning 0.127s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.6847
                       Mean reward: 4.23
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.8478
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.94s
                      Time elapsed: 00:04:56
                               ETA: 01:08:19

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 50588 steps/s (collection: 1.820s, learning 0.123s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 30.7092
                       Mean reward: 3.94
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 0.8568
     Episode_Reward/lifting_object: -0.0391
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.94s
                      Time elapsed: 00:04:58
                               ETA: 01:08:13

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 50792 steps/s (collection: 1.820s, learning 0.116s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0337
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.7299
                       Mean reward: 4.17
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.8571
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.94s
                      Time elapsed: 00:05:00
                               ETA: 01:08:07

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 51656 steps/s (collection: 1.789s, learning 0.114s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0382
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.7518
                       Mean reward: 4.54
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 0.0065
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.90s
                      Time elapsed: 00:05:02
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 52476 steps/s (collection: 1.777s, learning 0.097s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.7882
                       Mean reward: 4.17
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 0.0026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.87s
                      Time elapsed: 00:05:04
                               ETA: 01:07:55

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 52010 steps/s (collection: 1.803s, learning 0.088s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0422
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.8183
                       Mean reward: 4.16
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: 0.0174
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.89s
                      Time elapsed: 00:05:06
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50691 steps/s (collection: 1.832s, learning 0.107s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0380
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.8629
                       Mean reward: 4.36
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 0.8607
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.94s
                      Time elapsed: 00:05:08
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 50171 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0364
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.8996
                       Mean reward: 4.29
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 0.0037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.96s
                      Time elapsed: 00:05:10
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50738 steps/s (collection: 1.838s, learning 0.100s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.9550
                       Mean reward: 4.03
               Mean episode length: 222.91
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 0.0067
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.94s
                      Time elapsed: 00:05:11
                               ETA: 01:07:33

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 50677 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1095
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.9952
                       Mean reward: 4.05
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 0.8227
     Episode_Reward/lifting_object: 0.0238
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.94s
                      Time elapsed: 00:05:13
                               ETA: 01:07:27

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50940 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.3953
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.0025
                       Mean reward: 3.74
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 0.8424
     Episode_Reward/lifting_object: -0.0270
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.93s
                      Time elapsed: 00:05:15
                               ETA: 01:07:22

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 51277 steps/s (collection: 1.817s, learning 0.100s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0388
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0097
                       Mean reward: 4.69
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.8692
     Episode_Reward/lifting_object: 0.0232
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.92s
                      Time elapsed: 00:05:17
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 51078 steps/s (collection: 1.835s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.0323
                       Mean reward: 4.38
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 0.8849
     Episode_Reward/lifting_object: 0.0094
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.92s
                      Time elapsed: 00:05:19
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 51783 steps/s (collection: 1.804s, learning 0.094s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0991
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 31.0650
                       Mean reward: 4.39
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 0.8832
     Episode_Reward/lifting_object: -0.0123
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.90s
                      Time elapsed: 00:05:21
                               ETA: 01:07:05

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 51327 steps/s (collection: 1.826s, learning 0.089s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.0730
                       Mean reward: 4.49
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.8887
     Episode_Reward/lifting_object: -0.0026
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.92s
                      Time elapsed: 00:05:23
                               ETA: 01:07:00

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 52544 steps/s (collection: 1.786s, learning 0.085s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 31.0834
                       Mean reward: 4.49
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.8992
     Episode_Reward/lifting_object: 0.0042
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.87s
                      Time elapsed: 00:05:25
                               ETA: 01:06:54

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 52048 steps/s (collection: 1.784s, learning 0.104s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.0913
                       Mean reward: 3.82
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.9080
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.89s
                      Time elapsed: 00:05:27
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 50589 steps/s (collection: 1.843s, learning 0.100s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0462
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 31.1068
                       Mean reward: 3.90
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.8970
     Episode_Reward/lifting_object: -0.0387
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.94s
                      Time elapsed: 00:05:29
                               ETA: 01:06:44

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 51225 steps/s (collection: 1.822s, learning 0.097s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.1109
                       Mean reward: 4.63
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.92s
                      Time elapsed: 00:05:31
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 48063 steps/s (collection: 1.916s, learning 0.130s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.1205
                       Mean reward: 4.42
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 0.0059
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.05s
                      Time elapsed: 00:05:33
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 50689 steps/s (collection: 1.840s, learning 0.100s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0489
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.1338
                       Mean reward: 4.39
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.8943
     Episode_Reward/lifting_object: 0.0126
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.94s
                      Time elapsed: 00:05:35
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 49970 steps/s (collection: 1.871s, learning 0.097s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0439
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.1609
                       Mean reward: 4.37
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.9002
     Episode_Reward/lifting_object: -0.0007
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.97s
                      Time elapsed: 00:05:37
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 47495 steps/s (collection: 1.971s, learning 0.099s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.2027
                       Mean reward: 4.45
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.8932
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.07s
                      Time elapsed: 00:05:39
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 51313 steps/s (collection: 1.810s, learning 0.106s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0354
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.2405
                       Mean reward: 4.62
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 0.9146
     Episode_Reward/lifting_object: -0.0044
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.92s
                      Time elapsed: 00:05:41
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49861 steps/s (collection: 1.854s, learning 0.118s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1551
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.2687
                       Mean reward: 4.50
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.9055
     Episode_Reward/lifting_object: -0.0048
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.97s
                      Time elapsed: 00:05:43
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 51560 steps/s (collection: 1.810s, learning 0.097s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.2771
                       Mean reward: 4.41
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.9110
     Episode_Reward/lifting_object: -0.0182
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.91s
                      Time elapsed: 00:05:44
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 51065 steps/s (collection: 1.840s, learning 0.085s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0706
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.2900
                       Mean reward: 4.46
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.9145
     Episode_Reward/lifting_object: 0.0074
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.93s
                      Time elapsed: 00:05:46
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 50052 steps/s (collection: 1.862s, learning 0.102s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2077
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.3016
                       Mean reward: 4.09
               Mean episode length: 217.30
    Episode_Reward/reaching_object: 0.9004
     Episode_Reward/lifting_object: -0.0005
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.96s
                      Time elapsed: 00:05:48
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49305 steps/s (collection: 1.880s, learning 0.114s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 31.3164
                       Mean reward: 4.51
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 0.8829
     Episode_Reward/lifting_object: 0.0182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.99s
                      Time elapsed: 00:05:50
                               ETA: 01:05:55

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 46526 steps/s (collection: 1.978s, learning 0.135s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.3438
                       Mean reward: 4.47
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 0.9045
     Episode_Reward/lifting_object: 0.0106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.11s
                      Time elapsed: 00:05:52
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 46680 steps/s (collection: 2.014s, learning 0.092s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0451
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.3721
                       Mean reward: 4.54
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.9315
     Episode_Reward/lifting_object: 0.0111
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.11s
                      Time elapsed: 00:05:55
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 48002 steps/s (collection: 1.929s, learning 0.119s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1038
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.3851
                       Mean reward: 4.53
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 0.9055
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.05s
                      Time elapsed: 00:05:57
                               ETA: 01:05:47

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 47669 steps/s (collection: 1.964s, learning 0.098s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1372
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.4089
                       Mean reward: 4.56
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 0.9056
     Episode_Reward/lifting_object: 0.0110
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.06s
                      Time elapsed: 00:05:59
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 50005 steps/s (collection: 1.870s, learning 0.096s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0449
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.4244
                       Mean reward: 4.76
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.9160
     Episode_Reward/lifting_object: 0.0020
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.97s
                      Time elapsed: 00:06:01
                               ETA: 01:05:39

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 50616 steps/s (collection: 1.858s, learning 0.084s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0532
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.4545
                       Mean reward: 4.55
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 0.9307
     Episode_Reward/lifting_object: 0.0060
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.94s
                      Time elapsed: 00:06:03
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 50821 steps/s (collection: 1.841s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1229
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.4619
                       Mean reward: 4.00
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.9257
     Episode_Reward/lifting_object: -0.0618
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.93s
                      Time elapsed: 00:06:04
                               ETA: 01:05:30

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 49945 steps/s (collection: 1.860s, learning 0.109s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1111
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.4734
                       Mean reward: 4.61
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 0.0055
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.97s
                      Time elapsed: 00:06:06
                               ETA: 01:05:26

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 48926 steps/s (collection: 1.883s, learning 0.126s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1747
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.4999
                       Mean reward: 4.84
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 0.9248
     Episode_Reward/lifting_object: 0.0068
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.01s
                      Time elapsed: 00:06:08
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 49060 steps/s (collection: 1.896s, learning 0.108s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1217
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.5204
                       Mean reward: 4.85
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 0.9276
     Episode_Reward/lifting_object: -0.0000
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.00s
                      Time elapsed: 00:06:10
                               ETA: 01:05:19

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 48081 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1303
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.5389
                       Mean reward: 4.71
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 0.9189
     Episode_Reward/lifting_object: 0.0532
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.04s
                      Time elapsed: 00:06:12
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 47757 steps/s (collection: 1.960s, learning 0.099s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2199
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.5496
                       Mean reward: 5.66
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 0.9136
     Episode_Reward/lifting_object: 0.0712
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.06s
                      Time elapsed: 00:06:15
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 50118 steps/s (collection: 1.868s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2510
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.5673
                       Mean reward: 4.63
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 0.8907
     Episode_Reward/lifting_object: 0.0429
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.96s
                      Time elapsed: 00:06:17
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 46706 steps/s (collection: 1.961s, learning 0.144s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5289
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 31.5797
                       Mean reward: 4.37
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 0.8904
     Episode_Reward/lifting_object: 0.0703
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.10s
                      Time elapsed: 00:06:19
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 48250 steps/s (collection: 1.946s, learning 0.091s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.6745
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.6064
                       Mean reward: 4.84
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 0.1001
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.04s
                      Time elapsed: 00:06:21
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 49653 steps/s (collection: 1.886s, learning 0.094s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.0454
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.6360
                       Mean reward: 5.14
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 0.8680
     Episode_Reward/lifting_object: 0.1950
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.98s
                      Time elapsed: 00:06:23
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 45318 steps/s (collection: 2.073s, learning 0.096s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.1856
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 31.6635
                       Mean reward: 5.85
               Mean episode length: 220.20
    Episode_Reward/reaching_object: 0.8460
     Episode_Reward/lifting_object: 0.2565
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.17s
                      Time elapsed: 00:06:25
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 49177 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.6388
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.6954
                       Mean reward: 6.22
               Mean episode length: 202.87
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: 0.4481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.00s
                      Time elapsed: 00:06:27
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 43824 steps/s (collection: 2.040s, learning 0.203s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.9732
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.7238
                       Mean reward: 6.53
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 0.8068
     Episode_Reward/lifting_object: 0.4895
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.24s
                      Time elapsed: 00:06:29
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 46127 steps/s (collection: 1.988s, learning 0.143s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.5794
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.7377
                       Mean reward: 8.26
               Mean episode length: 198.34
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 0.7212
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.13s
                      Time elapsed: 00:06:31
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 46224 steps/s (collection: 2.022s, learning 0.104s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.8543
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.7562
                       Mean reward: 6.82
               Mean episode length: 208.28
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 0.7263
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.13s
                      Time elapsed: 00:06:33
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 46648 steps/s (collection: 2.015s, learning 0.092s)
             Mean action noise std: 1.43
          Mean value_function loss: 4.3436
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.7811
                       Mean reward: 9.74
               Mean episode length: 201.67
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 0.9860
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.11s
                      Time elapsed: 00:06:35
                               ETA: 01:04:46

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 47474 steps/s (collection: 1.961s, learning 0.110s)
             Mean action noise std: 1.43
          Mean value_function loss: 5.3914
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.8059
                       Mean reward: 9.98
               Mean episode length: 203.38
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 1.0872
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.07s
                      Time elapsed: 00:06:37
                               ETA: 01:04:43

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 46612 steps/s (collection: 1.984s, learning 0.125s)
             Mean action noise std: 1.43
          Mean value_function loss: 5.5805
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 31.8224
                       Mean reward: 7.72
               Mean episode length: 205.99
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 1.1492
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.11s
                      Time elapsed: 00:06:40
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 47422 steps/s (collection: 1.978s, learning 0.095s)
             Mean action noise std: 1.43
          Mean value_function loss: 7.0672
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 31.8293
                       Mean reward: 10.23
               Mean episode length: 187.05
    Episode_Reward/reaching_object: 0.6591
     Episode_Reward/lifting_object: 1.3674
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.07s
                      Time elapsed: 00:06:42
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 46679 steps/s (collection: 2.011s, learning 0.095s)
             Mean action noise std: 1.44
          Mean value_function loss: 9.0554
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.8394
                       Mean reward: 13.30
               Mean episode length: 195.15
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 1.5997
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.11s
                      Time elapsed: 00:06:44
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 45236 steps/s (collection: 2.023s, learning 0.150s)
             Mean action noise std: 1.44
          Mean value_function loss: 6.9880
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 31.8561
                       Mean reward: 15.39
               Mean episode length: 195.61
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: 1.8076
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.17s
                      Time elapsed: 00:06:46
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 45860 steps/s (collection: 2.044s, learning 0.099s)
             Mean action noise std: 1.44
          Mean value_function loss: 8.3410
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 31.8729
                       Mean reward: 11.86
               Mean episode length: 164.65
    Episode_Reward/reaching_object: 0.5921
     Episode_Reward/lifting_object: 1.5859
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.14s
                      Time elapsed: 00:06:48
                               ETA: 01:04:31

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 45468 steps/s (collection: 2.020s, learning 0.142s)
             Mean action noise std: 1.44
          Mean value_function loss: 11.1591
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 31.8865
                       Mean reward: 15.10
               Mean episode length: 180.38
    Episode_Reward/reaching_object: 0.6072
     Episode_Reward/lifting_object: 2.1882
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.16s
                      Time elapsed: 00:06:50
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 43138 steps/s (collection: 2.162s, learning 0.116s)
             Mean action noise std: 1.44
          Mean value_function loss: 8.4545
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 31.8984
                       Mean reward: 15.64
               Mean episode length: 165.04
    Episode_Reward/reaching_object: 0.5616
     Episode_Reward/lifting_object: 2.3693
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.28s
                      Time elapsed: 00:06:53
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 47071 steps/s (collection: 1.999s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 9.4163
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.9114
                       Mean reward: 11.91
               Mean episode length: 171.03
    Episode_Reward/reaching_object: 0.5449
     Episode_Reward/lifting_object: 2.2620
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.09s
                      Time elapsed: 00:06:55
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 47952 steps/s (collection: 1.951s, learning 0.099s)
             Mean action noise std: 1.44
          Mean value_function loss: 8.9555
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 31.9352
                       Mean reward: 15.02
               Mean episode length: 164.86
    Episode_Reward/reaching_object: 0.5196
     Episode_Reward/lifting_object: 2.4535
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.05s
                      Time elapsed: 00:06:57
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 45822 steps/s (collection: 2.042s, learning 0.104s)
             Mean action noise std: 1.44
          Mean value_function loss: 12.0378
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 31.9503
                       Mean reward: 19.45
               Mean episode length: 173.28
    Episode_Reward/reaching_object: 0.5322
     Episode_Reward/lifting_object: 2.7761
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.15s
                      Time elapsed: 00:06:59
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 45714 steps/s (collection: 2.039s, learning 0.111s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.6944
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 31.9670
                       Mean reward: 18.09
               Mean episode length: 181.93
    Episode_Reward/reaching_object: 0.5214
     Episode_Reward/lifting_object: 2.6860
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.15s
                      Time elapsed: 00:07:01
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 47630 steps/s (collection: 1.968s, learning 0.096s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.3720
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 31.9877
                       Mean reward: 13.38
               Mean episode length: 167.42
    Episode_Reward/reaching_object: 0.5236
     Episode_Reward/lifting_object: 2.6832
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.06s
                      Time elapsed: 00:07:03
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 44995 steps/s (collection: 2.047s, learning 0.137s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.8340
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 32.0042
                       Mean reward: 17.98
               Mean episode length: 180.51
    Episode_Reward/reaching_object: 0.5214
     Episode_Reward/lifting_object: 3.0094
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.18s
                      Time elapsed: 00:07:05
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 46324 steps/s (collection: 2.007s, learning 0.116s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.3930
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 32.0197
                       Mean reward: 17.63
               Mean episode length: 156.24
    Episode_Reward/reaching_object: 0.4715
     Episode_Reward/lifting_object: 2.7713
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.12s
                      Time elapsed: 00:07:07
                               ETA: 01:04:12

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 43858 steps/s (collection: 2.131s, learning 0.111s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.1488
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 32.0419
                       Mean reward: 15.82
               Mean episode length: 155.41
    Episode_Reward/reaching_object: 0.4768
     Episode_Reward/lifting_object: 2.9541
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.24s
                      Time elapsed: 00:07:10
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 46299 steps/s (collection: 2.025s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 13.6806
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 32.0476
                       Mean reward: 18.06
               Mean episode length: 154.49
    Episode_Reward/reaching_object: 0.4739
     Episode_Reward/lifting_object: 3.0462
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.12s
                      Time elapsed: 00:07:12
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 46847 steps/s (collection: 1.996s, learning 0.102s)
             Mean action noise std: 1.45
          Mean value_function loss: 12.8521
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 32.0522
                       Mean reward: 19.53
               Mean episode length: 157.54
    Episode_Reward/reaching_object: 0.4518
     Episode_Reward/lifting_object: 2.9733
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.10s
                      Time elapsed: 00:07:14
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 45750 steps/s (collection: 2.045s, learning 0.104s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.7996
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 32.0553
                       Mean reward: 17.78
               Mean episode length: 141.78
    Episode_Reward/reaching_object: 0.4117
     Episode_Reward/lifting_object: 2.9450
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.15s
                      Time elapsed: 00:07:16
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 44526 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 1.45
          Mean value_function loss: 15.4865
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 32.0650
                       Mean reward: 16.35
               Mean episode length: 145.32
    Episode_Reward/reaching_object: 0.4234
     Episode_Reward/lifting_object: 3.1266
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.21s
                      Time elapsed: 00:07:18
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 46929 steps/s (collection: 1.983s, learning 0.112s)
             Mean action noise std: 1.45
          Mean value_function loss: 11.5565
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 32.0796
                       Mean reward: 15.61
               Mean episode length: 129.05
    Episode_Reward/reaching_object: 0.4159
     Episode_Reward/lifting_object: 3.0905
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.09s
                      Time elapsed: 00:07:20
                               ETA: 01:04:00

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 44121 steps/s (collection: 2.069s, learning 0.159s)
             Mean action noise std: 1.46
          Mean value_function loss: 17.7377
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 32.0962
                       Mean reward: 17.06
               Mean episode length: 138.29
    Episode_Reward/reaching_object: 0.4146
     Episode_Reward/lifting_object: 3.4102
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.23s
                      Time elapsed: 00:07:22
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 45231 steps/s (collection: 2.035s, learning 0.139s)
             Mean action noise std: 1.46
          Mean value_function loss: 14.8039
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 32.1207
                       Mean reward: 17.54
               Mean episode length: 143.82
    Episode_Reward/reaching_object: 0.4063
     Episode_Reward/lifting_object: 3.1411
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.17s
                      Time elapsed: 00:07:25
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 45864 steps/s (collection: 2.037s, learning 0.107s)
             Mean action noise std: 1.46
          Mean value_function loss: 13.7925
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 32.1465
                       Mean reward: 18.79
               Mean episode length: 135.44
    Episode_Reward/reaching_object: 0.4091
     Episode_Reward/lifting_object: 3.2936
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.14s
                      Time elapsed: 00:07:27
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 45057 steps/s (collection: 2.058s, learning 0.124s)
             Mean action noise std: 1.46
          Mean value_function loss: 12.7146
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 32.1738
                       Mean reward: 19.50
               Mean episode length: 129.96
    Episode_Reward/reaching_object: 0.4065
     Episode_Reward/lifting_object: 3.6820
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.18s
                      Time elapsed: 00:07:29
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 47594 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 1.46
          Mean value_function loss: 14.6910
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 32.1959
                       Mean reward: 19.16
               Mean episode length: 151.56
    Episode_Reward/reaching_object: 0.4059
     Episode_Reward/lifting_object: 3.6336
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.07s
                      Time elapsed: 00:07:31
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 46364 steps/s (collection: 1.973s, learning 0.148s)
             Mean action noise std: 1.47
          Mean value_function loss: 16.5014
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 32.2152
                       Mean reward: 19.98
               Mean episode length: 139.36
    Episode_Reward/reaching_object: 0.4032
     Episode_Reward/lifting_object: 3.4492
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.12s
                      Time elapsed: 00:07:33
                               ETA: 01:03:48

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 45800 steps/s (collection: 1.977s, learning 0.170s)
             Mean action noise std: 1.47
          Mean value_function loss: 13.6137
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 32.2301
                       Mean reward: 23.74
               Mean episode length: 140.89
    Episode_Reward/reaching_object: 0.4023
     Episode_Reward/lifting_object: 3.7131
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.15s
                      Time elapsed: 00:07:35
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 44684 steps/s (collection: 2.019s, learning 0.181s)
             Mean action noise std: 1.47
          Mean value_function loss: 15.0586
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 32.2431
                       Mean reward: 16.90
               Mean episode length: 138.99
    Episode_Reward/reaching_object: 0.3836
     Episode_Reward/lifting_object: 3.6608
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.20s
                      Time elapsed: 00:07:38
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 46370 steps/s (collection: 2.031s, learning 0.089s)
             Mean action noise std: 1.47
          Mean value_function loss: 17.7359
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 32.2558
                       Mean reward: 27.40
               Mean episode length: 153.50
    Episode_Reward/reaching_object: 0.3996
     Episode_Reward/lifting_object: 4.2463
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.12s
                      Time elapsed: 00:07:40
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 46105 steps/s (collection: 2.013s, learning 0.120s)
             Mean action noise std: 1.47
          Mean value_function loss: 16.2845
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 32.2724
                       Mean reward: 24.56
               Mean episode length: 146.14
    Episode_Reward/reaching_object: 0.3998
     Episode_Reward/lifting_object: 4.1971
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.13s
                      Time elapsed: 00:07:42
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 46187 steps/s (collection: 2.027s, learning 0.101s)
             Mean action noise std: 1.47
          Mean value_function loss: 19.1883
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 32.2956
                       Mean reward: 25.32
               Mean episode length: 155.90
    Episode_Reward/reaching_object: 0.4075
     Episode_Reward/lifting_object: 4.2837
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.13s
                      Time elapsed: 00:07:44
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 46870 steps/s (collection: 1.993s, learning 0.104s)
             Mean action noise std: 1.47
          Mean value_function loss: 17.0919
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 32.3097
                       Mean reward: 28.31
               Mean episode length: 135.48
    Episode_Reward/reaching_object: 0.3946
     Episode_Reward/lifting_object: 4.5397
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.10s
                      Time elapsed: 00:07:46
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 46125 steps/s (collection: 2.042s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 18.3450
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 32.3308
                       Mean reward: 28.19
               Mean episode length: 140.88
    Episode_Reward/reaching_object: 0.3968
     Episode_Reward/lifting_object: 4.9852
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.13s
                      Time elapsed: 00:07:48
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 45739 steps/s (collection: 2.032s, learning 0.118s)
             Mean action noise std: 1.48
          Mean value_function loss: 21.5941
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 32.3477
                       Mean reward: 28.06
               Mean episode length: 152.77
    Episode_Reward/reaching_object: 0.4061
     Episode_Reward/lifting_object: 4.3669
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.15s
                      Time elapsed: 00:07:50
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 46105 steps/s (collection: 2.005s, learning 0.127s)
             Mean action noise std: 1.48
          Mean value_function loss: 21.3986
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 32.3612
                       Mean reward: 26.60
               Mean episode length: 137.76
    Episode_Reward/reaching_object: 0.3982
     Episode_Reward/lifting_object: 4.5522
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.13s
                      Time elapsed: 00:07:52
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 41733 steps/s (collection: 2.217s, learning 0.139s)
             Mean action noise std: 1.48
          Mean value_function loss: 20.4842
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 32.3736
                       Mean reward: 25.44
               Mean episode length: 149.43
    Episode_Reward/reaching_object: 0.3944
     Episode_Reward/lifting_object: 4.7811
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.36s
                      Time elapsed: 00:07:55
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 46258 steps/s (collection: 2.010s, learning 0.115s)
             Mean action noise std: 1.48
          Mean value_function loss: 22.4174
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 32.3961
                       Mean reward: 23.01
               Mean episode length: 146.05
    Episode_Reward/reaching_object: 0.4000
     Episode_Reward/lifting_object: 4.9620
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.13s
                      Time elapsed: 00:07:57
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 46601 steps/s (collection: 2.009s, learning 0.101s)
             Mean action noise std: 1.48
          Mean value_function loss: 19.7953
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 32.4178
                       Mean reward: 27.09
               Mean episode length: 142.62
    Episode_Reward/reaching_object: 0.3909
     Episode_Reward/lifting_object: 4.8631
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.11s
                      Time elapsed: 00:07:59
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 47408 steps/s (collection: 1.987s, learning 0.086s)
             Mean action noise std: 1.48
          Mean value_function loss: 19.5691
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 32.4427
                       Mean reward: 26.31
               Mean episode length: 141.15
    Episode_Reward/reaching_object: 0.3868
     Episode_Reward/lifting_object: 5.2794
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.07s
                      Time elapsed: 00:08:01
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 47268 steps/s (collection: 1.979s, learning 0.100s)
             Mean action noise std: 1.49
          Mean value_function loss: 19.8919
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 32.4534
                       Mean reward: 25.33
               Mean episode length: 126.93
    Episode_Reward/reaching_object: 0.3980
     Episode_Reward/lifting_object: 5.2762
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.08s
                      Time elapsed: 00:08:03
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 45686 steps/s (collection: 2.013s, learning 0.139s)
             Mean action noise std: 1.49
          Mean value_function loss: 24.3834
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 32.4665
                       Mean reward: 28.62
               Mean episode length: 135.02
    Episode_Reward/reaching_object: 0.3772
     Episode_Reward/lifting_object: 5.1860
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.15s
                      Time elapsed: 00:08:05
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 46246 steps/s (collection: 2.008s, learning 0.117s)
             Mean action noise std: 1.49
          Mean value_function loss: 21.1151
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 32.4784
                       Mean reward: 30.53
               Mean episode length: 133.30
    Episode_Reward/reaching_object: 0.3748
     Episode_Reward/lifting_object: 5.3544
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.13s
                      Time elapsed: 00:08:07
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 43096 steps/s (collection: 2.121s, learning 0.160s)
             Mean action noise std: 1.49
          Mean value_function loss: 23.5996
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 32.4928
                       Mean reward: 31.73
               Mean episode length: 137.60
    Episode_Reward/reaching_object: 0.3895
     Episode_Reward/lifting_object: 5.4013
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.28s
                      Time elapsed: 00:08:10
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 45364 steps/s (collection: 2.056s, learning 0.111s)
             Mean action noise std: 1.49
          Mean value_function loss: 22.0695
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 32.5137
                       Mean reward: 29.05
               Mean episode length: 140.98
    Episode_Reward/reaching_object: 0.3769
     Episode_Reward/lifting_object: 5.3572
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.17s
                      Time elapsed: 00:08:12
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 47251 steps/s (collection: 1.967s, learning 0.113s)
             Mean action noise std: 1.49
          Mean value_function loss: 24.8649
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 32.5278
                       Mean reward: 25.45
               Mean episode length: 124.95
    Episode_Reward/reaching_object: 0.3679
     Episode_Reward/lifting_object: 5.3367
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.08s
                      Time elapsed: 00:08:14
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 48114 steps/s (collection: 1.946s, learning 0.098s)
             Mean action noise std: 1.49
          Mean value_function loss: 24.5236
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 32.5382
                       Mean reward: 33.99
               Mean episode length: 142.65
    Episode_Reward/reaching_object: 0.3596
     Episode_Reward/lifting_object: 5.3714
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.04s
                      Time elapsed: 00:08:16
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 47509 steps/s (collection: 1.963s, learning 0.107s)
             Mean action noise std: 1.49
          Mean value_function loss: 26.8132
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 32.5492
                       Mean reward: 31.72
               Mean episode length: 144.00
    Episode_Reward/reaching_object: 0.3801
     Episode_Reward/lifting_object: 5.7925
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.07s
                      Time elapsed: 00:08:18
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 46979 steps/s (collection: 1.992s, learning 0.101s)
             Mean action noise std: 1.50
          Mean value_function loss: 22.9483
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 32.5654
                       Mean reward: 27.93
               Mean episode length: 134.99
    Episode_Reward/reaching_object: 0.3653
     Episode_Reward/lifting_object: 5.2620
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.09s
                      Time elapsed: 00:08:20
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.020s, learning 0.092s)
             Mean action noise std: 1.50
          Mean value_function loss: 23.9372
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 32.5809
                       Mean reward: 28.02
               Mean episode length: 122.47
    Episode_Reward/reaching_object: 0.3643
     Episode_Reward/lifting_object: 5.7497
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.11s
                      Time elapsed: 00:08:22
                               ETA: 01:02:58

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 42424 steps/s (collection: 2.140s, learning 0.177s)
             Mean action noise std: 1.50
          Mean value_function loss: 21.8809
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 32.5996
                       Mean reward: 36.28
               Mean episode length: 137.84
    Episode_Reward/reaching_object: 0.3581
     Episode_Reward/lifting_object: 6.0567
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.32s
                      Time elapsed: 00:08:25
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 46964 steps/s (collection: 1.993s, learning 0.100s)
             Mean action noise std: 1.50
          Mean value_function loss: 25.6438
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 32.6066
                       Mean reward: 34.04
               Mean episode length: 130.90
    Episode_Reward/reaching_object: 0.3624
     Episode_Reward/lifting_object: 5.7835
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.09s
                      Time elapsed: 00:08:27
                               ETA: 01:02:54

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 44647 steps/s (collection: 2.031s, learning 0.171s)
             Mean action noise std: 1.50
          Mean value_function loss: 25.6641
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 32.6099
                       Mean reward: 31.84
               Mean episode length: 130.49
    Episode_Reward/reaching_object: 0.3420
     Episode_Reward/lifting_object: 5.6009
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.20s
                      Time elapsed: 00:08:29
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.110s, learning 0.122s)
             Mean action noise std: 1.50
          Mean value_function loss: 26.8985
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 32.6180
                       Mean reward: 32.30
               Mean episode length: 131.87
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: 5.7228
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.23s
                      Time elapsed: 00:08:31
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 46285 steps/s (collection: 2.018s, learning 0.106s)
             Mean action noise std: 1.50
          Mean value_function loss: 29.5313
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 32.6301
                       Mean reward: 30.92
               Mean episode length: 117.89
    Episode_Reward/reaching_object: 0.3346
     Episode_Reward/lifting_object: 5.6943
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.12s
                      Time elapsed: 00:08:33
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 46068 steps/s (collection: 2.040s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 27.0367
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 32.6464
                       Mean reward: 31.71
               Mean episode length: 116.56
    Episode_Reward/reaching_object: 0.3386
     Episode_Reward/lifting_object: 5.9290
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.13s
                      Time elapsed: 00:08:35
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 46475 steps/s (collection: 1.973s, learning 0.142s)
             Mean action noise std: 1.50
          Mean value_function loss: 26.5175
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 32.6622
                       Mean reward: 32.59
               Mean episode length: 122.00
    Episode_Reward/reaching_object: 0.3328
     Episode_Reward/lifting_object: 5.7752
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.12s
                      Time elapsed: 00:08:37
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 45493 steps/s (collection: 2.060s, learning 0.101s)
             Mean action noise std: 1.50
          Mean value_function loss: 28.2696
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 32.6725
                       Mean reward: 27.76
               Mean episode length: 120.00
    Episode_Reward/reaching_object: 0.3228
     Episode_Reward/lifting_object: 5.3536
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.16s
                      Time elapsed: 00:08:40
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 47195 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 27.9478
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 32.6773
                       Mean reward: 32.36
               Mean episode length: 127.84
    Episode_Reward/reaching_object: 0.3324
     Episode_Reward/lifting_object: 5.8778
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.08s
                      Time elapsed: 00:08:42
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 48448 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 23.0073
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 32.6797
                       Mean reward: 35.76
               Mean episode length: 126.24
    Episode_Reward/reaching_object: 0.3331
     Episode_Reward/lifting_object: 6.2590
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.03s
                      Time elapsed: 00:08:44
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 47639 steps/s (collection: 1.973s, learning 0.091s)
             Mean action noise std: 1.50
          Mean value_function loss: 32.5675
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 32.6810
                       Mean reward: 31.89
               Mean episode length: 118.41
    Episode_Reward/reaching_object: 0.3299
     Episode_Reward/lifting_object: 6.1952
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.06s
                      Time elapsed: 00:08:46
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 47858 steps/s (collection: 1.956s, learning 0.098s)
             Mean action noise std: 1.51
          Mean value_function loss: 26.7486
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 32.6865
                       Mean reward: 32.26
               Mean episode length: 123.48
    Episode_Reward/reaching_object: 0.3292
     Episode_Reward/lifting_object: 6.4060
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.05s
                      Time elapsed: 00:08:48
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 46530 steps/s (collection: 2.019s, learning 0.093s)
             Mean action noise std: 1.51
          Mean value_function loss: 26.2014
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 32.7003
                       Mean reward: 29.51
               Mean episode length: 116.58
    Episode_Reward/reaching_object: 0.3231
     Episode_Reward/lifting_object: 6.2275
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.11s
                      Time elapsed: 00:08:50
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 46096 steps/s (collection: 2.026s, learning 0.107s)
             Mean action noise std: 1.51
          Mean value_function loss: 28.2628
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 32.7103
                       Mean reward: 33.32
               Mean episode length: 115.44
    Episode_Reward/reaching_object: 0.3226
     Episode_Reward/lifting_object: 6.3429
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.13s
                      Time elapsed: 00:08:52
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 46947 steps/s (collection: 1.997s, learning 0.097s)
             Mean action noise std: 1.51
          Mean value_function loss: 28.7464
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 32.7199
                       Mean reward: 28.96
               Mean episode length: 114.46
    Episode_Reward/reaching_object: 0.3170
     Episode_Reward/lifting_object: 5.8264
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.09s
                      Time elapsed: 00:08:54
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 46409 steps/s (collection: 2.016s, learning 0.102s)
             Mean action noise std: 1.51
          Mean value_function loss: 29.2525
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 32.7326
                       Mean reward: 36.51
               Mean episode length: 116.07
    Episode_Reward/reaching_object: 0.3268
     Episode_Reward/lifting_object: 6.5457
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.12s
                      Time elapsed: 00:08:56
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 45118 steps/s (collection: 2.023s, learning 0.155s)
             Mean action noise std: 1.51
          Mean value_function loss: 32.0847
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 32.7484
                       Mean reward: 32.71
               Mean episode length: 111.02
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 6.5553
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.18s
                      Time elapsed: 00:08:59
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.012s, learning 0.120s)
             Mean action noise std: 1.51
          Mean value_function loss: 30.2846
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 32.7651
                       Mean reward: 34.25
               Mean episode length: 122.27
    Episode_Reward/reaching_object: 0.3270
     Episode_Reward/lifting_object: 6.1731
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.13s
                      Time elapsed: 00:09:01
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 46769 steps/s (collection: 1.972s, learning 0.130s)
             Mean action noise std: 1.51
          Mean value_function loss: 37.5498
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 32.7749
                       Mean reward: 28.55
               Mean episode length: 108.19
    Episode_Reward/reaching_object: 0.3110
     Episode_Reward/lifting_object: 6.4850
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.10s
                      Time elapsed: 00:09:03
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 47713 steps/s (collection: 1.955s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 29.7368
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 32.7885
                       Mean reward: 30.66
               Mean episode length: 111.39
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 6.0258
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.06s
                      Time elapsed: 00:09:05
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48208 steps/s (collection: 1.937s, learning 0.102s)
             Mean action noise std: 1.52
          Mean value_function loss: 33.2999
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 32.8047
                       Mean reward: 32.49
               Mean episode length: 119.55
    Episode_Reward/reaching_object: 0.3174
     Episode_Reward/lifting_object: 6.5328
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.04s
                      Time elapsed: 00:09:07
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 47062 steps/s (collection: 1.996s, learning 0.093s)
             Mean action noise std: 1.52
          Mean value_function loss: 29.7279
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 32.8208
                       Mean reward: 34.31
               Mean episode length: 123.37
    Episode_Reward/reaching_object: 0.3212
     Episode_Reward/lifting_object: 6.9379
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.09s
                      Time elapsed: 00:09:09
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 46845 steps/s (collection: 1.981s, learning 0.117s)
             Mean action noise std: 1.52
          Mean value_function loss: 37.5899
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 32.8331
                       Mean reward: 37.52
               Mean episode length: 128.94
    Episode_Reward/reaching_object: 0.3334
     Episode_Reward/lifting_object: 7.1050
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.10s
                      Time elapsed: 00:09:11
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 46752 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 1.52
          Mean value_function loss: 26.5640
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 32.8480
                       Mean reward: 37.05
               Mean episode length: 122.31
    Episode_Reward/reaching_object: 0.3272
     Episode_Reward/lifting_object: 6.6648
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.10s
                      Time elapsed: 00:09:13
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 46693 steps/s (collection: 2.017s, learning 0.088s)
             Mean action noise std: 1.52
          Mean value_function loss: 33.5117
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 32.8563
                       Mean reward: 39.05
               Mean episode length: 120.88
    Episode_Reward/reaching_object: 0.3301
     Episode_Reward/lifting_object: 7.4061
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.11s
                      Time elapsed: 00:09:15
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 46664 steps/s (collection: 1.997s, learning 0.110s)
             Mean action noise std: 1.52
          Mean value_function loss: 36.2645
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 32.8724
                       Mean reward: 38.38
               Mean episode length: 119.42
    Episode_Reward/reaching_object: 0.3238
     Episode_Reward/lifting_object: 6.8268
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.11s
                      Time elapsed: 00:09:17
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 46124 steps/s (collection: 2.013s, learning 0.118s)
             Mean action noise std: 1.52
          Mean value_function loss: 36.5261
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 32.8833
                       Mean reward: 30.84
               Mean episode length: 121.23
    Episode_Reward/reaching_object: 0.3232
     Episode_Reward/lifting_object: 6.8353
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.13s
                      Time elapsed: 00:09:19
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 46206 steps/s (collection: 2.006s, learning 0.121s)
             Mean action noise std: 1.52
          Mean value_function loss: 29.8506
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 32.8945
                       Mean reward: 37.34
               Mean episode length: 114.27
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 7.0674
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.13s
                      Time elapsed: 00:09:22
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 46315 steps/s (collection: 2.016s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 33.5281
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 32.8989
                       Mean reward: 37.40
               Mean episode length: 117.05
    Episode_Reward/reaching_object: 0.3259
     Episode_Reward/lifting_object: 7.2550
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.12s
                      Time elapsed: 00:09:24
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 46367 steps/s (collection: 2.023s, learning 0.098s)
             Mean action noise std: 1.53
          Mean value_function loss: 32.6947
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 32.9059
                       Mean reward: 34.75
               Mean episode length: 105.27
    Episode_Reward/reaching_object: 0.3080
     Episode_Reward/lifting_object: 7.0306
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.12s
                      Time elapsed: 00:09:26
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 46372 steps/s (collection: 2.011s, learning 0.109s)
             Mean action noise std: 1.53
          Mean value_function loss: 36.5906
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.9213
                       Mean reward: 37.48
               Mean episode length: 110.68
    Episode_Reward/reaching_object: 0.3090
     Episode_Reward/lifting_object: 7.1026
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.12s
                      Time elapsed: 00:09:28
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 45284 steps/s (collection: 2.047s, learning 0.124s)
             Mean action noise std: 1.53
          Mean value_function loss: 36.5429
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 32.9367
                       Mean reward: 38.92
               Mean episode length: 106.38
    Episode_Reward/reaching_object: 0.3070
     Episode_Reward/lifting_object: 7.1257
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.17s
                      Time elapsed: 00:09:30
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 46024 steps/s (collection: 2.036s, learning 0.100s)
             Mean action noise std: 1.53
          Mean value_function loss: 28.3997
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 32.9434
                       Mean reward: 38.07
               Mean episode length: 121.84
    Episode_Reward/reaching_object: 0.3182
     Episode_Reward/lifting_object: 7.4238
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.14s
                      Time elapsed: 00:09:32
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 47946 steps/s (collection: 1.963s, learning 0.087s)
             Mean action noise std: 1.53
          Mean value_function loss: 36.2201
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 32.9512
                       Mean reward: 40.54
               Mean episode length: 113.72
    Episode_Reward/reaching_object: 0.3206
     Episode_Reward/lifting_object: 7.4580
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.05s
                      Time elapsed: 00:09:34
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 46829 steps/s (collection: 2.005s, learning 0.095s)
             Mean action noise std: 1.53
          Mean value_function loss: 39.2635
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 32.9636
                       Mean reward: 45.84
               Mean episode length: 124.68
    Episode_Reward/reaching_object: 0.3247
     Episode_Reward/lifting_object: 7.8871
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.10s
                      Time elapsed: 00:09:36
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 43992 steps/s (collection: 2.053s, learning 0.182s)
             Mean action noise std: 1.53
          Mean value_function loss: 36.0625
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 32.9777
                       Mean reward: 38.85
               Mean episode length: 113.92
    Episode_Reward/reaching_object: 0.3217
     Episode_Reward/lifting_object: 7.6678
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.23s
                      Time elapsed: 00:09:39
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 45246 steps/s (collection: 2.067s, learning 0.106s)
             Mean action noise std: 1.53
          Mean value_function loss: 36.5463
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 32.9851
                       Mean reward: 37.12
               Mean episode length: 108.62
    Episode_Reward/reaching_object: 0.3074
     Episode_Reward/lifting_object: 7.2575
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 34.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.17s
                      Time elapsed: 00:09:41
                               ETA: 01:01:35

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 47467 steps/s (collection: 1.971s, learning 0.100s)
             Mean action noise std: 1.53
          Mean value_function loss: 34.8265
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 32.9906
                       Mean reward: 35.11
               Mean episode length: 105.07
    Episode_Reward/reaching_object: 0.3182
     Episode_Reward/lifting_object: 7.6402
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.07s
                      Time elapsed: 00:09:43
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 46473 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 1.53
          Mean value_function loss: 39.9819
               Mean surrogate loss: 0.0192
                 Mean entropy loss: 32.9956
                       Mean reward: 40.25
               Mean episode length: 119.19
    Episode_Reward/reaching_object: 0.3169
     Episode_Reward/lifting_object: 8.0256
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.12s
                      Time elapsed: 00:09:45
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 46377 steps/s (collection: 2.002s, learning 0.118s)
             Mean action noise std: 1.53
          Mean value_function loss: 28.1796
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 32.9962
                       Mean reward: 39.18
               Mean episode length: 115.00
    Episode_Reward/reaching_object: 0.3295
     Episode_Reward/lifting_object: 8.0524
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.12s
                      Time elapsed: 00:09:47
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.034s, learning 0.099s)
             Mean action noise std: 1.53
          Mean value_function loss: 28.6266
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.0011
                       Mean reward: 44.28
               Mean episode length: 128.30
    Episode_Reward/reaching_object: 0.3230
     Episode_Reward/lifting_object: 7.8524
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.13s
                      Time elapsed: 00:09:49
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 43769 steps/s (collection: 2.120s, learning 0.126s)
             Mean action noise std: 1.53
          Mean value_function loss: 31.7153
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.0107
                       Mean reward: 48.21
               Mean episode length: 130.37
    Episode_Reward/reaching_object: 0.3448
     Episode_Reward/lifting_object: 8.5507
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.25s
                      Time elapsed: 00:09:52
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 45415 steps/s (collection: 2.029s, learning 0.136s)
             Mean action noise std: 1.54
          Mean value_function loss: 30.6526
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 33.0172
                       Mean reward: 48.30
               Mean episode length: 124.05
    Episode_Reward/reaching_object: 0.3427
     Episode_Reward/lifting_object: 8.6712
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.16s
                      Time elapsed: 00:09:54
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 45642 steps/s (collection: 2.055s, learning 0.099s)
             Mean action noise std: 1.54
          Mean value_function loss: 32.1446
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 33.0225
                       Mean reward: 46.06
               Mean episode length: 128.63
    Episode_Reward/reaching_object: 0.3308
     Episode_Reward/lifting_object: 8.3652
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.15s
                      Time elapsed: 00:09:56
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 46689 steps/s (collection: 2.014s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 30.3926
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.0333
                       Mean reward: 41.07
               Mean episode length: 115.68
    Episode_Reward/reaching_object: 0.3283
     Episode_Reward/lifting_object: 8.5380
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.11s
                      Time elapsed: 00:09:58
                               ETA: 01:01:18

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 44462 steps/s (collection: 2.026s, learning 0.185s)
             Mean action noise std: 1.54
          Mean value_function loss: 34.6634
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 33.0456
                       Mean reward: 43.76
               Mean episode length: 119.58
    Episode_Reward/reaching_object: 0.3244
     Episode_Reward/lifting_object: 8.3653
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.21s
                      Time elapsed: 00:10:00
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 43370 steps/s (collection: 2.161s, learning 0.106s)
             Mean action noise std: 1.54
          Mean value_function loss: 34.7068
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 33.0592
                       Mean reward: 40.52
               Mean episode length: 113.41
    Episode_Reward/reaching_object: 0.3253
     Episode_Reward/lifting_object: 8.0957
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.27s
                      Time elapsed: 00:10:02
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 45207 steps/s (collection: 2.061s, learning 0.113s)
             Mean action noise std: 1.54
          Mean value_function loss: 33.6401
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 33.0691
                       Mean reward: 39.58
               Mean episode length: 116.74
    Episode_Reward/reaching_object: 0.3168
     Episode_Reward/lifting_object: 8.0708
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.17s
                      Time elapsed: 00:10:05
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 46100 steps/s (collection: 2.046s, learning 0.086s)
             Mean action noise std: 1.54
          Mean value_function loss: 35.7754
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 33.0735
                       Mean reward: 40.95
               Mean episode length: 109.14
    Episode_Reward/reaching_object: 0.3137
     Episode_Reward/lifting_object: 8.5081
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.13s
                      Time elapsed: 00:10:07
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 45974 steps/s (collection: 2.034s, learning 0.105s)
             Mean action noise std: 1.54
          Mean value_function loss: 34.0899
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 33.0813
                       Mean reward: 46.79
               Mean episode length: 117.52
    Episode_Reward/reaching_object: 0.3193
     Episode_Reward/lifting_object: 8.4266
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.14s
                      Time elapsed: 00:10:09
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 45396 steps/s (collection: 2.069s, learning 0.096s)
             Mean action noise std: 1.54
          Mean value_function loss: 32.2273
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.0875
                       Mean reward: 44.61
               Mean episode length: 113.49
    Episode_Reward/reaching_object: 0.3238
     Episode_Reward/lifting_object: 9.0476
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.17s
                      Time elapsed: 00:10:11
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 47251 steps/s (collection: 1.983s, learning 0.097s)
             Mean action noise std: 1.54
          Mean value_function loss: 34.8807
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 33.0985
                       Mean reward: 43.75
               Mean episode length: 117.09
    Episode_Reward/reaching_object: 0.3213
     Episode_Reward/lifting_object: 8.5470
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.08s
                      Time elapsed: 00:10:13
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 46810 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 36.4194
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 33.1122
                       Mean reward: 50.60
               Mean episode length: 127.22
    Episode_Reward/reaching_object: 0.3070
     Episode_Reward/lifting_object: 8.3073
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.10s
                      Time elapsed: 00:10:15
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 47218 steps/s (collection: 1.990s, learning 0.092s)
             Mean action noise std: 1.55
          Mean value_function loss: 39.3423
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 33.1267
                       Mean reward: 48.38
               Mean episode length: 122.83
    Episode_Reward/reaching_object: 0.3199
     Episode_Reward/lifting_object: 8.9000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.08s
                      Time elapsed: 00:10:17
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 47888 steps/s (collection: 1.948s, learning 0.105s)
             Mean action noise std: 1.55
          Mean value_function loss: 38.4623
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.1366
                       Mean reward: 44.93
               Mean episode length: 111.14
    Episode_Reward/reaching_object: 0.3133
     Episode_Reward/lifting_object: 9.0371
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.05s
                      Time elapsed: 00:10:19
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 47814 steps/s (collection: 1.949s, learning 0.107s)
             Mean action noise std: 1.55
          Mean value_function loss: 37.4469
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 33.1458
                       Mean reward: 44.37
               Mean episode length: 109.63
    Episode_Reward/reaching_object: 0.3098
     Episode_Reward/lifting_object: 8.5001
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.06s
                      Time elapsed: 00:10:21
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.971s, learning 0.092s)
             Mean action noise std: 1.55
          Mean value_function loss: 36.1340
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 33.1547
                       Mean reward: 43.87
               Mean episode length: 106.47
    Episode_Reward/reaching_object: 0.3163
     Episode_Reward/lifting_object: 8.7829
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.06s
                      Time elapsed: 00:10:23
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 46835 steps/s (collection: 2.005s, learning 0.094s)
             Mean action noise std: 1.55
          Mean value_function loss: 35.9116
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 33.1648
                       Mean reward: 47.77
               Mean episode length: 107.14
    Episode_Reward/reaching_object: 0.3011
     Episode_Reward/lifting_object: 8.5426
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.10s
                      Time elapsed: 00:10:26
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 44538 steps/s (collection: 2.107s, learning 0.101s)
             Mean action noise std: 1.55
          Mean value_function loss: 34.7986
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 33.1713
                       Mean reward: 41.27
               Mean episode length: 111.07
    Episode_Reward/reaching_object: 0.3094
     Episode_Reward/lifting_object: 8.9883
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.21s
                      Time elapsed: 00:10:28
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 45814 steps/s (collection: 2.049s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 35.8852
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 33.1763
                       Mean reward: 47.97
               Mean episode length: 114.96
    Episode_Reward/reaching_object: 0.3078
     Episode_Reward/lifting_object: 9.0695
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.15s
                      Time elapsed: 00:10:30
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 46855 steps/s (collection: 2.009s, learning 0.090s)
             Mean action noise std: 1.55
          Mean value_function loss: 38.8185
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 33.1820
                       Mean reward: 45.28
               Mean episode length: 112.17
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: 8.6056
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.10s
                      Time elapsed: 00:10:32
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 47434 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 1.55
          Mean value_function loss: 35.1107
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.1901
                       Mean reward: 41.94
               Mean episode length: 104.64
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 8.5786
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.07s
                      Time elapsed: 00:10:34
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 46476 steps/s (collection: 2.016s, learning 0.099s)
             Mean action noise std: 1.55
          Mean value_function loss: 45.0482
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.1978
                       Mean reward: 42.23
               Mean episode length: 101.44
    Episode_Reward/reaching_object: 0.2940
     Episode_Reward/lifting_object: 8.7648
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 40.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.12s
                      Time elapsed: 00:10:36
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 46350 steps/s (collection: 2.007s, learning 0.114s)
             Mean action noise std: 1.55
          Mean value_function loss: 36.2708
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.2038
                       Mean reward: 42.29
               Mean episode length: 103.39
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 8.6628
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.12s
                      Time elapsed: 00:10:38
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47057 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 41.6644
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.2088
                       Mean reward: 43.84
               Mean episode length: 99.39
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: 8.8397
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.09s
                      Time elapsed: 00:10:40
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 46786 steps/s (collection: 2.008s, learning 0.094s)
             Mean action noise std: 1.55
          Mean value_function loss: 36.7817
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 33.2123
                       Mean reward: 49.87
               Mean episode length: 110.91
    Episode_Reward/reaching_object: 0.2902
     Episode_Reward/lifting_object: 8.9031
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.10s
                      Time elapsed: 00:10:42
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 46378 steps/s (collection: 2.018s, learning 0.102s)
             Mean action noise std: 1.55
          Mean value_function loss: 39.5788
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.2211
                       Mean reward: 45.59
               Mean episode length: 107.58
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 8.9211
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.12s
                      Time elapsed: 00:10:45
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 46627 steps/s (collection: 2.009s, learning 0.099s)
             Mean action noise std: 1.55
          Mean value_function loss: 42.2247
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 33.2287
                       Mean reward: 42.78
               Mean episode length: 101.06
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 8.7632
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.11s
                      Time elapsed: 00:10:47
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 45999 steps/s (collection: 2.029s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.4807
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 33.2368
                       Mean reward: 44.85
               Mean episode length: 94.62
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: 8.7720
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.14s
                      Time elapsed: 00:10:49
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 45903 steps/s (collection: 2.039s, learning 0.103s)
             Mean action noise std: 1.56
          Mean value_function loss: 40.3415
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 33.2459
                       Mean reward: 45.98
               Mean episode length: 106.46
    Episode_Reward/reaching_object: 0.2810
     Episode_Reward/lifting_object: 8.6095
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.14s
                      Time elapsed: 00:10:51
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 45869 steps/s (collection: 2.037s, learning 0.107s)
             Mean action noise std: 1.56
          Mean value_function loss: 37.2173
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.2525
                       Mean reward: 50.25
               Mean episode length: 100.43
    Episode_Reward/reaching_object: 0.2785
     Episode_Reward/lifting_object: 8.5826
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.14s
                      Time elapsed: 00:10:53
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 46926 steps/s (collection: 2.003s, learning 0.092s)
             Mean action noise std: 1.56
          Mean value_function loss: 46.8635
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.2588
                       Mean reward: 41.79
               Mean episode length: 103.91
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: 9.0241
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 39.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.09s
                      Time elapsed: 00:10:55
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 47182 steps/s (collection: 1.988s, learning 0.096s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.5223
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.2656
                       Mean reward: 45.73
               Mean episode length: 101.78
    Episode_Reward/reaching_object: 0.2861
     Episode_Reward/lifting_object: 9.1783
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 43.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.08s
                      Time elapsed: 00:10:57
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 47112 steps/s (collection: 1.992s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.1286
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 33.2730
                       Mean reward: 42.96
               Mean episode length: 107.70
    Episode_Reward/reaching_object: 0.2749
     Episode_Reward/lifting_object: 8.5173
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.09s
                      Time elapsed: 00:10:59
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 45371 steps/s (collection: 2.066s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.3596
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.2828
                       Mean reward: 45.60
               Mean episode length: 100.42
    Episode_Reward/reaching_object: 0.2744
     Episode_Reward/lifting_object: 8.7745
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.17s
                      Time elapsed: 00:11:02
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 46668 steps/s (collection: 2.002s, learning 0.104s)
             Mean action noise std: 1.56
          Mean value_function loss: 45.4351
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 33.2966
                       Mean reward: 43.53
               Mean episode length: 90.83
    Episode_Reward/reaching_object: 0.2682
     Episode_Reward/lifting_object: 8.7837
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.11s
                      Time elapsed: 00:11:04
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 45712 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 44.1375
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.3021
                       Mean reward: 46.50
               Mean episode length: 98.72
    Episode_Reward/reaching_object: 0.2754
     Episode_Reward/lifting_object: 9.2141
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.15s
                      Time elapsed: 00:11:06
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 46765 steps/s (collection: 2.006s, learning 0.097s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.6580
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.3120
                       Mean reward: 48.75
               Mean episode length: 102.23
    Episode_Reward/reaching_object: 0.2812
     Episode_Reward/lifting_object: 9.2960
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.10s
                      Time elapsed: 00:11:08
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 46293 steps/s (collection: 2.024s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 43.9118
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 33.3179
                       Mean reward: 49.00
               Mean episode length: 98.11
    Episode_Reward/reaching_object: 0.2791
     Episode_Reward/lifting_object: 9.1910
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.12s
                      Time elapsed: 00:11:10
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 45383 steps/s (collection: 2.072s, learning 0.094s)
             Mean action noise std: 1.56
          Mean value_function loss: 45.0795
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 33.3217
                       Mean reward: 42.07
               Mean episode length: 94.73
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 8.8822
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.17s
                      Time elapsed: 00:11:12
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 46353 steps/s (collection: 2.012s, learning 0.109s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.4503
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 33.3239
                       Mean reward: 44.25
               Mean episode length: 100.02
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: 9.0783
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.12s
                      Time elapsed: 00:11:14
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 46477 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 47.4599
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 33.3307
                       Mean reward: 52.58
               Mean episode length: 96.18
    Episode_Reward/reaching_object: 0.2735
     Episode_Reward/lifting_object: 9.0687
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.12s
                      Time elapsed: 00:11:16
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 45141 steps/s (collection: 2.075s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.9724
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 33.3361
                       Mean reward: 46.14
               Mean episode length: 91.61
    Episode_Reward/reaching_object: 0.2711
     Episode_Reward/lifting_object: 8.9808
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.18s
                      Time elapsed: 00:11:19
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 45639 steps/s (collection: 2.045s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 47.9137
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.3409
                       Mean reward: 42.06
               Mean episode length: 96.09
    Episode_Reward/reaching_object: 0.2726
     Episode_Reward/lifting_object: 8.9748
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.15s
                      Time elapsed: 00:11:21
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 44767 steps/s (collection: 2.085s, learning 0.111s)
             Mean action noise std: 1.57
          Mean value_function loss: 44.1570
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.3501
                       Mean reward: 44.23
               Mean episode length: 87.30
    Episode_Reward/reaching_object: 0.2680
     Episode_Reward/lifting_object: 8.6252
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.20s
                      Time elapsed: 00:11:23
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 45606 steps/s (collection: 2.058s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.6784
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.3597
                       Mean reward: 45.26
               Mean episode length: 92.01
    Episode_Reward/reaching_object: 0.2665
     Episode_Reward/lifting_object: 8.8865
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.16s
                      Time elapsed: 00:11:25
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 46274 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 46.6519
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 33.3650
                       Mean reward: 48.18
               Mean episode length: 90.26
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 9.1039
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.12s
                      Time elapsed: 00:11:27
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 45639 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.1004
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.3700
                       Mean reward: 39.21
               Mean episode length: 81.88
    Episode_Reward/reaching_object: 0.2650
     Episode_Reward/lifting_object: 8.8505
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 42.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.15s
                      Time elapsed: 00:11:29
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 46825 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.0867
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 33.3764
                       Mean reward: 41.99
               Mean episode length: 88.53
    Episode_Reward/reaching_object: 0.2756
     Episode_Reward/lifting_object: 8.9839
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.10s
                      Time elapsed: 00:11:32
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 45753 steps/s (collection: 2.039s, learning 0.110s)
             Mean action noise std: 1.57
          Mean value_function loss: 45.0683
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 33.3870
                       Mean reward: 44.31
               Mean episode length: 90.50
    Episode_Reward/reaching_object: 0.2685
     Episode_Reward/lifting_object: 9.0695
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.15s
                      Time elapsed: 00:11:34
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46114 steps/s (collection: 2.031s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 45.2559
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 33.3952
                       Mean reward: 50.12
               Mean episode length: 96.55
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 9.4260
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.13s
                      Time elapsed: 00:11:36
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 45986 steps/s (collection: 2.046s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 40.1105
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 33.3969
                       Mean reward: 51.77
               Mean episode length: 94.66
    Episode_Reward/reaching_object: 0.2660
     Episode_Reward/lifting_object: 9.3686
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.14s
                      Time elapsed: 00:11:38
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 45918 steps/s (collection: 2.041s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.9553
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 33.3989
                       Mean reward: 45.00
               Mean episode length: 89.83
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 9.5149
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.14s
                      Time elapsed: 00:11:40
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 46312 steps/s (collection: 2.029s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 44.9346
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 33.4021
                       Mean reward: 45.91
               Mean episode length: 91.92
    Episode_Reward/reaching_object: 0.2628
     Episode_Reward/lifting_object: 9.5130
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.12s
                      Time elapsed: 00:11:42
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 45427 steps/s (collection: 2.061s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 43.1758
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 33.4081
                       Mean reward: 53.03
               Mean episode length: 99.80
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 9.9913
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.16s
                      Time elapsed: 00:11:44
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 46357 steps/s (collection: 2.025s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 48.1911
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.4150
                       Mean reward: 50.55
               Mean episode length: 94.56
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 9.9171
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.12s
                      Time elapsed: 00:11:46
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 45018 steps/s (collection: 2.075s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 47.3098
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 33.4227
                       Mean reward: 44.02
               Mean episode length: 87.74
    Episode_Reward/reaching_object: 0.2671
     Episode_Reward/lifting_object: 9.6407
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.18s
                      Time elapsed: 00:11:49
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 45421 steps/s (collection: 2.054s, learning 0.111s)
             Mean action noise std: 1.58
          Mean value_function loss: 45.8146
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 33.4299
                       Mean reward: 48.07
               Mean episode length: 93.43
    Episode_Reward/reaching_object: 0.2695
     Episode_Reward/lifting_object: 10.1358
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 42.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.16s
                      Time elapsed: 00:11:51
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18457 steps/s (collection: 5.205s, learning 0.122s)
             Mean action noise std: 1.58
          Mean value_function loss: 46.1246
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 33.4383
                       Mean reward: 56.88
               Mean episode length: 94.68
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 9.9254
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 44.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.33s
                      Time elapsed: 00:11:56
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14592 steps/s (collection: 6.612s, learning 0.125s)
             Mean action noise std: 1.58
          Mean value_function loss: 46.3635
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 33.4409
                       Mean reward: 48.57
               Mean episode length: 85.87
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: 9.8463
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 43.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.74s
                      Time elapsed: 00:12:03
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14240 steps/s (collection: 6.781s, learning 0.122s)
             Mean action noise std: 1.58
          Mean value_function loss: 45.4403
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 33.4467
                       Mean reward: 55.46
               Mean episode length: 96.82
    Episode_Reward/reaching_object: 0.2641
     Episode_Reward/lifting_object: 9.9722
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 44.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.90s
                      Time elapsed: 00:12:10
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14422 steps/s (collection: 6.699s, learning 0.118s)
             Mean action noise std: 1.58
          Mean value_function loss: 45.9312
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 33.4501
                       Mean reward: 50.68
               Mean episode length: 90.48
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 9.9927
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.82s
                      Time elapsed: 00:12:17
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14176 steps/s (collection: 6.815s, learning 0.120s)
             Mean action noise std: 1.58
          Mean value_function loss: 49.1024
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 33.4549
                       Mean reward: 53.26
               Mean episode length: 88.22
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 10.1716
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.93s
                      Time elapsed: 00:12:24
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14392 steps/s (collection: 6.713s, learning 0.117s)
             Mean action noise std: 1.58
          Mean value_function loss: 47.6813
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.4615
                       Mean reward: 53.56
               Mean episode length: 97.09
    Episode_Reward/reaching_object: 0.2650
     Episode_Reward/lifting_object: 10.2097
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.83s
                      Time elapsed: 00:12:30
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14262 steps/s (collection: 6.780s, learning 0.113s)
             Mean action noise std: 1.58
          Mean value_function loss: 47.2296
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 33.4668
                       Mean reward: 49.58
               Mean episode length: 89.71
    Episode_Reward/reaching_object: 0.2585
     Episode_Reward/lifting_object: 9.9575
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.89s
                      Time elapsed: 00:12:37
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14698 steps/s (collection: 6.577s, learning 0.111s)
             Mean action noise std: 1.58
          Mean value_function loss: 56.6240
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.4712
                       Mean reward: 51.56
               Mean episode length: 89.20
    Episode_Reward/reaching_object: 0.2596
     Episode_Reward/lifting_object: 10.1924
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.69s
                      Time elapsed: 00:12:44
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13308 steps/s (collection: 7.275s, learning 0.112s)
             Mean action noise std: 1.58
          Mean value_function loss: 47.9521
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 33.4780
                       Mean reward: 54.05
               Mean episode length: 92.79
    Episode_Reward/reaching_object: 0.2640
     Episode_Reward/lifting_object: 10.3309
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.39s
                      Time elapsed: 00:12:51
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 46251 steps/s (collection: 2.032s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 49.2553
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.4794
                       Mean reward: 43.95
               Mean episode length: 84.38
    Episode_Reward/reaching_object: 0.2627
     Episode_Reward/lifting_object: 10.2246
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.13s
                      Time elapsed: 00:12:53
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 46868 steps/s (collection: 1.996s, learning 0.101s)
             Mean action noise std: 1.58
          Mean value_function loss: 53.5394
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.4806
                       Mean reward: 51.52
               Mean episode length: 90.70
    Episode_Reward/reaching_object: 0.2623
     Episode_Reward/lifting_object: 10.1652
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.10s
                      Time elapsed: 00:12:56
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 47402 steps/s (collection: 1.982s, learning 0.092s)
             Mean action noise std: 1.58
          Mean value_function loss: 49.6339
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.4825
                       Mean reward: 53.68
               Mean episode length: 90.27
    Episode_Reward/reaching_object: 0.2652
     Episode_Reward/lifting_object: 10.2807
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.07s
                      Time elapsed: 00:12:58
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 45750 steps/s (collection: 2.059s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 48.5602
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 33.4852
                       Mean reward: 52.14
               Mean episode length: 86.82
    Episode_Reward/reaching_object: 0.2601
     Episode_Reward/lifting_object: 10.0845
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 47.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.15s
                      Time elapsed: 00:13:00
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 46967 steps/s (collection: 1.998s, learning 0.095s)
             Mean action noise std: 1.58
          Mean value_function loss: 48.6664
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.4849
                       Mean reward: 54.83
               Mean episode length: 89.09
    Episode_Reward/reaching_object: 0.2629
     Episode_Reward/lifting_object: 10.6046
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.09s
                      Time elapsed: 00:13:02
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.012s, learning 0.100s)
             Mean action noise std: 1.58
          Mean value_function loss: 46.4968
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.4911
                       Mean reward: 51.53
               Mean episode length: 84.83
    Episode_Reward/reaching_object: 0.2580
     Episode_Reward/lifting_object: 10.3654
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.11s
                      Time elapsed: 00:13:04
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 46805 steps/s (collection: 1.991s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 48.8012
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.4971
                       Mean reward: 53.55
               Mean episode length: 89.50
    Episode_Reward/reaching_object: 0.2567
     Episode_Reward/lifting_object: 10.2636
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.10s
                      Time elapsed: 00:13:06
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 47699 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 55.0861
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.5052
                       Mean reward: 47.12
               Mean episode length: 85.45
    Episode_Reward/reaching_object: 0.2646
     Episode_Reward/lifting_object: 10.5140
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.06s
                      Time elapsed: 00:13:08
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 47698 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 52.6571
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 33.5094
                       Mean reward: 55.80
               Mean episode length: 88.70
    Episode_Reward/reaching_object: 0.2613
     Episode_Reward/lifting_object: 10.2373
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.06s
                      Time elapsed: 00:13:10
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 47098 steps/s (collection: 1.986s, learning 0.101s)
             Mean action noise std: 1.58
          Mean value_function loss: 50.4427
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.5162
                       Mean reward: 51.43
               Mean episode length: 89.67
    Episode_Reward/reaching_object: 0.2720
     Episode_Reward/lifting_object: 11.0522
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 47.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.09s
                      Time elapsed: 00:13:12
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 47371 steps/s (collection: 1.977s, learning 0.099s)
             Mean action noise std: 1.58
          Mean value_function loss: 54.5098
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 33.5244
                       Mean reward: 57.40
               Mean episode length: 88.75
    Episode_Reward/reaching_object: 0.2608
     Episode_Reward/lifting_object: 10.6283
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.08s
                      Time elapsed: 00:13:14
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 48165 steps/s (collection: 1.941s, learning 0.100s)
             Mean action noise std: 1.59
          Mean value_function loss: 49.7357
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.5305
                       Mean reward: 47.32
               Mean episode length: 79.01
    Episode_Reward/reaching_object: 0.2671
     Episode_Reward/lifting_object: 10.7041
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.04s
                      Time elapsed: 00:13:16
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 48369 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 53.3816
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 33.5408
                       Mean reward: 57.36
               Mean episode length: 85.57
    Episode_Reward/reaching_object: 0.2617
     Episode_Reward/lifting_object: 10.7900
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.03s
                      Time elapsed: 00:13:18
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48404 steps/s (collection: 1.936s, learning 0.095s)
             Mean action noise std: 1.59
          Mean value_function loss: 50.5267
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 33.5502
                       Mean reward: 61.86
               Mean episode length: 94.84
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 10.7259
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.03s
                      Time elapsed: 00:13:20
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 48245 steps/s (collection: 1.947s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 49.9068
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 33.5544
                       Mean reward: 56.00
               Mean episode length: 86.49
    Episode_Reward/reaching_object: 0.2538
     Episode_Reward/lifting_object: 10.5504
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.04s
                      Time elapsed: 00:13:23
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 47843 steps/s (collection: 1.965s, learning 0.090s)
             Mean action noise std: 1.59
          Mean value_function loss: 49.4217
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.5563
                       Mean reward: 55.99
               Mean episode length: 91.51
    Episode_Reward/reaching_object: 0.2639
     Episode_Reward/lifting_object: 10.6112
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.05s
                      Time elapsed: 00:13:25
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 48442 steps/s (collection: 1.944s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 52.1158
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 33.5585
                       Mean reward: 60.77
               Mean episode length: 94.49
    Episode_Reward/reaching_object: 0.2636
     Episode_Reward/lifting_object: 11.0744
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 45.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.03s
                      Time elapsed: 00:13:27
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 46827 steps/s (collection: 2.012s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 52.0036
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 33.5612
                       Mean reward: 59.58
               Mean episode length: 89.67
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 11.3929
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 47.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.10s
                      Time elapsed: 00:13:29
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 48511 steps/s (collection: 1.938s, learning 0.089s)
             Mean action noise std: 1.59
          Mean value_function loss: 56.0483
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 33.5665
                       Mean reward: 54.58
               Mean episode length: 90.36
    Episode_Reward/reaching_object: 0.2648
     Episode_Reward/lifting_object: 10.9656
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.03s
                      Time elapsed: 00:13:31
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 46893 steps/s (collection: 1.989s, learning 0.107s)
             Mean action noise std: 1.59
          Mean value_function loss: 53.7667
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.5686
                       Mean reward: 54.20
               Mean episode length: 90.81
    Episode_Reward/reaching_object: 0.2653
     Episode_Reward/lifting_object: 10.7401
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 47.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.10s
                      Time elapsed: 00:13:33
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 48022 steps/s (collection: 1.946s, learning 0.101s)
             Mean action noise std: 1.59
          Mean value_function loss: 56.2825
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.5733
                       Mean reward: 53.03
               Mean episode length: 90.53
    Episode_Reward/reaching_object: 0.2681
     Episode_Reward/lifting_object: 11.2232
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.05s
                      Time elapsed: 00:13:35
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 47570 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 57.6705
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 33.5795
                       Mean reward: 56.33
               Mean episode length: 87.91
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 10.9245
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.07s
                      Time elapsed: 00:13:37
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 48518 steps/s (collection: 1.940s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 57.1542
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 33.5853
                       Mean reward: 57.34
               Mean episode length: 91.59
    Episode_Reward/reaching_object: 0.2716
     Episode_Reward/lifting_object: 10.9266
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.03s
                      Time elapsed: 00:13:39
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 47805 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 1.59
          Mean value_function loss: 54.2626
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 33.5901
                       Mean reward: 57.28
               Mean episode length: 87.66
    Episode_Reward/reaching_object: 0.2727
     Episode_Reward/lifting_object: 11.3212
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.06s
                      Time elapsed: 00:13:41
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 47903 steps/s (collection: 1.956s, learning 0.096s)
             Mean action noise std: 1.59
          Mean value_function loss: 54.2452
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.5916
                       Mean reward: 57.82
               Mean episode length: 84.70
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 11.5896
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.05s
                      Time elapsed: 00:13:43
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 49051 steps/s (collection: 1.919s, learning 0.085s)
             Mean action noise std: 1.59
          Mean value_function loss: 56.1276
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 33.5953
                       Mean reward: 58.10
               Mean episode length: 89.04
    Episode_Reward/reaching_object: 0.2782
     Episode_Reward/lifting_object: 11.4735
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.00s
                      Time elapsed: 00:13:45
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.961s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 59.4555
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 33.5988
                       Mean reward: 57.07
               Mean episode length: 89.33
    Episode_Reward/reaching_object: 0.2859
     Episode_Reward/lifting_object: 11.6967
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.05s
                      Time elapsed: 00:13:47
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 48546 steps/s (collection: 1.937s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 57.9038
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 33.6019
                       Mean reward: 52.77
               Mean episode length: 83.18
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: 11.9712
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 45.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.02s
                      Time elapsed: 00:13:49
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 48112 steps/s (collection: 1.957s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 54.4970
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.6075
                       Mean reward: 59.99
               Mean episode length: 90.88
    Episode_Reward/reaching_object: 0.2819
     Episode_Reward/lifting_object: 11.8276
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.04s
                      Time elapsed: 00:13:51
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 48390 steps/s (collection: 1.946s, learning 0.085s)
             Mean action noise std: 1.59
          Mean value_function loss: 61.0306
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.6135
                       Mean reward: 56.87
               Mean episode length: 91.04
    Episode_Reward/reaching_object: 0.2916
     Episode_Reward/lifting_object: 11.8503
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.03s
                      Time elapsed: 00:13:53
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 47962 steps/s (collection: 1.963s, learning 0.086s)
             Mean action noise std: 1.59
          Mean value_function loss: 74.8284
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 33.6196
                       Mean reward: 59.69
               Mean episode length: 91.64
    Episode_Reward/reaching_object: 0.2875
     Episode_Reward/lifting_object: 11.9315
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.05s
                      Time elapsed: 00:13:55
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 47930 steps/s (collection: 1.962s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 63.2760
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.6240
                       Mean reward: 58.96
               Mean episode length: 94.20
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 11.4448
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 47.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.05s
                      Time elapsed: 00:13:57
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 48164 steps/s (collection: 1.952s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 55.0022
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 33.6295
                       Mean reward: 59.15
               Mean episode length: 93.55
    Episode_Reward/reaching_object: 0.2811
     Episode_Reward/lifting_object: 11.5660
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.04s
                      Time elapsed: 00:13:59
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 48244 steps/s (collection: 1.952s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 55.6632
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.6374
                       Mean reward: 56.77
               Mean episode length: 91.78
    Episode_Reward/reaching_object: 0.2809
     Episode_Reward/lifting_object: 11.3684
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.04s
                      Time elapsed: 00:14:01
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 47521 steps/s (collection: 1.958s, learning 0.111s)
             Mean action noise std: 1.60
          Mean value_function loss: 62.4885
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 33.6444
                       Mean reward: 62.64
               Mean episode length: 90.44
    Episode_Reward/reaching_object: 0.2932
     Episode_Reward/lifting_object: 12.1097
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 42.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.07s
                      Time elapsed: 00:14:03
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 46732 steps/s (collection: 1.991s, learning 0.113s)
             Mean action noise std: 1.60
          Mean value_function loss: 70.3479
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 33.6491
                       Mean reward: 67.26
               Mean episode length: 96.78
    Episode_Reward/reaching_object: 0.2893
     Episode_Reward/lifting_object: 12.0067
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 43.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.10s
                      Time elapsed: 00:14:06
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 47868 steps/s (collection: 1.951s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 66.6554
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.6534
                       Mean reward: 61.60
               Mean episode length: 92.91
    Episode_Reward/reaching_object: 0.2977
     Episode_Reward/lifting_object: 12.0689
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 42.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.05s
                      Time elapsed: 00:14:08
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 47546 steps/s (collection: 1.974s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 65.7533
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 33.6595
                       Mean reward: 69.00
               Mean episode length: 89.65
    Episode_Reward/reaching_object: 0.2970
     Episode_Reward/lifting_object: 12.3828
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.07s
                      Time elapsed: 00:14:10
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 44831 steps/s (collection: 2.086s, learning 0.106s)
             Mean action noise std: 1.60
          Mean value_function loss: 65.9556
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 33.6610
                       Mean reward: 58.57
               Mean episode length: 92.47
    Episode_Reward/reaching_object: 0.2965
     Episode_Reward/lifting_object: 11.9628
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.19s
                      Time elapsed: 00:14:12
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 45450 steps/s (collection: 2.067s, learning 0.095s)
             Mean action noise std: 1.60
          Mean value_function loss: 64.4124
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 33.6616
                       Mean reward: 65.35
               Mean episode length: 101.36
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 12.8747
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.16s
                      Time elapsed: 00:14:14
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 46856 steps/s (collection: 1.998s, learning 0.100s)
             Mean action noise std: 1.60
          Mean value_function loss: 59.9011
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.6631
                       Mean reward: 56.71
               Mean episode length: 84.47
    Episode_Reward/reaching_object: 0.2977
     Episode_Reward/lifting_object: 12.1772
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.10s
                      Time elapsed: 00:14:16
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 46383 steps/s (collection: 2.035s, learning 0.084s)
             Mean action noise std: 1.60
          Mean value_function loss: 65.2658
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 33.6685
                       Mean reward: 65.59
               Mean episode length: 97.93
    Episode_Reward/reaching_object: 0.3054
     Episode_Reward/lifting_object: 12.3729
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.12s
                      Time elapsed: 00:14:18
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 47386 steps/s (collection: 1.988s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 64.4629
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 33.6717
                       Mean reward: 64.30
               Mean episode length: 94.81
    Episode_Reward/reaching_object: 0.3022
     Episode_Reward/lifting_object: 12.4603
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 43.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.07s
                      Time elapsed: 00:14:20
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 47122 steps/s (collection: 1.985s, learning 0.101s)
             Mean action noise std: 1.60
          Mean value_function loss: 59.4550
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.6740
                       Mean reward: 64.48
               Mean episode length: 91.60
    Episode_Reward/reaching_object: 0.3099
     Episode_Reward/lifting_object: 12.9635
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.09s
                      Time elapsed: 00:14:22
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 46999 steps/s (collection: 1.983s, learning 0.109s)
             Mean action noise std: 1.60
          Mean value_function loss: 60.7621
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 33.6781
                       Mean reward: 66.60
               Mean episode length: 93.76
    Episode_Reward/reaching_object: 0.3048
     Episode_Reward/lifting_object: 12.6465
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 42.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.09s
                      Time elapsed: 00:14:25
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 45750 steps/s (collection: 1.998s, learning 0.151s)
             Mean action noise std: 1.60
          Mean value_function loss: 65.6199
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 33.6821
                       Mean reward: 64.47
               Mean episode length: 92.24
    Episode_Reward/reaching_object: 0.3038
     Episode_Reward/lifting_object: 12.8957
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 42.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.15s
                      Time elapsed: 00:14:27
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 46678 steps/s (collection: 2.020s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 63.7225
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 33.6848
                       Mean reward: 71.81
               Mean episode length: 95.84
    Episode_Reward/reaching_object: 0.3013
     Episode_Reward/lifting_object: 13.1350
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.11s
                      Time elapsed: 00:14:29
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 46466 steps/s (collection: 2.015s, learning 0.100s)
             Mean action noise std: 1.60
          Mean value_function loss: 64.2924
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 33.6896
                       Mean reward: 68.90
               Mean episode length: 98.28
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 13.6173
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.12s
                      Time elapsed: 00:14:31
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 47113 steps/s (collection: 1.990s, learning 0.097s)
             Mean action noise std: 1.60
          Mean value_function loss: 68.1988
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.6957
                       Mean reward: 63.72
               Mean episode length: 91.75
    Episode_Reward/reaching_object: 0.3043
     Episode_Reward/lifting_object: 13.2070
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.09s
                      Time elapsed: 00:14:33
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 45699 steps/s (collection: 2.049s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 61.6055
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.7065
                       Mean reward: 65.26
               Mean episode length: 96.33
    Episode_Reward/reaching_object: 0.2988
     Episode_Reward/lifting_object: 13.0597
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.15s
                      Time elapsed: 00:14:35
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 44683 steps/s (collection: 2.106s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 62.8970
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 33.7129
                       Mean reward: 69.14
               Mean episode length: 92.51
    Episode_Reward/reaching_object: 0.3036
     Episode_Reward/lifting_object: 13.3698
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.20s
                      Time elapsed: 00:14:37
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 46555 steps/s (collection: 2.025s, learning 0.087s)
             Mean action noise std: 1.60
          Mean value_function loss: 66.4478
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.7164
                       Mean reward: 62.76
               Mean episode length: 87.37
    Episode_Reward/reaching_object: 0.3074
     Episode_Reward/lifting_object: 13.5022
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.11s
                      Time elapsed: 00:14:39
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 45542 steps/s (collection: 2.040s, learning 0.118s)
             Mean action noise std: 1.61
          Mean value_function loss: 66.1424
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.7223
                       Mean reward: 71.77
               Mean episode length: 97.27
    Episode_Reward/reaching_object: 0.2977
     Episode_Reward/lifting_object: 13.2321
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.16s
                      Time elapsed: 00:14:42
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 44323 steps/s (collection: 2.086s, learning 0.132s)
             Mean action noise std: 1.61
          Mean value_function loss: 64.3388
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.7289
                       Mean reward: 67.17
               Mean episode length: 90.15
    Episode_Reward/reaching_object: 0.2925
     Episode_Reward/lifting_object: 12.9831
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.22s
                      Time elapsed: 00:14:44
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 47576 steps/s (collection: 1.979s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 60.8881
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 33.7340
                       Mean reward: 65.65
               Mean episode length: 90.71
    Episode_Reward/reaching_object: 0.3086
     Episode_Reward/lifting_object: 13.7855
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.07s
                      Time elapsed: 00:14:46
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 46869 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 65.5539
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.7383
                       Mean reward: 64.95
               Mean episode length: 89.27
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 13.5893
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.10s
                      Time elapsed: 00:14:48
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 46451 steps/s (collection: 1.996s, learning 0.120s)
             Mean action noise std: 1.61
          Mean value_function loss: 62.9005
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 33.7434
                       Mean reward: 70.75
               Mean episode length: 95.72
    Episode_Reward/reaching_object: 0.3038
     Episode_Reward/lifting_object: 13.5826
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.12s
                      Time elapsed: 00:14:50
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 46918 steps/s (collection: 2.003s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 67.0910
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 33.7492
                       Mean reward: 65.46
               Mean episode length: 85.50
    Episode_Reward/reaching_object: 0.3007
     Episode_Reward/lifting_object: 13.5474
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.10s
                      Time elapsed: 00:14:52
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 46904 steps/s (collection: 2.004s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 62.1840
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.7547
                       Mean reward: 73.12
               Mean episode length: 94.24
    Episode_Reward/reaching_object: 0.3089
     Episode_Reward/lifting_object: 14.1440
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.10s
                      Time elapsed: 00:14:54
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 46798 steps/s (collection: 2.014s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 66.6733
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 33.7608
                       Mean reward: 68.68
               Mean episode length: 92.47
    Episode_Reward/reaching_object: 0.2966
     Episode_Reward/lifting_object: 13.7553
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.10s
                      Time elapsed: 00:14:56
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 45993 steps/s (collection: 2.039s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 65.2738
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 33.7673
                       Mean reward: 69.58
               Mean episode length: 91.58
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 13.9091
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.14s
                      Time elapsed: 00:14:59
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 46135 steps/s (collection: 2.026s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 66.3087
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 33.7706
                       Mean reward: 73.39
               Mean episode length: 95.89
    Episode_Reward/reaching_object: 0.2963
     Episode_Reward/lifting_object: 13.5919
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.13s
                      Time elapsed: 00:15:01
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 44171 steps/s (collection: 2.091s, learning 0.134s)
             Mean action noise std: 1.61
          Mean value_function loss: 63.3710
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 33.7714
                       Mean reward: 76.72
               Mean episode length: 100.90
    Episode_Reward/reaching_object: 0.3103
     Episode_Reward/lifting_object: 14.5084
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.23s
                      Time elapsed: 00:15:03
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 44227 steps/s (collection: 2.117s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 68.8117
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 33.7721
                       Mean reward: 76.38
               Mean episode length: 96.09
    Episode_Reward/reaching_object: 0.2986
     Episode_Reward/lifting_object: 13.8549
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.22s
                      Time elapsed: 00:15:05
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 42039 steps/s (collection: 2.245s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 71.3817
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 33.7727
                       Mean reward: 79.41
               Mean episode length: 106.64
    Episode_Reward/reaching_object: 0.3071
     Episode_Reward/lifting_object: 14.4278
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.34s
                      Time elapsed: 00:15:07
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 46150 steps/s (collection: 2.034s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 72.3174
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 33.7728
                       Mean reward: 75.42
               Mean episode length: 95.97
    Episode_Reward/reaching_object: 0.3035
     Episode_Reward/lifting_object: 14.0876
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 42.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.13s
                      Time elapsed: 00:15:10
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 45200 steps/s (collection: 2.058s, learning 0.117s)
             Mean action noise std: 1.61
          Mean value_function loss: 64.4895
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 33.7730
                       Mean reward: 70.86
               Mean episode length: 95.39
    Episode_Reward/reaching_object: 0.3080
     Episode_Reward/lifting_object: 14.1603
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.17s
                      Time elapsed: 00:15:12
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 42659 steps/s (collection: 2.200s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 66.1602
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 33.7731
                       Mean reward: 76.78
               Mean episode length: 98.73
    Episode_Reward/reaching_object: 0.3011
     Episode_Reward/lifting_object: 13.7780
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.30s
                      Time elapsed: 00:15:14
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 46023 steps/s (collection: 2.017s, learning 0.119s)
             Mean action noise std: 1.61
          Mean value_function loss: 63.4791
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 33.7733
                       Mean reward: 73.11
               Mean episode length: 97.49
    Episode_Reward/reaching_object: 0.3106
     Episode_Reward/lifting_object: 14.5745
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 39.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.14s
                      Time elapsed: 00:15:16
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 43873 steps/s (collection: 2.118s, learning 0.123s)
             Mean action noise std: 1.61
          Mean value_function loss: 72.3513
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 33.7742
                       Mean reward: 75.72
               Mean episode length: 105.17
    Episode_Reward/reaching_object: 0.3130
     Episode_Reward/lifting_object: 14.5111
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 44.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.24s
                      Time elapsed: 00:15:18
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 46638 steps/s (collection: 2.001s, learning 0.107s)
             Mean action noise std: 1.61
          Mean value_function loss: 78.6367
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 33.7760
                       Mean reward: 71.28
               Mean episode length: 94.66
    Episode_Reward/reaching_object: 0.3123
     Episode_Reward/lifting_object: 14.1074
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.11s
                      Time elapsed: 00:15:21
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 46960 steps/s (collection: 2.004s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 70.1505
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.7783
                       Mean reward: 69.80
               Mean episode length: 91.79
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 14.2950
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 39.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.09s
                      Time elapsed: 00:15:23
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 46608 steps/s (collection: 2.012s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 80.2682
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.7807
                       Mean reward: 76.22
               Mean episode length: 103.51
    Episode_Reward/reaching_object: 0.3161
     Episode_Reward/lifting_object: 14.6924
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.11s
                      Time elapsed: 00:15:25
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 46112 steps/s (collection: 1.996s, learning 0.136s)
             Mean action noise std: 1.61
          Mean value_function loss: 72.6706
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 33.7856
                       Mean reward: 73.32
               Mean episode length: 92.75
    Episode_Reward/reaching_object: 0.3283
     Episode_Reward/lifting_object: 15.2208
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.13s
                      Time elapsed: 00:15:27
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 46519 steps/s (collection: 2.017s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 87.4696
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.7888
                       Mean reward: 68.65
               Mean episode length: 93.16
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: 14.8115
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 42.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.11s
                      Time elapsed: 00:15:29
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 45044 steps/s (collection: 2.082s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 78.7685
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.7933
                       Mean reward: 77.63
               Mean episode length: 97.23
    Episode_Reward/reaching_object: 0.3197
     Episode_Reward/lifting_object: 14.5427
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.18s
                      Time elapsed: 00:15:31
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 45271 steps/s (collection: 2.081s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 85.2037
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.8004
                       Mean reward: 72.75
               Mean episode length: 95.79
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 14.7002
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 41.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.17s
                      Time elapsed: 00:15:33
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 45999 steps/s (collection: 2.054s, learning 0.083s)
             Mean action noise std: 1.61
          Mean value_function loss: 75.4540
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 33.8048
                       Mean reward: 81.42
               Mean episode length: 107.16
    Episode_Reward/reaching_object: 0.3343
     Episode_Reward/lifting_object: 15.3142
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.14s
                      Time elapsed: 00:15:35
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 43347 steps/s (collection: 2.143s, learning 0.124s)
             Mean action noise std: 1.61
          Mean value_function loss: 74.0211
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 33.8058
                       Mean reward: 73.88
               Mean episode length: 96.82
    Episode_Reward/reaching_object: 0.3245
     Episode_Reward/lifting_object: 14.9263
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.27s
                      Time elapsed: 00:15:38
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 45122 steps/s (collection: 2.074s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 71.5532
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 33.8073
                       Mean reward: 78.43
               Mean episode length: 104.87
    Episode_Reward/reaching_object: 0.3298
     Episode_Reward/lifting_object: 15.1866
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.18s
                      Time elapsed: 00:15:40
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 46625 steps/s (collection: 2.021s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 72.2860
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 33.8106
                       Mean reward: 80.21
               Mean episode length: 105.77
    Episode_Reward/reaching_object: 0.3322
     Episode_Reward/lifting_object: 15.4079
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.11s
                      Time elapsed: 00:15:42
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 45907 steps/s (collection: 2.041s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 76.1822
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.8137
                       Mean reward: 68.00
               Mean episode length: 93.04
    Episode_Reward/reaching_object: 0.3303
     Episode_Reward/lifting_object: 15.4162
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.14s
                      Time elapsed: 00:15:44
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 43283 steps/s (collection: 2.123s, learning 0.149s)
             Mean action noise std: 1.61
          Mean value_function loss: 74.8350
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.8170
                       Mean reward: 83.85
               Mean episode length: 101.47
    Episode_Reward/reaching_object: 0.3354
     Episode_Reward/lifting_object: 15.8662
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.27s
                      Time elapsed: 00:15:46
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 43658 steps/s (collection: 2.162s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 78.1114
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.8223
                       Mean reward: 82.05
               Mean episode length: 104.20
    Episode_Reward/reaching_object: 0.3462
     Episode_Reward/lifting_object: 16.2836
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.25s
                      Time elapsed: 00:15:49
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 42620 steps/s (collection: 2.189s, learning 0.118s)
             Mean action noise std: 1.62
          Mean value_function loss: 67.9602
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.8305
                       Mean reward: 83.80
               Mean episode length: 106.26
    Episode_Reward/reaching_object: 0.3411
     Episode_Reward/lifting_object: 16.1936
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 40.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.31s
                      Time elapsed: 00:15:51
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 44579 steps/s (collection: 2.100s, learning 0.105s)
             Mean action noise std: 1.62
          Mean value_function loss: 78.5704
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 33.8357
                       Mean reward: 82.22
               Mean episode length: 101.58
    Episode_Reward/reaching_object: 0.3349
     Episode_Reward/lifting_object: 15.9691
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.21s
                      Time elapsed: 00:15:53
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 44122 steps/s (collection: 2.118s, learning 0.110s)
             Mean action noise std: 1.62
          Mean value_function loss: 84.0145
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 33.8391
                       Mean reward: 86.27
               Mean episode length: 105.55
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: 16.4966
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.23s
                      Time elapsed: 00:15:55
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 42906 steps/s (collection: 2.186s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 70.6964
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 33.8407
                       Mean reward: 91.85
               Mean episode length: 106.68
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: 16.6165
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.29s
                      Time elapsed: 00:15:58
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 42909 steps/s (collection: 2.161s, learning 0.130s)
             Mean action noise std: 1.62
          Mean value_function loss: 76.2978
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 33.8433
                       Mean reward: 90.58
               Mean episode length: 109.40
    Episode_Reward/reaching_object: 0.3413
     Episode_Reward/lifting_object: 16.4925
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.29s
                      Time elapsed: 00:16:00
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 45404 steps/s (collection: 2.079s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 86.5248
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 33.8485
                       Mean reward: 91.12
               Mean episode length: 112.31
    Episode_Reward/reaching_object: 0.3474
     Episode_Reward/lifting_object: 16.4452
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.17s
                      Time elapsed: 00:16:02
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 43059 steps/s (collection: 2.114s, learning 0.169s)
             Mean action noise std: 1.62
          Mean value_function loss: 79.3045
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.8521
                       Mean reward: 89.47
               Mean episode length: 108.41
    Episode_Reward/reaching_object: 0.3504
     Episode_Reward/lifting_object: 16.5004
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.28s
                      Time elapsed: 00:16:04
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 45456 steps/s (collection: 2.072s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 83.3009
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.8556
                       Mean reward: 87.60
               Mean episode length: 109.75
    Episode_Reward/reaching_object: 0.3555
     Episode_Reward/lifting_object: 16.9141
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.16s
                      Time elapsed: 00:16:07
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 44431 steps/s (collection: 2.084s, learning 0.129s)
             Mean action noise std: 1.62
          Mean value_function loss: 83.3989
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 33.8569
                       Mean reward: 80.04
               Mean episode length: 100.77
    Episode_Reward/reaching_object: 0.3631
     Episode_Reward/lifting_object: 16.9677
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.21s
                      Time elapsed: 00:16:09
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 44951 steps/s (collection: 2.095s, learning 0.092s)
             Mean action noise std: 1.62
          Mean value_function loss: 88.2321
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.8599
                       Mean reward: 86.24
               Mean episode length: 108.25
    Episode_Reward/reaching_object: 0.3563
     Episode_Reward/lifting_object: 17.1752
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.19s
                      Time elapsed: 00:16:11
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 45029 steps/s (collection: 2.093s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 80.3494
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.8649
                       Mean reward: 87.72
               Mean episode length: 104.05
    Episode_Reward/reaching_object: 0.3612
     Episode_Reward/lifting_object: 16.8641
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.18s
                      Time elapsed: 00:16:13
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 44374 steps/s (collection: 2.126s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 83.2301
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 33.8684
                       Mean reward: 97.54
               Mean episode length: 113.48
    Episode_Reward/reaching_object: 0.3770
     Episode_Reward/lifting_object: 18.1270
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.22s
                      Time elapsed: 00:16:15
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 44164 steps/s (collection: 2.134s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 79.6211
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 33.8701
                       Mean reward: 87.73
               Mean episode length: 108.10
    Episode_Reward/reaching_object: 0.3684
     Episode_Reward/lifting_object: 17.1947
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.23s
                      Time elapsed: 00:16:18
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 41717 steps/s (collection: 2.226s, learning 0.130s)
             Mean action noise std: 1.62
          Mean value_function loss: 74.4818
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 33.8722
                       Mean reward: 92.73
               Mean episode length: 113.05
    Episode_Reward/reaching_object: 0.3814
     Episode_Reward/lifting_object: 18.3131
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.36s
                      Time elapsed: 00:16:20
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 45033 steps/s (collection: 2.071s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 76.9818
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.8766
                       Mean reward: 97.55
               Mean episode length: 116.79
    Episode_Reward/reaching_object: 0.3871
     Episode_Reward/lifting_object: 18.5787
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.18s
                      Time elapsed: 00:16:22
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 45292 steps/s (collection: 2.064s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 75.2597
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.8804
                       Mean reward: 92.16
               Mean episode length: 113.67
    Episode_Reward/reaching_object: 0.3878
     Episode_Reward/lifting_object: 18.5970
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.17s
                      Time elapsed: 00:16:24
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 44294 steps/s (collection: 2.083s, learning 0.136s)
             Mean action noise std: 1.62
          Mean value_function loss: 83.1651
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 33.8849
                       Mean reward: 96.45
               Mean episode length: 115.69
    Episode_Reward/reaching_object: 0.3982
     Episode_Reward/lifting_object: 19.5007
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.22s
                      Time elapsed: 00:16:27
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 44980 steps/s (collection: 2.096s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 78.1868
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.8936
                       Mean reward: 107.60
               Mean episode length: 124.89
    Episode_Reward/reaching_object: 0.3969
     Episode_Reward/lifting_object: 19.3008
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.19s
                      Time elapsed: 00:16:29
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 45303 steps/s (collection: 2.073s, learning 0.097s)
             Mean action noise std: 1.62
          Mean value_function loss: 77.9865
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.8981
                       Mean reward: 100.55
               Mean episode length: 113.60
    Episode_Reward/reaching_object: 0.3939
     Episode_Reward/lifting_object: 19.3166
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.17s
                      Time elapsed: 00:16:31
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 45028 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 1.62
          Mean value_function loss: 72.9575
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 33.9007
                       Mean reward: 83.23
               Mean episode length: 101.15
    Episode_Reward/reaching_object: 0.3907
     Episode_Reward/lifting_object: 19.3731
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.18s
                      Time elapsed: 00:16:33
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 46120 steps/s (collection: 2.037s, learning 0.095s)
             Mean action noise std: 1.62
          Mean value_function loss: 80.2469
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.9032
                       Mean reward: 94.06
               Mean episode length: 112.29
    Episode_Reward/reaching_object: 0.4001
     Episode_Reward/lifting_object: 19.2131
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.13s
                      Time elapsed: 00:16:35
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 43003 steps/s (collection: 2.181s, learning 0.104s)
             Mean action noise std: 1.62
          Mean value_function loss: 74.7795
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.9049
                       Mean reward: 109.22
               Mean episode length: 125.22
    Episode_Reward/reaching_object: 0.4145
     Episode_Reward/lifting_object: 20.2177
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.29s
                      Time elapsed: 00:16:38
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 45092 steps/s (collection: 2.089s, learning 0.091s)
             Mean action noise std: 1.63
          Mean value_function loss: 79.9005
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 33.9074
                       Mean reward: 101.52
               Mean episode length: 121.43
    Episode_Reward/reaching_object: 0.4168
     Episode_Reward/lifting_object: 20.0853
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.18s
                      Time elapsed: 00:16:40
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 45146 steps/s (collection: 2.067s, learning 0.110s)
             Mean action noise std: 1.63
          Mean value_function loss: 78.1795
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 33.9094
                       Mean reward: 94.85
               Mean episode length: 116.14
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: 20.2613
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.18s
                      Time elapsed: 00:16:42
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 45255 steps/s (collection: 2.080s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 77.8041
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.9115
                       Mean reward: 105.24
               Mean episode length: 115.21
    Episode_Reward/reaching_object: 0.4154
     Episode_Reward/lifting_object: 20.3269
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.17s
                      Time elapsed: 00:16:44
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 42439 steps/s (collection: 2.217s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 79.1626
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 33.9145
                       Mean reward: 119.77
               Mean episode length: 139.93
    Episode_Reward/reaching_object: 0.4391
     Episode_Reward/lifting_object: 21.2847
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.32s
                      Time elapsed: 00:16:46
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 44597 steps/s (collection: 2.115s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 85.2354
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.9149
                       Mean reward: 108.30
               Mean episode length: 121.38
    Episode_Reward/reaching_object: 0.4232
     Episode_Reward/lifting_object: 20.4702
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.20s
                      Time elapsed: 00:16:49
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 43422 steps/s (collection: 2.169s, learning 0.094s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.7341
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 33.9164
                       Mean reward: 103.20
               Mean episode length: 118.84
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 20.9933
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.26s
                      Time elapsed: 00:16:51
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 45232 steps/s (collection: 2.054s, learning 0.119s)
             Mean action noise std: 1.63
          Mean value_function loss: 76.2960
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 33.9168
                       Mean reward: 108.21
               Mean episode length: 122.50
    Episode_Reward/reaching_object: 0.4273
     Episode_Reward/lifting_object: 21.1769
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.17s
                      Time elapsed: 00:16:53
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 46071 steps/s (collection: 2.036s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 95.3768
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 33.9170
                       Mean reward: 97.71
               Mean episode length: 116.61
    Episode_Reward/reaching_object: 0.4390
     Episode_Reward/lifting_object: 21.7903
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.13s
                      Time elapsed: 00:16:55
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 44053 steps/s (collection: 2.131s, learning 0.101s)
             Mean action noise std: 1.63
          Mean value_function loss: 84.1386
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.9168
                       Mean reward: 118.58
               Mean episode length: 134.87
    Episode_Reward/reaching_object: 0.4414
     Episode_Reward/lifting_object: 22.0691
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.23s
                      Time elapsed: 00:16:57
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 44792 steps/s (collection: 2.088s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 83.2441
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.9169
                       Mean reward: 105.48
               Mean episode length: 123.67
    Episode_Reward/reaching_object: 0.4460
     Episode_Reward/lifting_object: 21.9871
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.19s
                      Time elapsed: 00:17:00
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 45131 steps/s (collection: 2.077s, learning 0.102s)
             Mean action noise std: 1.63
          Mean value_function loss: 77.4748
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.9173
                       Mean reward: 119.74
               Mean episode length: 130.44
    Episode_Reward/reaching_object: 0.4553
     Episode_Reward/lifting_object: 23.2255
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.18s
                      Time elapsed: 00:17:02
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 44534 steps/s (collection: 2.095s, learning 0.113s)
             Mean action noise std: 1.63
          Mean value_function loss: 83.2073
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.9178
                       Mean reward: 116.21
               Mean episode length: 128.91
    Episode_Reward/reaching_object: 0.4479
     Episode_Reward/lifting_object: 22.6496
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.21s
                      Time elapsed: 00:17:04
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 43122 steps/s (collection: 2.164s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 85.8877
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 33.9168
                       Mean reward: 125.54
               Mean episode length: 139.81
    Episode_Reward/reaching_object: 0.4638
     Episode_Reward/lifting_object: 23.6750
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.28s
                      Time elapsed: 00:17:06
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 43163 steps/s (collection: 2.150s, learning 0.127s)
             Mean action noise std: 1.63
          Mean value_function loss: 76.8769
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.9161
                       Mean reward: 128.88
               Mean episode length: 136.18
    Episode_Reward/reaching_object: 0.4429
     Episode_Reward/lifting_object: 23.1858
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.28s
                      Time elapsed: 00:17:09
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 44576 steps/s (collection: 2.112s, learning 0.094s)
             Mean action noise std: 1.63
          Mean value_function loss: 89.5487
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 33.9171
                       Mean reward: 125.24
               Mean episode length: 133.45
    Episode_Reward/reaching_object: 0.4490
     Episode_Reward/lifting_object: 23.7020
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.21s
                      Time elapsed: 00:17:11
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 43746 steps/s (collection: 2.133s, learning 0.114s)
             Mean action noise std: 1.63
          Mean value_function loss: 79.1569
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 33.9185
                       Mean reward: 110.75
               Mean episode length: 123.37
    Episode_Reward/reaching_object: 0.4608
     Episode_Reward/lifting_object: 24.4722
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.25s
                      Time elapsed: 00:17:13
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 39984 steps/s (collection: 2.258s, learning 0.200s)
             Mean action noise std: 1.63
          Mean value_function loss: 85.4974
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 33.9186
                       Mean reward: 130.27
               Mean episode length: 142.06
    Episode_Reward/reaching_object: 0.4526
     Episode_Reward/lifting_object: 23.6143
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.46s
                      Time elapsed: 00:17:15
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 43791 steps/s (collection: 2.157s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 84.2557
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 33.9187
                       Mean reward: 114.79
               Mean episode length: 129.91
    Episode_Reward/reaching_object: 0.4466
     Episode_Reward/lifting_object: 23.3016
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.24s
                      Time elapsed: 00:17:18
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 44011 steps/s (collection: 2.134s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 83.7435
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.9191
                       Mean reward: 121.63
               Mean episode length: 135.04
    Episode_Reward/reaching_object: 0.4498
     Episode_Reward/lifting_object: 23.7898
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.23s
                      Time elapsed: 00:17:20
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 41143 steps/s (collection: 2.254s, learning 0.135s)
             Mean action noise std: 1.63
          Mean value_function loss: 78.4401
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.9208
                       Mean reward: 128.27
               Mean episode length: 136.83
    Episode_Reward/reaching_object: 0.4628
     Episode_Reward/lifting_object: 24.2005
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.39s
                      Time elapsed: 00:17:22
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 39603 steps/s (collection: 2.274s, learning 0.208s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.7762
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 33.9234
                       Mean reward: 125.89
               Mean episode length: 134.87
    Episode_Reward/reaching_object: 0.4703
     Episode_Reward/lifting_object: 24.9738
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.48s
                      Time elapsed: 00:17:25
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 43486 steps/s (collection: 2.166s, learning 0.094s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.9614
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 33.9273
                       Mean reward: 118.75
               Mean episode length: 131.66
    Episode_Reward/reaching_object: 0.4723
     Episode_Reward/lifting_object: 24.5711
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.26s
                      Time elapsed: 00:17:27
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 45196 steps/s (collection: 2.065s, learning 0.110s)
             Mean action noise std: 1.63
          Mean value_function loss: 95.2513
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 33.9294
                       Mean reward: 117.62
               Mean episode length: 129.86
    Episode_Reward/reaching_object: 0.4723
     Episode_Reward/lifting_object: 24.7915
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.18s
                      Time elapsed: 00:17:29
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 45238 steps/s (collection: 2.076s, learning 0.097s)
             Mean action noise std: 1.63
          Mean value_function loss: 84.4162
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.9305
                       Mean reward: 132.76
               Mean episode length: 134.19
    Episode_Reward/reaching_object: 0.4718
     Episode_Reward/lifting_object: 25.2096
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.17s
                      Time elapsed: 00:17:31
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 43526 steps/s (collection: 2.161s, learning 0.097s)
             Mean action noise std: 1.63
          Mean value_function loss: 87.3058
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 33.9343
                       Mean reward: 127.16
               Mean episode length: 134.59
    Episode_Reward/reaching_object: 0.4706
     Episode_Reward/lifting_object: 25.3304
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.26s
                      Time elapsed: 00:17:34
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 43679 steps/s (collection: 2.151s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 90.0030
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 33.9398
                       Mean reward: 139.24
               Mean episode length: 148.58
    Episode_Reward/reaching_object: 0.4700
     Episode_Reward/lifting_object: 25.2672
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.25s
                      Time elapsed: 00:17:36
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 43167 steps/s (collection: 2.160s, learning 0.118s)
             Mean action noise std: 1.63
          Mean value_function loss: 92.5030
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.9437
                       Mean reward: 120.60
               Mean episode length: 136.53
    Episode_Reward/reaching_object: 0.5042
     Episode_Reward/lifting_object: 26.7549
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.28s
                      Time elapsed: 00:17:38
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 44123 steps/s (collection: 2.121s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 88.4765
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 33.9488
                       Mean reward: 137.21
               Mean episode length: 147.04
    Episode_Reward/reaching_object: 0.4947
     Episode_Reward/lifting_object: 25.9505
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.23s
                      Time elapsed: 00:17:40
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 44400 steps/s (collection: 2.114s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 84.6879
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 33.9506
                       Mean reward: 131.26
               Mean episode length: 140.40
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: 26.0767
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.21s
                      Time elapsed: 00:17:43
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 44592 steps/s (collection: 2.112s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 90.5552
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 33.9502
                       Mean reward: 137.18
               Mean episode length: 146.49
    Episode_Reward/reaching_object: 0.4935
     Episode_Reward/lifting_object: 26.1873
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.20s
                      Time elapsed: 00:17:45
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 44571 steps/s (collection: 2.116s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 89.5603
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 33.9494
                       Mean reward: 134.43
               Mean episode length: 149.20
    Episode_Reward/reaching_object: 0.5012
     Episode_Reward/lifting_object: 26.5994
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.21s
                      Time elapsed: 00:17:47
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 42273 steps/s (collection: 2.128s, learning 0.198s)
             Mean action noise std: 1.63
          Mean value_function loss: 86.1349
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 33.9488
                       Mean reward: 138.96
               Mean episode length: 152.49
    Episode_Reward/reaching_object: 0.5215
     Episode_Reward/lifting_object: 27.8660
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.33s
                      Time elapsed: 00:17:49
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 44881 steps/s (collection: 2.093s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.1202
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 33.9491
                       Mean reward: 156.07
               Mean episode length: 157.93
    Episode_Reward/reaching_object: 0.5397
     Episode_Reward/lifting_object: 28.8960
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.19s
                      Time elapsed: 00:17:52
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 44579 steps/s (collection: 2.098s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 85.2819
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 33.9496
                       Mean reward: 137.85
               Mean episode length: 150.66
    Episode_Reward/reaching_object: 0.5332
     Episode_Reward/lifting_object: 28.4985
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.21s
                      Time elapsed: 00:17:54
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 44613 steps/s (collection: 2.105s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 88.3473
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 33.9518
                       Mean reward: 148.68
               Mean episode length: 151.14
    Episode_Reward/reaching_object: 0.5287
     Episode_Reward/lifting_object: 28.9882
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.20s
                      Time elapsed: 00:17:56
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 44144 steps/s (collection: 2.084s, learning 0.143s)
             Mean action noise std: 1.63
          Mean value_function loss: 93.7320
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 33.9559
                       Mean reward: 149.95
               Mean episode length: 154.42
    Episode_Reward/reaching_object: 0.5468
     Episode_Reward/lifting_object: 30.2719
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.23s
                      Time elapsed: 00:17:58
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 44156 steps/s (collection: 2.095s, learning 0.131s)
             Mean action noise std: 1.63
          Mean value_function loss: 86.9220
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 33.9578
                       Mean reward: 144.94
               Mean episode length: 155.69
    Episode_Reward/reaching_object: 0.5189
     Episode_Reward/lifting_object: 28.4163
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.23s
                      Time elapsed: 00:18:00
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 45033 steps/s (collection: 2.085s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 93.7727
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.9579
                       Mean reward: 139.90
               Mean episode length: 143.72
    Episode_Reward/reaching_object: 0.5104
     Episode_Reward/lifting_object: 28.2850
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.18s
                      Time elapsed: 00:18:03
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 44263 steps/s (collection: 2.113s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 95.5635
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 33.9581
                       Mean reward: 140.38
               Mean episode length: 145.53
    Episode_Reward/reaching_object: 0.5228
     Episode_Reward/lifting_object: 28.3223
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.22s
                      Time elapsed: 00:18:05
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 43853 steps/s (collection: 2.135s, learning 0.106s)
             Mean action noise std: 1.63
          Mean value_function loss: 86.4807
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.9590
                       Mean reward: 140.85
               Mean episode length: 153.52
    Episode_Reward/reaching_object: 0.5377
     Episode_Reward/lifting_object: 29.6602
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.24s
                      Time elapsed: 00:18:07
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 43882 steps/s (collection: 2.121s, learning 0.119s)
             Mean action noise std: 1.63
          Mean value_function loss: 92.4557
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.9598
                       Mean reward: 159.52
               Mean episode length: 161.95
    Episode_Reward/reaching_object: 0.5510
     Episode_Reward/lifting_object: 30.8873
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.24s
                      Time elapsed: 00:18:09
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 42339 steps/s (collection: 2.157s, learning 0.164s)
             Mean action noise std: 1.63
          Mean value_function loss: 87.3525
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 33.9598
                       Mean reward: 132.38
               Mean episode length: 144.61
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: 28.5476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.32s
                      Time elapsed: 00:18:12
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 43536 steps/s (collection: 2.162s, learning 0.096s)
             Mean action noise std: 1.63
          Mean value_function loss: 88.6686
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 33.9594
                       Mean reward: 132.29
               Mean episode length: 134.29
    Episode_Reward/reaching_object: 0.5249
     Episode_Reward/lifting_object: 29.7740
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.26s
                      Time elapsed: 00:18:14
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 44986 steps/s (collection: 2.092s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 84.8718
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.9639
                       Mean reward: 132.57
               Mean episode length: 136.54
    Episode_Reward/reaching_object: 0.5398
     Episode_Reward/lifting_object: 30.1182
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.19s
                      Time elapsed: 00:18:16
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 43549 steps/s (collection: 2.119s, learning 0.138s)
             Mean action noise std: 1.63
          Mean value_function loss: 91.5401
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 33.9675
                       Mean reward: 162.34
               Mean episode length: 160.03
    Episode_Reward/reaching_object: 0.5453
     Episode_Reward/lifting_object: 30.8423
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.26s
                      Time elapsed: 00:18:18
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 43220 steps/s (collection: 2.175s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 88.3392
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 33.9684
                       Mean reward: 155.95
               Mean episode length: 145.90
    Episode_Reward/reaching_object: 0.5359
     Episode_Reward/lifting_object: 30.4638
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.27s
                      Time elapsed: 00:18:21
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 44651 steps/s (collection: 2.095s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.0360
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.9691
                       Mean reward: 153.85
               Mean episode length: 150.79
    Episode_Reward/reaching_object: 0.5525
     Episode_Reward/lifting_object: 31.0952
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.20s
                      Time elapsed: 00:18:23
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 44510 steps/s (collection: 2.119s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 81.6163
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 33.9696
                       Mean reward: 156.97
               Mean episode length: 157.19
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 30.8509
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.21s
                      Time elapsed: 00:18:25
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 40742 steps/s (collection: 2.259s, learning 0.154s)
             Mean action noise std: 1.63
          Mean value_function loss: 87.5457
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 33.9697
                       Mean reward: 130.95
               Mean episode length: 139.35
    Episode_Reward/reaching_object: 0.5666
     Episode_Reward/lifting_object: 32.1776
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.41s
                      Time elapsed: 00:18:27
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 44305 steps/s (collection: 2.126s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 79.6703
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 33.9697
                       Mean reward: 155.21
               Mean episode length: 156.65
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 33.7118
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.22s
                      Time elapsed: 00:18:30
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 41269 steps/s (collection: 2.271s, learning 0.111s)
             Mean action noise std: 1.63
          Mean value_function loss: 91.0814
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 33.9697
                       Mean reward: 148.50
               Mean episode length: 153.41
    Episode_Reward/reaching_object: 0.5412
     Episode_Reward/lifting_object: 30.1937
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.38s
                      Time elapsed: 00:18:32
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 42490 steps/s (collection: 2.182s, learning 0.132s)
             Mean action noise std: 1.63
          Mean value_function loss: 90.8020
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 33.9694
                       Mean reward: 161.66
               Mean episode length: 166.05
    Episode_Reward/reaching_object: 0.5858
     Episode_Reward/lifting_object: 32.6901
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.31s
                      Time elapsed: 00:18:34
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 43044 steps/s (collection: 2.161s, learning 0.123s)
             Mean action noise std: 1.63
          Mean value_function loss: 94.5565
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 33.9692
                       Mean reward: 146.13
               Mean episode length: 157.13
    Episode_Reward/reaching_object: 0.5738
     Episode_Reward/lifting_object: 31.2768
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.28s
                      Time elapsed: 00:18:37
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 43040 steps/s (collection: 2.178s, learning 0.106s)
             Mean action noise std: 1.63
          Mean value_function loss: 97.3868
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.9696
                       Mean reward: 150.95
               Mean episode length: 161.17
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 29.7171
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.28s
                      Time elapsed: 00:18:39
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 41306 steps/s (collection: 2.278s, learning 0.102s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.5794
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 33.9708
                       Mean reward: 153.69
               Mean episode length: 154.07
    Episode_Reward/reaching_object: 0.5742
     Episode_Reward/lifting_object: 30.3652
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.38s
                      Time elapsed: 00:18:41
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 44512 steps/s (collection: 2.094s, learning 0.115s)
             Mean action noise std: 1.63
          Mean value_function loss: 82.5016
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 33.9716
                       Mean reward: 148.79
               Mean episode length: 150.75
    Episode_Reward/reaching_object: 0.5742
     Episode_Reward/lifting_object: 31.0515
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.21s
                      Time elapsed: 00:18:43
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 43794 steps/s (collection: 2.130s, learning 0.115s)
             Mean action noise std: 1.63
          Mean value_function loss: 75.6444
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 33.9719
                       Mean reward: 160.99
               Mean episode length: 167.23
    Episode_Reward/reaching_object: 0.5894
     Episode_Reward/lifting_object: 31.3576
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.24s
                      Time elapsed: 00:18:46
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 41983 steps/s (collection: 2.243s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 76.8496
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 33.9718
                       Mean reward: 156.81
               Mean episode length: 164.86
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 32.3001
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.34s
                      Time elapsed: 00:18:48
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 43692 steps/s (collection: 2.146s, learning 0.103s)
             Mean action noise std: 1.63
          Mean value_function loss: 83.1649
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 33.9718
                       Mean reward: 171.29
               Mean episode length: 165.24
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 32.6211
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.25s
                      Time elapsed: 00:18:50
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 40693 steps/s (collection: 2.247s, learning 0.169s)
             Mean action noise std: 1.63
          Mean value_function loss: 88.8657
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 33.9718
                       Mean reward: 145.12
               Mean episode length: 156.11
    Episode_Reward/reaching_object: 0.5756
     Episode_Reward/lifting_object: 30.2870
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.42s
                      Time elapsed: 00:18:53
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 44345 steps/s (collection: 2.112s, learning 0.105s)
             Mean action noise std: 1.63
          Mean value_function loss: 86.1158
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 33.9723
                       Mean reward: 168.04
               Mean episode length: 180.59
    Episode_Reward/reaching_object: 0.6072
     Episode_Reward/lifting_object: 32.5684
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.22s
                      Time elapsed: 00:18:55
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 39136 steps/s (collection: 2.337s, learning 0.175s)
             Mean action noise std: 1.64
          Mean value_function loss: 83.9220
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.9733
                       Mean reward: 152.49
               Mean episode length: 163.01
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 32.7391
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.51s
                      Time elapsed: 00:18:57
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 45092 steps/s (collection: 2.078s, learning 0.103s)
             Mean action noise std: 1.64
          Mean value_function loss: 92.9930
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.9742
                       Mean reward: 137.19
               Mean episode length: 152.31
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 31.1062
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.18s
                      Time elapsed: 00:19:00
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 39464 steps/s (collection: 2.335s, learning 0.156s)
             Mean action noise std: 1.64
          Mean value_function loss: 101.4539
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.9749
                       Mean reward: 153.93
               Mean episode length: 159.54
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 32.9267
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.49s
                      Time elapsed: 00:19:02
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 41217 steps/s (collection: 2.285s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 90.9362
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.9754
                       Mean reward: 150.82
               Mean episode length: 161.06
    Episode_Reward/reaching_object: 0.5840
     Episode_Reward/lifting_object: 31.0403
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.39s
                      Time elapsed: 00:19:05
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 44679 steps/s (collection: 2.108s, learning 0.092s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.1918
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 33.9761
                       Mean reward: 162.02
               Mean episode length: 164.26
    Episode_Reward/reaching_object: 0.6149
     Episode_Reward/lifting_object: 33.0507
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.20s
                      Time elapsed: 00:19:07
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 44756 steps/s (collection: 2.096s, learning 0.100s)
             Mean action noise std: 1.64
          Mean value_function loss: 90.3543
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.9782
                       Mean reward: 147.63
               Mean episode length: 154.90
    Episode_Reward/reaching_object: 0.5940
     Episode_Reward/lifting_object: 33.0663
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.20s
                      Time elapsed: 00:19:09
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 42365 steps/s (collection: 2.224s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 91.9955
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.9795
                       Mean reward: 166.37
               Mean episode length: 166.25
    Episode_Reward/reaching_object: 0.5905
     Episode_Reward/lifting_object: 32.4472
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.32s
                      Time elapsed: 00:19:11
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 42297 steps/s (collection: 2.129s, learning 0.195s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.0967
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 33.9809
                       Mean reward: 149.57
               Mean episode length: 149.66
    Episode_Reward/reaching_object: 0.5748
     Episode_Reward/lifting_object: 31.6350
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.32s
                      Time elapsed: 00:19:14
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 45642 steps/s (collection: 2.050s, learning 0.104s)
             Mean action noise std: 1.64
          Mean value_function loss: 88.1568
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 33.9810
                       Mean reward: 158.29
               Mean episode length: 153.74
    Episode_Reward/reaching_object: 0.6007
     Episode_Reward/lifting_object: 33.9775
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.15s
                      Time elapsed: 00:19:16
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 44481 steps/s (collection: 2.115s, learning 0.095s)
             Mean action noise std: 1.64
          Mean value_function loss: 79.9318
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 33.9831
                       Mean reward: 165.17
               Mean episode length: 159.26
    Episode_Reward/reaching_object: 0.5948
     Episode_Reward/lifting_object: 33.9072
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.21s
                      Time elapsed: 00:19:18
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 45218 steps/s (collection: 2.075s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 90.5406
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 33.9843
                       Mean reward: 186.50
               Mean episode length: 179.30
    Episode_Reward/reaching_object: 0.5964
     Episode_Reward/lifting_object: 34.2987
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.17s
                      Time elapsed: 00:19:20
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 43782 steps/s (collection: 2.142s, learning 0.103s)
             Mean action noise std: 1.64
          Mean value_function loss: 83.3200
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.9842
                       Mean reward: 186.34
               Mean episode length: 178.65
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 35.5392
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.25s
                      Time elapsed: 00:19:22
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 43227 steps/s (collection: 2.140s, learning 0.134s)
             Mean action noise std: 1.64
          Mean value_function loss: 90.0247
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.9849
                       Mean reward: 153.08
               Mean episode length: 158.28
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 33.0803
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.27s
                      Time elapsed: 00:19:25
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 44316 steps/s (collection: 2.119s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 87.6151
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 33.9857
                       Mean reward: 168.97
               Mean episode length: 163.08
    Episode_Reward/reaching_object: 0.5923
     Episode_Reward/lifting_object: 34.2117
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.22s
                      Time elapsed: 00:19:27
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 44272 steps/s (collection: 2.106s, learning 0.115s)
             Mean action noise std: 1.64
          Mean value_function loss: 96.1376
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 33.9863
                       Mean reward: 161.71
               Mean episode length: 158.34
    Episode_Reward/reaching_object: 0.5783
     Episode_Reward/lifting_object: 33.4178
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.22s
                      Time elapsed: 00:19:29
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 44858 steps/s (collection: 2.090s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 97.1934
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 33.9871
                       Mean reward: 165.53
               Mean episode length: 160.60
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 33.3671
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.19s
                      Time elapsed: 00:19:31
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 40829 steps/s (collection: 2.180s, learning 0.228s)
             Mean action noise std: 1.64
          Mean value_function loss: 93.5552
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.9900
                       Mean reward: 166.98
               Mean episode length: 163.13
    Episode_Reward/reaching_object: 0.5715
     Episode_Reward/lifting_object: 33.1211
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.41s
                      Time elapsed: 00:19:34
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 41740 steps/s (collection: 2.178s, learning 0.177s)
             Mean action noise std: 1.64
          Mean value_function loss: 87.6313
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.9907
                       Mean reward: 168.36
               Mean episode length: 161.91
    Episode_Reward/reaching_object: 0.5817
     Episode_Reward/lifting_object: 34.0401
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.36s
                      Time elapsed: 00:19:36
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 44290 steps/s (collection: 2.124s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 90.4862
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.9911
                       Mean reward: 178.78
               Mean episode length: 172.72
    Episode_Reward/reaching_object: 0.5691
     Episode_Reward/lifting_object: 33.2861
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.22s
                      Time elapsed: 00:19:38
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 45629 steps/s (collection: 2.051s, learning 0.103s)
             Mean action noise std: 1.64
          Mean value_function loss: 86.1356
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.9922
                       Mean reward: 164.72
               Mean episode length: 161.21
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: 33.5201
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.15s
                      Time elapsed: 00:19:40
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 43800 steps/s (collection: 2.051s, learning 0.194s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.7402
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 33.9925
                       Mean reward: 180.22
               Mean episode length: 168.11
    Episode_Reward/reaching_object: 0.5689
     Episode_Reward/lifting_object: 33.7108
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.24s
                      Time elapsed: 00:19:43
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 41295 steps/s (collection: 2.285s, learning 0.095s)
             Mean action noise std: 1.64
          Mean value_function loss: 91.2679
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 33.9921
                       Mean reward: 173.00
               Mean episode length: 165.98
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 34.6033
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.38s
                      Time elapsed: 00:19:45
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 43925 steps/s (collection: 2.144s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 87.5968
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.9926
                       Mean reward: 195.02
               Mean episode length: 180.15
    Episode_Reward/reaching_object: 0.5881
     Episode_Reward/lifting_object: 35.0287
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.24s
                      Time elapsed: 00:19:47
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 40262 steps/s (collection: 2.289s, learning 0.153s)
             Mean action noise std: 1.64
          Mean value_function loss: 86.8023
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.9929
                       Mean reward: 202.47
               Mean episode length: 186.00
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 37.7003
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.44s
                      Time elapsed: 00:19:50
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 44696 steps/s (collection: 2.102s, learning 0.097s)
             Mean action noise std: 1.64
          Mean value_function loss: 87.2142
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 33.9931
                       Mean reward: 184.24
               Mean episode length: 172.70
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 37.0718
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.20s
                      Time elapsed: 00:19:52
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 41992 steps/s (collection: 2.202s, learning 0.139s)
             Mean action noise std: 1.64
          Mean value_function loss: 86.5433
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.9918
                       Mean reward: 167.48
               Mean episode length: 157.96
    Episode_Reward/reaching_object: 0.5886
     Episode_Reward/lifting_object: 34.9879
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.34s
                      Time elapsed: 00:19:54
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 45312 steps/s (collection: 2.078s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.7042
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 33.9911
                       Mean reward: 179.31
               Mean episode length: 169.37
    Episode_Reward/reaching_object: 0.6083
     Episode_Reward/lifting_object: 36.8162
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.17s
                      Time elapsed: 00:19:56
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 41991 steps/s (collection: 2.150s, learning 0.192s)
             Mean action noise std: 1.64
          Mean value_function loss: 95.7994
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 33.9897
                       Mean reward: 186.80
               Mean episode length: 169.32
    Episode_Reward/reaching_object: 0.6027
     Episode_Reward/lifting_object: 37.1038
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.34s
                      Time elapsed: 00:19:59
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 42384 steps/s (collection: 2.227s, learning 0.093s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.2265
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 33.9900
                       Mean reward: 184.09
               Mean episode length: 166.51
    Episode_Reward/reaching_object: 0.5862
     Episode_Reward/lifting_object: 35.9482
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.32s
                      Time elapsed: 00:20:01
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 40495 steps/s (collection: 2.318s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 98.7183
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 33.9902
                       Mean reward: 196.46
               Mean episode length: 173.60
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 37.7074
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.43s
                      Time elapsed: 00:20:04
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 43680 steps/s (collection: 2.146s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 92.0612
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.9905
                       Mean reward: 194.23
               Mean episode length: 180.20
    Episode_Reward/reaching_object: 0.6041
     Episode_Reward/lifting_object: 37.5782
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.25s
                      Time elapsed: 00:20:06
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 43507 steps/s (collection: 2.142s, learning 0.118s)
             Mean action noise std: 1.64
          Mean value_function loss: 93.3315
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.9921
                       Mean reward: 194.70
               Mean episode length: 170.56
    Episode_Reward/reaching_object: 0.5937
     Episode_Reward/lifting_object: 37.6486
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.26s
                      Time elapsed: 00:20:08
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 44022 steps/s (collection: 2.126s, learning 0.107s)
             Mean action noise std: 1.64
          Mean value_function loss: 97.7407
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 33.9962
                       Mean reward: 190.71
               Mean episode length: 169.55
    Episode_Reward/reaching_object: 0.5979
     Episode_Reward/lifting_object: 37.2486
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.23s
                      Time elapsed: 00:20:10
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 44355 steps/s (collection: 2.102s, learning 0.114s)
             Mean action noise std: 1.64
          Mean value_function loss: 100.0227
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 33.9999
                       Mean reward: 184.96
               Mean episode length: 167.26
    Episode_Reward/reaching_object: 0.6031
     Episode_Reward/lifting_object: 37.6179
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.22s
                      Time elapsed: 00:20:12
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 43918 steps/s (collection: 2.145s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.2056
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 34.0019
                       Mean reward: 191.88
               Mean episode length: 173.71
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 37.9984
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.24s
                      Time elapsed: 00:20:15
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 44181 steps/s (collection: 2.100s, learning 0.125s)
             Mean action noise std: 1.64
          Mean value_function loss: 94.9394
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.0022
                       Mean reward: 192.16
               Mean episode length: 170.90
    Episode_Reward/reaching_object: 0.5966
     Episode_Reward/lifting_object: 37.6061
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.23s
                      Time elapsed: 00:20:17
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 42865 steps/s (collection: 2.142s, learning 0.152s)
             Mean action noise std: 1.64
          Mean value_function loss: 97.2618
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.0019
                       Mean reward: 197.65
               Mean episode length: 175.20
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 37.3241
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.29s
                      Time elapsed: 00:20:19
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 43225 steps/s (collection: 2.183s, learning 0.092s)
             Mean action noise std: 1.64
          Mean value_function loss: 96.2291
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.0028
                       Mean reward: 193.78
               Mean episode length: 169.80
    Episode_Reward/reaching_object: 0.6081
     Episode_Reward/lifting_object: 39.0991
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.27s
                      Time elapsed: 00:20:21
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 43511 steps/s (collection: 2.118s, learning 0.142s)
             Mean action noise std: 1.64
          Mean value_function loss: 99.3336
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.0029
                       Mean reward: 192.46
               Mean episode length: 164.40
    Episode_Reward/reaching_object: 0.5928
     Episode_Reward/lifting_object: 37.9245
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.26s
                      Time elapsed: 00:20:24
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 44387 steps/s (collection: 2.103s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 95.3052
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.0018
                       Mean reward: 186.80
               Mean episode length: 167.90
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 38.0816
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.21s
                      Time elapsed: 00:20:26
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 45045 steps/s (collection: 2.084s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 97.0889
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 34.0014
                       Mean reward: 190.26
               Mean episode length: 168.13
    Episode_Reward/reaching_object: 0.6035
     Episode_Reward/lifting_object: 39.1822
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.18s
                      Time elapsed: 00:20:28
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 42466 steps/s (collection: 2.194s, learning 0.121s)
             Mean action noise std: 1.64
          Mean value_function loss: 105.0090
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.0016
                       Mean reward: 211.06
               Mean episode length: 179.55
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 39.5784
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.31s
                      Time elapsed: 00:20:30
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 43663 steps/s (collection: 2.117s, learning 0.134s)
             Mean action noise std: 1.64
          Mean value_function loss: 100.0131
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.0026
                       Mean reward: 197.55
               Mean episode length: 173.10
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 40.5582
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.25s
                      Time elapsed: 00:20:33
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 45101 steps/s (collection: 2.078s, learning 0.102s)
             Mean action noise std: 1.64
          Mean value_function loss: 103.6609
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.0036
                       Mean reward: 194.42
               Mean episode length: 166.64
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 39.3904
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.18s
                      Time elapsed: 00:20:35
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 44949 steps/s (collection: 2.083s, learning 0.104s)
             Mean action noise std: 1.64
          Mean value_function loss: 108.8712
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.0048
                       Mean reward: 199.49
               Mean episode length: 173.16
    Episode_Reward/reaching_object: 0.5962
     Episode_Reward/lifting_object: 38.7886
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.19s
                      Time elapsed: 00:20:37
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 43549 steps/s (collection: 2.145s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 103.1884
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0068
                       Mean reward: 204.51
               Mean episode length: 174.96
    Episode_Reward/reaching_object: 0.6090
     Episode_Reward/lifting_object: 39.5594
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.26s
                      Time elapsed: 00:20:39
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 45812 steps/s (collection: 2.055s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 117.9449
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.0075
                       Mean reward: 199.75
               Mean episode length: 168.61
    Episode_Reward/reaching_object: 0.6013
     Episode_Reward/lifting_object: 38.9475
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.15s
                      Time elapsed: 00:20:41
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 44291 steps/s (collection: 2.091s, learning 0.128s)
             Mean action noise std: 1.64
          Mean value_function loss: 108.3112
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.0085
                       Mean reward: 199.64
               Mean episode length: 170.11
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 39.0354
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.22s
                      Time elapsed: 00:20:44
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 44901 steps/s (collection: 2.063s, learning 0.127s)
             Mean action noise std: 1.64
          Mean value_function loss: 104.5335
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.0084
                       Mean reward: 173.85
               Mean episode length: 149.07
    Episode_Reward/reaching_object: 0.5730
     Episode_Reward/lifting_object: 37.8796
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.19s
                      Time elapsed: 00:20:46
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 44794 steps/s (collection: 2.094s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 101.5737
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.0083
                       Mean reward: 199.14
               Mean episode length: 169.28
    Episode_Reward/reaching_object: 0.6062
     Episode_Reward/lifting_object: 40.7581
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.19s
                      Time elapsed: 00:20:48
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 44556 steps/s (collection: 2.102s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 103.3380
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 34.0087
                       Mean reward: 209.40
               Mean episode length: 172.92
    Episode_Reward/reaching_object: 0.6118
     Episode_Reward/lifting_object: 40.5143
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.21s
                      Time elapsed: 00:20:50
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45293 steps/s (collection: 2.057s, learning 0.114s)
             Mean action noise std: 1.64
          Mean value_function loss: 114.4593
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.0089
                       Mean reward: 212.97
               Mean episode length: 171.96
    Episode_Reward/reaching_object: 0.6004
     Episode_Reward/lifting_object: 40.2440
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.17s
                      Time elapsed: 00:20:52
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 44847 steps/s (collection: 2.092s, learning 0.100s)
             Mean action noise std: 1.64
          Mean value_function loss: 104.3408
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 34.0093
                       Mean reward: 195.73
               Mean episode length: 161.96
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: 40.7012
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.19s
                      Time elapsed: 00:20:55
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 43321 steps/s (collection: 2.139s, learning 0.131s)
             Mean action noise std: 1.64
          Mean value_function loss: 97.5520
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.0095
                       Mean reward: 210.26
               Mean episode length: 169.66
    Episode_Reward/reaching_object: 0.5853
     Episode_Reward/lifting_object: 39.6785
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.27s
                      Time elapsed: 00:20:57
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 43580 steps/s (collection: 2.155s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 112.0644
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.0103
                       Mean reward: 224.40
               Mean episode length: 177.37
    Episode_Reward/reaching_object: 0.6136
     Episode_Reward/lifting_object: 41.9637
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.26s
                      Time elapsed: 00:20:59
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 43367 steps/s (collection: 2.166s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 117.9428
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.0117
                       Mean reward: 186.17
               Mean episode length: 161.16
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 41.3615
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.27s
                      Time elapsed: 00:21:01
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 45285 steps/s (collection: 2.080s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 116.8699
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.0112
                       Mean reward: 198.28
               Mean episode length: 171.29
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 40.0732
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.17s
                      Time elapsed: 00:21:04
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 45164 steps/s (collection: 2.070s, learning 0.107s)
             Mean action noise std: 1.64
          Mean value_function loss: 118.7497
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.0117
                       Mean reward: 217.08
               Mean episode length: 179.02
    Episode_Reward/reaching_object: 0.5992
     Episode_Reward/lifting_object: 40.1639
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.18s
                      Time elapsed: 00:21:06
                               ETA: 00:53:22

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 45576 steps/s (collection: 2.059s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 107.9574
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.0131
                       Mean reward: 221.78
               Mean episode length: 183.76
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 40.8270
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.16s
                      Time elapsed: 00:21:08
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 45175 steps/s (collection: 2.076s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 115.7514
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 34.0149
                       Mean reward: 215.31
               Mean episode length: 173.77
    Episode_Reward/reaching_object: 0.6184
     Episode_Reward/lifting_object: 42.6466
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.18s
                      Time elapsed: 00:21:10
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 43861 steps/s (collection: 2.130s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 123.1358
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.0155
                       Mean reward: 213.00
               Mean episode length: 174.21
    Episode_Reward/reaching_object: 0.5973
     Episode_Reward/lifting_object: 40.5686
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.24s
                      Time elapsed: 00:21:12
                               ETA: 00:53:15

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 45469 steps/s (collection: 2.063s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 110.7904
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.0174
                       Mean reward: 219.41
               Mean episode length: 180.16
    Episode_Reward/reaching_object: 0.5989
     Episode_Reward/lifting_object: 40.8622
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.16s
                      Time elapsed: 00:21:15
                               ETA: 00:53:13

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 44729 steps/s (collection: 2.105s, learning 0.093s)
             Mean action noise std: 1.64
          Mean value_function loss: 116.7601
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.0186
                       Mean reward: 219.25
               Mean episode length: 175.20
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 41.1690
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.20s
                      Time elapsed: 00:21:17
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 44849 steps/s (collection: 2.091s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 115.1671
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.0201
                       Mean reward: 220.19
               Mean episode length: 172.24
    Episode_Reward/reaching_object: 0.5757
     Episode_Reward/lifting_object: 39.1322
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.19s
                      Time elapsed: 00:21:19
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 45210 steps/s (collection: 2.069s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 117.2070
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 34.0215
                       Mean reward: 205.40
               Mean episode length: 167.75
    Episode_Reward/reaching_object: 0.5917
     Episode_Reward/lifting_object: 40.7351
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.17s
                      Time elapsed: 00:21:21
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 43982 steps/s (collection: 2.121s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 107.8823
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 34.0222
                       Mean reward: 234.06
               Mean episode length: 185.86
    Episode_Reward/reaching_object: 0.6176
     Episode_Reward/lifting_object: 42.4067
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.24s
                      Time elapsed: 00:21:23
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 44851 steps/s (collection: 2.088s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 105.5894
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.0234
                       Mean reward: 207.68
               Mean episode length: 174.23
    Episode_Reward/reaching_object: 0.6004
     Episode_Reward/lifting_object: 41.2864
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.19s
                      Time elapsed: 00:21:26
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 45141 steps/s (collection: 2.083s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 123.8525
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.0253
                       Mean reward: 204.45
               Mean episode length: 167.54
    Episode_Reward/reaching_object: 0.6010
     Episode_Reward/lifting_object: 41.7017
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.18s
                      Time elapsed: 00:21:28
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 45386 steps/s (collection: 2.069s, learning 0.097s)
             Mean action noise std: 1.65
          Mean value_function loss: 125.2014
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.0251
                       Mean reward: 213.33
               Mean episode length: 178.05
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 40.2335
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.17s
                      Time elapsed: 00:21:30
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 44317 steps/s (collection: 2.120s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 111.8680
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.0250
                       Mean reward: 207.55
               Mean episode length: 171.94
    Episode_Reward/reaching_object: 0.6148
     Episode_Reward/lifting_object: 42.1156
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.22s
                      Time elapsed: 00:21:32
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 45411 steps/s (collection: 2.068s, learning 0.097s)
             Mean action noise std: 1.65
          Mean value_function loss: 116.8828
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.0262
                       Mean reward: 197.10
               Mean episode length: 159.43
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 42.3602
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.16s
                      Time elapsed: 00:21:34
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 45431 steps/s (collection: 2.061s, learning 0.103s)
             Mean action noise std: 1.65
          Mean value_function loss: 116.7400
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.0266
                       Mean reward: 213.37
               Mean episode length: 172.91
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: 42.5786
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.16s
                      Time elapsed: 00:21:36
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 45406 steps/s (collection: 2.061s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 114.9237
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.0258
                       Mean reward: 215.23
               Mean episode length: 168.18
    Episode_Reward/reaching_object: 0.6158
     Episode_Reward/lifting_object: 42.8853
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.16s
                      Time elapsed: 00:21:39
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 44723 steps/s (collection: 2.093s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 102.1948
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.0271
                       Mean reward: 201.75
               Mean episode length: 165.60
    Episode_Reward/reaching_object: 0.6255
     Episode_Reward/lifting_object: 44.0621
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.20s
                      Time elapsed: 00:21:41
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 45075 steps/s (collection: 2.072s, learning 0.109s)
             Mean action noise std: 1.65
          Mean value_function loss: 122.6512
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.0305
                       Mean reward: 232.30
               Mean episode length: 179.66
    Episode_Reward/reaching_object: 0.6211
     Episode_Reward/lifting_object: 45.0142
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.18s
                      Time elapsed: 00:21:43
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 43719 steps/s (collection: 2.141s, learning 0.107s)
             Mean action noise std: 1.65
          Mean value_function loss: 116.4415
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.0340
                       Mean reward: 224.36
               Mean episode length: 175.19
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 42.6853
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.25s
                      Time elapsed: 00:21:45
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 44409 steps/s (collection: 2.116s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 115.8111
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.0361
                       Mean reward: 209.11
               Mean episode length: 165.66
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 42.1710
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.21s
                      Time elapsed: 00:21:47
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 44521 steps/s (collection: 2.114s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 115.3163
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.0366
                       Mean reward: 231.58
               Mean episode length: 181.89
    Episode_Reward/reaching_object: 0.5998
     Episode_Reward/lifting_object: 42.3102
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.21s
                      Time elapsed: 00:21:50
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 44649 steps/s (collection: 2.102s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 129.4287
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 34.0365
                       Mean reward: 206.94
               Mean episode length: 161.37
    Episode_Reward/reaching_object: 0.6097
     Episode_Reward/lifting_object: 44.0908
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.20s
                      Time elapsed: 00:21:52
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 43875 steps/s (collection: 2.138s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 124.6243
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 34.0367
                       Mean reward: 207.02
               Mean episode length: 162.73
    Episode_Reward/reaching_object: 0.5860
     Episode_Reward/lifting_object: 40.9539
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.24s
                      Time elapsed: 00:21:54
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 43660 steps/s (collection: 2.135s, learning 0.117s)
             Mean action noise std: 1.65
          Mean value_function loss: 120.7695
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0370
                       Mean reward: 219.35
               Mean episode length: 172.07
    Episode_Reward/reaching_object: 0.5982
     Episode_Reward/lifting_object: 41.4933
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.25s
                      Time elapsed: 00:21:56
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 44816 steps/s (collection: 2.086s, learning 0.107s)
             Mean action noise std: 1.65
          Mean value_function loss: 126.5817
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0365
                       Mean reward: 230.85
               Mean episode length: 172.33
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 43.3587
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.19s
                      Time elapsed: 00:21:59
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 43556 steps/s (collection: 2.163s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 126.1381
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.0359
                       Mean reward: 230.61
               Mean episode length: 178.45
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 44.1309
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.26s
                      Time elapsed: 00:22:01
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 43991 steps/s (collection: 2.123s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 123.1904
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.0369
                       Mean reward: 212.36
               Mean episode length: 164.82
    Episode_Reward/reaching_object: 0.6240
     Episode_Reward/lifting_object: 44.1430
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.23s
                      Time elapsed: 00:22:03
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 44766 steps/s (collection: 2.095s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 108.7508
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.0378
                       Mean reward: 204.18
               Mean episode length: 161.34
    Episode_Reward/reaching_object: 0.6173
     Episode_Reward/lifting_object: 43.2777
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.20s
                      Time elapsed: 00:22:05
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 43371 steps/s (collection: 2.154s, learning 0.113s)
             Mean action noise std: 1.65
          Mean value_function loss: 109.6276
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.0382
                       Mean reward: 212.79
               Mean episode length: 163.57
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 44.0867
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.27s
                      Time elapsed: 00:22:07
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 44262 steps/s (collection: 2.122s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 110.7980
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.0385
                       Mean reward: 218.41
               Mean episode length: 170.06
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 44.0110
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.22s
                      Time elapsed: 00:22:10
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 44211 steps/s (collection: 2.127s, learning 0.097s)
             Mean action noise std: 1.65
          Mean value_function loss: 123.5173
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 34.0384
                       Mean reward: 221.28
               Mean episode length: 168.84
    Episode_Reward/reaching_object: 0.6107
     Episode_Reward/lifting_object: 43.9639
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.22s
                      Time elapsed: 00:22:12
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 45198 steps/s (collection: 2.059s, learning 0.116s)
             Mean action noise std: 1.65
          Mean value_function loss: 114.4290
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.0388
                       Mean reward: 236.26
               Mean episode length: 173.97
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 44.7514
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.17s
                      Time elapsed: 00:22:14
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 44756 steps/s (collection: 2.075s, learning 0.122s)
             Mean action noise std: 1.65
          Mean value_function loss: 121.0656
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 34.0397
                       Mean reward: 215.63
               Mean episode length: 170.71
    Episode_Reward/reaching_object: 0.6336
     Episode_Reward/lifting_object: 45.6181
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.20s
                      Time elapsed: 00:22:16
                               ETA: 00:52:08

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 44643 steps/s (collection: 2.104s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 121.5038
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.0395
                       Mean reward: 207.01
               Mean episode length: 159.13
    Episode_Reward/reaching_object: 0.6128
     Episode_Reward/lifting_object: 43.4692
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.20s
                      Time elapsed: 00:22:18
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 45759 steps/s (collection: 2.055s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 119.1861
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.0376
                       Mean reward: 216.09
               Mean episode length: 165.96
    Episode_Reward/reaching_object: 0.6205
     Episode_Reward/lifting_object: 44.5266
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.15s
                      Time elapsed: 00:22:21
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 43294 steps/s (collection: 2.177s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 144.1498
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.0367
                       Mean reward: 259.51
               Mean episode length: 192.34
    Episode_Reward/reaching_object: 0.6464
     Episode_Reward/lifting_object: 47.0320
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.27s
                      Time elapsed: 00:22:23
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 45451 steps/s (collection: 2.069s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 117.6377
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.0392
                       Mean reward: 226.77
               Mean episode length: 173.92
    Episode_Reward/reaching_object: 0.6429
     Episode_Reward/lifting_object: 46.8590
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.16s
                      Time elapsed: 00:22:25
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45274 steps/s (collection: 2.075s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 130.4508
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.0399
                       Mean reward: 233.40
               Mean episode length: 173.03
    Episode_Reward/reaching_object: 0.6316
     Episode_Reward/lifting_object: 46.7028
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.17s
                      Time elapsed: 00:22:27
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 45042 steps/s (collection: 2.080s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 111.3813
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.0390
                       Mean reward: 247.95
               Mean episode length: 183.64
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 46.6335
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.18s
                      Time elapsed: 00:22:29
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 44121 steps/s (collection: 2.132s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 128.5438
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 34.0375
                       Mean reward: 221.06
               Mean episode length: 168.03
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 43.8682
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.23s
                      Time elapsed: 00:22:32
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45744 steps/s (collection: 2.055s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 128.3847
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.0378
                       Mean reward: 240.16
               Mean episode length: 177.89
    Episode_Reward/reaching_object: 0.6117
     Episode_Reward/lifting_object: 44.8468
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.15s
                      Time elapsed: 00:22:34
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44809 steps/s (collection: 2.098s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 118.6402
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.0386
                       Mean reward: 252.25
               Mean episode length: 186.48
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 46.7921
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.19s
                      Time elapsed: 00:22:36
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 44454 steps/s (collection: 2.116s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 119.2974
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.0393
                       Mean reward: 264.19
               Mean episode length: 193.95
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 49.1959
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.21s
                      Time elapsed: 00:22:38
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 44992 steps/s (collection: 2.091s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 115.0949
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.0398
                       Mean reward: 227.43
               Mean episode length: 173.74
    Episode_Reward/reaching_object: 0.6477
     Episode_Reward/lifting_object: 47.2155
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.18s
                      Time elapsed: 00:22:40
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 44900 steps/s (collection: 2.074s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 122.3657
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.0399
                       Mean reward: 221.72
               Mean episode length: 167.21
    Episode_Reward/reaching_object: 0.6279
     Episode_Reward/lifting_object: 46.9668
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.19s
                      Time elapsed: 00:22:43
                               ETA: 00:51:40

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 45200 steps/s (collection: 2.073s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 128.4973
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.0404
                       Mean reward: 227.42
               Mean episode length: 169.56
    Episode_Reward/reaching_object: 0.6311
     Episode_Reward/lifting_object: 46.3973
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.17s
                      Time elapsed: 00:22:45
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 44306 steps/s (collection: 2.120s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 132.8355
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.0418
                       Mean reward: 214.09
               Mean episode length: 160.51
    Episode_Reward/reaching_object: 0.6140
     Episode_Reward/lifting_object: 45.1149
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.22s
                      Time elapsed: 00:22:47
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 45334 steps/s (collection: 2.069s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 116.0949
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 34.0427
                       Mean reward: 222.19
               Mean episode length: 162.77
    Episode_Reward/reaching_object: 0.6179
     Episode_Reward/lifting_object: 46.1681
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.17s
                      Time elapsed: 00:22:49
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 45179 steps/s (collection: 2.078s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 113.2554
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.0432
                       Mean reward: 265.55
               Mean episode length: 195.75
    Episode_Reward/reaching_object: 0.6667
     Episode_Reward/lifting_object: 49.2822
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.18s
                      Time elapsed: 00:22:51
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 44407 steps/s (collection: 2.121s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 119.9472
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.0437
                       Mean reward: 249.64
               Mean episode length: 183.77
    Episode_Reward/reaching_object: 0.6447
     Episode_Reward/lifting_object: 48.8083
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.21s
                      Time elapsed: 00:22:54
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45583 steps/s (collection: 2.061s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 118.2203
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.0456
                       Mean reward: 232.75
               Mean episode length: 169.69
    Episode_Reward/reaching_object: 0.6349
     Episode_Reward/lifting_object: 47.6581
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.16s
                      Time elapsed: 00:22:56
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.144s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 126.1259
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.0487
                       Mean reward: 238.03
               Mean episode length: 175.33
    Episode_Reward/reaching_object: 0.6448
     Episode_Reward/lifting_object: 49.0344
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.24s
                      Time elapsed: 00:22:58
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 43943 steps/s (collection: 2.134s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 134.8082
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.0499
                       Mean reward: 245.71
               Mean episode length: 178.36
    Episode_Reward/reaching_object: 0.6403
     Episode_Reward/lifting_object: 47.9969
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.24s
                      Time elapsed: 00:23:00
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 44796 steps/s (collection: 2.093s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 126.5811
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.0509
                       Mean reward: 240.46
               Mean episode length: 169.40
    Episode_Reward/reaching_object: 0.6394
     Episode_Reward/lifting_object: 48.3700
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.19s
                      Time elapsed: 00:23:02
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45598 steps/s (collection: 2.057s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 135.7795
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.0536
                       Mean reward: 249.48
               Mean episode length: 173.75
    Episode_Reward/reaching_object: 0.6398
     Episode_Reward/lifting_object: 48.9461
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.16s
                      Time elapsed: 00:23:05
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 45640 steps/s (collection: 2.054s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 137.1593
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.0560
                       Mean reward: 238.22
               Mean episode length: 169.32
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 47.7147
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.15s
                      Time elapsed: 00:23:07
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 44632 steps/s (collection: 2.106s, learning 0.097s)
             Mean action noise std: 1.65
          Mean value_function loss: 121.6597
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.0595
                       Mean reward: 250.60
               Mean episode length: 177.75
    Episode_Reward/reaching_object: 0.6264
     Episode_Reward/lifting_object: 47.7743
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.20s
                      Time elapsed: 00:23:09
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 45662 steps/s (collection: 2.052s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 119.5084
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.0629
                       Mean reward: 259.41
               Mean episode length: 180.92
    Episode_Reward/reaching_object: 0.6383
     Episode_Reward/lifting_object: 48.7344
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.15s
                      Time elapsed: 00:23:11
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 45922 steps/s (collection: 2.036s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 115.3161
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.0605
                       Mean reward: 243.85
               Mean episode length: 175.27
    Episode_Reward/reaching_object: 0.6390
     Episode_Reward/lifting_object: 48.5758
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.14s
                      Time elapsed: 00:23:13
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 45631 steps/s (collection: 2.052s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 125.1108
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.0576
                       Mean reward: 262.55
               Mean episode length: 185.82
    Episode_Reward/reaching_object: 0.6593
     Episode_Reward/lifting_object: 50.2581
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.15s
                      Time elapsed: 00:23:15
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 45221 steps/s (collection: 2.075s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 120.5752
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.0570
                       Mean reward: 258.96
               Mean episode length: 176.92
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 50.8336
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.17s
                      Time elapsed: 00:23:17
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 45797 steps/s (collection: 2.050s, learning 0.096s)
             Mean action noise std: 1.65
          Mean value_function loss: 122.3693
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.0574
                       Mean reward: 244.19
               Mean episode length: 172.00
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: 50.3780
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.15s
                      Time elapsed: 00:23:20
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 46022 steps/s (collection: 2.043s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 127.0363
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.0588
                       Mean reward: 240.52
               Mean episode length: 172.01
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 50.7412
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.14s
                      Time elapsed: 00:23:22
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 45596 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 145.7898
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.0627
                       Mean reward: 243.87
               Mean episode length: 170.11
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 48.6492
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.16s
                      Time elapsed: 00:23:24
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 45423 steps/s (collection: 2.060s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 131.0785
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.0655
                       Mean reward: 252.15
               Mean episode length: 180.24
    Episode_Reward/reaching_object: 0.6318
     Episode_Reward/lifting_object: 48.1060
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.16s
                      Time elapsed: 00:23:26
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 43795 steps/s (collection: 2.109s, learning 0.136s)
             Mean action noise std: 1.65
          Mean value_function loss: 129.6853
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 34.0657
                       Mean reward: 275.99
               Mean episode length: 191.55
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 51.0309
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.24s
                      Time elapsed: 00:23:28
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 44664 steps/s (collection: 2.098s, learning 0.103s)
             Mean action noise std: 1.65
          Mean value_function loss: 119.2234
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.0661
                       Mean reward: 264.69
               Mean episode length: 180.40
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 51.1666
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.20s
                      Time elapsed: 00:23:31
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 44943 steps/s (collection: 2.093s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 143.2796
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.0664
                       Mean reward: 252.22
               Mean episode length: 173.38
    Episode_Reward/reaching_object: 0.6475
     Episode_Reward/lifting_object: 49.6696
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.19s
                      Time elapsed: 00:23:33
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 44479 steps/s (collection: 2.120s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 141.2821
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.0666
                       Mean reward: 270.96
               Mean episode length: 186.08
    Episode_Reward/reaching_object: 0.6579
     Episode_Reward/lifting_object: 50.6428
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.21s
                      Time elapsed: 00:23:35
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 45816 steps/s (collection: 2.052s, learning 0.093s)
             Mean action noise std: 1.66
          Mean value_function loss: 153.3480
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.0671
                       Mean reward: 252.84
               Mean episode length: 175.95
    Episode_Reward/reaching_object: 0.6433
     Episode_Reward/lifting_object: 49.7699
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.15s
                      Time elapsed: 00:23:37
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 44231 steps/s (collection: 2.122s, learning 0.100s)
             Mean action noise std: 1.66
          Mean value_function loss: 137.4398
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 34.0676
                       Mean reward: 246.03
               Mean episode length: 172.18
    Episode_Reward/reaching_object: 0.6469
     Episode_Reward/lifting_object: 50.1009
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.22s
                      Time elapsed: 00:23:39
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 44847 steps/s (collection: 2.083s, learning 0.109s)
             Mean action noise std: 1.66
          Mean value_function loss: 143.4153
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.0685
                       Mean reward: 261.59
               Mean episode length: 178.46
    Episode_Reward/reaching_object: 0.6521
     Episode_Reward/lifting_object: 50.4193
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.19s
                      Time elapsed: 00:23:41
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 45614 steps/s (collection: 2.039s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 138.2387
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.0706
                       Mean reward: 259.11
               Mean episode length: 181.22
    Episode_Reward/reaching_object: 0.6483
     Episode_Reward/lifting_object: 50.1391
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.16s
                      Time elapsed: 00:23:44
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 44952 steps/s (collection: 2.088s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 139.8113
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.0719
                       Mean reward: 233.32
               Mean episode length: 167.32
    Episode_Reward/reaching_object: 0.6414
     Episode_Reward/lifting_object: 49.7789
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.19s
                      Time elapsed: 00:23:46
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 44978 steps/s (collection: 2.088s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 143.4348
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.0739
                       Mean reward: 263.49
               Mean episode length: 187.34
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 49.3827
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.19s
                      Time elapsed: 00:23:48
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 45020 steps/s (collection: 2.090s, learning 0.094s)
             Mean action noise std: 1.66
          Mean value_function loss: 150.5351
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.0743
                       Mean reward: 265.79
               Mean episode length: 176.75
    Episode_Reward/reaching_object: 0.6449
     Episode_Reward/lifting_object: 51.5576
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.18s
                      Time elapsed: 00:23:50
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 45751 steps/s (collection: 2.050s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 145.2056
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0746
                       Mean reward: 258.81
               Mean episode length: 175.02
    Episode_Reward/reaching_object: 0.6392
     Episode_Reward/lifting_object: 49.6466
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.15s
                      Time elapsed: 00:23:52
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 44869 steps/s (collection: 2.100s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 159.8047
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.0754
                       Mean reward: 250.66
               Mean episode length: 172.17
    Episode_Reward/reaching_object: 0.6350
     Episode_Reward/lifting_object: 49.9699
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.19s
                      Time elapsed: 00:23:55
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 45470 steps/s (collection: 2.064s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 146.2417
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0747
                       Mean reward: 261.80
               Mean episode length: 181.13
    Episode_Reward/reaching_object: 0.6499
     Episode_Reward/lifting_object: 50.9768
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.16s
                      Time elapsed: 00:23:57
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 45324 steps/s (collection: 2.073s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 159.1926
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.0741
                       Mean reward: 252.36
               Mean episode length: 178.57
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 49.3462
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.17s
                      Time elapsed: 00:23:59
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 44757 steps/s (collection: 2.091s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 147.6607
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.0751
                       Mean reward: 257.34
               Mean episode length: 173.40
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 50.1017
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.20s
                      Time elapsed: 00:24:01
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 44807 steps/s (collection: 2.089s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 150.1260
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.0739
                       Mean reward: 224.33
               Mean episode length: 160.90
    Episode_Reward/reaching_object: 0.6306
     Episode_Reward/lifting_object: 49.4900
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.19s
                      Time elapsed: 00:24:03
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 43939 steps/s (collection: 2.142s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 146.9814
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.0763
                       Mean reward: 228.02
               Mean episode length: 164.24
    Episode_Reward/reaching_object: 0.6389
     Episode_Reward/lifting_object: 49.7250
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.24s
                      Time elapsed: 00:24:06
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 45453 steps/s (collection: 2.070s, learning 0.093s)
             Mean action noise std: 1.66
          Mean value_function loss: 158.2772
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.0793
                       Mean reward: 239.98
               Mean episode length: 165.91
    Episode_Reward/reaching_object: 0.6226
     Episode_Reward/lifting_object: 49.5940
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.16s
                      Time elapsed: 00:24:08
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 45575 steps/s (collection: 2.060s, learning 0.097s)
             Mean action noise std: 1.66
          Mean value_function loss: 143.2340
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.0802
                       Mean reward: 229.74
               Mean episode length: 157.92
    Episode_Reward/reaching_object: 0.6487
     Episode_Reward/lifting_object: 51.4218
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.16s
                      Time elapsed: 00:24:10
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 44826 steps/s (collection: 2.088s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 163.6775
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.0805
                       Mean reward: 255.32
               Mean episode length: 181.54
    Episode_Reward/reaching_object: 0.6444
     Episode_Reward/lifting_object: 49.7484
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.19s
                      Time elapsed: 00:24:12
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45134 steps/s (collection: 2.074s, learning 0.104s)
             Mean action noise std: 1.66
          Mean value_function loss: 139.0687
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.0801
                       Mean reward: 259.15
               Mean episode length: 178.51
    Episode_Reward/reaching_object: 0.6723
     Episode_Reward/lifting_object: 53.2577
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.18s
                      Time elapsed: 00:24:14
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 45148 steps/s (collection: 2.062s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 154.2045
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.0809
                       Mean reward: 242.25
               Mean episode length: 171.09
    Episode_Reward/reaching_object: 0.6409
     Episode_Reward/lifting_object: 50.2034
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.18s
                      Time elapsed: 00:24:16
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 45023 steps/s (collection: 2.066s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 138.0724
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.0794
                       Mean reward: 242.39
               Mean episode length: 170.19
    Episode_Reward/reaching_object: 0.6544
     Episode_Reward/lifting_object: 51.4935
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.18s
                      Time elapsed: 00:24:19
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 45318 steps/s (collection: 2.075s, learning 0.094s)
             Mean action noise std: 1.66
          Mean value_function loss: 168.5970
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 34.0796
                       Mean reward: 264.35
               Mean episode length: 175.75
    Episode_Reward/reaching_object: 0.6332
     Episode_Reward/lifting_object: 50.1851
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.17s
                      Time elapsed: 00:24:21
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 45826 steps/s (collection: 2.046s, learning 0.100s)
             Mean action noise std: 1.66
          Mean value_function loss: 143.3948
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.0813
                       Mean reward: 242.73
               Mean episode length: 169.50
    Episode_Reward/reaching_object: 0.6351
     Episode_Reward/lifting_object: 49.3545
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.15s
                      Time elapsed: 00:24:23
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 44974 steps/s (collection: 2.096s, learning 0.090s)
             Mean action noise std: 1.66
          Mean value_function loss: 167.6074
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 34.0807
                       Mean reward: 261.10
               Mean episode length: 176.88
    Episode_Reward/reaching_object: 0.6202
     Episode_Reward/lifting_object: 48.0362
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.19s
                      Time elapsed: 00:24:25
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 45169 steps/s (collection: 2.085s, learning 0.092s)
             Mean action noise std: 1.66
          Mean value_function loss: 157.3016
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.0809
                       Mean reward: 264.60
               Mean episode length: 174.95
    Episode_Reward/reaching_object: 0.6540
     Episode_Reward/lifting_object: 51.7741
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.18s
                      Time elapsed: 00:24:27
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 44570 steps/s (collection: 2.114s, learning 0.092s)
             Mean action noise std: 1.66
          Mean value_function loss: 152.6583
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.0819
                       Mean reward: 271.32
               Mean episode length: 183.44
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 52.5965
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.21s
                      Time elapsed: 00:24:29
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 45317 steps/s (collection: 2.079s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 155.9969
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.0805
                       Mean reward: 258.69
               Mean episode length: 178.85
    Episode_Reward/reaching_object: 0.6489
     Episode_Reward/lifting_object: 50.5310
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.17s
                      Time elapsed: 00:24:32
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.043s, learning 0.093s)
             Mean action noise std: 1.66
          Mean value_function loss: 128.1521
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.0808
                       Mean reward: 254.85
               Mean episode length: 176.41
    Episode_Reward/reaching_object: 0.6416
     Episode_Reward/lifting_object: 50.3382
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.14s
                      Time elapsed: 00:24:34
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 45096 steps/s (collection: 2.085s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 150.1945
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.0803
                       Mean reward: 247.58
               Mean episode length: 167.50
    Episode_Reward/reaching_object: 0.6506
     Episode_Reward/lifting_object: 51.1271
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.18s
                      Time elapsed: 00:24:36
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 45356 steps/s (collection: 2.074s, learning 0.093s)
             Mean action noise std: 1.66
          Mean value_function loss: 137.0804
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.0794
                       Mean reward: 276.22
               Mean episode length: 188.91
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 51.9429
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.17s
                      Time elapsed: 00:24:38
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 45002 steps/s (collection: 2.081s, learning 0.103s)
             Mean action noise std: 1.66
          Mean value_function loss: 132.1903
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.0799
                       Mean reward: 266.26
               Mean episode length: 181.74
    Episode_Reward/reaching_object: 0.6401
     Episode_Reward/lifting_object: 50.2271
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.18s
                      Time elapsed: 00:24:40
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 44892 steps/s (collection: 2.092s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 154.2286
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.0813
                       Mean reward: 294.96
               Mean episode length: 189.83
    Episode_Reward/reaching_object: 0.6578
     Episode_Reward/lifting_object: 52.4580
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.19s
                      Time elapsed: 00:24:42
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 27274 steps/s (collection: 3.495s, learning 0.109s)
             Mean action noise std: 1.66
          Mean value_function loss: 133.0205
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.0829
                       Mean reward: 274.76
               Mean episode length: 177.35
    Episode_Reward/reaching_object: 0.6634
     Episode_Reward/lifting_object: 53.1572
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.60s
                      Time elapsed: 00:24:46
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 15047 steps/s (collection: 6.393s, learning 0.141s)
             Mean action noise std: 1.66
          Mean value_function loss: 152.5376
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.0840
                       Mean reward: 261.32
               Mean episode length: 183.27
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 50.5144
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.53s
                      Time elapsed: 00:24:53
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14038 steps/s (collection: 6.885s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 139.4254
               Mean surrogate loss: 0.0199
                 Mean entropy loss: 34.0865
                       Mean reward: 267.25
               Mean episode length: 176.41
    Episode_Reward/reaching_object: 0.6558
     Episode_Reward/lifting_object: 51.4389
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.00s
                      Time elapsed: 00:25:00
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14360 steps/s (collection: 6.728s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 151.6916
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.0868
                       Mean reward: 277.66
               Mean episode length: 186.37
    Episode_Reward/reaching_object: 0.6790
     Episode_Reward/lifting_object: 53.9821
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.85s
                      Time elapsed: 00:25:06
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14008 steps/s (collection: 6.899s, learning 0.118s)
             Mean action noise std: 1.66
          Mean value_function loss: 141.7819
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.0876
                       Mean reward: 267.31
               Mean episode length: 176.40
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 53.3802
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.02s
                      Time elapsed: 00:25:13
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14516 steps/s (collection: 6.656s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 152.0719
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.0887
                       Mean reward: 263.42
               Mean episode length: 177.52
    Episode_Reward/reaching_object: 0.6664
     Episode_Reward/lifting_object: 52.8029
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.77s
                      Time elapsed: 00:25:20
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13993 steps/s (collection: 6.909s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 165.7268
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.0892
                       Mean reward: 260.03
               Mean episode length: 174.49
    Episode_Reward/reaching_object: 0.6494
     Episode_Reward/lifting_object: 52.1758
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.03s
                      Time elapsed: 00:25:27
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14096 steps/s (collection: 6.847s, learning 0.127s)
             Mean action noise std: 1.66
          Mean value_function loss: 146.4868
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.0896
                       Mean reward: 277.59
               Mean episode length: 184.03
    Episode_Reward/reaching_object: 0.6805
     Episode_Reward/lifting_object: 54.1244
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.97s
                      Time elapsed: 00:25:34
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13958 steps/s (collection: 6.917s, learning 0.126s)
             Mean action noise std: 1.66
          Mean value_function loss: 174.4594
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.0901
                       Mean reward: 273.85
               Mean episode length: 179.22
    Episode_Reward/reaching_object: 0.6758
     Episode_Reward/lifting_object: 55.2455
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.04s
                      Time elapsed: 00:25:41
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21824 steps/s (collection: 4.416s, learning 0.088s)
             Mean action noise std: 1.66
          Mean value_function loss: 228.8179
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.0924
                       Mean reward: 269.93
               Mean episode length: 174.51
    Episode_Reward/reaching_object: 0.6447
     Episode_Reward/lifting_object: 52.2912
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.50s
                      Time elapsed: 00:25:46
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 45677 steps/s (collection: 2.029s, learning 0.124s)
             Mean action noise std: 1.66
          Mean value_function loss: 149.8156
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.0972
                       Mean reward: 240.24
               Mean episode length: 165.62
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: 52.9747
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.15s
                      Time elapsed: 00:25:48
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 47203 steps/s (collection: 1.962s, learning 0.121s)
             Mean action noise std: 1.66
          Mean value_function loss: 136.0197
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.1014
                       Mean reward: 283.84
               Mean episode length: 183.00
    Episode_Reward/reaching_object: 0.6590
     Episode_Reward/lifting_object: 53.4996
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.08s
                      Time elapsed: 00:25:50
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 43893 steps/s (collection: 2.082s, learning 0.158s)
             Mean action noise std: 1.66
          Mean value_function loss: 149.9509
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.1022
                       Mean reward: 268.81
               Mean episode length: 172.40
    Episode_Reward/reaching_object: 0.6460
     Episode_Reward/lifting_object: 53.7354
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.24s
                      Time elapsed: 00:25:52
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 47175 steps/s (collection: 1.978s, learning 0.106s)
             Mean action noise std: 1.66
          Mean value_function loss: 157.9672
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.1039
                       Mean reward: 252.57
               Mean episode length: 166.65
    Episode_Reward/reaching_object: 0.6550
     Episode_Reward/lifting_object: 53.1404
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.08s
                      Time elapsed: 00:25:54
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 46902 steps/s (collection: 1.985s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 142.7945
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 34.1033
                       Mean reward: 269.29
               Mean episode length: 177.79
    Episode_Reward/reaching_object: 0.6567
     Episode_Reward/lifting_object: 53.9135
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.10s
                      Time elapsed: 00:25:56
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 48090 steps/s (collection: 1.955s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 150.0828
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.1007
                       Mean reward: 273.07
               Mean episode length: 179.99
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 52.5755
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.04s
                      Time elapsed: 00:25:58
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 47963 steps/s (collection: 1.960s, learning 0.090s)
             Mean action noise std: 1.66
          Mean value_function loss: 144.3598
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.1003
                       Mean reward: 271.81
               Mean episode length: 177.10
    Episode_Reward/reaching_object: 0.6754
     Episode_Reward/lifting_object: 55.4542
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.05s
                      Time elapsed: 00:26:01
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 47400 steps/s (collection: 1.989s, learning 0.085s)
             Mean action noise std: 1.66
          Mean value_function loss: 163.7426
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.1004
                       Mean reward: 279.62
               Mean episode length: 182.57
    Episode_Reward/reaching_object: 0.6680
     Episode_Reward/lifting_object: 54.4096
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.07s
                      Time elapsed: 00:26:03
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 45854 steps/s (collection: 2.016s, learning 0.128s)
             Mean action noise std: 1.66
          Mean value_function loss: 156.4376
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.1008
                       Mean reward: 263.85
               Mean episode length: 168.06
    Episode_Reward/reaching_object: 0.6678
     Episode_Reward/lifting_object: 54.9500
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.14s
                      Time elapsed: 00:26:05
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 46482 steps/s (collection: 2.005s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 171.6074
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.1013
                       Mean reward: 279.52
               Mean episode length: 181.30
    Episode_Reward/reaching_object: 0.6535
     Episode_Reward/lifting_object: 53.6179
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.11s
                      Time elapsed: 00:26:07
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 44296 steps/s (collection: 2.062s, learning 0.158s)
             Mean action noise std: 1.66
          Mean value_function loss: 149.0212
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.1015
                       Mean reward: 281.96
               Mean episode length: 181.97
    Episode_Reward/reaching_object: 0.6650
     Episode_Reward/lifting_object: 54.4038
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.22s
                      Time elapsed: 00:26:09
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 45931 steps/s (collection: 2.028s, learning 0.113s)
             Mean action noise std: 1.66
          Mean value_function loss: 149.2844
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.1019
                       Mean reward: 283.13
               Mean episode length: 176.90
    Episode_Reward/reaching_object: 0.6770
     Episode_Reward/lifting_object: 56.1060
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.14s
                      Time elapsed: 00:26:11
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 48113 steps/s (collection: 1.957s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 157.0149
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.1041
                       Mean reward: 278.35
               Mean episode length: 181.92
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: 53.5721
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.04s
                      Time elapsed: 00:26:13
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 43982 steps/s (collection: 2.148s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 171.3281
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.1067
                       Mean reward: 258.12
               Mean episode length: 174.18
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 54.2682
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.24s
                      Time elapsed: 00:26:16
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 47382 steps/s (collection: 1.987s, learning 0.088s)
             Mean action noise std: 1.66
          Mean value_function loss: 153.4913
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.1086
                       Mean reward: 278.30
               Mean episode length: 179.39
    Episode_Reward/reaching_object: 0.6525
     Episode_Reward/lifting_object: 52.7481
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.07s
                      Time elapsed: 00:26:18
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 46938 steps/s (collection: 1.999s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 155.4918
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.1073
                       Mean reward: 261.86
               Mean episode length: 176.42
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 54.0193
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.09s
                      Time elapsed: 00:26:20
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47756 steps/s (collection: 1.966s, learning 0.092s)
             Mean action noise std: 1.66
          Mean value_function loss: 144.6613
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.1064
                       Mean reward: 299.01
               Mean episode length: 187.50
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 55.9585
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.06s
                      Time elapsed: 00:26:22
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 46635 steps/s (collection: 2.021s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 153.9175
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.1105
                       Mean reward: 291.12
               Mean episode length: 188.50
    Episode_Reward/reaching_object: 0.7095
     Episode_Reward/lifting_object: 58.3228
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.11s
                      Time elapsed: 00:26:24
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47606 steps/s (collection: 1.974s, learning 0.091s)
             Mean action noise std: 1.67
          Mean value_function loss: 147.7264
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.1112
                       Mean reward: 283.32
               Mean episode length: 180.02
    Episode_Reward/reaching_object: 0.6666
     Episode_Reward/lifting_object: 54.6537
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.06s
                      Time elapsed: 00:26:26
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 44187 steps/s (collection: 2.058s, learning 0.167s)
             Mean action noise std: 1.67
          Mean value_function loss: 142.6619
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.1097
                       Mean reward: 291.05
               Mean episode length: 186.71
    Episode_Reward/reaching_object: 0.6985
     Episode_Reward/lifting_object: 58.1345
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.22s
                      Time elapsed: 00:26:28
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 46309 steps/s (collection: 1.990s, learning 0.133s)
             Mean action noise std: 1.67
          Mean value_function loss: 160.5269
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.1100
                       Mean reward: 285.27
               Mean episode length: 184.27
    Episode_Reward/reaching_object: 0.6862
     Episode_Reward/lifting_object: 56.0689
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.12s
                      Time elapsed: 00:26:30
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 47287 steps/s (collection: 1.976s, learning 0.103s)
             Mean action noise std: 1.67
          Mean value_function loss: 152.4492
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.1168
                       Mean reward: 281.18
               Mean episode length: 177.37
    Episode_Reward/reaching_object: 0.6788
     Episode_Reward/lifting_object: 56.2311
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.08s
                      Time elapsed: 00:26:32
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 48007 steps/s (collection: 1.953s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 138.7047
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.1190
                       Mean reward: 304.78
               Mean episode length: 195.22
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: 56.3398
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.05s
                      Time elapsed: 00:26:34
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 48066 steps/s (collection: 1.959s, learning 0.086s)
             Mean action noise std: 1.67
          Mean value_function loss: 141.1115
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.1203
                       Mean reward: 266.36
               Mean episode length: 172.94
    Episode_Reward/reaching_object: 0.6729
     Episode_Reward/lifting_object: 55.9942
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.05s
                      Time elapsed: 00:26:36
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47058 steps/s (collection: 1.997s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 145.8017
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.1222
                       Mean reward: 299.23
               Mean episode length: 185.58
    Episode_Reward/reaching_object: 0.6884
     Episode_Reward/lifting_object: 58.0361
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.09s
                      Time elapsed: 00:26:39
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 46309 steps/s (collection: 1.999s, learning 0.124s)
             Mean action noise std: 1.67
          Mean value_function loss: 149.0843
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.1240
                       Mean reward: 305.67
               Mean episode length: 190.68
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 57.8795
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.12s
                      Time elapsed: 00:26:41
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 44447 steps/s (collection: 2.112s, learning 0.100s)
             Mean action noise std: 1.67
          Mean value_function loss: 148.2358
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.1255
                       Mean reward: 301.25
               Mean episode length: 187.28
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: 57.4969
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.21s
                      Time elapsed: 00:26:43
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 46604 steps/s (collection: 2.014s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 166.0523
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.1268
                       Mean reward: 279.83
               Mean episode length: 181.14
    Episode_Reward/reaching_object: 0.6648
     Episode_Reward/lifting_object: 54.9860
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.11s
                      Time elapsed: 00:26:45
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 42158 steps/s (collection: 2.119s, learning 0.213s)
             Mean action noise std: 1.67
          Mean value_function loss: 155.4087
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.1267
                       Mean reward: 287.65
               Mean episode length: 189.27
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 57.5493
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.33s
                      Time elapsed: 00:26:47
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 45011 steps/s (collection: 2.097s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 145.8060
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.1270
                       Mean reward: 293.75
               Mean episode length: 185.17
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 58.1249
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.18s
                      Time elapsed: 00:26:49
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 47490 steps/s (collection: 1.976s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 144.6925
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.1297
                       Mean reward: 275.78
               Mean episode length: 169.00
    Episode_Reward/reaching_object: 0.6883
     Episode_Reward/lifting_object: 57.8103
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.07s
                      Time elapsed: 00:26:52
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 47575 steps/s (collection: 1.972s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 133.5048
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 34.1330
                       Mean reward: 307.94
               Mean episode length: 185.77
    Episode_Reward/reaching_object: 0.6924
     Episode_Reward/lifting_object: 58.5785
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.07s
                      Time elapsed: 00:26:54
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 48259 steps/s (collection: 1.950s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 144.0740
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 34.1338
                       Mean reward: 288.00
               Mean episode length: 177.93
    Episode_Reward/reaching_object: 0.6911
     Episode_Reward/lifting_object: 58.0875
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.04s
                      Time elapsed: 00:26:56
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 46391 steps/s (collection: 2.010s, learning 0.109s)
             Mean action noise std: 1.67
          Mean value_function loss: 151.9643
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 34.1340
                       Mean reward: 283.64
               Mean episode length: 177.98
    Episode_Reward/reaching_object: 0.6775
     Episode_Reward/lifting_object: 56.8198
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.12s
                      Time elapsed: 00:26:58
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 46921 steps/s (collection: 2.009s, learning 0.086s)
             Mean action noise std: 1.67
          Mean value_function loss: 147.7918
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.1342
                       Mean reward: 305.26
               Mean episode length: 188.77
    Episode_Reward/reaching_object: 0.7075
     Episode_Reward/lifting_object: 59.8123
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.10s
                      Time elapsed: 00:27:00
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 44476 steps/s (collection: 2.067s, learning 0.143s)
             Mean action noise std: 1.67
          Mean value_function loss: 140.0029
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.1338
                       Mean reward: 308.50
               Mean episode length: 192.68
    Episode_Reward/reaching_object: 0.6878
     Episode_Reward/lifting_object: 57.5996
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.21s
                      Time elapsed: 00:27:02
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 42300 steps/s (collection: 2.228s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 137.2669
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.1318
                       Mean reward: 283.79
               Mean episode length: 173.56
    Episode_Reward/reaching_object: 0.6880
     Episode_Reward/lifting_object: 58.0203
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.32s
                      Time elapsed: 00:27:04
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 45316 steps/s (collection: 2.058s, learning 0.111s)
             Mean action noise std: 1.67
          Mean value_function loss: 174.2150
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.1301
                       Mean reward: 302.98
               Mean episode length: 193.24
    Episode_Reward/reaching_object: 0.6934
     Episode_Reward/lifting_object: 58.6424
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.17s
                      Time elapsed: 00:27:07
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 44677 steps/s (collection: 2.105s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 150.8436
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.1302
                       Mean reward: 300.22
               Mean episode length: 187.69
    Episode_Reward/reaching_object: 0.6987
     Episode_Reward/lifting_object: 59.0405
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.20s
                      Time elapsed: 00:27:09
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 46270 steps/s (collection: 2.036s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 156.3098
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 34.1325
                       Mean reward: 315.18
               Mean episode length: 193.25
    Episode_Reward/reaching_object: 0.6858
     Episode_Reward/lifting_object: 58.4977
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.12s
                      Time elapsed: 00:27:11
                               ETA: 00:48:47

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 46172 steps/s (collection: 2.037s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 141.3470
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.1337
                       Mean reward: 301.41
               Mean episode length: 191.26
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 58.8985
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.13s
                      Time elapsed: 00:27:13
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 45791 steps/s (collection: 2.013s, learning 0.134s)
             Mean action noise std: 1.67
          Mean value_function loss: 147.9216
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.1346
                       Mean reward: 287.07
               Mean episode length: 182.39
    Episode_Reward/reaching_object: 0.6710
     Episode_Reward/lifting_object: 56.6724
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.15s
                      Time elapsed: 00:27:15
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 46029 steps/s (collection: 2.016s, learning 0.120s)
             Mean action noise std: 1.67
          Mean value_function loss: 166.0482
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.1350
                       Mean reward: 268.65
               Mean episode length: 173.15
    Episode_Reward/reaching_object: 0.6577
     Episode_Reward/lifting_object: 55.6594
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.14s
                      Time elapsed: 00:27:17
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 46535 steps/s (collection: 2.002s, learning 0.111s)
             Mean action noise std: 1.67
          Mean value_function loss: 150.3266
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.1352
                       Mean reward: 318.90
               Mean episode length: 194.70
    Episode_Reward/reaching_object: 0.6922
     Episode_Reward/lifting_object: 59.4063
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.11s
                      Time elapsed: 00:27:19
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 46370 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 1.67
          Mean value_function loss: 163.2878
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.1356
                       Mean reward: 300.36
               Mean episode length: 186.14
    Episode_Reward/reaching_object: 0.6879
     Episode_Reward/lifting_object: 58.9486
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.12s
                      Time elapsed: 00:27:22
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 46808 steps/s (collection: 2.002s, learning 0.098s)
             Mean action noise std: 1.67
          Mean value_function loss: 166.2780
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.1374
                       Mean reward: 287.13
               Mean episode length: 182.37
    Episode_Reward/reaching_object: 0.6800
     Episode_Reward/lifting_object: 57.9164
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.10s
                      Time elapsed: 00:27:24
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 45931 steps/s (collection: 2.040s, learning 0.100s)
             Mean action noise std: 1.67
          Mean value_function loss: 158.6403
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.1390
                       Mean reward: 301.25
               Mean episode length: 187.56
    Episode_Reward/reaching_object: 0.6858
     Episode_Reward/lifting_object: 58.7064
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.14s
                      Time elapsed: 00:27:26
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 46837 steps/s (collection: 1.998s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 148.5901
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.1417
                       Mean reward: 318.74
               Mean episode length: 193.93
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 61.0552
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.10s
                      Time elapsed: 00:27:28
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 46498 steps/s (collection: 2.015s, learning 0.099s)
             Mean action noise std: 1.67
          Mean value_function loss: 157.0282
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.1471
                       Mean reward: 281.94
               Mean episode length: 177.20
    Episode_Reward/reaching_object: 0.6761
     Episode_Reward/lifting_object: 58.1869
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.11s
                      Time elapsed: 00:27:30
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 46613 steps/s (collection: 2.017s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 162.8547
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.1520
                       Mean reward: 270.80
               Mean episode length: 168.74
    Episode_Reward/reaching_object: 0.6741
     Episode_Reward/lifting_object: 57.8506
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.11s
                      Time elapsed: 00:27:32
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 46523 steps/s (collection: 2.015s, learning 0.098s)
             Mean action noise std: 1.67
          Mean value_function loss: 151.9331
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.1547
                       Mean reward: 318.23
               Mean episode length: 192.25
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 61.1754
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.11s
                      Time elapsed: 00:27:34
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 45696 steps/s (collection: 2.057s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 161.4347
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.1567
                       Mean reward: 300.00
               Mean episode length: 182.21
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 60.5994
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.15s
                      Time elapsed: 00:27:36
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 46839 steps/s (collection: 2.004s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 162.9312
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.1583
                       Mean reward: 283.49
               Mean episode length: 171.73
    Episode_Reward/reaching_object: 0.6722
     Episode_Reward/lifting_object: 58.0520
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.10s
                      Time elapsed: 00:27:38
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 47316 steps/s (collection: 1.980s, learning 0.098s)
             Mean action noise std: 1.67
          Mean value_function loss: 158.6327
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.1623
                       Mean reward: 288.77
               Mean episode length: 177.61
    Episode_Reward/reaching_object: 0.6769
     Episode_Reward/lifting_object: 58.5862
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.08s
                      Time elapsed: 00:27:41
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 46269 steps/s (collection: 2.022s, learning 0.103s)
             Mean action noise std: 1.67
          Mean value_function loss: 177.4390
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.1679
                       Mean reward: 297.78
               Mean episode length: 180.33
    Episode_Reward/reaching_object: 0.6738
     Episode_Reward/lifting_object: 57.5034
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.12s
                      Time elapsed: 00:27:43
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 45551 steps/s (collection: 2.057s, learning 0.101s)
             Mean action noise std: 1.68
          Mean value_function loss: 164.8102
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.1706
                       Mean reward: 310.69
               Mean episode length: 186.51
    Episode_Reward/reaching_object: 0.6600
     Episode_Reward/lifting_object: 56.7032
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.16s
                      Time elapsed: 00:27:45
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 43259 steps/s (collection: 2.129s, learning 0.144s)
             Mean action noise std: 1.68
          Mean value_function loss: 148.3230
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.1727
                       Mean reward: 315.65
               Mean episode length: 188.31
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: 59.6773
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.27s
                      Time elapsed: 00:27:47
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 45817 steps/s (collection: 2.034s, learning 0.112s)
             Mean action noise std: 1.68
          Mean value_function loss: 161.5478
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.1703
                       Mean reward: 285.14
               Mean episode length: 171.56
    Episode_Reward/reaching_object: 0.6898
     Episode_Reward/lifting_object: 59.2893
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.15s
                      Time elapsed: 00:27:49
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 46567 steps/s (collection: 2.021s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 167.4960
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.1688
                       Mean reward: 288.31
               Mean episode length: 174.61
    Episode_Reward/reaching_object: 0.6787
     Episode_Reward/lifting_object: 58.3028
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.11s
                      Time elapsed: 00:27:51
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 47023 steps/s (collection: 2.004s, learning 0.087s)
             Mean action noise std: 1.68
          Mean value_function loss: 143.0723
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.1681
                       Mean reward: 308.07
               Mean episode length: 186.50
    Episode_Reward/reaching_object: 0.7028
     Episode_Reward/lifting_object: 60.7211
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.09s
                      Time elapsed: 00:27:53
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 46481 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 171.1262
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.1729
                       Mean reward: 282.31
               Mean episode length: 176.21
    Episode_Reward/reaching_object: 0.6964
     Episode_Reward/lifting_object: 59.9070
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.11s
                      Time elapsed: 00:27:56
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 46469 steps/s (collection: 2.005s, learning 0.111s)
             Mean action noise std: 1.68
          Mean value_function loss: 155.6149
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.1802
                       Mean reward: 307.23
               Mean episode length: 185.85
    Episode_Reward/reaching_object: 0.6867
     Episode_Reward/lifting_object: 58.8111
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.12s
                      Time elapsed: 00:27:58
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 47263 steps/s (collection: 1.988s, learning 0.092s)
             Mean action noise std: 1.68
          Mean value_function loss: 201.4538
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.1803
                       Mean reward: 305.05
               Mean episode length: 180.85
    Episode_Reward/reaching_object: 0.6827
     Episode_Reward/lifting_object: 59.6494
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.08s
                      Time elapsed: 00:28:00
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 46273 steps/s (collection: 2.019s, learning 0.105s)
             Mean action noise std: 1.68
          Mean value_function loss: 189.5127
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.1835
                       Mean reward: 283.60
               Mean episode length: 167.72
    Episode_Reward/reaching_object: 0.6843
     Episode_Reward/lifting_object: 59.2577
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.12s
                      Time elapsed: 00:28:02
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 46142 steps/s (collection: 2.041s, learning 0.089s)
             Mean action noise std: 1.68
          Mean value_function loss: 167.5538
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.1849
                       Mean reward: 294.05
               Mean episode length: 184.43
    Episode_Reward/reaching_object: 0.7106
     Episode_Reward/lifting_object: 61.0589
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.13s
                      Time elapsed: 00:28:04
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 46203 steps/s (collection: 2.034s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 161.7384
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.1863
                       Mean reward: 305.00
               Mean episode length: 180.07
    Episode_Reward/reaching_object: 0.6953
     Episode_Reward/lifting_object: 60.9081
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.13s
                      Time elapsed: 00:28:06
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 42500 steps/s (collection: 2.133s, learning 0.180s)
             Mean action noise std: 1.68
          Mean value_function loss: 171.5867
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 34.1874
                       Mean reward: 285.13
               Mean episode length: 172.86
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: 59.9967
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.31s
                      Time elapsed: 00:28:08
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 45279 steps/s (collection: 2.073s, learning 0.098s)
             Mean action noise std: 1.68
          Mean value_function loss: 159.7325
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.1880
                       Mean reward: 317.53
               Mean episode length: 184.04
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: 61.1269
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.17s
                      Time elapsed: 00:28:11
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 46858 steps/s (collection: 2.013s, learning 0.085s)
             Mean action noise std: 1.68
          Mean value_function loss: 251.0036
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.1891
                       Mean reward: 321.01
               Mean episode length: 188.30
    Episode_Reward/reaching_object: 0.6872
     Episode_Reward/lifting_object: 59.1706
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.10s
                      Time elapsed: 00:28:13
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 44907 steps/s (collection: 2.099s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 238.0503
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.1913
                       Mean reward: 314.25
               Mean episode length: 187.78
    Episode_Reward/reaching_object: 0.6826
     Episode_Reward/lifting_object: 59.6590
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.19s
                      Time elapsed: 00:28:15
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 46830 steps/s (collection: 1.996s, learning 0.104s)
             Mean action noise std: 1.68
          Mean value_function loss: 149.7531
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.1937
                       Mean reward: 307.44
               Mean episode length: 183.26
    Episode_Reward/reaching_object: 0.6941
     Episode_Reward/lifting_object: 60.2269
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.10s
                      Time elapsed: 00:28:17
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 46444 steps/s (collection: 2.011s, learning 0.105s)
             Mean action noise std: 1.68
          Mean value_function loss: 142.2600
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.1949
                       Mean reward: 309.12
               Mean episode length: 183.84
    Episode_Reward/reaching_object: 0.6791
     Episode_Reward/lifting_object: 59.0920
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.12s
                      Time elapsed: 00:28:19
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 45985 steps/s (collection: 2.032s, learning 0.106s)
             Mean action noise std: 1.68
          Mean value_function loss: 153.2783
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.1952
                       Mean reward: 302.04
               Mean episode length: 182.45
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 62.0165
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.14s
                      Time elapsed: 00:28:21
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 46114 steps/s (collection: 2.022s, learning 0.110s)
             Mean action noise std: 1.68
          Mean value_function loss: 157.0861
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.1935
                       Mean reward: 326.91
               Mean episode length: 195.39
    Episode_Reward/reaching_object: 0.7118
     Episode_Reward/lifting_object: 62.5396
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.13s
                      Time elapsed: 00:28:23
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 47431 steps/s (collection: 1.985s, learning 0.088s)
             Mean action noise std: 1.68
          Mean value_function loss: 160.3679
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.1950
                       Mean reward: 316.70
               Mean episode length: 186.36
    Episode_Reward/reaching_object: 0.7187
     Episode_Reward/lifting_object: 62.2083
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.07s
                      Time elapsed: 00:28:25
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 46830 steps/s (collection: 2.012s, learning 0.088s)
             Mean action noise std: 1.68
          Mean value_function loss: 169.1366
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.1987
                       Mean reward: 302.71
               Mean episode length: 184.88
    Episode_Reward/reaching_object: 0.7259
     Episode_Reward/lifting_object: 63.3752
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.10s
                      Time elapsed: 00:28:28
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 43833 steps/s (collection: 2.120s, learning 0.123s)
             Mean action noise std: 1.68
          Mean value_function loss: 147.6617
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.1973
                       Mean reward: 320.78
               Mean episode length: 189.35
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 62.0175
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.24s
                      Time elapsed: 00:28:30
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 45062 steps/s (collection: 2.051s, learning 0.130s)
             Mean action noise std: 1.68
          Mean value_function loss: 146.8526
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.1976
                       Mean reward: 298.77
               Mean episode length: 179.06
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: 61.0362
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.18s
                      Time elapsed: 00:28:32
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 41147 steps/s (collection: 2.239s, learning 0.150s)
             Mean action noise std: 1.68
          Mean value_function loss: 165.0543
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.1977
                       Mean reward: 293.08
               Mean episode length: 175.16
    Episode_Reward/reaching_object: 0.7015
     Episode_Reward/lifting_object: 62.0579
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.39s
                      Time elapsed: 00:28:34
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 44048 steps/s (collection: 2.138s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 152.3937
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.1966
                       Mean reward: 321.13
               Mean episode length: 192.08
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: 61.5197
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.23s
                      Time elapsed: 00:28:37
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 44990 steps/s (collection: 2.082s, learning 0.103s)
             Mean action noise std: 1.68
          Mean value_function loss: 163.7553
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.1949
                       Mean reward: 293.63
               Mean episode length: 178.76
    Episode_Reward/reaching_object: 0.6980
     Episode_Reward/lifting_object: 60.9116
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.19s
                      Time elapsed: 00:28:39
                               ETA: 00:47:05

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 44656 steps/s (collection: 2.035s, learning 0.166s)
             Mean action noise std: 1.68
          Mean value_function loss: 148.4345
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.1933
                       Mean reward: 301.98
               Mean episode length: 183.72
    Episode_Reward/reaching_object: 0.7075
     Episode_Reward/lifting_object: 62.4136
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.20s
                      Time elapsed: 00:28:41
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 43183 steps/s (collection: 2.163s, learning 0.114s)
             Mean action noise std: 1.68
          Mean value_function loss: 163.5730
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.1920
                       Mean reward: 302.72
               Mean episode length: 179.17
    Episode_Reward/reaching_object: 0.6865
     Episode_Reward/lifting_object: 60.8514
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.28s
                      Time elapsed: 00:28:43
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 43600 steps/s (collection: 2.150s, learning 0.104s)
             Mean action noise std: 1.68
          Mean value_function loss: 164.0107
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.1908
                       Mean reward: 295.82
               Mean episode length: 175.25
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 61.4985
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.25s
                      Time elapsed: 00:28:46
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 44588 steps/s (collection: 2.054s, learning 0.150s)
             Mean action noise std: 1.68
          Mean value_function loss: 163.0733
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.1930
                       Mean reward: 295.22
               Mean episode length: 178.24
    Episode_Reward/reaching_object: 0.6870
     Episode_Reward/lifting_object: 59.9793
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.20s
                      Time elapsed: 00:28:48
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 42902 steps/s (collection: 2.151s, learning 0.140s)
             Mean action noise std: 1.68
          Mean value_function loss: 146.1771
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.1983
                       Mean reward: 317.27
               Mean episode length: 185.63
    Episode_Reward/reaching_object: 0.7110
     Episode_Reward/lifting_object: 63.4820
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.29s
                      Time elapsed: 00:28:50
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 46998 steps/s (collection: 1.997s, learning 0.095s)
             Mean action noise std: 1.68
          Mean value_function loss: 155.4973
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.1985
                       Mean reward: 307.99
               Mean episode length: 177.67
    Episode_Reward/reaching_object: 0.7052
     Episode_Reward/lifting_object: 63.2002
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.09s
                      Time elapsed: 00:28:52
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 45766 steps/s (collection: 2.062s, learning 0.086s)
             Mean action noise std: 1.68
          Mean value_function loss: 148.0965
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 34.1985
                       Mean reward: 315.87
               Mean episode length: 187.04
    Episode_Reward/reaching_object: 0.7109
     Episode_Reward/lifting_object: 63.0651
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.15s
                      Time elapsed: 00:28:54
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 45890 steps/s (collection: 2.043s, learning 0.100s)
             Mean action noise std: 1.68
          Mean value_function loss: 155.5898
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 34.1985
                       Mean reward: 304.22
               Mean episode length: 183.48
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 62.8375
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.14s
                      Time elapsed: 00:28:56
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 46460 steps/s (collection: 2.030s, learning 0.086s)
             Mean action noise std: 1.68
          Mean value_function loss: 170.2841
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 34.1990
                       Mean reward: 306.45
               Mean episode length: 181.59
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 61.4451
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.12s
                      Time elapsed: 00:28:59
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 46431 steps/s (collection: 2.023s, learning 0.095s)
             Mean action noise std: 1.68
          Mean value_function loss: 149.2085
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.1996
                       Mean reward: 335.14
               Mean episode length: 194.58
    Episode_Reward/reaching_object: 0.7071
     Episode_Reward/lifting_object: 63.6573
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.12s
                      Time elapsed: 00:29:01
                               ETA: 00:46:41

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 47152 steps/s (collection: 2.000s, learning 0.085s)
             Mean action noise std: 1.68
          Mean value_function loss: 157.0646
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.1993
                       Mean reward: 295.30
               Mean episode length: 169.21
    Episode_Reward/reaching_object: 0.6987
     Episode_Reward/lifting_object: 62.4608
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.08s
                      Time elapsed: 00:29:03
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 43734 steps/s (collection: 2.083s, learning 0.165s)
             Mean action noise std: 1.68
          Mean value_function loss: 156.2637
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.1970
                       Mean reward: 288.08
               Mean episode length: 172.48
    Episode_Reward/reaching_object: 0.6821
     Episode_Reward/lifting_object: 60.7901
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.25s
                      Time elapsed: 00:29:05
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 47193 steps/s (collection: 1.970s, learning 0.113s)
             Mean action noise std: 1.68
          Mean value_function loss: 159.8601
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.1968
                       Mean reward: 325.78
               Mean episode length: 190.29
    Episode_Reward/reaching_object: 0.6786
     Episode_Reward/lifting_object: 60.7974
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.08s
                      Time elapsed: 00:29:07
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 46737 steps/s (collection: 2.006s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 155.9446
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.1985
                       Mean reward: 323.68
               Mean episode length: 189.49
    Episode_Reward/reaching_object: 0.7033
     Episode_Reward/lifting_object: 62.1293
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.10s
                      Time elapsed: 00:29:09
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 46291 steps/s (collection: 2.029s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 269.4868
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.2056
                       Mean reward: 320.16
               Mean episode length: 186.22
    Episode_Reward/reaching_object: 0.6907
     Episode_Reward/lifting_object: 62.2404
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.12s
                      Time elapsed: 00:29:11
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 47250 steps/s (collection: 1.978s, learning 0.103s)
             Mean action noise std: 1.69
          Mean value_function loss: 248.5610
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.2133
                       Mean reward: 312.45
               Mean episode length: 179.37
    Episode_Reward/reaching_object: 0.6888
     Episode_Reward/lifting_object: 61.7244
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.08s
                      Time elapsed: 00:29:13
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 46967 steps/s (collection: 2.004s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 152.2850
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.2191
                       Mean reward: 323.61
               Mean episode length: 187.15
    Episode_Reward/reaching_object: 0.7099
     Episode_Reward/lifting_object: 64.1886
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.09s
                      Time elapsed: 00:29:15
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 47433 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 164.9659
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.2214
                       Mean reward: 280.70
               Mean episode length: 168.44
    Episode_Reward/reaching_object: 0.6972
     Episode_Reward/lifting_object: 62.4253
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.07s
                      Time elapsed: 00:29:18
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 47349 steps/s (collection: 1.984s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 163.0666
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.2236
                       Mean reward: 336.52
               Mean episode length: 191.91
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 62.9459
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.08s
                      Time elapsed: 00:29:20
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 47153 steps/s (collection: 1.994s, learning 0.090s)
             Mean action noise std: 1.69
          Mean value_function loss: 159.8462
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.2276
                       Mean reward: 313.41
               Mean episode length: 182.82
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 62.9693
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.08s
                      Time elapsed: 00:29:22
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 47498 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 158.0913
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.2316
                       Mean reward: 316.27
               Mean episode length: 181.93
    Episode_Reward/reaching_object: 0.7117
     Episode_Reward/lifting_object: 63.6974
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.07s
                      Time elapsed: 00:29:24
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 47315 steps/s (collection: 1.989s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 158.6746
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.2323
                       Mean reward: 310.86
               Mean episode length: 180.73
    Episode_Reward/reaching_object: 0.7118
     Episode_Reward/lifting_object: 63.6129
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.08s
                      Time elapsed: 00:29:26
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 43069 steps/s (collection: 2.134s, learning 0.148s)
             Mean action noise std: 1.69
          Mean value_function loss: 163.2110
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.2321
                       Mean reward: 326.39
               Mean episode length: 190.39
    Episode_Reward/reaching_object: 0.7036
     Episode_Reward/lifting_object: 63.4545
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.28s
                      Time elapsed: 00:29:28
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 43162 steps/s (collection: 2.147s, learning 0.131s)
             Mean action noise std: 1.69
          Mean value_function loss: 184.3664
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.2331
                       Mean reward: 327.25
               Mean episode length: 185.18
    Episode_Reward/reaching_object: 0.6954
     Episode_Reward/lifting_object: 63.4628
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.28s
                      Time elapsed: 00:29:30
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 45667 steps/s (collection: 2.053s, learning 0.100s)
             Mean action noise std: 1.69
          Mean value_function loss: 170.9601
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.2362
                       Mean reward: 344.15
               Mean episode length: 193.80
    Episode_Reward/reaching_object: 0.7033
     Episode_Reward/lifting_object: 63.8591
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.15s
                      Time elapsed: 00:29:33
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 46655 steps/s (collection: 2.015s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 181.6763
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.2376
                       Mean reward: 318.91
               Mean episode length: 180.31
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 64.1264
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.11s
                      Time elapsed: 00:29:35
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 46848 steps/s (collection: 2.009s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 167.1737
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.2379
                       Mean reward: 324.13
               Mean episode length: 188.09
    Episode_Reward/reaching_object: 0.7101
     Episode_Reward/lifting_object: 64.4108
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.10s
                      Time elapsed: 00:29:37
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 46786 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 1.69
          Mean value_function loss: 179.6344
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.2390
                       Mean reward: 316.74
               Mean episode length: 178.83
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 63.2676
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.10s
                      Time elapsed: 00:29:39
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 47219 steps/s (collection: 1.991s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 177.9260
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.2430
                       Mean reward: 339.09
               Mean episode length: 194.67
    Episode_Reward/reaching_object: 0.7204
     Episode_Reward/lifting_object: 65.5659
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.08s
                      Time elapsed: 00:29:41
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 46630 steps/s (collection: 2.014s, learning 0.094s)
             Mean action noise std: 1.69
          Mean value_function loss: 199.7350
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.2488
                       Mean reward: 313.35
               Mean episode length: 178.97
    Episode_Reward/reaching_object: 0.7040
     Episode_Reward/lifting_object: 64.4879
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.11s
                      Time elapsed: 00:29:43
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 46270 steps/s (collection: 2.000s, learning 0.125s)
             Mean action noise std: 1.69
          Mean value_function loss: 192.6988
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.2536
                       Mean reward: 320.46
               Mean episode length: 179.09
    Episode_Reward/reaching_object: 0.6819
     Episode_Reward/lifting_object: 62.0068
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.12s
                      Time elapsed: 00:29:45
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 45882 steps/s (collection: 2.057s, learning 0.086s)
             Mean action noise std: 1.69
          Mean value_function loss: 183.9091
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.2564
                       Mean reward: 308.85
               Mean episode length: 173.49
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 60.8301
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.14s
                      Time elapsed: 00:29:47
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 44941 steps/s (collection: 2.103s, learning 0.084s)
             Mean action noise std: 1.69
          Mean value_function loss: 186.8938
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.2577
                       Mean reward: 340.46
               Mean episode length: 188.85
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 63.5001
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.19s
                      Time elapsed: 00:29:49
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 47211 steps/s (collection: 1.993s, learning 0.090s)
             Mean action noise std: 1.69
          Mean value_function loss: 180.5347
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.2559
                       Mean reward: 316.00
               Mean episode length: 181.27
    Episode_Reward/reaching_object: 0.6896
     Episode_Reward/lifting_object: 62.9689
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.08s
                      Time elapsed: 00:29:52
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 45188 steps/s (collection: 2.077s, learning 0.098s)
             Mean action noise std: 1.69
          Mean value_function loss: 173.0151
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.2526
                       Mean reward: 334.86
               Mean episode length: 190.37
    Episode_Reward/reaching_object: 0.7136
     Episode_Reward/lifting_object: 64.3977
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.18s
                      Time elapsed: 00:29:54
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 46955 steps/s (collection: 2.003s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 171.2377
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.2507
                       Mean reward: 325.36
               Mean episode length: 183.76
    Episode_Reward/reaching_object: 0.7124
     Episode_Reward/lifting_object: 65.2705
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.09s
                      Time elapsed: 00:29:56
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 46731 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 1.69
          Mean value_function loss: 172.4844
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.2522
                       Mean reward: 339.72
               Mean episode length: 189.56
    Episode_Reward/reaching_object: 0.7150
     Episode_Reward/lifting_object: 65.9451
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.10s
                      Time elapsed: 00:29:58
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 44392 steps/s (collection: 2.067s, learning 0.147s)
             Mean action noise std: 1.69
          Mean value_function loss: 173.8423
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 34.2536
                       Mean reward: 301.66
               Mean episode length: 174.76
    Episode_Reward/reaching_object: 0.6930
     Episode_Reward/lifting_object: 62.7059
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.21s
                      Time elapsed: 00:30:00
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 44359 steps/s (collection: 2.043s, learning 0.173s)
             Mean action noise std: 1.69
          Mean value_function loss: 153.9001
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.2548
                       Mean reward: 314.64
               Mean episode length: 178.74
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 63.5214
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.22s
                      Time elapsed: 00:30:02
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 43989 steps/s (collection: 2.144s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 166.4531
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.2551
                       Mean reward: 348.82
               Mean episode length: 193.97
    Episode_Reward/reaching_object: 0.7147
     Episode_Reward/lifting_object: 65.6288
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.23s
                      Time elapsed: 00:30:05
                               ETA: 00:45:26

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 43526 steps/s (collection: 2.137s, learning 0.121s)
             Mean action noise std: 1.69
          Mean value_function loss: 183.3221
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.2554
                       Mean reward: 343.83
               Mean episode length: 192.22
    Episode_Reward/reaching_object: 0.7069
     Episode_Reward/lifting_object: 65.6160
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.26s
                      Time elapsed: 00:30:07
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 45676 steps/s (collection: 2.054s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 176.4573
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.2588
                       Mean reward: 322.91
               Mean episode length: 183.79
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: 64.6718
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.15s
                      Time elapsed: 00:30:09
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 44138 steps/s (collection: 2.092s, learning 0.135s)
             Mean action noise std: 1.70
          Mean value_function loss: 182.2888
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.2605
                       Mean reward: 311.78
               Mean episode length: 178.36
    Episode_Reward/reaching_object: 0.6921
     Episode_Reward/lifting_object: 63.4064
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.23s
                      Time elapsed: 00:30:11
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 45466 steps/s (collection: 2.055s, learning 0.107s)
             Mean action noise std: 1.70
          Mean value_function loss: 181.6021
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 34.2619
                       Mean reward: 337.93
               Mean episode length: 187.37
    Episode_Reward/reaching_object: 0.6953
     Episode_Reward/lifting_object: 64.6589
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.16s
                      Time elapsed: 00:30:13
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 45836 steps/s (collection: 2.062s, learning 0.083s)
             Mean action noise std: 1.70
          Mean value_function loss: 206.9293
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.2630
                       Mean reward: 314.87
               Mean episode length: 175.87
    Episode_Reward/reaching_object: 0.7105
     Episode_Reward/lifting_object: 66.2202
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.14s
                      Time elapsed: 00:30:16
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 46012 steps/s (collection: 2.037s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 173.4359
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.2662
                       Mean reward: 324.42
               Mean episode length: 180.43
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 64.4448
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.14s
                      Time elapsed: 00:30:18
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 44719 steps/s (collection: 2.098s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 173.5116
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.2686
                       Mean reward: 318.59
               Mean episode length: 176.87
    Episode_Reward/reaching_object: 0.7011
     Episode_Reward/lifting_object: 64.9574
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.20s
                      Time elapsed: 00:30:20
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.137s, learning 0.102s)
             Mean action noise std: 1.70
          Mean value_function loss: 155.1860
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.2730
                       Mean reward: 336.86
               Mean episode length: 189.78
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 67.7213
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.24s
                      Time elapsed: 00:30:22
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 45538 steps/s (collection: 2.067s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 166.0125
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.2731
                       Mean reward: 334.83
               Mean episode length: 184.69
    Episode_Reward/reaching_object: 0.7088
     Episode_Reward/lifting_object: 65.4326
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.16s
                      Time elapsed: 00:30:24
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 46724 steps/s (collection: 2.013s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 171.8247
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.2700
                       Mean reward: 337.27
               Mean episode length: 189.83
    Episode_Reward/reaching_object: 0.7156
     Episode_Reward/lifting_object: 66.0249
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.10s
                      Time elapsed: 00:30:26
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 45506 steps/s (collection: 2.058s, learning 0.102s)
             Mean action noise std: 1.70
          Mean value_function loss: 190.3982
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.2697
                       Mean reward: 305.39
               Mean episode length: 174.38
    Episode_Reward/reaching_object: 0.6953
     Episode_Reward/lifting_object: 64.5562
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.16s
                      Time elapsed: 00:30:29
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 45064 steps/s (collection: 2.079s, learning 0.103s)
             Mean action noise std: 1.70
          Mean value_function loss: 175.2959
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.2743
                       Mean reward: 346.64
               Mean episode length: 191.56
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 69.0045
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.18s
                      Time elapsed: 00:30:31
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 46187 steps/s (collection: 2.021s, learning 0.107s)
             Mean action noise std: 1.70
          Mean value_function loss: 165.7851
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.2806
                       Mean reward: 340.38
               Mean episode length: 186.00
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 68.2316
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.13s
                      Time elapsed: 00:30:33
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 45752 steps/s (collection: 2.057s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 179.7863
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.2870
                       Mean reward: 313.59
               Mean episode length: 176.96
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 66.9665
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.15s
                      Time elapsed: 00:30:35
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 45516 steps/s (collection: 2.064s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 162.9549
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.2973
                       Mean reward: 361.81
               Mean episode length: 198.43
    Episode_Reward/reaching_object: 0.7219
     Episode_Reward/lifting_object: 66.8370
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.16s
                      Time elapsed: 00:30:37
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 45222 steps/s (collection: 2.076s, learning 0.098s)
             Mean action noise std: 1.70
          Mean value_function loss: 180.6877
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 34.3024
                       Mean reward: 348.07
               Mean episode length: 189.82
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 67.8208
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.17s
                      Time elapsed: 00:30:39
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 40236 steps/s (collection: 2.344s, learning 0.099s)
             Mean action noise std: 1.70
          Mean value_function loss: 169.7279
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.3011
                       Mean reward: 337.26
               Mean episode length: 185.58
    Episode_Reward/reaching_object: 0.7090
     Episode_Reward/lifting_object: 65.8450
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.44s
                      Time elapsed: 00:30:42
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 36608 steps/s (collection: 2.318s, learning 0.367s)
             Mean action noise std: 1.70
          Mean value_function loss: 192.5892
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 34.3010
                       Mean reward: 339.57
               Mean episode length: 185.27
    Episode_Reward/reaching_object: 0.7094
     Episode_Reward/lifting_object: 65.3528
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.69s
                      Time elapsed: 00:30:44
                               ETA: 00:44:44

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 38100 steps/s (collection: 2.476s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 175.8843
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.3041
                       Mean reward: 330.12
               Mean episode length: 182.16
    Episode_Reward/reaching_object: 0.7051
     Episode_Reward/lifting_object: 65.5891
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.58s
                      Time elapsed: 00:30:47
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 40530 steps/s (collection: 2.307s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 166.5603
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 34.3105
                       Mean reward: 314.17
               Mean episode length: 174.67
    Episode_Reward/reaching_object: 0.6920
     Episode_Reward/lifting_object: 64.4917
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.43s
                      Time elapsed: 00:30:49
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 45190 steps/s (collection: 2.083s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 164.0952
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.3124
                       Mean reward: 346.38
               Mean episode length: 194.68
    Episode_Reward/reaching_object: 0.7276
     Episode_Reward/lifting_object: 67.8805
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.18s
                      Time elapsed: 00:30:52
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 46419 steps/s (collection: 2.028s, learning 0.090s)
             Mean action noise std: 1.70
          Mean value_function loss: 181.4041
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.3139
                       Mean reward: 320.62
               Mean episode length: 179.66
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: 65.8434
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.12s
                      Time elapsed: 00:30:54
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 45664 steps/s (collection: 2.048s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 165.3679
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.3124
                       Mean reward: 348.57
               Mean episode length: 191.01
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 67.0783
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.15s
                      Time elapsed: 00:30:56
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 45914 steps/s (collection: 2.038s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 179.6892
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.3122
                       Mean reward: 307.50
               Mean episode length: 172.98
    Episode_Reward/reaching_object: 0.7013
     Episode_Reward/lifting_object: 65.3780
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.14s
                      Time elapsed: 00:30:58
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 46472 steps/s (collection: 2.017s, learning 0.098s)
             Mean action noise std: 1.71
          Mean value_function loss: 169.9683
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.3116
                       Mean reward: 346.03
               Mean episode length: 189.34
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 64.8039
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.12s
                      Time elapsed: 00:31:00
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 46638 steps/s (collection: 1.999s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 167.3347
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 34.3169
                       Mean reward: 316.46
               Mean episode length: 176.32
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 67.5952
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.11s
                      Time elapsed: 00:31:02
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 45958 steps/s (collection: 2.036s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 171.6309
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.3193
                       Mean reward: 324.01
               Mean episode length: 178.64
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 68.0406
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.14s
                      Time elapsed: 00:31:04
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 45417 steps/s (collection: 2.046s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 167.7358
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.3208
                       Mean reward: 355.45
               Mean episode length: 196.67
    Episode_Reward/reaching_object: 0.7282
     Episode_Reward/lifting_object: 68.2943
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.16s
                      Time elapsed: 00:31:07
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 44993 steps/s (collection: 2.094s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 169.5360
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.3179
                       Mean reward: 345.00
               Mean episode length: 183.62
    Episode_Reward/reaching_object: 0.7329
     Episode_Reward/lifting_object: 69.6615
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.18s
                      Time elapsed: 00:31:09
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.049s, learning 0.087s)
             Mean action noise std: 1.71
          Mean value_function loss: 175.3859
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.3180
                       Mean reward: 368.93
               Mean episode length: 201.93
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 68.2229
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.14s
                      Time elapsed: 00:31:11
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 45185 steps/s (collection: 2.056s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 173.4215
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.3257
                       Mean reward: 349.40
               Mean episode length: 190.40
    Episode_Reward/reaching_object: 0.7072
     Episode_Reward/lifting_object: 66.4216
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.18s
                      Time elapsed: 00:31:13
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 43745 steps/s (collection: 2.124s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 171.1200
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 34.3306
                       Mean reward: 334.37
               Mean episode length: 181.30
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 67.5123
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.25s
                      Time elapsed: 00:31:15
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 42140 steps/s (collection: 2.179s, learning 0.153s)
             Mean action noise std: 1.71
          Mean value_function loss: 181.0365
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.3303
                       Mean reward: 327.92
               Mean episode length: 177.24
    Episode_Reward/reaching_object: 0.6912
     Episode_Reward/lifting_object: 65.5264
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.33s
                      Time elapsed: 00:31:18
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 42362 steps/s (collection: 2.233s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 179.5159
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.3342
                       Mean reward: 328.38
               Mean episode length: 178.48
    Episode_Reward/reaching_object: 0.7112
     Episode_Reward/lifting_object: 67.0961
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.32s
                      Time elapsed: 00:31:20
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 39383 steps/s (collection: 2.249s, learning 0.247s)
             Mean action noise std: 1.71
          Mean value_function loss: 168.6363
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.3393
                       Mean reward: 344.53
               Mean episode length: 189.34
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 70.6143
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.50s
                      Time elapsed: 00:31:22
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 37604 steps/s (collection: 2.509s, learning 0.106s)
             Mean action noise std: 1.71
          Mean value_function loss: 174.2699
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.3401
                       Mean reward: 333.35
               Mean episode length: 178.07
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 67.2082
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.61s
                      Time elapsed: 00:31:25
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 39211 steps/s (collection: 2.341s, learning 0.166s)
             Mean action noise std: 1.71
          Mean value_function loss: 172.3180
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.3392
                       Mean reward: 340.33
               Mean episode length: 184.06
    Episode_Reward/reaching_object: 0.7254
     Episode_Reward/lifting_object: 69.3491
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.51s
                      Time elapsed: 00:31:28
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 42685 steps/s (collection: 2.206s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 192.8657
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.3356
                       Mean reward: 338.95
               Mean episode length: 184.51
    Episode_Reward/reaching_object: 0.7053
     Episode_Reward/lifting_object: 67.4042
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.30s
                      Time elapsed: 00:31:30
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 43335 steps/s (collection: 2.074s, learning 0.195s)
             Mean action noise std: 1.71
          Mean value_function loss: 183.8864
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.3399
                       Mean reward: 342.91
               Mean episode length: 186.05
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: 69.2827
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.27s
                      Time elapsed: 00:31:32
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 41946 steps/s (collection: 2.247s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 185.2474
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.3430
                       Mean reward: 357.30
               Mean episode length: 187.21
    Episode_Reward/reaching_object: 0.7170
     Episode_Reward/lifting_object: 68.2464
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.34s
                      Time elapsed: 00:31:35
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 42724 steps/s (collection: 2.198s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 181.7289
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 34.3445
                       Mean reward: 353.03
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 0.7185
     Episode_Reward/lifting_object: 68.2869
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.30s
                      Time elapsed: 00:31:37
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 45029 steps/s (collection: 2.092s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 182.9726
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.3452
                       Mean reward: 370.97
               Mean episode length: 196.05
    Episode_Reward/reaching_object: 0.7118
     Episode_Reward/lifting_object: 68.2402
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.18s
                      Time elapsed: 00:31:39
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 45464 steps/s (collection: 2.077s, learning 0.085s)
             Mean action noise std: 1.71
          Mean value_function loss: 189.0478
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 34.3467
                       Mean reward: 319.46
               Mean episode length: 174.47
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 66.2876
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.16s
                      Time elapsed: 00:31:41
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 47090 steps/s (collection: 1.994s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 176.3893
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.3469
                       Mean reward: 346.81
               Mean episode length: 186.76
    Episode_Reward/reaching_object: 0.7068
     Episode_Reward/lifting_object: 67.3575
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.09s
                      Time elapsed: 00:31:43
                               ETA: 00:43:45

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 45138 steps/s (collection: 2.067s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 179.8721
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.3475
                       Mean reward: 365.68
               Mean episode length: 192.37
    Episode_Reward/reaching_object: 0.7235
     Episode_Reward/lifting_object: 68.6798
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.18s
                      Time elapsed: 00:31:45
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 45987 steps/s (collection: 2.023s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 181.0070
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.3467
                       Mean reward: 338.89
               Mean episode length: 180.32
    Episode_Reward/reaching_object: 0.7232
     Episode_Reward/lifting_object: 69.3161
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.14s
                      Time elapsed: 00:31:48
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 46541 steps/s (collection: 2.008s, learning 0.105s)
             Mean action noise std: 1.71
          Mean value_function loss: 190.7285
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.3493
                       Mean reward: 338.02
               Mean episode length: 182.29
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 65.9650
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.11s
                      Time elapsed: 00:31:50
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 46673 steps/s (collection: 2.006s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 181.4169
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.3528
                       Mean reward: 334.35
               Mean episode length: 181.21
    Episode_Reward/reaching_object: 0.7090
     Episode_Reward/lifting_object: 67.4333
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.11s
                      Time elapsed: 00:31:52
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 45114 steps/s (collection: 2.087s, learning 0.092s)
             Mean action noise std: 1.71
          Mean value_function loss: 177.1017
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.3588
                       Mean reward: 323.45
               Mean episode length: 173.87
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: 69.0031
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.18s
                      Time elapsed: 00:31:54
                               ETA: 00:43:33

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 47114 steps/s (collection: 1.989s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 174.6425
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.3694
                       Mean reward: 371.60
               Mean episode length: 195.01
    Episode_Reward/reaching_object: 0.7210
     Episode_Reward/lifting_object: 69.2255
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.09s
                      Time elapsed: 00:31:56
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 46639 steps/s (collection: 2.016s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 179.0342
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 34.3744
                       Mean reward: 327.88
               Mean episode length: 179.97
    Episode_Reward/reaching_object: 0.7130
     Episode_Reward/lifting_object: 67.2827
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.11s
                      Time elapsed: 00:31:58
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 46125 steps/s (collection: 2.030s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 179.3995
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 34.3789
                       Mean reward: 337.14
               Mean episode length: 179.40
    Episode_Reward/reaching_object: 0.7144
     Episode_Reward/lifting_object: 68.4896
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.13s
                      Time elapsed: 00:32:00
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 46451 steps/s (collection: 2.020s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 182.5699
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.3805
                       Mean reward: 357.15
               Mean episode length: 191.32
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 69.0282
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.12s
                      Time elapsed: 00:32:02
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 45901 steps/s (collection: 2.039s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 194.8463
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.3810
                       Mean reward: 376.43
               Mean episode length: 195.16
    Episode_Reward/reaching_object: 0.7256
     Episode_Reward/lifting_object: 68.9759
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.14s
                      Time elapsed: 00:32:05
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 46436 steps/s (collection: 2.010s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 180.8124
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.3820
                       Mean reward: 367.42
               Mean episode length: 199.35
    Episode_Reward/reaching_object: 0.7290
     Episode_Reward/lifting_object: 68.6105
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.12s
                      Time elapsed: 00:32:07
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 44464 steps/s (collection: 2.108s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 180.3942
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.3870
                       Mean reward: 343.27
               Mean episode length: 181.42
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 70.4557
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.21s
                      Time elapsed: 00:32:09
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 46739 steps/s (collection: 2.002s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 177.5970
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.3891
                       Mean reward: 376.90
               Mean episode length: 199.09
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 70.7776
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.10s
                      Time elapsed: 00:32:11
                               ETA: 00:43:14

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 46913 steps/s (collection: 1.998s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 166.4124
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.3896
                       Mean reward: 347.36
               Mean episode length: 186.22
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 69.5883
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.10s
                      Time elapsed: 00:32:13
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 45622 steps/s (collection: 2.054s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 225.3479
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.3889
                       Mean reward: 345.41
               Mean episode length: 183.07
    Episode_Reward/reaching_object: 0.7306
     Episode_Reward/lifting_object: 69.6046
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.15s
                      Time elapsed: 00:32:15
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 46454 steps/s (collection: 2.012s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 248.8396
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.3913
                       Mean reward: 346.35
               Mean episode length: 186.92
    Episode_Reward/reaching_object: 0.7200
     Episode_Reward/lifting_object: 67.4291
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.12s
                      Time elapsed: 00:32:17
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.039s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 195.1493
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 34.3933
                       Mean reward: 339.81
               Mean episode length: 180.68
    Episode_Reward/reaching_object: 0.7323
     Episode_Reward/lifting_object: 68.5716
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.14s
                      Time elapsed: 00:32:19
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 46809 steps/s (collection: 2.012s, learning 0.089s)
             Mean action noise std: 1.72
          Mean value_function loss: 180.4325
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.3933
                       Mean reward: 350.59
               Mean episode length: 183.61
    Episode_Reward/reaching_object: 0.7250
     Episode_Reward/lifting_object: 68.3332
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.10s
                      Time elapsed: 00:32:22
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 45653 steps/s (collection: 2.063s, learning 0.090s)
             Mean action noise std: 1.72
          Mean value_function loss: 211.6863
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.3929
                       Mean reward: 353.94
               Mean episode length: 184.40
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 67.9212
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.15s
                      Time elapsed: 00:32:24
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 46010 steps/s (collection: 2.046s, learning 0.090s)
             Mean action noise std: 1.72
          Mean value_function loss: 190.3202
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.3927
                       Mean reward: 341.63
               Mean episode length: 182.21
    Episode_Reward/reaching_object: 0.7207
     Episode_Reward/lifting_object: 67.0527
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.14s
                      Time elapsed: 00:32:26
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 45337 steps/s (collection: 2.081s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 183.6609
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.3934
                       Mean reward: 344.66
               Mean episode length: 183.35
    Episode_Reward/reaching_object: 0.7195
     Episode_Reward/lifting_object: 67.4719
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.17s
                      Time elapsed: 00:32:28
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 45774 steps/s (collection: 2.060s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 179.5776
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.3933
                       Mean reward: 337.01
               Mean episode length: 183.10
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 68.2655
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.15s
                      Time elapsed: 00:32:30
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 46834 steps/s (collection: 2.011s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 187.6103
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 34.3950
                       Mean reward: 347.70
               Mean episode length: 186.50
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 68.9513
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.10s
                      Time elapsed: 00:32:32
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 45392 steps/s (collection: 2.058s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 178.2391
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.3958
                       Mean reward: 338.30
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 0.7225
     Episode_Reward/lifting_object: 67.8068
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.17s
                      Time elapsed: 00:32:34
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 45828 steps/s (collection: 2.026s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 187.2712
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.3961
                       Mean reward: 367.52
               Mean episode length: 195.41
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 69.4300
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.15s
                      Time elapsed: 00:32:37
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 46473 steps/s (collection: 2.008s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 186.1720
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.4016
                       Mean reward: 343.05
               Mean episode length: 185.62
    Episode_Reward/reaching_object: 0.7255
     Episode_Reward/lifting_object: 67.5147
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.12s
                      Time elapsed: 00:32:39
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 47315 steps/s (collection: 1.990s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 184.6557
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.4086
                       Mean reward: 357.44
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 0.7215
     Episode_Reward/lifting_object: 67.1469
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.08s
                      Time elapsed: 00:32:41
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 47129 steps/s (collection: 1.988s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 186.7907
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.4158
                       Mean reward: 360.45
               Mean episode length: 191.51
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 69.3799
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.09s
                      Time elapsed: 00:32:43
                               ETA: 00:42:37

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 46790 steps/s (collection: 1.999s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 184.6001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.4172
                       Mean reward: 345.64
               Mean episode length: 185.40
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 69.5695
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.10s
                      Time elapsed: 00:32:45
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 46061 steps/s (collection: 2.038s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 179.5273
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.4201
                       Mean reward: 328.59
               Mean episode length: 174.83
    Episode_Reward/reaching_object: 0.7407
     Episode_Reward/lifting_object: 69.9365
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.13s
                      Time elapsed: 00:32:47
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 45389 steps/s (collection: 2.073s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 181.6612
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.4279
                       Mean reward: 365.23
               Mean episode length: 190.90
    Episode_Reward/reaching_object: 0.7400
     Episode_Reward/lifting_object: 69.6390
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.17s
                      Time elapsed: 00:32:49
                               ETA: 00:42:30

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 46014 steps/s (collection: 2.044s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 180.8533
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.4333
                       Mean reward: 318.72
               Mean episode length: 175.43
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 68.8801
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.14s
                      Time elapsed: 00:32:51
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 44176 steps/s (collection: 2.127s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 188.2962
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.4361
                       Mean reward: 367.39
               Mean episode length: 187.11
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 70.8001
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.23s
                      Time elapsed: 00:32:54
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 45779 steps/s (collection: 2.055s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 195.0177
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.4408
                       Mean reward: 375.42
               Mean episode length: 198.56
    Episode_Reward/reaching_object: 0.7364
     Episode_Reward/lifting_object: 69.7871
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.15s
                      Time elapsed: 00:32:56
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 45360 steps/s (collection: 2.072s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 189.0666
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.4434
                       Mean reward: 344.71
               Mean episode length: 183.21
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: 69.2206
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.17s
                      Time elapsed: 00:32:58
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 45108 steps/s (collection: 2.080s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 169.7174
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.4423
                       Mean reward: 350.44
               Mean episode length: 184.65
    Episode_Reward/reaching_object: 0.7317
     Episode_Reward/lifting_object: 68.4711
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.18s
                      Time elapsed: 00:33:00
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 46109 steps/s (collection: 2.038s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 167.9634
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 34.4438
                       Mean reward: 360.99
               Mean episode length: 188.23
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 72.1921
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.13s
                      Time elapsed: 00:33:02
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 45137 steps/s (collection: 2.084s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 181.0647
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.4475
                       Mean reward: 370.79
               Mean episode length: 191.83
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 73.0349
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.18s
                      Time elapsed: 00:33:04
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 45421 steps/s (collection: 2.046s, learning 0.118s)
             Mean action noise std: 1.73
          Mean value_function loss: 189.6702
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.4485
                       Mean reward: 368.63
               Mean episode length: 193.20
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 71.3307
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.16s
                      Time elapsed: 00:33:07
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 45303 steps/s (collection: 2.072s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 176.1627
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.4497
                       Mean reward: 376.07
               Mean episode length: 197.93
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 72.2432
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.17s
                      Time elapsed: 00:33:09
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 45873 steps/s (collection: 2.047s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 195.7266
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.4537
                       Mean reward: 375.24
               Mean episode length: 194.11
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 73.7766
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.14s
                      Time elapsed: 00:33:11
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 46196 steps/s (collection: 2.032s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 184.0214
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.4583
                       Mean reward: 364.38
               Mean episode length: 192.26
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 72.3507
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.13s
                      Time elapsed: 00:33:13
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 45459 steps/s (collection: 2.066s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 186.6095
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.4653
                       Mean reward: 343.43
               Mean episode length: 180.83
    Episode_Reward/reaching_object: 0.7399
     Episode_Reward/lifting_object: 70.5418
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.16s
                      Time elapsed: 00:33:15
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 46288 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 205.8580
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.4770
                       Mean reward: 390.01
               Mean episode length: 200.45
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 74.7628
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.12s
                      Time elapsed: 00:33:17
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 45778 steps/s (collection: 2.049s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 198.2922
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.4862
                       Mean reward: 384.22
               Mean episode length: 197.52
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 74.9428
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.15s
                      Time elapsed: 00:33:19
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 46520 steps/s (collection: 2.016s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 163.6767
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.4881
                       Mean reward: 360.40
               Mean episode length: 189.64
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 72.8701
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.11s
                      Time elapsed: 00:33:22
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 45307 steps/s (collection: 2.063s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 183.6515
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.4886
                       Mean reward: 379.41
               Mean episode length: 194.23
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 72.7329
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.17s
                      Time elapsed: 00:33:24
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 46285 steps/s (collection: 2.018s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 172.5525
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.4891
                       Mean reward: 392.59
               Mean episode length: 203.74
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 74.1124
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.12s
                      Time elapsed: 00:33:26
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 44237 steps/s (collection: 2.121s, learning 0.101s)
             Mean action noise std: 1.74
          Mean value_function loss: 166.9649
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.4959
                       Mean reward: 398.37
               Mean episode length: 207.80
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 72.3673
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.22s
                      Time elapsed: 00:33:28
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 45358 steps/s (collection: 2.076s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 175.7393
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.4992
                       Mean reward: 336.55
               Mean episode length: 177.70
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 73.7694
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.17s
                      Time elapsed: 00:33:30
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 45448 steps/s (collection: 2.056s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 185.2082
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.5010
                       Mean reward: 359.38
               Mean episode length: 184.92
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 73.3136
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.16s
                      Time elapsed: 00:33:32
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 45557 steps/s (collection: 2.044s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 171.8248
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.5067
                       Mean reward: 393.47
               Mean episode length: 207.95
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 73.2463
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.16s
                      Time elapsed: 00:33:35
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 45870 steps/s (collection: 2.043s, learning 0.101s)
             Mean action noise std: 1.74
          Mean value_function loss: 188.7969
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.5103
                       Mean reward: 384.32
               Mean episode length: 196.89
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 73.4502
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.14s
                      Time elapsed: 00:33:37
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 45732 steps/s (collection: 2.043s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 167.2711
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.5141
                       Mean reward: 363.39
               Mean episode length: 188.57
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 72.2837
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.15s
                      Time elapsed: 00:33:39
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 45417 steps/s (collection: 2.061s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 178.8580
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.5225
                       Mean reward: 379.25
               Mean episode length: 200.64
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 73.6237
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.16s
                      Time elapsed: 00:33:41
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 45958 steps/s (collection: 2.049s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 177.4431
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.5261
                       Mean reward: 359.28
               Mean episode length: 188.37
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 73.0466
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.14s
                      Time elapsed: 00:33:43
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 45322 steps/s (collection: 2.065s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 167.4619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.5282
                       Mean reward: 388.90
               Mean episode length: 204.92
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 76.0116
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.17s
                      Time elapsed: 00:33:45
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 45269 steps/s (collection: 2.058s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 171.5967
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.5271
                       Mean reward: 385.83
               Mean episode length: 200.28
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 74.5751
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.17s
                      Time elapsed: 00:33:48
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 45538 steps/s (collection: 2.050s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 188.8691
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.5270
                       Mean reward: 378.99
               Mean episode length: 197.30
    Episode_Reward/reaching_object: 0.7941
     Episode_Reward/lifting_object: 75.8093
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.16s
                      Time elapsed: 00:33:50
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 44077 steps/s (collection: 2.132s, learning 0.098s)
             Mean action noise std: 1.74
          Mean value_function loss: 181.0303
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.5316
                       Mean reward: 366.79
               Mean episode length: 190.76
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 71.6901
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.23s
                      Time elapsed: 00:33:52
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 45610 steps/s (collection: 2.060s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 175.0611
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.5374
                       Mean reward: 369.61
               Mean episode length: 192.61
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 75.5445
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.16s
                      Time elapsed: 00:33:54
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 45228 steps/s (collection: 2.079s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 171.2012
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.5390
                       Mean reward: 374.42
               Mean episode length: 195.76
    Episode_Reward/reaching_object: 0.7929
     Episode_Reward/lifting_object: 75.8752
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.17s
                      Time elapsed: 00:33:56
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 44938 steps/s (collection: 2.084s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 167.8764
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.5350
                       Mean reward: 367.25
               Mean episode length: 191.10
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 72.4910
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.19s
                      Time elapsed: 00:33:58
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 45153 steps/s (collection: 2.085s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 170.8719
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.5275
                       Mean reward: 374.04
               Mean episode length: 193.48
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 74.0023
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.18s
                      Time elapsed: 00:34:01
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 44557 steps/s (collection: 2.098s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 181.1656
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.5261
                       Mean reward: 385.68
               Mean episode length: 200.67
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 74.6168
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.21s
                      Time elapsed: 00:34:03
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 43683 steps/s (collection: 2.155s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 173.7473
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.5293
                       Mean reward: 389.00
               Mean episode length: 199.91
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 73.6166
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.25s
                      Time elapsed: 00:34:05
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 45028 steps/s (collection: 2.094s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 175.0373
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.5369
                       Mean reward: 392.03
               Mean episode length: 201.40
    Episode_Reward/reaching_object: 0.7957
     Episode_Reward/lifting_object: 77.0693
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.18s
                      Time elapsed: 00:34:07
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 44759 steps/s (collection: 2.104s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 169.2032
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.5442
                       Mean reward: 384.87
               Mean episode length: 202.72
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 74.3879
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.20s
                      Time elapsed: 00:34:09
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 45428 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 168.7140
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.5471
                       Mean reward: 386.37
               Mean episode length: 195.62
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 76.6386
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.16s
                      Time elapsed: 00:34:12
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 45350 steps/s (collection: 2.065s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 176.0391
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.5490
                       Mean reward: 374.49
               Mean episode length: 199.74
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 75.8608
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.17s
                      Time elapsed: 00:34:14
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 45310 steps/s (collection: 2.068s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 178.3430
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.5490
                       Mean reward: 377.52
               Mean episode length: 191.20
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 75.2332
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.17s
                      Time elapsed: 00:34:16
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 45039 steps/s (collection: 2.065s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 180.1884
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.5506
                       Mean reward: 398.99
               Mean episode length: 201.83
    Episode_Reward/reaching_object: 0.7872
     Episode_Reward/lifting_object: 76.4983
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.18s
                      Time elapsed: 00:34:18
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 45096 steps/s (collection: 2.078s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 172.3623
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.5512
                       Mean reward: 374.62
               Mean episode length: 192.47
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 73.6091
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.18s
                      Time elapsed: 00:34:20
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 45647 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 1.75
          Mean value_function loss: 181.9533
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.5495
                       Mean reward: 356.98
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 72.7309
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.15s
                      Time elapsed: 00:34:22
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 45236 steps/s (collection: 2.080s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 192.5728
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.5514
                       Mean reward: 380.29
               Mean episode length: 192.40
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 73.6788
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.17s
                      Time elapsed: 00:34:25
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 44941 steps/s (collection: 2.081s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 179.2946
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.5546
                       Mean reward: 374.23
               Mean episode length: 190.90
    Episode_Reward/reaching_object: 0.7908
     Episode_Reward/lifting_object: 77.0736
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.19s
                      Time elapsed: 00:34:27
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 44919 steps/s (collection: 2.088s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 171.6956
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.5566
                       Mean reward: 353.72
               Mean episode length: 184.20
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 74.1648
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.19s
                      Time elapsed: 00:34:29
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 42207 steps/s (collection: 2.235s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 178.6168
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.5566
                       Mean reward: 421.82
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 0.7961
     Episode_Reward/lifting_object: 78.4008
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.33s
                      Time elapsed: 00:34:31
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 43515 steps/s (collection: 2.164s, learning 0.095s)
             Mean action noise std: 1.75
          Mean value_function loss: 183.7908
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.5554
                       Mean reward: 391.23
               Mean episode length: 199.53
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 78.1965
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.26s
                      Time elapsed: 00:34:34
                               ETA: 00:40:37

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 44934 steps/s (collection: 2.082s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 177.7542
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.5551
                       Mean reward: 381.96
               Mean episode length: 197.95
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 75.2786
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.19s
                      Time elapsed: 00:34:36
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 45012 steps/s (collection: 2.074s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 174.3943
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.5599
                       Mean reward: 396.88
               Mean episode length: 203.35
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 77.2205
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.18s
                      Time elapsed: 00:34:38
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 44132 steps/s (collection: 2.109s, learning 0.118s)
             Mean action noise std: 1.75
          Mean value_function loss: 171.0918
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.5695
                       Mean reward: 372.58
               Mean episode length: 194.19
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 75.0028
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.23s
                      Time elapsed: 00:34:40
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 44606 steps/s (collection: 2.108s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 186.8277
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.5790
                       Mean reward: 393.74
               Mean episode length: 203.89
    Episode_Reward/reaching_object: 0.8018
     Episode_Reward/lifting_object: 77.9379
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.20s
                      Time elapsed: 00:34:42
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 45220 steps/s (collection: 2.074s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 171.2347
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.5812
                       Mean reward: 408.41
               Mean episode length: 205.75
    Episode_Reward/reaching_object: 0.8043
     Episode_Reward/lifting_object: 78.6431
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.17s
                      Time elapsed: 00:34:45
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 44248 steps/s (collection: 2.118s, learning 0.104s)
             Mean action noise std: 1.75
          Mean value_function loss: 175.9063
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.5819
                       Mean reward: 410.43
               Mean episode length: 205.85
    Episode_Reward/reaching_object: 0.7941
     Episode_Reward/lifting_object: 77.2856
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.22s
                      Time elapsed: 00:34:47
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 43498 steps/s (collection: 2.166s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 189.9245
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.5845
                       Mean reward: 393.02
               Mean episode length: 196.63
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 74.6879
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.26s
                      Time elapsed: 00:34:49
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 44859 steps/s (collection: 2.096s, learning 0.095s)
             Mean action noise std: 1.75
          Mean value_function loss: 170.6746
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.5920
                       Mean reward: 386.33
               Mean episode length: 194.58
    Episode_Reward/reaching_object: 0.7931
     Episode_Reward/lifting_object: 78.1645
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.19s
                      Time elapsed: 00:34:51
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 43980 steps/s (collection: 2.134s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 172.3412
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.5951
                       Mean reward: 382.20
               Mean episode length: 196.07
    Episode_Reward/reaching_object: 0.7913
     Episode_Reward/lifting_object: 77.6137
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.24s
                      Time elapsed: 00:34:53
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 45409 steps/s (collection: 2.067s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 175.1822
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.5950
                       Mean reward: 392.16
               Mean episode length: 197.38
    Episode_Reward/reaching_object: 0.7981
     Episode_Reward/lifting_object: 78.8859
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.16s
                      Time elapsed: 00:34:56
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 44997 steps/s (collection: 2.071s, learning 0.114s)
             Mean action noise std: 1.76
          Mean value_function loss: 192.5362
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.5973
                       Mean reward: 383.01
               Mean episode length: 192.71
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 77.4573
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.18s
                      Time elapsed: 00:34:58
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 45234 steps/s (collection: 2.076s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 165.3549
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.6005
                       Mean reward: 422.92
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 0.7986
     Episode_Reward/lifting_object: 78.3626
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.17s
                      Time elapsed: 00:35:00
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 44440 steps/s (collection: 2.098s, learning 0.114s)
             Mean action noise std: 1.76
          Mean value_function loss: 171.7099
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 34.6056
                       Mean reward: 406.14
               Mean episode length: 203.66
    Episode_Reward/reaching_object: 0.8047
     Episode_Reward/lifting_object: 79.6111
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.21s
                      Time elapsed: 00:35:02
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 45329 steps/s (collection: 2.069s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 181.2098
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.6066
                       Mean reward: 369.95
               Mean episode length: 184.88
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 77.2549
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.17s
                      Time elapsed: 00:35:04
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 44966 steps/s (collection: 2.078s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 170.3831
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.6084
                       Mean reward: 390.31
               Mean episode length: 196.18
    Episode_Reward/reaching_object: 0.8066
     Episode_Reward/lifting_object: 79.2830
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.19s
                      Time elapsed: 00:35:07
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 44908 steps/s (collection: 2.082s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 175.7625
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.6091
                       Mean reward: 377.09
               Mean episode length: 188.32
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 76.6906
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.19s
                      Time elapsed: 00:35:09
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 45318 steps/s (collection: 2.064s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 168.7758
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.6107
                       Mean reward: 424.20
               Mean episode length: 207.62
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 80.7986
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.17s
                      Time elapsed: 00:35:11
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 44930 steps/s (collection: 2.090s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 191.4161
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.6141
                       Mean reward: 393.25
               Mean episode length: 194.63
    Episode_Reward/reaching_object: 0.8029
     Episode_Reward/lifting_object: 79.6406
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.19s
                      Time elapsed: 00:35:13
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 45094 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 193.0939
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.6144
                       Mean reward: 382.68
               Mean episode length: 191.33
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 77.4436
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.18s
                      Time elapsed: 00:35:15
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 45049 steps/s (collection: 2.082s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 167.0566
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.6171
                       Mean reward: 400.22
               Mean episode length: 200.24
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 79.0320
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.18s
                      Time elapsed: 00:35:17
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 44776 steps/s (collection: 2.098s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 178.8180
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.6240
                       Mean reward: 407.25
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 0.8044
     Episode_Reward/lifting_object: 80.2200
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.20s
                      Time elapsed: 00:35:20
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44559 steps/s (collection: 2.091s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 178.8711
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.6264
                       Mean reward: 433.94
               Mean episode length: 213.58
    Episode_Reward/reaching_object: 0.8149
     Episode_Reward/lifting_object: 81.2183
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.21s
                      Time elapsed: 00:35:22
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 45120 steps/s (collection: 2.076s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 170.2292
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.6282
                       Mean reward: 409.55
               Mean episode length: 206.99
    Episode_Reward/reaching_object: 0.8312
     Episode_Reward/lifting_object: 82.3384
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.18s
                      Time elapsed: 00:35:24
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 45754 steps/s (collection: 2.057s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 188.9314
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.6329
                       Mean reward: 416.62
               Mean episode length: 201.23
    Episode_Reward/reaching_object: 0.7932
     Episode_Reward/lifting_object: 80.1058
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.15s
                      Time elapsed: 00:35:26
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 44664 steps/s (collection: 2.097s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 178.0236
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.6347
                       Mean reward: 390.95
               Mean episode length: 191.16
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 79.9359
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.20s
                      Time elapsed: 00:35:28
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 45017 steps/s (collection: 2.089s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 189.8828
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.6337
                       Mean reward: 403.19
               Mean episode length: 196.71
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 78.9523
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.18s
                      Time elapsed: 00:35:31
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 45199 steps/s (collection: 2.080s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 218.2239
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.6322
                       Mean reward: 392.85
               Mean episode length: 194.46
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 79.0146
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.17s
                      Time elapsed: 00:35:33
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 44899 steps/s (collection: 2.086s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 178.6643
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.6342
                       Mean reward: 395.64
               Mean episode length: 197.04
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 78.8846
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.19s
                      Time elapsed: 00:35:35
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 44882 steps/s (collection: 2.080s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 175.2888
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.6386
                       Mean reward: 406.87
               Mean episode length: 197.15
    Episode_Reward/reaching_object: 0.8038
     Episode_Reward/lifting_object: 81.2462
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.19s
                      Time elapsed: 00:35:37
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 44453 steps/s (collection: 2.112s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 190.9134
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.6446
                       Mean reward: 419.37
               Mean episode length: 203.34
    Episode_Reward/reaching_object: 0.7971
     Episode_Reward/lifting_object: 80.9316
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.21s
                      Time elapsed: 00:35:39
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 44780 steps/s (collection: 2.094s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 174.2557
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.6524
                       Mean reward: 398.83
               Mean episode length: 195.29
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 79.9784
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.20s
                      Time elapsed: 00:35:42
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 44095 steps/s (collection: 2.132s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 174.6469
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.6560
                       Mean reward: 418.50
               Mean episode length: 209.76
    Episode_Reward/reaching_object: 0.8104
     Episode_Reward/lifting_object: 81.5148
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.23s
                      Time elapsed: 00:35:44
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 44824 steps/s (collection: 2.093s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 181.6172
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.6620
                       Mean reward: 407.50
               Mean episode length: 202.82
    Episode_Reward/reaching_object: 0.8237
     Episode_Reward/lifting_object: 82.3925
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.19s
                      Time elapsed: 00:35:46
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43993 steps/s (collection: 2.128s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 165.8630
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.6652
                       Mean reward: 430.26
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 81.6832
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.23s
                      Time elapsed: 00:35:48
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 41307 steps/s (collection: 2.277s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 193.1848
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.6642
                       Mean reward: 428.47
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 0.8148
     Episode_Reward/lifting_object: 82.5793
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.38s
                      Time elapsed: 00:35:51
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 41590 steps/s (collection: 2.253s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 185.3915
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.6689
                       Mean reward: 415.99
               Mean episode length: 201.92
    Episode_Reward/reaching_object: 0.7917
     Episode_Reward/lifting_object: 80.3260
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.36s
                      Time elapsed: 00:35:53
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 38316 steps/s (collection: 2.383s, learning 0.183s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.2112
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.6739
                       Mean reward: 419.47
               Mean episode length: 202.39
    Episode_Reward/reaching_object: 0.8068
     Episode_Reward/lifting_object: 81.5612
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.57s
                      Time elapsed: 00:35:56
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 35330 steps/s (collection: 2.590s, learning 0.193s)
             Mean action noise std: 1.77
          Mean value_function loss: 181.6426
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.6782
                       Mean reward: 425.39
               Mean episode length: 204.63
    Episode_Reward/reaching_object: 0.8078
     Episode_Reward/lifting_object: 82.4257
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.78s
                      Time elapsed: 00:35:58
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 36967 steps/s (collection: 2.543s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 191.3330
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.6881
                       Mean reward: 408.02
               Mean episode length: 197.07
    Episode_Reward/reaching_object: 0.8180
     Episode_Reward/lifting_object: 83.5180
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.66s
                      Time elapsed: 00:36:01
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 38388 steps/s (collection: 2.396s, learning 0.165s)
             Mean action noise std: 1.77
          Mean value_function loss: 178.7141
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.6933
                       Mean reward: 432.96
               Mean episode length: 207.42
    Episode_Reward/reaching_object: 0.8081
     Episode_Reward/lifting_object: 82.5726
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.56s
                      Time elapsed: 00:36:04
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 40643 steps/s (collection: 2.247s, learning 0.172s)
             Mean action noise std: 1.77
          Mean value_function loss: 182.8500
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.6936
                       Mean reward: 404.00
               Mean episode length: 196.69
    Episode_Reward/reaching_object: 0.8308
     Episode_Reward/lifting_object: 84.9385
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.42s
                      Time elapsed: 00:36:06
                               ETA: 00:39:04

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 39288 steps/s (collection: 2.352s, learning 0.150s)
             Mean action noise std: 1.77
          Mean value_function loss: 197.6833
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.6971
                       Mean reward: 406.46
               Mean episode length: 200.15
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 79.5751
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.50s
                      Time elapsed: 00:36:08
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 40174 steps/s (collection: 2.263s, learning 0.184s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.0628
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.7001
                       Mean reward: 421.63
               Mean episode length: 202.14
    Episode_Reward/reaching_object: 0.8145
     Episode_Reward/lifting_object: 82.7371
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.45s
                      Time elapsed: 00:36:11
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 38130 steps/s (collection: 2.430s, learning 0.148s)
             Mean action noise std: 1.77
          Mean value_function loss: 173.1324
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 34.7025
                       Mean reward: 380.15
               Mean episode length: 187.43
    Episode_Reward/reaching_object: 0.7986
     Episode_Reward/lifting_object: 81.1261
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.58s
                      Time elapsed: 00:36:13
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 40487 steps/s (collection: 2.331s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 164.3544
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 34.7034
                       Mean reward: 424.16
               Mean episode length: 205.56
    Episode_Reward/reaching_object: 0.8512
     Episode_Reward/lifting_object: 87.2560
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.43s
                      Time elapsed: 00:36:16
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 42803 steps/s (collection: 2.191s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 181.2400
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.7043
                       Mean reward: 415.79
               Mean episode length: 207.68
    Episode_Reward/reaching_object: 0.8268
     Episode_Reward/lifting_object: 84.3197
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.30s
                      Time elapsed: 00:36:18
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 41913 steps/s (collection: 2.236s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 170.9270
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.7060
                       Mean reward: 414.43
               Mean episode length: 199.02
    Episode_Reward/reaching_object: 0.8033
     Episode_Reward/lifting_object: 82.9862
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.35s
                      Time elapsed: 00:36:21
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 43597 steps/s (collection: 2.150s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 189.2242
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7093
                       Mean reward: 424.38
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 0.8280
     Episode_Reward/lifting_object: 84.5193
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.25s
                      Time elapsed: 00:36:23
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 39832 steps/s (collection: 2.268s, learning 0.200s)
             Mean action noise std: 1.77
          Mean value_function loss: 191.8823
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7076
                       Mean reward: 423.43
               Mean episode length: 207.08
    Episode_Reward/reaching_object: 0.8220
     Episode_Reward/lifting_object: 83.7294
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.47s
                      Time elapsed: 00:36:25
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 34188 steps/s (collection: 2.698s, learning 0.177s)
             Mean action noise std: 1.77
          Mean value_function loss: 176.9157
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7067
                       Mean reward: 429.51
               Mean episode length: 209.42
    Episode_Reward/reaching_object: 0.8260
     Episode_Reward/lifting_object: 84.5424
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.88s
                      Time elapsed: 00:36:28
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 35314 steps/s (collection: 2.610s, learning 0.174s)
             Mean action noise std: 1.78
          Mean value_function loss: 168.9918
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.7052
                       Mean reward: 418.13
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 0.8288
     Episode_Reward/lifting_object: 84.8952
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.78s
                      Time elapsed: 00:36:31
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 41211 steps/s (collection: 2.268s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 207.2375
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.7077
                       Mean reward: 455.09
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: 87.2477
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.39s
                      Time elapsed: 00:36:33
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 41015 steps/s (collection: 2.278s, learning 0.118s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.1927
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.7169
                       Mean reward: 409.72
               Mean episode length: 202.19
    Episode_Reward/reaching_object: 0.8259
     Episode_Reward/lifting_object: 84.5319
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.40s
                      Time elapsed: 00:36:36
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 38188 steps/s (collection: 2.387s, learning 0.187s)
             Mean action noise std: 1.78
          Mean value_function loss: 170.0808
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 34.7212
                       Mean reward: 416.21
               Mean episode length: 202.52
    Episode_Reward/reaching_object: 0.8053
     Episode_Reward/lifting_object: 83.1309
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.57s
                      Time elapsed: 00:36:38
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 35408 steps/s (collection: 2.620s, learning 0.157s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.1830
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.7218
                       Mean reward: 438.84
               Mean episode length: 213.08
    Episode_Reward/reaching_object: 0.8219
     Episode_Reward/lifting_object: 85.4511
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.78s
                      Time elapsed: 00:36:41
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 37696 steps/s (collection: 2.438s, learning 0.170s)
             Mean action noise std: 1.78
          Mean value_function loss: 174.6468
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.7210
                       Mean reward: 441.52
               Mean episode length: 212.46
    Episode_Reward/reaching_object: 0.8330
     Episode_Reward/lifting_object: 86.3816
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.61s
                      Time elapsed: 00:36:44
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 33795 steps/s (collection: 2.716s, learning 0.193s)
             Mean action noise std: 1.78
          Mean value_function loss: 184.9490
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.7255
                       Mean reward: 436.27
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 0.8262
     Episode_Reward/lifting_object: 86.3091
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.91s
                      Time elapsed: 00:36:47
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 39056 steps/s (collection: 2.368s, learning 0.149s)
             Mean action noise std: 1.78
          Mean value_function loss: 182.7152
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 34.7312
                       Mean reward: 435.75
               Mean episode length: 209.52
    Episode_Reward/reaching_object: 0.8373
     Episode_Reward/lifting_object: 86.8337
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.52s
                      Time elapsed: 00:36:49
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 40394 steps/s (collection: 2.332s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.3082
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.7329
                       Mean reward: 409.82
               Mean episode length: 200.64
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 87.2778
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.43s
                      Time elapsed: 00:36:52
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 40289 steps/s (collection: 2.286s, learning 0.154s)
             Mean action noise std: 1.78
          Mean value_function loss: 174.4183
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.7326
                       Mean reward: 451.13
               Mean episode length: 214.43
    Episode_Reward/reaching_object: 0.8382
     Episode_Reward/lifting_object: 87.8107
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.44s
                      Time elapsed: 00:36:54
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 38553 steps/s (collection: 2.384s, learning 0.166s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.1114
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.7342
                       Mean reward: 438.36
               Mean episode length: 206.26
    Episode_Reward/reaching_object: 0.8193
     Episode_Reward/lifting_object: 84.9708
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.55s
                      Time elapsed: 00:36:57
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 39214 steps/s (collection: 2.387s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 176.3846
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.7316
                       Mean reward: 452.50
               Mean episode length: 210.12
    Episode_Reward/reaching_object: 0.8123
     Episode_Reward/lifting_object: 85.4290
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.51s
                      Time elapsed: 00:36:59
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 34717 steps/s (collection: 2.530s, learning 0.302s)
             Mean action noise std: 1.78
          Mean value_function loss: 163.2630
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.7233
                       Mean reward: 437.10
               Mean episode length: 209.23
    Episode_Reward/reaching_object: 0.8242
     Episode_Reward/lifting_object: 85.8490
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.83s
                      Time elapsed: 00:37:02
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 32808 steps/s (collection: 2.824s, learning 0.172s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.6188
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.7189
                       Mean reward: 445.54
               Mean episode length: 209.06
    Episode_Reward/reaching_object: 0.8012
     Episode_Reward/lifting_object: 84.0577
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 3.00s
                      Time elapsed: 00:37:05
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 31626 steps/s (collection: 2.863s, learning 0.245s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.6028
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7143
                       Mean reward: 440.35
               Mean episode length: 207.97
    Episode_Reward/reaching_object: 0.8207
     Episode_Reward/lifting_object: 86.6579
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 3.11s
                      Time elapsed: 00:37:08
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 31468 steps/s (collection: 2.997s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 162.2599
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.7131
                       Mean reward: 457.43
               Mean episode length: 217.03
    Episode_Reward/reaching_object: 0.8300
     Episode_Reward/lifting_object: 86.7680
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 3.12s
                      Time elapsed: 00:37:11
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 39501 steps/s (collection: 2.286s, learning 0.203s)
             Mean action noise std: 1.78
          Mean value_function loss: 203.8409
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.7128
                       Mean reward: 462.52
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 87.9346
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.49s
                      Time elapsed: 00:37:14
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 39649 steps/s (collection: 2.310s, learning 0.169s)
             Mean action noise std: 1.78
          Mean value_function loss: 200.7008
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.7141
                       Mean reward: 433.35
               Mean episode length: 201.28
    Episode_Reward/reaching_object: 0.8089
     Episode_Reward/lifting_object: 84.8347
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.48s
                      Time elapsed: 00:37:16
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 39005 steps/s (collection: 2.400s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 159.5736
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 34.7165
                       Mean reward: 430.79
               Mean episode length: 204.77
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 89.4395
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.52s
                      Time elapsed: 00:37:19
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 42410 steps/s (collection: 2.183s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.1400
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.7164
                       Mean reward: 451.49
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 90.0368
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.32s
                      Time elapsed: 00:37:21
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 42858 steps/s (collection: 2.192s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.5160
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.7152
                       Mean reward: 425.87
               Mean episode length: 204.01
    Episode_Reward/reaching_object: 0.8172
     Episode_Reward/lifting_object: 86.9527
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.29s
                      Time elapsed: 00:37:23
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 44219 steps/s (collection: 2.093s, learning 0.130s)
             Mean action noise std: 1.78
          Mean value_function loss: 203.6798
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.7158
                       Mean reward: 475.71
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 0.8237
     Episode_Reward/lifting_object: 87.6076
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.22s
                      Time elapsed: 00:37:25
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 43874 steps/s (collection: 2.137s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 173.7227
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.7238
                       Mean reward: 428.04
               Mean episode length: 202.17
    Episode_Reward/reaching_object: 0.8289
     Episode_Reward/lifting_object: 87.7729
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.24s
                      Time elapsed: 00:37:28
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 44879 steps/s (collection: 2.090s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.1865
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.7267
                       Mean reward: 442.77
               Mean episode length: 208.10
    Episode_Reward/reaching_object: 0.8205
     Episode_Reward/lifting_object: 87.6736
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.19s
                      Time elapsed: 00:37:30
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 44150 steps/s (collection: 2.130s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.6087
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.7249
                       Mean reward: 467.69
               Mean episode length: 218.25
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 91.3520
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.23s
                      Time elapsed: 00:37:32
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 44676 steps/s (collection: 2.102s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 191.0879
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.7257
                       Mean reward: 454.02
               Mean episode length: 204.13
    Episode_Reward/reaching_object: 0.8072
     Episode_Reward/lifting_object: 86.5938
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.20s
                      Time elapsed: 00:37:34
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 42827 steps/s (collection: 2.190s, learning 0.106s)
             Mean action noise std: 1.79
          Mean value_function loss: 168.7298
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.7328
                       Mean reward: 464.61
               Mean episode length: 212.35
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 90.3094
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.30s
                      Time elapsed: 00:37:37
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 45080 steps/s (collection: 2.062s, learning 0.119s)
             Mean action noise std: 1.79
          Mean value_function loss: 202.9545
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.7362
                       Mean reward: 481.66
               Mean episode length: 217.66
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 90.8238
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.18s
                      Time elapsed: 00:37:39
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 40384 steps/s (collection: 2.336s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 172.3009
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.7351
                       Mean reward: 454.10
               Mean episode length: 206.86
    Episode_Reward/reaching_object: 0.8479
     Episode_Reward/lifting_object: 91.6290
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.43s
                      Time elapsed: 00:37:41
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 43150 steps/s (collection: 2.173s, learning 0.106s)
             Mean action noise std: 1.79
          Mean value_function loss: 193.7991
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.7313
                       Mean reward: 443.75
               Mean episode length: 203.50
    Episode_Reward/reaching_object: 0.8263
     Episode_Reward/lifting_object: 89.6551
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.28s
                      Time elapsed: 00:37:43
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13853 steps/s (collection: 6.980s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 187.1058
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.7390
                       Mean reward: 453.22
               Mean episode length: 204.29
    Episode_Reward/reaching_object: 0.8446
     Episode_Reward/lifting_object: 92.9820
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.10s
                      Time elapsed: 00:37:51
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14431 steps/s (collection: 6.681s, learning 0.131s)
             Mean action noise std: 1.79
          Mean value_function loss: 179.6994
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.7457
                       Mean reward: 437.44
               Mean episode length: 200.62
    Episode_Reward/reaching_object: 0.8109
     Episode_Reward/lifting_object: 88.3285
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.81s
                      Time elapsed: 00:37:57
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14183 steps/s (collection: 6.820s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 184.5866
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.7468
                       Mean reward: 458.63
               Mean episode length: 207.77
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: 92.5573
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.93s
                      Time elapsed: 00:38:04
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14298 steps/s (collection: 6.765s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 208.2133
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.7427
                       Mean reward: 482.38
               Mean episode length: 212.41
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 92.9122
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.88s
                      Time elapsed: 00:38:11
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14545 steps/s (collection: 6.621s, learning 0.137s)
             Mean action noise std: 1.79
          Mean value_function loss: 205.4349
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7425
                       Mean reward: 474.35
               Mean episode length: 211.49
    Episode_Reward/reaching_object: 0.8544
     Episode_Reward/lifting_object: 94.1561
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.76s
                      Time elapsed: 00:38:18
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14521 steps/s (collection: 6.643s, learning 0.127s)
             Mean action noise std: 1.79
          Mean value_function loss: 193.2535
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.7486
                       Mean reward: 451.90
               Mean episode length: 204.61
    Episode_Reward/reaching_object: 0.8447
     Episode_Reward/lifting_object: 92.2096
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.77s
                      Time elapsed: 00:38:25
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14436 steps/s (collection: 6.689s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 185.1073
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.7479
                       Mean reward: 448.14
               Mean episode length: 202.46
    Episode_Reward/reaching_object: 0.8310
     Episode_Reward/lifting_object: 91.6656
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.81s
                      Time elapsed: 00:38:32
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14367 steps/s (collection: 6.722s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 187.4018
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.7451
                       Mean reward: 458.40
               Mean episode length: 202.55
    Episode_Reward/reaching_object: 0.8501
     Episode_Reward/lifting_object: 95.1942
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.84s
                      Time elapsed: 00:38:38
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17892 steps/s (collection: 5.389s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 234.9197
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.7441
                       Mean reward: 466.65
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 0.8196
     Episode_Reward/lifting_object: 90.2012
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.49s
                      Time elapsed: 00:38:44
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 45510 steps/s (collection: 2.069s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 235.9642
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.7552
                       Mean reward: 468.99
               Mean episode length: 206.41
    Episode_Reward/reaching_object: 0.8562
     Episode_Reward/lifting_object: 94.5239
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.16s
                      Time elapsed: 00:38:46
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 45477 steps/s (collection: 2.044s, learning 0.117s)
             Mean action noise std: 1.80
          Mean value_function loss: 197.8987
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.7647
                       Mean reward: 482.57
               Mean episode length: 212.91
    Episode_Reward/reaching_object: 0.8681
     Episode_Reward/lifting_object: 96.3546
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.16s
                      Time elapsed: 00:38:48
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 48125 steps/s (collection: 1.944s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 220.6716
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 34.7662
                       Mean reward: 441.72
               Mean episode length: 197.31
    Episode_Reward/reaching_object: 0.8091
     Episode_Reward/lifting_object: 90.4840
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.04s
                      Time elapsed: 00:38:50
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 47037 steps/s (collection: 1.983s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 191.2905
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.7642
                       Mean reward: 453.13
               Mean episode length: 202.75
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 94.1315
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.09s
                      Time elapsed: 00:38:52
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 47192 steps/s (collection: 1.958s, learning 0.125s)
             Mean action noise std: 1.80
          Mean value_function loss: 188.6717
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.7628
                       Mean reward: 450.12
               Mean episode length: 200.82
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 93.9056
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.08s
                      Time elapsed: 00:38:54
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 47815 steps/s (collection: 1.966s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 196.1142
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.7645
                       Mean reward: 478.38
               Mean episode length: 209.89
    Episode_Reward/reaching_object: 0.8405
     Episode_Reward/lifting_object: 94.3325
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.06s
                      Time elapsed: 00:38:56
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 47639 steps/s (collection: 1.979s, learning 0.084s)
             Mean action noise std: 1.80
          Mean value_function loss: 190.4877
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.7661
                       Mean reward: 470.92
               Mean episode length: 206.35
    Episode_Reward/reaching_object: 0.8649
     Episode_Reward/lifting_object: 97.6594
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.06s
                      Time elapsed: 00:38:58
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 47874 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 186.2246
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.7671
                       Mean reward: 466.73
               Mean episode length: 206.88
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 94.6000
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.05s
                      Time elapsed: 00:39:01
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 47769 steps/s (collection: 1.949s, learning 0.109s)
             Mean action noise std: 1.80
          Mean value_function loss: 193.5951
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.7658
                       Mean reward: 492.12
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 96.8802
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.06s
                      Time elapsed: 00:39:03
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 45217 steps/s (collection: 2.075s, learning 0.100s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.2708
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.7601
                       Mean reward: 485.34
               Mean episode length: 209.61
    Episode_Reward/reaching_object: 0.8688
     Episode_Reward/lifting_object: 98.8778
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.17s
                      Time elapsed: 00:39:05
                               ETA: 00:37:40

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 47665 steps/s (collection: 1.963s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 202.2994
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 34.7567
                       Mean reward: 500.58
               Mean episode length: 216.42
    Episode_Reward/reaching_object: 0.8546
     Episode_Reward/lifting_object: 97.8477
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.06s
                      Time elapsed: 00:39:07
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 46663 steps/s (collection: 1.999s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 191.5167
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.7580
                       Mean reward: 491.23
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 95.9337
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.11s
                      Time elapsed: 00:39:09
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 45890 steps/s (collection: 2.046s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 206.8600
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.7562
                       Mean reward: 488.56
               Mean episode length: 208.84
    Episode_Reward/reaching_object: 0.8382
     Episode_Reward/lifting_object: 95.3556
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.14s
                      Time elapsed: 00:39:11
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 46578 steps/s (collection: 2.013s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 185.2310
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.7547
                       Mean reward: 527.70
               Mean episode length: 220.33
    Episode_Reward/reaching_object: 0.8650
     Episode_Reward/lifting_object: 99.2715
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.11s
                      Time elapsed: 00:39:13
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 48073 steps/s (collection: 1.957s, learning 0.088s)
             Mean action noise std: 1.80
          Mean value_function loss: 185.4485
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 34.7543
                       Mean reward: 519.89
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.8691
     Episode_Reward/lifting_object: 100.6390
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.04s
                      Time elapsed: 00:39:15
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 47824 steps/s (collection: 1.969s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 190.9377
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.7535
                       Mean reward: 527.03
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 0.8637
     Episode_Reward/lifting_object: 100.1870
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.06s
                      Time elapsed: 00:39:17
                               ETA: 00:37:25

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 48115 steps/s (collection: 1.953s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 204.1849
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.7530
                       Mean reward: 489.67
               Mean episode length: 209.20
    Episode_Reward/reaching_object: 0.8381
     Episode_Reward/lifting_object: 97.1671
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.04s
                      Time elapsed: 00:39:19
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 46555 steps/s (collection: 2.026s, learning 0.086s)
             Mean action noise std: 1.80
          Mean value_function loss: 207.4288
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.7521
                       Mean reward: 488.78
               Mean episode length: 204.02
    Episode_Reward/reaching_object: 0.8544
     Episode_Reward/lifting_object: 100.5035
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.11s
                      Time elapsed: 00:39:21
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 47274 steps/s (collection: 1.981s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 216.7873
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.7524
                       Mean reward: 504.32
               Mean episode length: 211.42
    Episode_Reward/reaching_object: 0.8331
     Episode_Reward/lifting_object: 97.0072
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.08s
                      Time elapsed: 00:39:24
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 46400 steps/s (collection: 2.014s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 211.7173
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.7546
                       Mean reward: 507.63
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 98.7749
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.12s
                      Time elapsed: 00:39:26
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 46979 steps/s (collection: 1.991s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 237.4058
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.7571
                       Mean reward: 505.39
               Mean episode length: 209.14
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 97.9658
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.09s
                      Time elapsed: 00:39:28
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 48214 steps/s (collection: 1.950s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 202.9287
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.7588
                       Mean reward: 551.98
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.8748
     Episode_Reward/lifting_object: 103.3135
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.04s
                      Time elapsed: 00:39:30
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 47267 steps/s (collection: 1.987s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 204.6946
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 34.7600
                       Mean reward: 516.72
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 98.9271
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.08s
                      Time elapsed: 00:39:32
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 48085 steps/s (collection: 1.956s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 211.2882
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.7598
                       Mean reward: 518.92
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 0.8526
     Episode_Reward/lifting_object: 100.7919
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.04s
                      Time elapsed: 00:39:34
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 47652 steps/s (collection: 1.975s, learning 0.088s)
             Mean action noise std: 1.80
          Mean value_function loss: 283.7439
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 34.7599
                       Mean reward: 474.92
               Mean episode length: 197.13
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 98.9650
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.06s
                      Time elapsed: 00:39:36
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 47486 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 228.4355
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 34.7600
                       Mean reward: 484.13
               Mean episode length: 195.17
    Episode_Reward/reaching_object: 0.8328
     Episode_Reward/lifting_object: 100.3033
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.07s
                      Time elapsed: 00:39:38
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 45467 steps/s (collection: 2.068s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 208.2832
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 34.7597
                       Mean reward: 540.34
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 0.8664
     Episode_Reward/lifting_object: 104.4445
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.16s
                      Time elapsed: 00:39:40
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 47065 steps/s (collection: 1.995s, learning 0.094s)
             Mean action noise std: 1.80
          Mean value_function loss: 234.5415
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.7593
                       Mean reward: 535.77
               Mean episode length: 215.52
    Episode_Reward/reaching_object: 0.8619
     Episode_Reward/lifting_object: 104.3937
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.09s
                      Time elapsed: 00:39:42
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 46394 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 214.0907
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.7580
                       Mean reward: 474.96
               Mean episode length: 191.29
    Episode_Reward/reaching_object: 0.8173
     Episode_Reward/lifting_object: 99.1368
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.12s
                      Time elapsed: 00:39:44
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 46197 steps/s (collection: 2.039s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 207.5103
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.7529
                       Mean reward: 551.06
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.8743
     Episode_Reward/lifting_object: 106.9541
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.13s
                      Time elapsed: 00:39:47
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 44111 steps/s (collection: 2.116s, learning 0.113s)
             Mean action noise std: 1.80
          Mean value_function loss: 212.0673
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.7445
                       Mean reward: 573.59
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 0.8772
     Episode_Reward/lifting_object: 107.0065
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.23s
                      Time elapsed: 00:39:49
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 47079 steps/s (collection: 1.987s, learning 0.101s)
             Mean action noise std: 1.80
          Mean value_function loss: 207.2612
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.7375
                       Mean reward: 545.90
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 0.8803
     Episode_Reward/lifting_object: 109.4968
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.09s
                      Time elapsed: 00:39:51
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 47804 steps/s (collection: 1.959s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 233.7955
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.7392
                       Mean reward: 521.76
               Mean episode length: 206.26
    Episode_Reward/reaching_object: 0.8799
     Episode_Reward/lifting_object: 109.8893
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.06s
                      Time elapsed: 00:39:53
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 46326 steps/s (collection: 2.017s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 237.7263
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.7408
                       Mean reward: 515.02
               Mean episode length: 202.13
    Episode_Reward/reaching_object: 0.8690
     Episode_Reward/lifting_object: 108.7625
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.12s
                      Time elapsed: 00:39:55
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 48165 steps/s (collection: 1.948s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 233.2664
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.7383
                       Mean reward: 567.35
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 0.8690
     Episode_Reward/lifting_object: 111.8571
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.04s
                      Time elapsed: 00:39:57
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 43576 steps/s (collection: 2.146s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 232.7957
               Mean surrogate loss: 0.0286
                 Mean entropy loss: 34.7337
                       Mean reward: 552.38
               Mean episode length: 207.32
    Episode_Reward/reaching_object: 0.8722
     Episode_Reward/lifting_object: 111.5953
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.26s
                      Time elapsed: 00:39:59
                               ETA: 00:36:35

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 46901 steps/s (collection: 1.985s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 233.8549
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.7331
                       Mean reward: 589.95
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 0.8770
     Episode_Reward/lifting_object: 112.7247
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.10s
                      Time elapsed: 00:40:01
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 47172 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 226.7000
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.7274
                       Mean reward: 604.78
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 0.8798
     Episode_Reward/lifting_object: 114.6972
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.08s
                      Time elapsed: 00:40:04
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 47479 steps/s (collection: 1.984s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 276.1202
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.7173
                       Mean reward: 587.96
               Mean episode length: 216.31
    Episode_Reward/reaching_object: 0.8770
     Episode_Reward/lifting_object: 115.9210
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.07s
                      Time elapsed: 00:40:06
                               ETA: 00:36:27

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 47572 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 239.8596
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.7117
                       Mean reward: 567.93
               Mean episode length: 210.09
    Episode_Reward/reaching_object: 0.8433
     Episode_Reward/lifting_object: 111.6186
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.07s
                      Time elapsed: 00:40:08
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 47527 steps/s (collection: 1.982s, learning 0.086s)
             Mean action noise std: 1.80
          Mean value_function loss: 274.4051
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.7065
                       Mean reward: 557.48
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 111.7623
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.07s
                      Time elapsed: 00:40:10
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 45511 steps/s (collection: 2.068s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 288.1200
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.7028
                       Mean reward: 592.46
               Mean episode length: 213.45
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 117.3145
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.16s
                      Time elapsed: 00:40:12
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 47806 steps/s (collection: 1.969s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 261.8106
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.6975
                       Mean reward: 603.11
               Mean episode length: 212.74
    Episode_Reward/reaching_object: 0.8457
     Episode_Reward/lifting_object: 116.6488
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.06s
                      Time elapsed: 00:40:14
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 46428 steps/s (collection: 2.025s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 283.1237
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.6901
                       Mean reward: 609.75
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 0.8356
     Episode_Reward/lifting_object: 116.2798
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.12s
                      Time elapsed: 00:40:16
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 46528 steps/s (collection: 2.019s, learning 0.094s)
             Mean action noise std: 1.80
          Mean value_function loss: 245.8405
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 34.6882
                       Mean reward: 608.21
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 0.8665
     Episode_Reward/lifting_object: 122.7972
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.11s
                      Time elapsed: 00:40:18
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 49298 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 265.8740
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.6877
                       Mean reward: 664.17
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 0.8859
     Episode_Reward/lifting_object: 128.4182
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.99s
                      Time elapsed: 00:40:20
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 47736 steps/s (collection: 1.946s, learning 0.113s)
             Mean action noise std: 1.80
          Mean value_function loss: 254.0984
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.6850
                       Mean reward: 676.82
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 0.8770
     Episode_Reward/lifting_object: 128.8720
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.06s
                      Time elapsed: 00:40:22
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 44335 steps/s (collection: 2.042s, learning 0.175s)
             Mean action noise std: 1.80
          Mean value_function loss: 283.4305
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 34.6829
                       Mean reward: 662.04
               Mean episode length: 212.61
    Episode_Reward/reaching_object: 0.8676
     Episode_Reward/lifting_object: 129.9525
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.22s
                      Time elapsed: 00:40:24
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 46518 steps/s (collection: 2.018s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 260.8949
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 34.6825
                       Mean reward: 668.26
               Mean episode length: 216.28
    Episode_Reward/reaching_object: 0.8959
     Episode_Reward/lifting_object: 135.2941
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.11s
                      Time elapsed: 00:40:27
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 49338 steps/s (collection: 1.901s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 271.1237
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.6822
                       Mean reward: 670.39
               Mean episode length: 214.07
    Episode_Reward/reaching_object: 0.8775
     Episode_Reward/lifting_object: 133.1284
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.99s
                      Time elapsed: 00:40:29
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 50527 steps/s (collection: 1.853s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 289.7712
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 34.6814
                       Mean reward: 647.28
               Mean episode length: 204.77
    Episode_Reward/reaching_object: 0.8859
     Episode_Reward/lifting_object: 136.1699
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 1.95s
                      Time elapsed: 00:40:30
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 49744 steps/s (collection: 1.880s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 287.8263
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 34.6813
                       Mean reward: 634.26
               Mean episode length: 206.21
    Episode_Reward/reaching_object: 0.8487
     Episode_Reward/lifting_object: 130.3765
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 1.98s
                      Time elapsed: 00:40:32
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 49321 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 287.8186
               Mean surrogate loss: 0.0166
                 Mean entropy loss: 34.6813
                       Mean reward: 723.50
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 132.8170
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 1.99s
                      Time elapsed: 00:40:34
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 49724 steps/s (collection: 1.875s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 275.2614
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.6811
                       Mean reward: 654.40
               Mean episode length: 204.90
    Episode_Reward/reaching_object: 0.8796
     Episode_Reward/lifting_object: 137.9275
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 1.98s
                      Time elapsed: 00:40:36
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 50808 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 1.81
          Mean value_function loss: 287.8132
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 34.6822
                       Mean reward: 707.92
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 0.8693
     Episode_Reward/lifting_object: 135.8933
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 1.93s
                      Time elapsed: 00:40:38
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 49196 steps/s (collection: 1.906s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 264.0892
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 34.6822
                       Mean reward: 733.91
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 0.8903
     Episode_Reward/lifting_object: 143.1868
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.00s
                      Time elapsed: 00:40:40
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 49606 steps/s (collection: 1.894s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 255.0767
               Mean surrogate loss: 0.0150
                 Mean entropy loss: 34.6822
                       Mean reward: 747.37
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 0.8874
     Episode_Reward/lifting_object: 142.2630
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 1.98s
                      Time elapsed: 00:40:42
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 50057 steps/s (collection: 1.864s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 221.6828
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 34.6824
                       Mean reward: 762.48
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 0.9369
     Episode_Reward/lifting_object: 150.5247
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 1.96s
                      Time elapsed: 00:40:44
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 49458 steps/s (collection: 1.897s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 240.8603
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.6819
                       Mean reward: 768.24
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 0.9564
     Episode_Reward/lifting_object: 154.3800
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.99s
                      Time elapsed: 00:40:46
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 48971 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 269.6739
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.6800
                       Mean reward: 746.35
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 0.9228
     Episode_Reward/lifting_object: 147.1160
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.01s
                      Time elapsed: 00:40:48
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 49220 steps/s (collection: 1.902s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 226.9138
               Mean surrogate loss: 0.0149
                 Mean entropy loss: 34.6791
                       Mean reward: 763.11
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 149.9415
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.00s
                      Time elapsed: 00:40:50
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 50399 steps/s (collection: 1.860s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 236.5729
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.6791
                       Mean reward: 748.82
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 0.9644
     Episode_Reward/lifting_object: 153.1263
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.95s
                      Time elapsed: 00:40:52
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 48437 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 221.7370
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 34.6760
                       Mean reward: 797.82
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: 153.2466
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.03s
                      Time elapsed: 00:40:54
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.889s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 220.5381
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.6736
                       Mean reward: 777.87
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.9738
     Episode_Reward/lifting_object: 154.3559
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.99s
                      Time elapsed: 00:40:56
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 49231 steps/s (collection: 1.900s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 214.4391
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 34.6736
                       Mean reward: 775.80
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 0.9616
     Episode_Reward/lifting_object: 151.7215
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.00s
                      Time elapsed: 00:40:58
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 49699 steps/s (collection: 1.868s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 205.1682
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 34.6735
                       Mean reward: 769.19
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 0.9556
     Episode_Reward/lifting_object: 150.4470
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.98s
                      Time elapsed: 00:41:00
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 50693 steps/s (collection: 1.854s, learning 0.085s)
             Mean action noise std: 1.81
          Mean value_function loss: 212.7002
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.6741
                       Mean reward: 766.40
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 0.9541
     Episode_Reward/lifting_object: 149.0041
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 1.94s
                      Time elapsed: 00:41:02
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 50826 steps/s (collection: 1.849s, learning 0.085s)
             Mean action noise std: 1.81
          Mean value_function loss: 232.7655
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 34.6747
                       Mean reward: 713.55
               Mean episode length: 218.15
    Episode_Reward/reaching_object: 0.9189
     Episode_Reward/lifting_object: 143.2806
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 1.93s
                      Time elapsed: 00:41:04
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 49889 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 1.81
          Mean value_function loss: 229.1180
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 34.6752
                       Mean reward: 733.57
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 0.9599
     Episode_Reward/lifting_object: 150.5527
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 1.97s
                      Time elapsed: 00:41:06
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 49912 steps/s (collection: 1.884s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 233.1178
               Mean surrogate loss: 0.0189
                 Mean entropy loss: 34.6757
                       Mean reward: 726.57
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 0.9630
     Episode_Reward/lifting_object: 152.2307
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.97s
                      Time elapsed: 00:41:08
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 49321 steps/s (collection: 1.905s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 218.1966
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.6771
                       Mean reward: 801.15
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 155.7206
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 1.99s
                      Time elapsed: 00:41:10
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 49217 steps/s (collection: 1.909s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 210.1928
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.6806
                       Mean reward: 816.76
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.0061
     Episode_Reward/lifting_object: 160.8749
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.00s
                      Time elapsed: 00:41:12
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 50146 steps/s (collection: 1.874s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 216.6772
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 34.6835
                       Mean reward: 807.79
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 0.9966
     Episode_Reward/lifting_object: 159.3510
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.96s
                      Time elapsed: 00:41:14
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 48255 steps/s (collection: 1.948s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 210.3964
               Mean surrogate loss: 0.0267
                 Mean entropy loss: 34.6847
                       Mean reward: 798.76
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.9904
     Episode_Reward/lifting_object: 159.3406
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.04s
                      Time elapsed: 00:41:16
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 50225 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 210.7718
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.6851
                       Mean reward: 812.11
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 0.9756
     Episode_Reward/lifting_object: 157.0907
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 1.96s
                      Time elapsed: 00:41:18
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 49750 steps/s (collection: 1.870s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 205.0215
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.6843
                       Mean reward: 808.17
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.0136
     Episode_Reward/lifting_object: 164.7693
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.98s
                      Time elapsed: 00:41:20
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 46639 steps/s (collection: 2.012s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 215.1453
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.6818
                       Mean reward: 782.45
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 159.6090
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.11s
                      Time elapsed: 00:41:22
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 51030 steps/s (collection: 1.832s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 211.3709
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 34.6807
                       Mean reward: 792.82
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 159.2437
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.93s
                      Time elapsed: 00:41:24
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 51112 steps/s (collection: 1.831s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 234.3333
               Mean surrogate loss: 0.0157
                 Mean entropy loss: 34.6806
                       Mean reward: 804.66
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 0.9726
     Episode_Reward/lifting_object: 159.3449
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.92s
                      Time elapsed: 00:41:26
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 50590 steps/s (collection: 1.839s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 214.2731
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 34.6818
                       Mean reward: 808.84
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 0.9719
     Episode_Reward/lifting_object: 160.2434
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.94s
                      Time elapsed: 00:41:28
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 50610 steps/s (collection: 1.836s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 216.0619
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 34.6852
                       Mean reward: 803.54
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 0.9748
     Episode_Reward/lifting_object: 161.0740
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.94s
                      Time elapsed: 00:41:30
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 49778 steps/s (collection: 1.887s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 225.9743
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.6863
                       Mean reward: 776.22
               Mean episode length: 221.61
    Episode_Reward/reaching_object: 0.9453
     Episode_Reward/lifting_object: 155.9144
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.97s
                      Time elapsed: 00:41:32
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 51456 steps/s (collection: 1.822s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 233.3470
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 34.6877
                       Mean reward: 780.33
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 0.9555
     Episode_Reward/lifting_object: 157.4124
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.91s
                      Time elapsed: 00:41:34
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 49099 steps/s (collection: 1.910s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 200.8998
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.6869
                       Mean reward: 813.88
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 0.9813
     Episode_Reward/lifting_object: 162.8669
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.00s
                      Time elapsed: 00:41:36
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 51366 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 198.1418
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.6860
                       Mean reward: 812.42
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.0006
     Episode_Reward/lifting_object: 166.0147
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.91s
                      Time elapsed: 00:41:38
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 50312 steps/s (collection: 1.856s, learning 0.098s)
             Mean action noise std: 1.81
          Mean value_function loss: 191.0762
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.6901
                       Mean reward: 816.74
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 1.0014
     Episode_Reward/lifting_object: 165.5087
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 1.95s
                      Time elapsed: 00:41:40
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 51093 steps/s (collection: 1.831s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 185.2150
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 34.6997
                       Mean reward: 856.45
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.0067
     Episode_Reward/lifting_object: 166.4603
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.92s
                      Time elapsed: 00:41:42
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 49260 steps/s (collection: 1.887s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 186.4646
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 34.7020
                       Mean reward: 815.70
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 0.9827
     Episode_Reward/lifting_object: 162.9581
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.00s
                      Time elapsed: 00:41:44
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 49082 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.81
          Mean value_function loss: 182.8083
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.7040
                       Mean reward: 856.01
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.0264
     Episode_Reward/lifting_object: 170.1505
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.00s
                      Time elapsed: 00:41:46
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 50067 steps/s (collection: 1.872s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 184.9434
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.7144
                       Mean reward: 820.28
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.0378
     Episode_Reward/lifting_object: 172.4571
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 1.96s
                      Time elapsed: 00:41:47
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 47950 steps/s (collection: 1.932s, learning 0.118s)
             Mean action noise std: 1.82
          Mean value_function loss: 195.2092
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7202
                       Mean reward: 850.29
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.0034
     Episode_Reward/lifting_object: 166.2124
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.05s
                      Time elapsed: 00:41:50
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 50055 steps/s (collection: 1.854s, learning 0.110s)
             Mean action noise std: 1.82
          Mean value_function loss: 188.7964
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 34.7268
                       Mean reward: 838.73
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: 163.6896
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 1.96s
                      Time elapsed: 00:41:51
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 47662 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 206.5214
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.7286
                       Mean reward: 828.66
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.0175
     Episode_Reward/lifting_object: 167.6349
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.06s
                      Time elapsed: 00:41:54
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 48848 steps/s (collection: 1.912s, learning 0.101s)
             Mean action noise std: 1.82
          Mean value_function loss: 186.6866
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 34.7296
                       Mean reward: 812.69
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: 164.2754
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.01s
                      Time elapsed: 00:41:56
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 48940 steps/s (collection: 1.891s, learning 0.118s)
             Mean action noise std: 1.82
          Mean value_function loss: 196.3072
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 34.7302
                       Mean reward: 842.31
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.0191
     Episode_Reward/lifting_object: 167.7926
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.01s
                      Time elapsed: 00:41:58
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 48184 steps/s (collection: 1.938s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 185.9601
               Mean surrogate loss: 0.0183
                 Mean entropy loss: 34.7307
                       Mean reward: 829.85
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 1.0082
     Episode_Reward/lifting_object: 166.0349
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.04s
                      Time elapsed: 00:42:00
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 49180 steps/s (collection: 1.892s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 173.8241
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.7325
                       Mean reward: 849.22
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.0152
     Episode_Reward/lifting_object: 167.8237
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.00s
                      Time elapsed: 00:42:02
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 49671 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 1.82
          Mean value_function loss: 193.3577
               Mean surrogate loss: 0.0177
                 Mean entropy loss: 34.7365
                       Mean reward: 826.68
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 0.9805
     Episode_Reward/lifting_object: 162.9681
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.98s
                      Time elapsed: 00:42:04
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 49592 steps/s (collection: 1.888s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 193.1832
               Mean surrogate loss: 0.0216
                 Mean entropy loss: 34.7378
                       Mean reward: 844.59
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 165.8544
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.98s
                      Time elapsed: 00:42:06
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 49554 steps/s (collection: 1.886s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 199.4271
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 34.7389
                       Mean reward: 811.03
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 0.9870
     Episode_Reward/lifting_object: 163.2135
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.98s
                      Time elapsed: 00:42:08
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 49265 steps/s (collection: 1.902s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 201.2143
               Mean surrogate loss: 0.0192
                 Mean entropy loss: 34.7399
                       Mean reward: 828.96
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 166.6884
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.00s
                      Time elapsed: 00:42:10
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 50826 steps/s (collection: 1.842s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 223.1726
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 34.7410
                       Mean reward: 826.62
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 0.9743
     Episode_Reward/lifting_object: 160.9013
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.93s
                      Time elapsed: 00:42:11
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 46006 steps/s (collection: 2.006s, learning 0.131s)
             Mean action noise std: 1.82
          Mean value_function loss: 208.8649
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 34.7418
                       Mean reward: 813.22
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.0028
     Episode_Reward/lifting_object: 166.8079
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.14s
                      Time elapsed: 00:42:14
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 49513 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 210.7902
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.7434
                       Mean reward: 842.21
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.0078
     Episode_Reward/lifting_object: 167.2055
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.99s
                      Time elapsed: 00:42:16
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 48006 steps/s (collection: 1.956s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 202.2783
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.7470
                       Mean reward: 852.81
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 0.9882
     Episode_Reward/lifting_object: 163.4062
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.05s
                      Time elapsed: 00:42:18
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 50123 steps/s (collection: 1.870s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 192.4701
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.7549
                       Mean reward: 812.15
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.0100
     Episode_Reward/lifting_object: 167.7758
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 1.96s
                      Time elapsed: 00:42:20
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 50890 steps/s (collection: 1.840s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 196.7687
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 34.7591
                       Mean reward: 842.52
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 167.7630
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 1.93s
                      Time elapsed: 00:42:22
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 51785 steps/s (collection: 1.809s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 192.1079
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.7598
                       Mean reward: 852.85
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.0014
     Episode_Reward/lifting_object: 164.2854
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.90s
                      Time elapsed: 00:42:23
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 50089 steps/s (collection: 1.858s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 197.0797
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 34.7609
                       Mean reward: 824.19
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 1.0100
     Episode_Reward/lifting_object: 165.8288
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 1.96s
                      Time elapsed: 00:42:25
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 50100 steps/s (collection: 1.866s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 189.9129
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.7623
                       Mean reward: 832.91
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.0196
     Episode_Reward/lifting_object: 168.5460
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.96s
                      Time elapsed: 00:42:27
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 51047 steps/s (collection: 1.833s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 177.2132
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 34.7687
                       Mean reward: 860.35
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.0061
     Episode_Reward/lifting_object: 165.0620
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.93s
                      Time elapsed: 00:42:29
                               ETA: 00:33:25

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 50584 steps/s (collection: 1.847s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 225.4577
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 34.7718
                       Mean reward: 821.10
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 163.7642
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 1.94s
                      Time elapsed: 00:42:31
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 50414 steps/s (collection: 1.855s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 192.6382
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.7737
                       Mean reward: 858.19
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.0079
     Episode_Reward/lifting_object: 165.4127
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 1.95s
                      Time elapsed: 00:42:33
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 50247 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 189.7466
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.7772
                       Mean reward: 836.87
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 165.7849
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 1.96s
                      Time elapsed: 00:42:35
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 51123 steps/s (collection: 1.835s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 200.7089
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 34.7895
                       Mean reward: 828.81
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.0131
     Episode_Reward/lifting_object: 165.7358
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 1.92s
                      Time elapsed: 00:42:37
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 50511 steps/s (collection: 1.856s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 186.2910
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 34.7929
                       Mean reward: 827.36
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: 167.9590
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.95s
                      Time elapsed: 00:42:39
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 50979 steps/s (collection: 1.840s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 194.9767
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.7970
                       Mean reward: 830.57
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 1.0103
     Episode_Reward/lifting_object: 165.3746
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 1.93s
                      Time elapsed: 00:42:41
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 50283 steps/s (collection: 1.859s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 212.3594
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.8083
                       Mean reward: 823.51
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 0.9866
     Episode_Reward/lifting_object: 160.8328
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.95s
                      Time elapsed: 00:42:43
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 50039 steps/s (collection: 1.876s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 202.8912
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.8157
                       Mean reward: 826.32
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.0023
     Episode_Reward/lifting_object: 163.2714
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 1.96s
                      Time elapsed: 00:42:45
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 49827 steps/s (collection: 1.879s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 219.4548
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.8241
                       Mean reward: 799.64
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.0033
     Episode_Reward/lifting_object: 161.9869
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.97s
                      Time elapsed: 00:42:47
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 50460 steps/s (collection: 1.840s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 218.5879
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 34.8418
                       Mean reward: 836.91
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 1.0101
     Episode_Reward/lifting_object: 163.3249
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.95s
                      Time elapsed: 00:42:49
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 50651 steps/s (collection: 1.837s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 218.5356
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.8544
                       Mean reward: 837.04
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 1.0034
     Episode_Reward/lifting_object: 161.8304
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.94s
                      Time elapsed: 00:42:51
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 50847 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 219.8615
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.8652
                       Mean reward: 820.85
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 164.5349
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.93s
                      Time elapsed: 00:42:53
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 49769 steps/s (collection: 1.885s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 189.5595
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.8747
                       Mean reward: 849.85
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.0219
     Episode_Reward/lifting_object: 164.9136
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.98s
                      Time elapsed: 00:42:55
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 50452 steps/s (collection: 1.842s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 188.6358
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 34.8841
                       Mean reward: 840.87
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.0275
     Episode_Reward/lifting_object: 165.4370
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.95s
                      Time elapsed: 00:42:57
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 49237 steps/s (collection: 1.887s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 207.2407
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.8944
                       Mean reward: 838.79
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 1.0350
     Episode_Reward/lifting_object: 166.6800
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.00s
                      Time elapsed: 00:42:59
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 48728 steps/s (collection: 1.900s, learning 0.118s)
             Mean action noise std: 1.84
          Mean value_function loss: 190.6262
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.9180
                       Mean reward: 860.59
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.0274
     Episode_Reward/lifting_object: 164.3957
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.02s
                      Time elapsed: 00:43:01
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 50929 steps/s (collection: 1.838s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 188.9750
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.9304
                       Mean reward: 842.56
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 169.4254
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.93s
                      Time elapsed: 00:43:03
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 49345 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 209.9589
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 34.9345
                       Mean reward: 817.98
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 1.0106
     Episode_Reward/lifting_object: 161.8754
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 1.99s
                      Time elapsed: 00:43:05
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 51459 steps/s (collection: 1.826s, learning 0.085s)
             Mean action noise std: 1.84
          Mean value_function loss: 208.0544
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 34.9363
                       Mean reward: 857.59
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 162.6667
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 1.91s
                      Time elapsed: 00:43:06
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 50788 steps/s (collection: 1.838s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 181.9820
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 34.9385
                       Mean reward: 876.03
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 168.6673
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 1.94s
                      Time elapsed: 00:43:08
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 50320 steps/s (collection: 1.845s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 193.5545
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.9395
                       Mean reward: 852.41
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 170.1244
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.95s
                      Time elapsed: 00:43:10
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 50587 steps/s (collection: 1.852s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 215.7047
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.9403
                       Mean reward: 836.89
               Mean episode length: 232.08
    Episode_Reward/reaching_object: 1.0163
     Episode_Reward/lifting_object: 163.3188
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.94s
                      Time elapsed: 00:43:12
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 50486 steps/s (collection: 1.860s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 195.2188
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 34.9422
                       Mean reward: 844.11
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 161.4917
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 1.95s
                      Time elapsed: 00:43:14
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.845s, learning 0.084s)
             Mean action noise std: 1.84
          Mean value_function loss: 205.6714
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 34.9434
                       Mean reward: 840.84
               Mean episode length: 232.96
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 164.4036
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 1.93s
                      Time elapsed: 00:43:16
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 50949 steps/s (collection: 1.843s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 193.1212
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.9442
                       Mean reward: 832.92
               Mean episode length: 229.37
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 167.8616
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 1.93s
                      Time elapsed: 00:43:18
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 49757 steps/s (collection: 1.871s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 193.0509
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.9467
                       Mean reward: 820.22
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 1.0162
     Episode_Reward/lifting_object: 163.5087
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 1.98s
                      Time elapsed: 00:43:20
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 50474 steps/s (collection: 1.853s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 186.0443
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.9508
                       Mean reward: 835.02
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 167.2375
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.95s
                      Time elapsed: 00:43:22
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 50432 steps/s (collection: 1.857s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 197.0266
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.9568
                       Mean reward: 807.83
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 1.0054
     Episode_Reward/lifting_object: 162.0945
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 1.95s
                      Time elapsed: 00:43:24
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 50674 steps/s (collection: 1.843s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 189.2054
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.9668
                       Mean reward: 867.83
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 1.0247
     Episode_Reward/lifting_object: 165.7763
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.94s
                      Time elapsed: 00:43:26
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 50825 steps/s (collection: 1.832s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 185.1909
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.9814
                       Mean reward: 826.93
               Mean episode length: 227.35
    Episode_Reward/reaching_object: 1.0278
     Episode_Reward/lifting_object: 165.7896
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.93s
                      Time elapsed: 00:43:28
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 50437 steps/s (collection: 1.845s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 185.9529
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.9846
                       Mean reward: 851.42
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.0459
     Episode_Reward/lifting_object: 168.7161
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.95s
                      Time elapsed: 00:43:30
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 50719 steps/s (collection: 1.841s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 220.1527
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 34.9868
                       Mean reward: 809.14
               Mean episode length: 223.01
    Episode_Reward/reaching_object: 0.9929
     Episode_Reward/lifting_object: 161.0058
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 1.94s
                      Time elapsed: 00:43:32
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 50757 steps/s (collection: 1.853s, learning 0.084s)
             Mean action noise std: 1.85
          Mean value_function loss: 209.8908
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.9881
                       Mean reward: 813.30
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.0017
     Episode_Reward/lifting_object: 161.9902
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 1.94s
                      Time elapsed: 00:43:34
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 50787 steps/s (collection: 1.848s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 250.2015
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.9956
                       Mean reward: 799.51
               Mean episode length: 219.19
    Episode_Reward/reaching_object: 0.9786
     Episode_Reward/lifting_object: 157.8922
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 1.94s
                      Time elapsed: 00:43:36
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 50647 steps/s (collection: 1.851s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 266.6599
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.0085
                       Mean reward: 788.52
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 0.9526
     Episode_Reward/lifting_object: 153.7171
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.94s
                      Time elapsed: 00:43:38
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 49433 steps/s (collection: 1.893s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 274.8283
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.0158
                       Mean reward: 756.85
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 0.9352
     Episode_Reward/lifting_object: 150.7706
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 1.99s
                      Time elapsed: 00:43:39
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 50237 steps/s (collection: 1.869s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 251.6248
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.0169
                       Mean reward: 786.82
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 156.8504
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 1.96s
                      Time elapsed: 00:43:41
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 50696 steps/s (collection: 1.848s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 241.4773
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.0185
                       Mean reward: 805.65
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: 160.4862
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 1.94s
                      Time elapsed: 00:43:43
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 50946 steps/s (collection: 1.844s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 229.3778
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.0209
                       Mean reward: 831.60
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.0136
     Episode_Reward/lifting_object: 166.2236
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 1.93s
                      Time elapsed: 00:43:45
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 51228 steps/s (collection: 1.826s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 218.9458
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 35.0221
                       Mean reward: 819.66
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 1.0050
     Episode_Reward/lifting_object: 164.2881
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.92s
                      Time elapsed: 00:43:47
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 51088 steps/s (collection: 1.837s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 205.1781
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 35.0229
                       Mean reward: 854.24
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.0064
     Episode_Reward/lifting_object: 165.4040
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.92s
                      Time elapsed: 00:43:49
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 51621 steps/s (collection: 1.816s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 186.1232
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.0245
                       Mean reward: 849.41
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.0374
     Episode_Reward/lifting_object: 171.3130
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 1.90s
                      Time elapsed: 00:43:51
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 50255 steps/s (collection: 1.869s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 185.7237
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.0291
                       Mean reward: 832.67
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.0306
     Episode_Reward/lifting_object: 169.6305
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.96s
                      Time elapsed: 00:43:53
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 49001 steps/s (collection: 1.892s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 223.4981
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.0365
                       Mean reward: 858.19
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 1.0186
     Episode_Reward/lifting_object: 167.4347
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.01s
                      Time elapsed: 00:43:55
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 50031 steps/s (collection: 1.854s, learning 0.111s)
             Mean action noise std: 1.85
          Mean value_function loss: 188.9499
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 35.0411
                       Mean reward: 829.05
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 170.7439
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 1.96s
                      Time elapsed: 00:43:57
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 50431 steps/s (collection: 1.854s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 177.4123
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 35.0428
                       Mean reward: 847.66
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.0249
     Episode_Reward/lifting_object: 168.9254
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.95s
                      Time elapsed: 00:43:59
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 50025 steps/s (collection: 1.873s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 179.0153
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.0454
                       Mean reward: 868.30
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 171.5508
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.97s
                      Time elapsed: 00:44:01
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 50491 steps/s (collection: 1.856s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 162.6035
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.0527
                       Mean reward: 857.14
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 172.2213
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.95s
                      Time elapsed: 00:44:03
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 49519 steps/s (collection: 1.889s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 160.5446
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 35.0683
                       Mean reward: 852.37
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 174.0853
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.99s
                      Time elapsed: 00:44:05
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 50079 steps/s (collection: 1.871s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 158.0741
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.0748
                       Mean reward: 815.78
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 169.9145
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.96s
                      Time elapsed: 00:44:07
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 49922 steps/s (collection: 1.860s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 157.0872
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.0786
                       Mean reward: 861.34
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.0512
     Episode_Reward/lifting_object: 170.9433
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.97s
                      Time elapsed: 00:44:09
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 47428 steps/s (collection: 1.979s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 183.3133
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.0849
                       Mean reward: 858.19
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 167.3432
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.07s
                      Time elapsed: 00:44:11
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 48307 steps/s (collection: 1.927s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 159.6431
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.1006
                       Mean reward: 836.97
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 170.1479
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.03s
                      Time elapsed: 00:44:13
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 47518 steps/s (collection: 1.959s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 179.5632
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.1067
                       Mean reward: 862.91
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.0467
     Episode_Reward/lifting_object: 170.3598
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.07s
                      Time elapsed: 00:44:15
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 45531 steps/s (collection: 2.063s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 168.3354
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.1163
                       Mean reward: 853.30
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.0521
     Episode_Reward/lifting_object: 171.2882
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.16s
                      Time elapsed: 00:44:17
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 48016 steps/s (collection: 1.947s, learning 0.100s)
             Mean action noise std: 1.86
          Mean value_function loss: 194.3174
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.1216
                       Mean reward: 872.92
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 172.6938
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.05s
                      Time elapsed: 00:44:19
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 50809 steps/s (collection: 1.844s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 205.7493
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.1323
                       Mean reward: 841.50
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.0431
     Episode_Reward/lifting_object: 169.3846
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 1.93s
                      Time elapsed: 00:44:21
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 51135 steps/s (collection: 1.831s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 209.9864
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.1443
                       Mean reward: 865.76
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 172.6547
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 1.92s
                      Time elapsed: 00:44:23
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 50634 steps/s (collection: 1.846s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 205.8311
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.1635
                       Mean reward: 806.72
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 1.0212
     Episode_Reward/lifting_object: 165.1998
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.94s
                      Time elapsed: 00:44:25
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 50961 steps/s (collection: 1.831s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 174.7092
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.1729
                       Mean reward: 837.85
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 168.5579
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 1.93s
                      Time elapsed: 00:44:27
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 50538 steps/s (collection: 1.843s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 199.5716
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.1749
                       Mean reward: 811.40
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 1.0410
     Episode_Reward/lifting_object: 167.3868
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 1.95s
                      Time elapsed: 00:44:29
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 49663 steps/s (collection: 1.869s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 210.6549
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.1773
                       Mean reward: 854.71
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.0110
     Episode_Reward/lifting_object: 162.9662
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 1.98s
                      Time elapsed: 00:44:31
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 50863 steps/s (collection: 1.846s, learning 0.087s)
             Mean action noise std: 1.87
          Mean value_function loss: 227.1105
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 35.1793
                       Mean reward: 781.47
               Mean episode length: 215.73
    Episode_Reward/reaching_object: 1.0172
     Episode_Reward/lifting_object: 164.0030
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 1.93s
                      Time elapsed: 00:44:33
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 49882 steps/s (collection: 1.871s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 194.9163
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.1807
                       Mean reward: 856.86
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.0383
     Episode_Reward/lifting_object: 167.9509
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.97s
                      Time elapsed: 00:44:35
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 49943 steps/s (collection: 1.880s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 214.8425
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.1834
                       Mean reward: 839.62
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 169.4859
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.97s
                      Time elapsed: 00:44:37
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 49836 steps/s (collection: 1.867s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 185.6698
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.1855
                       Mean reward: 880.92
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 171.5532
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 1.97s
                      Time elapsed: 00:44:39
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 48835 steps/s (collection: 1.917s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 213.9331
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.1877
                       Mean reward: 856.11
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 172.5210
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.01s
                      Time elapsed: 00:44:41
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 49800 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 220.3549
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.2016
                       Mean reward: 828.52
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 169.3058
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 1.97s
                      Time elapsed: 00:44:43
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.881s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 203.9325
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.2109
                       Mean reward: 809.31
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 166.5295
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.97s
                      Time elapsed: 00:44:45
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 49993 steps/s (collection: 1.879s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 186.5930
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.2194
                       Mean reward: 879.66
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 1.0517
     Episode_Reward/lifting_object: 170.3997
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.97s
                      Time elapsed: 00:44:47
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 50059 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 179.6283
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.2284
                       Mean reward: 856.93
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.0316
     Episode_Reward/lifting_object: 166.4812
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.96s
                      Time elapsed: 00:44:49
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 49824 steps/s (collection: 1.884s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 197.7659
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.2328
                       Mean reward: 843.60
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 1.0436
     Episode_Reward/lifting_object: 168.9022
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 1.97s
                      Time elapsed: 00:44:51
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 49655 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 165.9265
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.2340
                       Mean reward: 882.08
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 168.4745
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.98s
                      Time elapsed: 00:44:52
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 49362 steps/s (collection: 1.897s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 217.8981
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.2382
                       Mean reward: 842.54
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.0531
     Episode_Reward/lifting_object: 170.8204
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.99s
                      Time elapsed: 00:44:54
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 48521 steps/s (collection: 1.928s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 187.7001
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.2563
                       Mean reward: 841.80
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 1.0504
     Episode_Reward/lifting_object: 169.9776
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.03s
                      Time elapsed: 00:44:57
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 48820 steps/s (collection: 1.903s, learning 0.111s)
             Mean action noise std: 1.88
          Mean value_function loss: 185.3704
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.2628
                       Mean reward: 829.74
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.0601
     Episode_Reward/lifting_object: 172.2013
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.01s
                      Time elapsed: 00:44:59
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 48138 steps/s (collection: 1.926s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 193.8123
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.2744
                       Mean reward: 821.64
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.0421
     Episode_Reward/lifting_object: 169.1181
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.04s
                      Time elapsed: 00:45:01
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 49587 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 1.88
          Mean value_function loss: 197.7810
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 35.2928
                       Mean reward: 863.69
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 170.7137
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.98s
                      Time elapsed: 00:45:03
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 49926 steps/s (collection: 1.880s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 192.9256
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.2959
                       Mean reward: 826.59
               Mean episode length: 226.81
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 168.4477
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.97s
                      Time elapsed: 00:45:05
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 47833 steps/s (collection: 1.949s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 201.9112
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.2995
                       Mean reward: 829.62
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.0332
     Episode_Reward/lifting_object: 166.7943
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.06s
                      Time elapsed: 00:45:07
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 46986 steps/s (collection: 1.997s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 192.9160
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.3036
                       Mean reward: 825.16
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: 164.9022
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.09s
                      Time elapsed: 00:45:09
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 45002 steps/s (collection: 2.084s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 185.0582
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.3138
                       Mean reward: 844.59
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.0370
     Episode_Reward/lifting_object: 167.8815
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.18s
                      Time elapsed: 00:45:11
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 47620 steps/s (collection: 1.965s, learning 0.100s)
             Mean action noise std: 1.89
          Mean value_function loss: 222.8504
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.3254
                       Mean reward: 818.88
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 1.0022
     Episode_Reward/lifting_object: 161.7242
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.06s
                      Time elapsed: 00:45:13
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 47987 steps/s (collection: 1.952s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 175.1163
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.3335
                       Mean reward: 854.26
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 171.2817
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.05s
                      Time elapsed: 00:45:15
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 47190 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 200.7881
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.3419
                       Mean reward: 810.56
               Mean episode length: 223.00
    Episode_Reward/reaching_object: 1.0291
     Episode_Reward/lifting_object: 165.4738
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.08s
                      Time elapsed: 00:45:17
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 48905 steps/s (collection: 1.913s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 194.7488
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.3421
                       Mean reward: 871.72
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 170.7215
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.01s
                      Time elapsed: 00:45:19
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 48606 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 173.4279
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.3395
                       Mean reward: 860.49
               Mean episode length: 235.18
    Episode_Reward/reaching_object: 1.0736
     Episode_Reward/lifting_object: 173.7511
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.02s
                      Time elapsed: 00:45:21
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 49284 steps/s (collection: 1.887s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 167.3587
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.3449
                       Mean reward: 893.27
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: 171.5181
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.99s
                      Time elapsed: 00:45:23
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 46458 steps/s (collection: 2.005s, learning 0.111s)
             Mean action noise std: 1.89
          Mean value_function loss: 178.2082
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3523
                       Mean reward: 852.26
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 170.6138
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.12s
                      Time elapsed: 00:45:25
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 46103 steps/s (collection: 2.009s, learning 0.123s)
             Mean action noise std: 1.89
          Mean value_function loss: 185.9648
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.3630
                       Mean reward: 839.31
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 1.0217
     Episode_Reward/lifting_object: 165.6181
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.13s
                      Time elapsed: 00:45:27
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 46741 steps/s (collection: 2.003s, learning 0.100s)
             Mean action noise std: 1.89
          Mean value_function loss: 198.4595
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.3678
                       Mean reward: 808.19
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 1.0411
     Episode_Reward/lifting_object: 169.5768
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.10s
                      Time elapsed: 00:45:29
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 48536 steps/s (collection: 1.934s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 205.4433
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.3765
                       Mean reward: 848.33
               Mean episode length: 230.24
    Episode_Reward/reaching_object: 0.9989
     Episode_Reward/lifting_object: 162.7694
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.03s
                      Time elapsed: 00:45:31
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 49543 steps/s (collection: 1.886s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 194.9208
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.3938
                       Mean reward: 851.86
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 168.2167
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.98s
                      Time elapsed: 00:45:33
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 49234 steps/s (collection: 1.901s, learning 0.096s)
             Mean action noise std: 1.90
          Mean value_function loss: 160.6806
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.4095
                       Mean reward: 881.91
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 170.0968
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.00s
                      Time elapsed: 00:45:35
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 49539 steps/s (collection: 1.878s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 194.6286
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.4077
                       Mean reward: 862.11
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 172.7718
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.98s
                      Time elapsed: 00:45:37
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 47897 steps/s (collection: 1.933s, learning 0.120s)
             Mean action noise std: 1.90
          Mean value_function loss: 183.2597
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.4078
                       Mean reward: 871.72
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 170.9874
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.05s
                      Time elapsed: 00:45:39
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 47480 steps/s (collection: 1.961s, learning 0.110s)
             Mean action noise std: 1.90
          Mean value_function loss: 180.6047
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.4112
                       Mean reward: 856.74
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 170.0435
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.07s
                      Time elapsed: 00:45:42
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 48849 steps/s (collection: 1.920s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 202.6881
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.4237
                       Mean reward: 850.15
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 174.2890
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.01s
                      Time elapsed: 00:45:44
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 49275 steps/s (collection: 1.894s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 218.8505
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.4358
                       Mean reward: 835.18
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 168.6086
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.99s
                      Time elapsed: 00:45:46
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 48475 steps/s (collection: 1.933s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 197.4841
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.4428
                       Mean reward: 855.05
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 1.0597
     Episode_Reward/lifting_object: 172.4293
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.03s
                      Time elapsed: 00:45:48
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 49142 steps/s (collection: 1.904s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 207.8056
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.4433
                       Mean reward: 864.75
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 172.3972
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.00s
                      Time elapsed: 00:45:50
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 48271 steps/s (collection: 1.926s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 179.9667
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.4458
                       Mean reward: 852.66
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.0480
     Episode_Reward/lifting_object: 171.0634
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.04s
                      Time elapsed: 00:45:52
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 47174 steps/s (collection: 1.967s, learning 0.117s)
             Mean action noise std: 1.91
          Mean value_function loss: 183.7730
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.4500
                       Mean reward: 816.77
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.0333
     Episode_Reward/lifting_object: 167.7508
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.08s
                      Time elapsed: 00:45:54
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 46990 steps/s (collection: 1.995s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 173.5266
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.4509
                       Mean reward: 844.34
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.0357
     Episode_Reward/lifting_object: 169.2192
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.09s
                      Time elapsed: 00:45:56
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 48073 steps/s (collection: 1.945s, learning 0.100s)
             Mean action noise std: 1.91
          Mean value_function loss: 216.1363
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.4504
                       Mean reward: 826.87
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 1.0275
     Episode_Reward/lifting_object: 167.4633
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.04s
                      Time elapsed: 00:45:58
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 48524 steps/s (collection: 1.918s, learning 0.108s)
             Mean action noise std: 1.91
          Mean value_function loss: 183.5494
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.4601
                       Mean reward: 858.27
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 171.6265
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.03s
                      Time elapsed: 00:46:00
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 48570 steps/s (collection: 1.929s, learning 0.095s)
             Mean action noise std: 1.91
          Mean value_function loss: 197.8257
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.4879
                       Mean reward: 847.56
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 169.3484
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.02s
                      Time elapsed: 00:46:02
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 48738 steps/s (collection: 1.921s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 154.7605
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.5044
                       Mean reward: 871.72
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 174.0560
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.02s
                      Time elapsed: 00:46:04
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 49105 steps/s (collection: 1.906s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 176.7159
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5151
                       Mean reward: 882.64
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 172.5879
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.00s
                      Time elapsed: 00:46:06
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 47840 steps/s (collection: 1.940s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 159.7587
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.5318
                       Mean reward: 857.77
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 170.1352
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.05s
                      Time elapsed: 00:46:08
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 47080 steps/s (collection: 1.974s, learning 0.114s)
             Mean action noise std: 1.92
          Mean value_function loss: 185.5561
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.5408
                       Mean reward: 860.26
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.0594
     Episode_Reward/lifting_object: 172.9502
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.09s
                      Time elapsed: 00:46:10
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 49119 steps/s (collection: 1.895s, learning 0.107s)
             Mean action noise std: 1.92
          Mean value_function loss: 182.0262
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 35.5419
                       Mean reward: 863.05
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 170.5354
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.00s
                      Time elapsed: 00:46:12
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 49640 steps/s (collection: 1.870s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 158.0856
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.5421
                       Mean reward: 843.77
               Mean episode length: 230.91
    Episode_Reward/reaching_object: 1.0493
     Episode_Reward/lifting_object: 170.2747
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 1.98s
                      Time elapsed: 00:46:14
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 49005 steps/s (collection: 1.898s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 161.9257
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.5425
                       Mean reward: 880.46
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.0569
     Episode_Reward/lifting_object: 172.5804
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.01s
                      Time elapsed: 00:46:16
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 49735 steps/s (collection: 1.874s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 172.4508
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.5432
                       Mean reward: 887.43
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 1.0480
     Episode_Reward/lifting_object: 170.6814
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 1.98s
                      Time elapsed: 00:46:18
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 47428 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 205.2972
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.5452
                       Mean reward: 830.46
               Mean episode length: 227.70
    Episode_Reward/reaching_object: 1.0319
     Episode_Reward/lifting_object: 167.7010
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.07s
                      Time elapsed: 00:46:20
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 48108 steps/s (collection: 1.948s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 190.9403
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.5511
                       Mean reward: 872.85
               Mean episode length: 236.54
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 167.9371
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.04s
                      Time elapsed: 00:46:22
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 48754 steps/s (collection: 1.920s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 191.0759
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.5642
                       Mean reward: 873.25
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 171.5686
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.02s
                      Time elapsed: 00:46:24
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 48373 steps/s (collection: 1.934s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 192.0342
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.5712
                       Mean reward: 891.73
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 1.0641
     Episode_Reward/lifting_object: 172.1292
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.03s
                      Time elapsed: 00:46:26
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 48142 steps/s (collection: 1.934s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 185.2715
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.5860
                       Mean reward: 826.30
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.0442
     Episode_Reward/lifting_object: 168.4977
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.04s
                      Time elapsed: 00:46:28
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 49022 steps/s (collection: 1.901s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 201.0992
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.6080
                       Mean reward: 838.44
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 1.0360
     Episode_Reward/lifting_object: 166.8395
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.01s
                      Time elapsed: 00:46:30
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 49076 steps/s (collection: 1.895s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 189.0346
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6251
                       Mean reward: 838.32
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 168.2082
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.00s
                      Time elapsed: 00:46:32
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 49699 steps/s (collection: 1.866s, learning 0.112s)
             Mean action noise std: 1.93
          Mean value_function loss: 204.7561
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.6416
                       Mean reward: 862.07
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 1.0314
     Episode_Reward/lifting_object: 165.3203
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.98s
                      Time elapsed: 00:46:34
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 49824 steps/s (collection: 1.875s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 220.4878
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6545
                       Mean reward: 839.73
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 1.0321
     Episode_Reward/lifting_object: 164.5793
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 1.97s
                      Time elapsed: 00:46:36
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 48382 steps/s (collection: 1.925s, learning 0.107s)
             Mean action noise std: 1.93
          Mean value_function loss: 165.5218
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.6736
                       Mean reward: 853.25
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 1.0604
     Episode_Reward/lifting_object: 169.6934
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.03s
                      Time elapsed: 00:46:38
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 46253 steps/s (collection: 2.008s, learning 0.118s)
             Mean action noise std: 1.93
          Mean value_function loss: 190.4800
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.6814
                       Mean reward: 903.37
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.0754
     Episode_Reward/lifting_object: 172.6389
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.13s
                      Time elapsed: 00:46:40
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 46076 steps/s (collection: 2.030s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 204.0747
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.6847
                       Mean reward: 855.57
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 1.0752
     Episode_Reward/lifting_object: 172.3026
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.13s
                      Time elapsed: 00:46:42
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 46995 steps/s (collection: 1.987s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 282.2911
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.6860
                       Mean reward: 827.36
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 167.3971
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.09s
                      Time elapsed: 00:46:45
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 48265 steps/s (collection: 1.937s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 247.2068
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.6907
                       Mean reward: 823.90
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.0277
     Episode_Reward/lifting_object: 164.1725
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.04s
                      Time elapsed: 00:46:47
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 48034 steps/s (collection: 1.939s, learning 0.108s)
             Mean action noise std: 1.94
          Mean value_function loss: 192.7860
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.7026
                       Mean reward: 846.72
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 169.1138
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.05s
                      Time elapsed: 00:46:49
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 47917 steps/s (collection: 1.953s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 211.0067
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7125
                       Mean reward: 815.89
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 169.2542
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.05s
                      Time elapsed: 00:46:51
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 44879 steps/s (collection: 2.078s, learning 0.112s)
             Mean action noise std: 1.94
          Mean value_function loss: 192.8509
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7228
                       Mean reward: 863.52
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 171.8247
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.19s
                      Time elapsed: 00:46:53
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 46674 steps/s (collection: 2.002s, learning 0.104s)
             Mean action noise std: 1.94
          Mean value_function loss: 222.4897
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7421
                       Mean reward: 829.17
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.0350
     Episode_Reward/lifting_object: 165.5121
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.11s
                      Time elapsed: 00:46:55
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 47853 steps/s (collection: 1.947s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 229.7000
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7530
                       Mean reward: 828.98
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 165.9183
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.05s
                      Time elapsed: 00:46:57
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 49273 steps/s (collection: 1.898s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 258.3792
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7634
                       Mean reward: 795.67
               Mean episode length: 218.34
    Episode_Reward/reaching_object: 1.0229
     Episode_Reward/lifting_object: 163.7089
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.00s
                      Time elapsed: 00:46:59
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 48796 steps/s (collection: 1.905s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 262.1026
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.7716
                       Mean reward: 850.43
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.0346
     Episode_Reward/lifting_object: 165.9988
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.01s
                      Time elapsed: 00:47:01
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 47778 steps/s (collection: 1.960s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 258.6821
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7767
                       Mean reward: 816.19
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.0393
     Episode_Reward/lifting_object: 166.9934
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.06s
                      Time elapsed: 00:47:03
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 46106 steps/s (collection: 2.006s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 265.5093
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.7849
                       Mean reward: 835.11
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 167.4492
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.13s
                      Time elapsed: 00:47:05
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 48333 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 254.9750
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.7927
                       Mean reward: 818.33
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 165.7078
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.03s
                      Time elapsed: 00:47:07
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 48866 steps/s (collection: 1.908s, learning 0.104s)
             Mean action noise std: 1.95
          Mean value_function loss: 221.7994
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.8029
                       Mean reward: 837.83
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 170.8237
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.01s
                      Time elapsed: 00:47:09
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 49343 steps/s (collection: 1.894s, learning 0.099s)
             Mean action noise std: 1.95
          Mean value_function loss: 191.5129
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.8162
                       Mean reward: 820.68
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 168.5227
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 1.99s
                      Time elapsed: 00:47:11
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 48592 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 1.95
          Mean value_function loss: 208.3869
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.8320
                       Mean reward: 857.29
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.0338
     Episode_Reward/lifting_object: 164.9537
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.02s
                      Time elapsed: 00:47:13
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 47430 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 1.95
          Mean value_function loss: 210.9675
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.8410
                       Mean reward: 859.72
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 168.2569
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.07s
                      Time elapsed: 00:47:15
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 48100 steps/s (collection: 1.945s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 258.4109
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8527
                       Mean reward: 867.18
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.0433
     Episode_Reward/lifting_object: 167.2606
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.04s
                      Time elapsed: 00:47:17
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 48721 steps/s (collection: 1.919s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 245.9296
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8747
                       Mean reward: 868.01
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 168.1211
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.02s
                      Time elapsed: 00:47:19
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 49367 steps/s (collection: 1.887s, learning 0.104s)
             Mean action noise std: 1.96
          Mean value_function loss: 231.4102
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.8947
                       Mean reward: 855.41
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.0311
     Episode_Reward/lifting_object: 165.2218
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 1.99s
                      Time elapsed: 00:47:21
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 48964 steps/s (collection: 1.913s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 209.1908
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.9178
                       Mean reward: 846.08
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.0636
     Episode_Reward/lifting_object: 171.1285
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.01s
                      Time elapsed: 00:47:23
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 47767 steps/s (collection: 1.953s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 210.6680
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.9292
                       Mean reward: 858.40
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 172.5103
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.06s
                      Time elapsed: 00:47:25
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 49191 steps/s (collection: 1.894s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 200.8137
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.9488
                       Mean reward: 873.46
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.0657
     Episode_Reward/lifting_object: 171.3243
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.00s
                      Time elapsed: 00:47:27
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 48658 steps/s (collection: 1.912s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 207.3488
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.9649
                       Mean reward: 860.89
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 170.0124
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.02s
                      Time elapsed: 00:47:30
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 47949 steps/s (collection: 1.929s, learning 0.122s)
             Mean action noise std: 1.97
          Mean value_function loss: 210.9297
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.9809
                       Mean reward: 850.53
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 167.9214
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.05s
                      Time elapsed: 00:47:32
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 48566 steps/s (collection: 1.916s, learning 0.108s)
             Mean action noise std: 1.97
          Mean value_function loss: 185.9367
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9894
                       Mean reward: 873.10
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 170.8469
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.02s
                      Time elapsed: 00:47:34
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 47667 steps/s (collection: 1.958s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 215.1147
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.0011
                       Mean reward: 872.18
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 170.8589
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.06s
                      Time elapsed: 00:47:36
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 47318 steps/s (collection: 1.968s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 194.3019
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.0225
                       Mean reward: 833.13
               Mean episode length: 227.46
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 169.0475
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.08s
                      Time elapsed: 00:47:38
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 48162 steps/s (collection: 1.940s, learning 0.102s)
             Mean action noise std: 1.98
          Mean value_function loss: 176.9685
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.0377
                       Mean reward: 871.64
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.0599
     Episode_Reward/lifting_object: 171.2614
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.04s
                      Time elapsed: 00:47:40
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 48297 steps/s (collection: 1.932s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 211.0472
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.0523
                       Mean reward: 829.51
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 170.2830
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.04s
                      Time elapsed: 00:47:42
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 49296 steps/s (collection: 1.896s, learning 0.098s)
             Mean action noise std: 1.98
          Mean value_function loss: 186.3872
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.0663
                       Mean reward: 870.29
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 170.8690
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 1.99s
                      Time elapsed: 00:47:44
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 48672 steps/s (collection: 1.918s, learning 0.102s)
             Mean action noise std: 1.98
          Mean value_function loss: 211.2468
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.0770
                       Mean reward: 875.04
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 168.7926
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.02s
                      Time elapsed: 00:47:46
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 47964 steps/s (collection: 1.951s, learning 0.098s)
             Mean action noise std: 1.98
          Mean value_function loss: 163.8623
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.0839
                       Mean reward: 872.40
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 172.4882
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.05s
                      Time elapsed: 00:47:48
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 48930 steps/s (collection: 1.913s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 159.1269
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.0949
                       Mean reward: 874.74
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 173.0323
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.01s
                      Time elapsed: 00:47:50
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 48584 steps/s (collection: 1.915s, learning 0.109s)
             Mean action noise std: 1.99
          Mean value_function loss: 147.6074
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.1045
                       Mean reward: 875.02
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 172.8888
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.02s
                      Time elapsed: 00:47:52
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 48512 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 1.99
          Mean value_function loss: 135.4442
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.1190
                       Mean reward: 896.85
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.0621
     Episode_Reward/lifting_object: 174.6617
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.03s
                      Time elapsed: 00:47:54
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 48715 steps/s (collection: 1.920s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 148.5179
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.1401
                       Mean reward: 902.17
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 1.0448
     Episode_Reward/lifting_object: 172.1343
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.02s
                      Time elapsed: 00:47:56
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 49164 steps/s (collection: 1.902s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 190.4991
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.1581
                       Mean reward: 816.13
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.0307
     Episode_Reward/lifting_object: 169.4405
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.00s
                      Time elapsed: 00:47:58
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 47823 steps/s (collection: 1.949s, learning 0.106s)
             Mean action noise std: 2.00
          Mean value_function loss: 226.2909
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.1756
                       Mean reward: 851.30
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.0052
     Episode_Reward/lifting_object: 165.8965
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.06s
                      Time elapsed: 00:48:00
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 48496 steps/s (collection: 1.916s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 216.2969
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.1947
                       Mean reward: 836.08
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.0202
     Episode_Reward/lifting_object: 167.8601
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.03s
                      Time elapsed: 00:48:02
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 47676 steps/s (collection: 1.956s, learning 0.106s)
             Mean action noise std: 2.00
          Mean value_function loss: 167.5803
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.2047
                       Mean reward: 860.85
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 1.0244
     Episode_Reward/lifting_object: 169.0707
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.06s
                      Time elapsed: 00:48:04
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 48463 steps/s (collection: 1.927s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 173.6154
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2141
                       Mean reward: 881.88
               Mean episode length: 236.85
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 172.9097
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.03s
                      Time elapsed: 00:48:06
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 49632 steps/s (collection: 1.883s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 186.2216
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.2222
                       Mean reward: 868.73
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 1.0294
     Episode_Reward/lifting_object: 170.0391
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 1.98s
                      Time elapsed: 00:48:08
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 48955 steps/s (collection: 1.899s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 191.3577
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.2276
                       Mean reward: 867.97
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 1.0345
     Episode_Reward/lifting_object: 171.6645
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.01s
                      Time elapsed: 00:48:10
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 49150 steps/s (collection: 1.885s, learning 0.115s)
             Mean action noise std: 2.00
          Mean value_function loss: 144.4571
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2369
                       Mean reward: 863.57
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 173.4480
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.00s
                      Time elapsed: 00:48:12
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 49887 steps/s (collection: 1.871s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 134.9236
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.2552
                       Mean reward: 865.33
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 175.8372
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 1.97s
                      Time elapsed: 00:48:14
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 47631 steps/s (collection: 1.967s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 168.6360
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.2721
                       Mean reward: 863.17
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.0312
     Episode_Reward/lifting_object: 169.9984
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.06s
                      Time elapsed: 00:48:16
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 45431 steps/s (collection: 2.064s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 153.0915
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2869
                       Mean reward: 880.23
               Mean episode length: 239.59
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 173.6768
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.16s
                      Time elapsed: 00:48:18
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 48730 steps/s (collection: 1.914s, learning 0.103s)
             Mean action noise std: 2.01
          Mean value_function loss: 153.5005
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.3113
                       Mean reward: 838.37
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 173.6855
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.02s
                      Time elapsed: 00:48:20
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 49653 steps/s (collection: 1.879s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 166.1430
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.3262
                       Mean reward: 872.63
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.0582
     Episode_Reward/lifting_object: 174.6128
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 1.98s
                      Time elapsed: 00:48:22
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 48951 steps/s (collection: 1.911s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 157.7259
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.3373
                       Mean reward: 870.14
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 174.2625
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.01s
                      Time elapsed: 00:48:24
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 49115 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 165.9894
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.3506
                       Mean reward: 889.95
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 175.5737
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.00s
                      Time elapsed: 00:48:26
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 48079 steps/s (collection: 1.938s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 128.4286
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.3555
                       Mean reward: 906.30
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 1.0831
     Episode_Reward/lifting_object: 179.0882
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.04s
                      Time elapsed: 00:48:28
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 48439 steps/s (collection: 1.917s, learning 0.112s)
             Mean action noise std: 2.02
          Mean value_function loss: 172.4083
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.3609
                       Mean reward: 872.79
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.0721
     Episode_Reward/lifting_object: 176.9755
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.03s
                      Time elapsed: 00:48:30
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 48827 steps/s (collection: 1.899s, learning 0.115s)
             Mean action noise std: 2.02
          Mean value_function loss: 139.5033
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.3708
                       Mean reward: 851.21
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.0311
     Episode_Reward/lifting_object: 169.6176
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.01s
                      Time elapsed: 00:48:32
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 47089 steps/s (collection: 1.972s, learning 0.116s)
             Mean action noise std: 2.02
          Mean value_function loss: 168.2778
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.3881
                       Mean reward: 866.92
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.0313
     Episode_Reward/lifting_object: 169.7870
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.09s
                      Time elapsed: 00:48:34
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 48875 steps/s (collection: 1.891s, learning 0.121s)
             Mean action noise std: 2.02
          Mean value_function loss: 134.6474
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.3998
                       Mean reward: 881.19
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 172.3668
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.01s
                      Time elapsed: 00:48:36
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 48523 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 2.02
          Mean value_function loss: 142.7320
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.4066
                       Mean reward: 869.74
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 171.3496
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.03s
                      Time elapsed: 00:48:39
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 49300 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 145.7362
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.4194
                       Mean reward: 833.48
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 173.5610
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 1.99s
                      Time elapsed: 00:48:41
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 48140 steps/s (collection: 1.948s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 167.7839
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.4418
                       Mean reward: 848.03
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: 172.7424
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.04s
                      Time elapsed: 00:48:43
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 49119 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 177.5430
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.4570
                       Mean reward: 874.63
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 1.0642
     Episode_Reward/lifting_object: 174.5091
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.00s
                      Time elapsed: 00:48:45
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 48651 steps/s (collection: 1.925s, learning 0.096s)
             Mean action noise std: 2.03
          Mean value_function loss: 172.1574
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.4677
                       Mean reward: 863.73
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 172.3018
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.02s
                      Time elapsed: 00:48:47
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 48794 steps/s (collection: 1.920s, learning 0.095s)
             Mean action noise std: 2.03
          Mean value_function loss: 163.5415
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.4862
                       Mean reward: 891.16
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.0688
     Episode_Reward/lifting_object: 174.2560
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.01s
                      Time elapsed: 00:48:49
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 49165 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 146.2572
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.5075
                       Mean reward: 888.83
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.0622
     Episode_Reward/lifting_object: 172.2153
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.00s
                      Time elapsed: 00:48:51
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 48941 steps/s (collection: 1.912s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 140.4296
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.5134
                       Mean reward: 872.82
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 174.2590
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.01s
                      Time elapsed: 00:48:53
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 49232 steps/s (collection: 1.896s, learning 0.101s)
             Mean action noise std: 2.04
          Mean value_function loss: 179.9985
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.5236
                       Mean reward: 808.45
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 1.0379
     Episode_Reward/lifting_object: 168.9043
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.00s
                      Time elapsed: 00:48:55
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 49363 steps/s (collection: 1.890s, learning 0.101s)
             Mean action noise std: 2.04
          Mean value_function loss: 155.8386
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.5384
                       Mean reward: 895.30
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.0659
     Episode_Reward/lifting_object: 173.4164
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 1.99s
                      Time elapsed: 00:48:57
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 49601 steps/s (collection: 1.878s, learning 0.104s)
             Mean action noise std: 2.04
          Mean value_function loss: 117.8217
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.5431
                       Mean reward: 883.41
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 174.3758
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 1.98s
                      Time elapsed: 00:48:59
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 49219 steps/s (collection: 1.898s, learning 0.100s)
             Mean action noise std: 2.04
          Mean value_function loss: 155.2845
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5447
                       Mean reward: 833.52
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.0517
     Episode_Reward/lifting_object: 171.9320
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.00s
                      Time elapsed: 00:49:01
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 49321 steps/s (collection: 1.885s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.1910
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.5459
                       Mean reward: 869.62
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 173.4262
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 1.99s
                      Time elapsed: 00:49:03
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 49120 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 2.04
          Mean value_function loss: 131.9092
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5485
                       Mean reward: 887.09
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 1.0810
     Episode_Reward/lifting_object: 176.9924
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.00s
                      Time elapsed: 00:49:05
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 48624 steps/s (collection: 1.909s, learning 0.113s)
             Mean action noise std: 2.04
          Mean value_function loss: 148.4260
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.5569
                       Mean reward: 851.26
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 172.1778
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.02s
                      Time elapsed: 00:49:07
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 48517 steps/s (collection: 1.915s, learning 0.112s)
             Mean action noise std: 2.04
          Mean value_function loss: 145.9447
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.5807
                       Mean reward: 864.93
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.0663
     Episode_Reward/lifting_object: 174.5316
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.03s
                      Time elapsed: 00:49:09
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 49017 steps/s (collection: 1.907s, learning 0.098s)
             Mean action noise std: 2.05
          Mean value_function loss: 171.8572
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.5941
                       Mean reward: 865.38
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 171.0227
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.01s
                      Time elapsed: 00:49:11
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 47697 steps/s (collection: 1.948s, learning 0.113s)
             Mean action noise std: 2.05
          Mean value_function loss: 166.4881
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.6086
                       Mean reward: 841.77
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 171.6069
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.06s
                      Time elapsed: 00:49:13
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 46262 steps/s (collection: 1.996s, learning 0.129s)
             Mean action noise std: 2.05
          Mean value_function loss: 150.2733
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.6242
                       Mean reward: 885.18
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 175.4354
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.12s
                      Time elapsed: 00:49:15
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 47551 steps/s (collection: 1.945s, learning 0.122s)
             Mean action noise std: 2.05
          Mean value_function loss: 134.0808
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.6436
                       Mean reward: 879.11
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 171.7820
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.07s
                      Time elapsed: 00:49:17
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 47574 steps/s (collection: 1.953s, learning 0.114s)
             Mean action noise std: 2.05
          Mean value_function loss: 152.8097
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.6570
                       Mean reward: 846.67
               Mean episode length: 231.70
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 173.7847
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.07s
                      Time elapsed: 00:49:19
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 47486 steps/s (collection: 1.963s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 125.8581
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.6699
                       Mean reward: 877.91
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 175.1238
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.07s
                      Time elapsed: 00:49:21
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 46516 steps/s (collection: 2.008s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 120.9048
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.6794
                       Mean reward: 895.14
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 1.0863
     Episode_Reward/lifting_object: 177.4880
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.11s
                      Time elapsed: 00:49:23
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 46941 steps/s (collection: 1.995s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 118.3514
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.6882
                       Mean reward: 900.60
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 176.5095
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.09s
                      Time elapsed: 00:49:25
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 46480 steps/s (collection: 2.008s, learning 0.107s)
             Mean action noise std: 2.06
          Mean value_function loss: 141.0867
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.6971
                       Mean reward: 862.73
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 173.9476
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.11s
                      Time elapsed: 00:49:27
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 46811 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 2.06
          Mean value_function loss: 144.7361
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7038
                       Mean reward: 863.91
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 172.6500
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.10s
                      Time elapsed: 00:49:29
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 47082 steps/s (collection: 1.977s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 128.8183
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.7087
                       Mean reward: 899.13
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 1.0735
     Episode_Reward/lifting_object: 175.3668
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.09s
                      Time elapsed: 00:49:32
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.973s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 116.8443
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.7154
                       Mean reward: 878.32
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.0707
     Episode_Reward/lifting_object: 175.9874
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.08s
                      Time elapsed: 00:49:34
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 49717 steps/s (collection: 1.871s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 115.0644
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.7263
                       Mean reward: 879.61
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.0881
     Episode_Reward/lifting_object: 178.5834
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 1.98s
                      Time elapsed: 00:49:36
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 48736 steps/s (collection: 1.913s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 129.8851
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.7388
                       Mean reward: 883.16
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 173.6893
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.02s
                      Time elapsed: 00:49:38
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19688 steps/s (collection: 4.871s, learning 0.123s)
             Mean action noise std: 2.07
          Mean value_function loss: 105.8380
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.7479
                       Mean reward: 861.71
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 170.3744
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 4.99s
                      Time elapsed: 00:49:43
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14929 steps/s (collection: 6.473s, learning 0.111s)
             Mean action noise std: 2.07
          Mean value_function loss: 115.6626
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.7545
                       Mean reward: 913.69
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 175.0253
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.58s
                      Time elapsed: 00:49:49
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14882 steps/s (collection: 6.467s, learning 0.138s)
             Mean action noise std: 2.07
          Mean value_function loss: 132.3470
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.7661
                       Mean reward: 903.52
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 1.0737
     Episode_Reward/lifting_object: 176.3649
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.61s
                      Time elapsed: 00:49:56
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14660 steps/s (collection: 6.598s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 124.5456
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.7783
                       Mean reward: 872.35
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 174.9447
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.71s
                      Time elapsed: 00:50:02
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14472 steps/s (collection: 6.662s, learning 0.130s)
             Mean action noise std: 2.07
          Mean value_function loss: 149.3302
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.7917
                       Mean reward: 840.53
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 1.0486
     Episode_Reward/lifting_object: 171.7361
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.79s
                      Time elapsed: 00:50:09
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14489 steps/s (collection: 6.657s, learning 0.128s)
             Mean action noise std: 2.07
          Mean value_function loss: 107.3860
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.8052
                       Mean reward: 891.12
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 177.0505
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.78s
                      Time elapsed: 00:50:16
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14665 steps/s (collection: 6.588s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 106.7908
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.8112
                       Mean reward: 896.48
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 1.0706
     Episode_Reward/lifting_object: 175.3688
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.70s
                      Time elapsed: 00:50:23
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14926 steps/s (collection: 6.473s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 99.5937
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.8149
                       Mean reward: 897.74
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.0817
     Episode_Reward/lifting_object: 177.3091
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.59s
                      Time elapsed: 00:50:29
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13472 steps/s (collection: 7.192s, learning 0.104s)
             Mean action noise std: 2.08
          Mean value_function loss: 176.8800
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.8174
                       Mean reward: 884.63
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 1.0697
     Episode_Reward/lifting_object: 175.0584
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.30s
                      Time elapsed: 00:50:37
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 44026 steps/s (collection: 2.134s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 118.9271
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.8333
                       Mean reward: 886.91
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 172.8153
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.23s
                      Time elapsed: 00:50:39
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 34552 steps/s (collection: 2.584s, learning 0.261s)
             Mean action noise std: 2.08
          Mean value_function loss: 135.1858
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.8472
                       Mean reward: 891.68
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 172.1705
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.85s
                      Time elapsed: 00:50:42
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 41388 steps/s (collection: 2.261s, learning 0.114s)
             Mean action noise std: 2.08
          Mean value_function loss: 158.2674
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.8622
                       Mean reward: 872.86
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 171.0252
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.38s
                      Time elapsed: 00:50:44
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 42510 steps/s (collection: 2.135s, learning 0.177s)
             Mean action noise std: 2.08
          Mean value_function loss: 142.6960
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.8731
                       Mean reward: 889.63
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 1.0708
     Episode_Reward/lifting_object: 174.8645
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.31s
                      Time elapsed: 00:50:46
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 40661 steps/s (collection: 2.308s, learning 0.110s)
             Mean action noise std: 2.08
          Mean value_function loss: 126.2220
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.8765
                       Mean reward: 884.34
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.0714
     Episode_Reward/lifting_object: 174.9166
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.42s
                      Time elapsed: 00:50:49
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 49638 steps/s (collection: 1.886s, learning 0.094s)
             Mean action noise std: 2.08
          Mean value_function loss: 124.9706
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.8810
                       Mean reward: 905.97
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 1.0883
     Episode_Reward/lifting_object: 178.3675
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.98s
                      Time elapsed: 00:50:51
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 47135 steps/s (collection: 1.962s, learning 0.124s)
             Mean action noise std: 2.09
          Mean value_function loss: 127.8214
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.8943
                       Mean reward: 892.06
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 1.0795
     Episode_Reward/lifting_object: 176.9382
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.09s
                      Time elapsed: 00:50:53
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 50229 steps/s (collection: 1.869s, learning 0.089s)
             Mean action noise std: 2.09
          Mean value_function loss: 135.5758
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.9103
                       Mean reward: 876.59
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.0740
     Episode_Reward/lifting_object: 175.7633
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.96s
                      Time elapsed: 00:50:55
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 50490 steps/s (collection: 1.852s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 104.3282
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.9234
                       Mean reward: 885.75
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.0802
     Episode_Reward/lifting_object: 177.6476
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.95s
                      Time elapsed: 00:50:57
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 46967 steps/s (collection: 1.969s, learning 0.124s)
             Mean action noise std: 2.09
          Mean value_function loss: 135.4958
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.9348
                       Mean reward: 871.56
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.0723
     Episode_Reward/lifting_object: 176.1340
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.09s
                      Time elapsed: 00:50:59
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 50933 steps/s (collection: 1.831s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 124.8422
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.9477
                       Mean reward: 857.60
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 175.6568
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.93s
                      Time elapsed: 00:51:01
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 50883 steps/s (collection: 1.841s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 155.6684
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.9587
                       Mean reward: 875.18
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 172.2713
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.93s
                      Time elapsed: 00:51:03
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 50873 steps/s (collection: 1.846s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 154.5716
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.9654
                       Mean reward: 904.54
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 1.0429
     Episode_Reward/lifting_object: 170.7198
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.93s
                      Time elapsed: 00:51:05
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 48955 steps/s (collection: 1.841s, learning 0.167s)
             Mean action noise std: 2.10
          Mean value_function loss: 144.2325
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.9763
                       Mean reward: 877.29
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 1.0662
     Episode_Reward/lifting_object: 174.7029
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.01s
                      Time elapsed: 00:51:07
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 48755 steps/s (collection: 1.906s, learning 0.111s)
             Mean action noise std: 2.10
          Mean value_function loss: 113.1429
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.9936
                       Mean reward: 883.08
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.0710
     Episode_Reward/lifting_object: 175.3995
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.02s
                      Time elapsed: 00:51:09
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51450 steps/s (collection: 1.812s, learning 0.099s)
             Mean action noise std: 2.10
          Mean value_function loss: 133.2515
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0033
                       Mean reward: 893.08
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.0474
     Episode_Reward/lifting_object: 171.2322
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.91s
                      Time elapsed: 00:51:11
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 50845 steps/s (collection: 1.782s, learning 0.152s)
             Mean action noise std: 2.10
          Mean value_function loss: 186.3925
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.0192
                       Mean reward: 847.13
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.0588
     Episode_Reward/lifting_object: 173.6456
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.93s
                      Time elapsed: 00:51:13
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 50769 steps/s (collection: 1.800s, learning 0.136s)
             Mean action noise std: 2.11
          Mean value_function loss: 153.8014
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.0419
                       Mean reward: 864.07
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 1.0503
     Episode_Reward/lifting_object: 171.1409
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 1.94s
                      Time elapsed: 00:51:14
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 49733 steps/s (collection: 1.814s, learning 0.162s)
             Mean action noise std: 2.11
          Mean value_function loss: 111.4634
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.0585
                       Mean reward: 862.27
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.0692
     Episode_Reward/lifting_object: 175.1391
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.98s
                      Time elapsed: 00:51:16
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 49607 steps/s (collection: 1.811s, learning 0.171s)
             Mean action noise std: 2.11
          Mean value_function loss: 134.0551
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0646
                       Mean reward: 881.26
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 1.0817
     Episode_Reward/lifting_object: 177.1597
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.98s
                      Time elapsed: 00:51:18
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 51574 steps/s (collection: 1.779s, learning 0.127s)
             Mean action noise std: 2.11
          Mean value_function loss: 100.3549
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0722
                       Mean reward: 905.32
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 1.0838
     Episode_Reward/lifting_object: 177.6652
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.91s
                      Time elapsed: 00:51:20
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 51524 steps/s (collection: 1.818s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 118.6294
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0793
                       Mean reward: 897.58
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.0819
     Episode_Reward/lifting_object: 178.3800
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.91s
                      Time elapsed: 00:51:22
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 51438 steps/s (collection: 1.821s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 110.1147
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.0868
                       Mean reward: 872.62
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 1.0763
     Episode_Reward/lifting_object: 176.2868
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.91s
                      Time elapsed: 00:51:24
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 50746 steps/s (collection: 1.819s, learning 0.118s)
             Mean action noise std: 2.11
          Mean value_function loss: 122.0156
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.0962
                       Mean reward: 867.91
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 1.0827
     Episode_Reward/lifting_object: 177.7872
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.94s
                      Time elapsed: 00:51:26
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 51209 steps/s (collection: 1.811s, learning 0.108s)
             Mean action noise std: 2.11
          Mean value_function loss: 144.8889
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.1035
                       Mean reward: 887.37
               Mean episode length: 237.80
    Episode_Reward/reaching_object: 1.0741
     Episode_Reward/lifting_object: 175.2141
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.92s
                      Time elapsed: 00:51:28
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 48252 steps/s (collection: 1.887s, learning 0.151s)
             Mean action noise std: 2.11
          Mean value_function loss: 169.0370
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.1128
                       Mean reward: 876.68
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 174.4297
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.04s
                      Time elapsed: 00:51:30
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 50285 steps/s (collection: 1.858s, learning 0.097s)
             Mean action noise std: 2.12
          Mean value_function loss: 151.1424
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.1263
                       Mean reward: 888.50
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.0821
     Episode_Reward/lifting_object: 176.5222
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.95s
                      Time elapsed: 00:51:32
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 50531 steps/s (collection: 1.858s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 142.7978
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.1341
                       Mean reward: 891.22
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.0819
     Episode_Reward/lifting_object: 176.8782
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.95s
                      Time elapsed: 00:51:34
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 52139 steps/s (collection: 1.798s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 165.2003
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1404
                       Mean reward: 897.28
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 1.0729
     Episode_Reward/lifting_object: 174.7158
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.89s
                      Time elapsed: 00:51:36
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 50821 steps/s (collection: 1.835s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 183.3485
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.1551
                       Mean reward: 888.29
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 1.0716
     Episode_Reward/lifting_object: 174.1496
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.93s
                      Time elapsed: 00:51:38
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 53604 steps/s (collection: 1.746s, learning 0.088s)
             Mean action noise std: 2.12
          Mean value_function loss: 143.8947
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1696
                       Mean reward: 860.60
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: 174.7601
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.83s
                      Time elapsed: 00:51:40
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 50401 steps/s (collection: 1.858s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 164.1233
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.1874
                       Mean reward: 864.22
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.0677
     Episode_Reward/lifting_object: 173.1408
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.95s
                      Time elapsed: 00:51:42
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 50870 steps/s (collection: 1.843s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 178.7828
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.2071
                       Mean reward: 837.66
               Mean episode length: 229.37
    Episode_Reward/reaching_object: 1.0517
     Episode_Reward/lifting_object: 169.6870
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.93s
                      Time elapsed: 00:51:44
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 52264 steps/s (collection: 1.790s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 160.9374
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.2231
                       Mean reward: 856.35
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.0746
     Episode_Reward/lifting_object: 173.6337
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.88s
                      Time elapsed: 00:51:45
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 50621 steps/s (collection: 1.850s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 162.9402
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.2391
                       Mean reward: 875.68
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 1.0789
     Episode_Reward/lifting_object: 175.0477
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.94s
                      Time elapsed: 00:51:47
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 52475 steps/s (collection: 1.780s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 183.6744
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2493
                       Mean reward: 872.65
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 170.9256
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.87s
                      Time elapsed: 00:51:49
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 51839 steps/s (collection: 1.801s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 148.5878
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.2544
                       Mean reward: 861.03
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 173.1258
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.90s
                      Time elapsed: 00:51:51
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 52643 steps/s (collection: 1.774s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 143.0997
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2649
                       Mean reward: 889.34
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.0933
     Episode_Reward/lifting_object: 177.3893
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 1.87s
                      Time elapsed: 00:51:53
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 51347 steps/s (collection: 1.829s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 169.5384
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 37.2746
                       Mean reward: 913.72
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 1.0889
     Episode_Reward/lifting_object: 176.6507
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.91s
                      Time elapsed: 00:51:55
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 51431 steps/s (collection: 1.811s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 143.3292
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.2802
                       Mean reward: 904.65
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 1.0815
     Episode_Reward/lifting_object: 175.6939
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.91s
                      Time elapsed: 00:51:57
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 51215 steps/s (collection: 1.815s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 127.2078
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.2878
                       Mean reward: 885.12
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 1.0909
     Episode_Reward/lifting_object: 177.5139
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.92s
                      Time elapsed: 00:51:59
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 51886 steps/s (collection: 1.795s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 108.1051
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.2996
                       Mean reward: 860.04
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.0877
     Episode_Reward/lifting_object: 177.6145
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.89s
                      Time elapsed: 00:52:01
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 50884 steps/s (collection: 1.839s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 157.4715
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3063
                       Mean reward: 849.63
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 1.0627
     Episode_Reward/lifting_object: 173.4134
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.93s
                      Time elapsed: 00:52:03
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 50992 steps/s (collection: 1.836s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 122.3758
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.3160
                       Mean reward: 885.12
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.0859
     Episode_Reward/lifting_object: 178.2788
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.93s
                      Time elapsed: 00:52:04
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 52265 steps/s (collection: 1.779s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 138.6788
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.3300
                       Mean reward: 899.43
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 173.2259
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 1.88s
                      Time elapsed: 00:52:06
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 51320 steps/s (collection: 1.830s, learning 0.086s)
             Mean action noise std: 2.14
          Mean value_function loss: 160.1263
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3414
                       Mean reward: 887.09
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.0732
     Episode_Reward/lifting_object: 175.8472
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.92s
                      Time elapsed: 00:52:08
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 52144 steps/s (collection: 1.800s, learning 0.086s)
             Mean action noise std: 2.14
          Mean value_function loss: 145.2922
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.3560
                       Mean reward: 862.13
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 173.3538
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 1.89s
                      Time elapsed: 00:52:10
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 52018 steps/s (collection: 1.806s, learning 0.084s)
             Mean action noise std: 2.15
          Mean value_function loss: 126.2809
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.3756
                       Mean reward: 898.05
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.0685
     Episode_Reward/lifting_object: 174.8920
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.89s
                      Time elapsed: 00:52:12
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 52001 steps/s (collection: 1.801s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 118.7167
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.3899
                       Mean reward: 899.58
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 1.0858
     Episode_Reward/lifting_object: 178.0761
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.89s
                      Time elapsed: 00:52:14
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 52011 steps/s (collection: 1.792s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 104.9581
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.3975
                       Mean reward: 893.89
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 1.0807
     Episode_Reward/lifting_object: 176.9100
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 1.89s
                      Time elapsed: 00:52:16
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 52591 steps/s (collection: 1.757s, learning 0.113s)
             Mean action noise std: 2.15
          Mean value_function loss: 118.3328
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.4064
                       Mean reward: 852.37
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 1.0694
     Episode_Reward/lifting_object: 174.4240
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.87s
                      Time elapsed: 00:52:18
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 52532 steps/s (collection: 1.770s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 106.1803
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.4219
                       Mean reward: 872.24
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.0723
     Episode_Reward/lifting_object: 174.6991
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.87s
                      Time elapsed: 00:52:20
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 51853 steps/s (collection: 1.806s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 110.0765
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.4351
                       Mean reward: 888.91
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0893
     Episode_Reward/lifting_object: 176.8181
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.90s
                      Time elapsed: 00:52:21
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 50977 steps/s (collection: 1.835s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 122.8604
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.4449
                       Mean reward: 873.91
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.0854
     Episode_Reward/lifting_object: 176.9690
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.93s
                      Time elapsed: 00:52:23
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 52111 steps/s (collection: 1.794s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 145.1182
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.4532
                       Mean reward: 902.46
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 1.0752
     Episode_Reward/lifting_object: 175.0870
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.89s
                      Time elapsed: 00:52:25
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 51728 steps/s (collection: 1.793s, learning 0.108s)
             Mean action noise std: 2.15
          Mean value_function loss: 156.3907
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.4582
                       Mean reward: 886.50
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 173.1434
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.90s
                      Time elapsed: 00:52:27
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 52142 steps/s (collection: 1.784s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 127.0498
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.4664
                       Mean reward: 909.26
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 1.0742
     Episode_Reward/lifting_object: 175.5066
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.89s
                      Time elapsed: 00:52:29
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 51164 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 130.4191
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.4742
                       Mean reward: 877.83
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 174.5941
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.92s
                      Time elapsed: 00:52:31
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 52087 steps/s (collection: 1.797s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 146.7796
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.4876
                       Mean reward: 852.29
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 1.0515
     Episode_Reward/lifting_object: 171.6505
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.89s
                      Time elapsed: 00:52:33
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 50988 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 147.0125
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.4998
                       Mean reward: 910.20
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 173.9119
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 1.93s
                      Time elapsed: 00:52:35
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 52361 steps/s (collection: 1.784s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 120.3132
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.5085
                       Mean reward: 892.85
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 174.3189
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.88s
                      Time elapsed: 00:52:37
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 51171 steps/s (collection: 1.832s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 116.4930
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.5198
                       Mean reward: 897.65
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 174.3161
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.92s
                      Time elapsed: 00:52:39
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 52493 steps/s (collection: 1.787s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 142.8389
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.5298
                       Mean reward: 869.78
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.0662
     Episode_Reward/lifting_object: 173.9720
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.87s
                      Time elapsed: 00:52:40
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 50622 steps/s (collection: 1.836s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 91.5550
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 37.5353
                       Mean reward: 887.80
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 179.0372
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.94s
                      Time elapsed: 00:52:42
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 52184 steps/s (collection: 1.781s, learning 0.103s)
             Mean action noise std: 2.17
          Mean value_function loss: 130.7585
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.5405
                       Mean reward: 896.48
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 1.0860
     Episode_Reward/lifting_object: 177.6921
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.88s
                      Time elapsed: 00:52:44
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 51238 steps/s (collection: 1.816s, learning 0.103s)
             Mean action noise std: 2.17
          Mean value_function loss: 126.4705
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.5483
                       Mean reward: 896.67
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 1.0520
     Episode_Reward/lifting_object: 171.6174
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.92s
                      Time elapsed: 00:52:46
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 52209 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 137.2326
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.5538
                       Mean reward: 863.16
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 174.5343
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.88s
                      Time elapsed: 00:52:48
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 50150 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 172.0587
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.5617
                       Mean reward: 896.83
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 173.2980
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 1.96s
                      Time elapsed: 00:52:50
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 51377 steps/s (collection: 1.823s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 161.7863
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.5745
                       Mean reward: 891.07
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 174.9627
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 1.91s
                      Time elapsed: 00:52:52
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 50252 steps/s (collection: 1.841s, learning 0.115s)
             Mean action noise std: 2.17
          Mean value_function loss: 170.3243
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.5838
                       Mean reward: 848.42
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 173.3535
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.96s
                      Time elapsed: 00:52:54
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 52020 steps/s (collection: 1.789s, learning 0.100s)
             Mean action noise std: 2.17
          Mean value_function loss: 147.8111
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.5876
                       Mean reward: 899.09
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 172.7238
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.89s
                      Time elapsed: 00:52:56
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 51489 steps/s (collection: 1.813s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 151.3195
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 37.5978
                       Mean reward: 878.79
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 173.6102
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 1.91s
                      Time elapsed: 00:52:58
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 51430 steps/s (collection: 1.824s, learning 0.088s)
             Mean action noise std: 2.17
          Mean value_function loss: 149.9175
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.6083
                       Mean reward: 879.40
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.0763
     Episode_Reward/lifting_object: 174.4679
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.91s
                      Time elapsed: 00:53:00
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 50935 steps/s (collection: 1.840s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 168.9472
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.6191
                       Mean reward: 872.87
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 171.5387
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.93s
                      Time elapsed: 00:53:02
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 51653 steps/s (collection: 1.815s, learning 0.088s)
             Mean action noise std: 2.18
          Mean value_function loss: 194.2439
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.6330
                       Mean reward: 806.66
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 171.1284
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.90s
                      Time elapsed: 00:53:03
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 51779 steps/s (collection: 1.812s, learning 0.087s)
             Mean action noise std: 2.18
          Mean value_function loss: 166.5613
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.6386
                       Mean reward: 890.10
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 1.0701
     Episode_Reward/lifting_object: 174.2273
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.90s
                      Time elapsed: 00:53:05
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 51667 steps/s (collection: 1.817s, learning 0.086s)
             Mean action noise std: 2.18
          Mean value_function loss: 136.8951
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.6476
                       Mean reward: 864.28
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.0665
     Episode_Reward/lifting_object: 173.4562
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.90s
                      Time elapsed: 00:53:07
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 52509 steps/s (collection: 1.780s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 178.8317
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.6556
                       Mean reward: 850.87
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 1.0505
     Episode_Reward/lifting_object: 170.8926
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.87s
                      Time elapsed: 00:53:09
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 52682 steps/s (collection: 1.780s, learning 0.086s)
             Mean action noise std: 2.18
          Mean value_function loss: 138.8196
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.6616
                       Mean reward: 853.86
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.0722
     Episode_Reward/lifting_object: 175.3194
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.87s
                      Time elapsed: 00:53:11
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 52501 steps/s (collection: 1.785s, learning 0.088s)
             Mean action noise std: 2.18
          Mean value_function loss: 133.4447
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.6669
                       Mean reward: 864.51
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.0665
     Episode_Reward/lifting_object: 173.7048
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.87s
                      Time elapsed: 00:53:13
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 50125 steps/s (collection: 1.852s, learning 0.109s)
             Mean action noise std: 2.18
          Mean value_function loss: 147.6841
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.6724
                       Mean reward: 892.73
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 173.0902
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.96s
                      Time elapsed: 00:53:15
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 51325 steps/s (collection: 1.815s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 128.3896
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.6819
                       Mean reward: 866.43
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.0616
     Episode_Reward/lifting_object: 173.5103
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.92s
                      Time elapsed: 00:53:17
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 51932 steps/s (collection: 1.799s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 154.5322
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 37.6855
                       Mean reward: 845.91
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 171.3429
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.89s
                      Time elapsed: 00:53:19
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 51188 steps/s (collection: 1.829s, learning 0.092s)
             Mean action noise std: 2.19
          Mean value_function loss: 147.1636
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.6936
                       Mean reward: 850.92
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 171.9167
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.92s
                      Time elapsed: 00:53:21
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 51027 steps/s (collection: 1.836s, learning 0.090s)
             Mean action noise std: 2.19
          Mean value_function loss: 113.6291
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.7063
                       Mean reward: 879.61
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.0735
     Episode_Reward/lifting_object: 175.1494
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.93s
                      Time elapsed: 00:53:22
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 51039 steps/s (collection: 1.840s, learning 0.087s)
             Mean action noise std: 2.19
          Mean value_function loss: 133.0348
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.7195
                       Mean reward: 867.71
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 176.9649
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.93s
                      Time elapsed: 00:53:24
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 51644 steps/s (collection: 1.816s, learning 0.087s)
             Mean action noise std: 2.19
          Mean value_function loss: 131.0390
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.7360
                       Mean reward: 894.30
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: 172.3698
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.90s
                      Time elapsed: 00:53:26
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 52319 steps/s (collection: 1.792s, learning 0.087s)
             Mean action noise std: 2.19
          Mean value_function loss: 129.9284
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.7489
                       Mean reward: 873.36
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.0774
     Episode_Reward/lifting_object: 176.5658
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.88s
                      Time elapsed: 00:53:28
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 51213 steps/s (collection: 1.830s, learning 0.089s)
             Mean action noise std: 2.19
          Mean value_function loss: 105.2006
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.7622
                       Mean reward: 864.43
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.0688
     Episode_Reward/lifting_object: 174.7585
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.92s
                      Time elapsed: 00:53:30
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 51826 steps/s (collection: 1.809s, learning 0.088s)
             Mean action noise std: 2.19
          Mean value_function loss: 129.1308
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.7703
                       Mean reward: 858.44
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 174.2474
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.90s
                      Time elapsed: 00:53:32
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 51369 steps/s (collection: 1.820s, learning 0.094s)
             Mean action noise std: 2.20
          Mean value_function loss: 139.5469
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.7736
                       Mean reward: 860.92
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 174.8385
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 1.91s
                      Time elapsed: 00:53:34
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 50071 steps/s (collection: 1.871s, learning 0.092s)
             Mean action noise std: 2.20
          Mean value_function loss: 134.8392
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.7772
                       Mean reward: 847.60
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 173.2294
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.96s
                      Time elapsed: 00:53:36
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 50822 steps/s (collection: 1.825s, learning 0.110s)
             Mean action noise std: 2.20
          Mean value_function loss: 103.4310
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.7817
                       Mean reward: 886.11
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.0708
     Episode_Reward/lifting_object: 175.6413
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.93s
                      Time elapsed: 00:53:38
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 51047 steps/s (collection: 1.833s, learning 0.093s)
             Mean action noise std: 2.20
          Mean value_function loss: 127.5156
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.7864
                       Mean reward: 837.03
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 1.0449
     Episode_Reward/lifting_object: 169.9181
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.93s
                      Time elapsed: 00:53:40
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 50584 steps/s (collection: 1.847s, learning 0.097s)
             Mean action noise std: 2.20
          Mean value_function loss: 125.9705
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7910
                       Mean reward: 868.33
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.0517
     Episode_Reward/lifting_object: 171.8561
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 1.94s
                      Time elapsed: 00:53:42
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 51241 steps/s (collection: 1.827s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 97.0808
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.7999
                       Mean reward: 877.53
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 1.0779
     Episode_Reward/lifting_object: 176.6462
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.92s
                      Time elapsed: 00:53:44
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.824s, learning 0.101s)
             Mean action noise std: 2.20
          Mean value_function loss: 124.5944
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.8202
                       Mean reward: 843.86
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 172.8463
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.93s
                      Time elapsed: 00:53:46
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 51011 steps/s (collection: 1.835s, learning 0.092s)
             Mean action noise std: 2.20
          Mean value_function loss: 118.0565
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.8415
                       Mean reward: 893.48
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 1.0762
     Episode_Reward/lifting_object: 176.1187
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.93s
                      Time elapsed: 00:53:47
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 50590 steps/s (collection: 1.841s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 143.3318
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.8557
                       Mean reward: 893.72
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 1.0923
     Episode_Reward/lifting_object: 178.4009
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.94s
                      Time elapsed: 00:53:49
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 50628 steps/s (collection: 1.848s, learning 0.094s)
             Mean action noise std: 2.21
          Mean value_function loss: 120.0625
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 37.8632
                       Mean reward: 890.47
               Mean episode length: 238.66
    Episode_Reward/reaching_object: 1.0727
     Episode_Reward/lifting_object: 175.5037
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.94s
                      Time elapsed: 00:53:51
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 50317 steps/s (collection: 1.850s, learning 0.104s)
             Mean action noise std: 2.21
          Mean value_function loss: 147.3800
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.8695
                       Mean reward: 873.83
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 172.7271
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.95s
                      Time elapsed: 00:53:53
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 49483 steps/s (collection: 1.888s, learning 0.099s)
             Mean action noise std: 2.21
          Mean value_function loss: 129.9576
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.8817
                       Mean reward: 887.80
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 173.2870
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.99s
                      Time elapsed: 00:53:55
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 51187 steps/s (collection: 1.828s, learning 0.092s)
             Mean action noise std: 2.21
          Mean value_function loss: 164.8665
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.8943
                       Mean reward: 880.81
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.0718
     Episode_Reward/lifting_object: 175.2585
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.92s
                      Time elapsed: 00:53:57
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 51019 steps/s (collection: 1.833s, learning 0.094s)
             Mean action noise std: 2.21
          Mean value_function loss: 137.5471
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.9005
                       Mean reward: 840.89
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 173.8169
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.93s
                      Time elapsed: 00:53:59
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 50288 steps/s (collection: 1.868s, learning 0.087s)
             Mean action noise std: 2.21
          Mean value_function loss: 148.2413
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.9094
                       Mean reward: 881.70
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 1.0701
     Episode_Reward/lifting_object: 174.3468
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.95s
                      Time elapsed: 00:54:01
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 50694 steps/s (collection: 1.836s, learning 0.104s)
             Mean action noise std: 2.21
          Mean value_function loss: 123.5261
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.9209
                       Mean reward: 890.31
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 177.7988
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.94s
                      Time elapsed: 00:54:03
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 50929 steps/s (collection: 1.836s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 166.1039
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.9324
                       Mean reward: 877.64
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 1.0812
     Episode_Reward/lifting_object: 176.4556
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.93s
                      Time elapsed: 00:54:05
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 51505 steps/s (collection: 1.817s, learning 0.092s)
             Mean action noise std: 2.22
          Mean value_function loss: 157.6327
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.9490
                       Mean reward: 887.57
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 1.0733
     Episode_Reward/lifting_object: 174.2145
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 1.91s
                      Time elapsed: 00:54:07
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 50898 steps/s (collection: 1.841s, learning 0.091s)
             Mean action noise std: 2.22
          Mean value_function loss: 142.1263
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.9676
                       Mean reward: 870.56
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 173.3029
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 1.93s
                      Time elapsed: 00:54:09
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 49796 steps/s (collection: 1.877s, learning 0.097s)
             Mean action noise std: 2.22
          Mean value_function loss: 204.8691
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 37.9777
                       Mean reward: 848.26
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 1.0461
     Episode_Reward/lifting_object: 169.8023
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 1.97s
                      Time elapsed: 00:54:11
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 49666 steps/s (collection: 1.879s, learning 0.100s)
             Mean action noise std: 2.22
          Mean value_function loss: 229.9565
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.9830
                       Mean reward: 839.79
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 1.0626
     Episode_Reward/lifting_object: 172.4089
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.98s
                      Time elapsed: 00:54:13
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 48283 steps/s (collection: 1.924s, learning 0.112s)
             Mean action noise std: 2.22
          Mean value_function loss: 212.2330
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.9892
                       Mean reward: 864.33
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 172.0281
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.04s
                      Time elapsed: 00:54:15
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 48670 steps/s (collection: 1.903s, learning 0.117s)
             Mean action noise std: 2.22
          Mean value_function loss: 191.1411
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 38.0005
                       Mean reward: 878.66
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 1.0588
     Episode_Reward/lifting_object: 170.9116
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.02s
                      Time elapsed: 00:54:17
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 49441 steps/s (collection: 1.875s, learning 0.113s)
             Mean action noise std: 2.22
          Mean value_function loss: 198.5730
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.0099
                       Mean reward: 831.59
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.0656
     Episode_Reward/lifting_object: 172.0542
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.99s
                      Time elapsed: 00:54:19
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 50786 steps/s (collection: 1.848s, learning 0.088s)
             Mean action noise std: 2.23
          Mean value_function loss: 155.6324
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.0201
                       Mean reward: 896.85
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 172.2245
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.94s
                      Time elapsed: 00:54:21
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 51085 steps/s (collection: 1.830s, learning 0.094s)
             Mean action noise std: 2.23
          Mean value_function loss: 156.4726
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 38.0263
                       Mean reward: 855.93
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.0601
     Episode_Reward/lifting_object: 170.6132
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 1.92s
                      Time elapsed: 00:54:23
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 51251 steps/s (collection: 1.828s, learning 0.090s)
             Mean action noise std: 2.23
          Mean value_function loss: 189.5752
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.0297
                       Mean reward: 885.08
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 171.2700
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.92s
                      Time elapsed: 00:54:25
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 50698 steps/s (collection: 1.844s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 136.2520
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.0336
                       Mean reward: 874.42
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.0782
     Episode_Reward/lifting_object: 173.4990
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.94s
                      Time elapsed: 00:54:27
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 49856 steps/s (collection: 1.865s, learning 0.107s)
             Mean action noise std: 2.23
          Mean value_function loss: 154.9568
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0422
                       Mean reward: 871.72
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 1.0813
     Episode_Reward/lifting_object: 175.0536
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.97s
                      Time elapsed: 00:54:28
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 49604 steps/s (collection: 1.874s, learning 0.108s)
             Mean action noise std: 2.23
          Mean value_function loss: 145.3594
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 38.0580
                       Mean reward: 856.61
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 171.9957
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.98s
                      Time elapsed: 00:54:30
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 49545 steps/s (collection: 1.868s, learning 0.117s)
             Mean action noise std: 2.23
          Mean value_function loss: 128.7265
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.0712
                       Mean reward: 857.50
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 173.6324
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.98s
                      Time elapsed: 00:54:32
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 50359 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 2.23
          Mean value_function loss: 129.0832
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.0812
                       Mean reward: 900.32
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 1.0984
     Episode_Reward/lifting_object: 177.4999
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.95s
                      Time elapsed: 00:54:34
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 49986 steps/s (collection: 1.863s, learning 0.104s)
             Mean action noise std: 2.23
          Mean value_function loss: 164.3409
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.0882
                       Mean reward: 888.07
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.0712
     Episode_Reward/lifting_object: 173.8365
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 1.97s
                      Time elapsed: 00:54:36
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 49518 steps/s (collection: 1.895s, learning 0.090s)
             Mean action noise std: 2.24
          Mean value_function loss: 157.3366
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.0919
                       Mean reward: 878.41
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: 172.1449
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 1.99s
                      Time elapsed: 00:54:38
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 49921 steps/s (collection: 1.878s, learning 0.091s)
             Mean action noise std: 2.24
          Mean value_function loss: 187.4759
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0954
                       Mean reward: 809.48
               Mean episode length: 222.04
    Episode_Reward/reaching_object: 1.0325
     Episode_Reward/lifting_object: 164.3718
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.97s
                      Time elapsed: 00:54:40
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 50795 steps/s (collection: 1.844s, learning 0.091s)
             Mean action noise std: 2.24
          Mean value_function loss: 188.6450
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.1002
                       Mean reward: 819.08
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 168.5344
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.94s
                      Time elapsed: 00:54:42
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 50596 steps/s (collection: 1.848s, learning 0.095s)
             Mean action noise std: 2.24
          Mean value_function loss: 157.6422
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.1069
                       Mean reward: 860.07
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.0789
     Episode_Reward/lifting_object: 173.5388
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.94s
                      Time elapsed: 00:54:44
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 49731 steps/s (collection: 1.864s, learning 0.113s)
             Mean action noise std: 2.24
          Mean value_function loss: 172.3738
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.1158
                       Mean reward: 863.65
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.0743
     Episode_Reward/lifting_object: 172.1142
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.98s
                      Time elapsed: 00:54:46
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 50001 steps/s (collection: 1.852s, learning 0.114s)
             Mean action noise std: 2.24
          Mean value_function loss: 148.0206
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.1235
                       Mean reward: 866.44
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.0740
     Episode_Reward/lifting_object: 171.9890
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 1.97s
                      Time elapsed: 00:54:48
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 49886 steps/s (collection: 1.860s, learning 0.111s)
             Mean action noise std: 2.24
          Mean value_function loss: 146.7292
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.1274
                       Mean reward: 891.46
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 1.1004
     Episode_Reward/lifting_object: 176.9660
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.97s
                      Time elapsed: 00:54:50
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 50343 steps/s (collection: 1.855s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 186.5985
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.1297
                       Mean reward: 848.40
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.0739
     Episode_Reward/lifting_object: 171.4064
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 1.95s
                      Time elapsed: 00:54:52
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 50152 steps/s (collection: 1.867s, learning 0.093s)
             Mean action noise std: 2.24
          Mean value_function loss: 178.7172
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.1355
                       Mean reward: 884.33
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 1.0802
     Episode_Reward/lifting_object: 172.3123
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.96s
                      Time elapsed: 00:54:54
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 50078 steps/s (collection: 1.868s, learning 0.095s)
             Mean action noise std: 2.24
          Mean value_function loss: 196.7104
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.1447
                       Mean reward: 852.54
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 1.0890
     Episode_Reward/lifting_object: 173.2334
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.96s
                      Time elapsed: 00:54:56
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 50279 steps/s (collection: 1.849s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 200.3088
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.1504
                       Mean reward: 835.18
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 1.0658
     Episode_Reward/lifting_object: 169.1711
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.96s
                      Time elapsed: 00:54:58
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 50181 steps/s (collection: 1.868s, learning 0.091s)
             Mean action noise std: 2.24
          Mean value_function loss: 171.4032
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.1545
                       Mean reward: 902.89
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 1.0855
     Episode_Reward/lifting_object: 173.2558
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 1.96s
                      Time elapsed: 00:55:00
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 49191 steps/s (collection: 1.898s, learning 0.101s)
             Mean action noise std: 2.25
          Mean value_function loss: 187.1609
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.1682
                       Mean reward: 842.20
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 1.0751
     Episode_Reward/lifting_object: 171.1823
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.00s
                      Time elapsed: 00:55:02
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 49021 steps/s (collection: 1.904s, learning 0.102s)
             Mean action noise std: 2.25
          Mean value_function loss: 205.6397
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1819
                       Mean reward: 874.42
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 1.0793
     Episode_Reward/lifting_object: 171.8892
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.01s
                      Time elapsed: 00:55:04
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 49722 steps/s (collection: 1.886s, learning 0.091s)
             Mean action noise std: 2.25
          Mean value_function loss: 161.3301
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.1900
                       Mean reward: 854.49
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 1.0661
     Episode_Reward/lifting_object: 169.8744
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.98s
                      Time elapsed: 00:55:06
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 50054 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 2.25
          Mean value_function loss: 181.8372
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.1998
                       Mean reward: 878.24
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.0849
     Episode_Reward/lifting_object: 172.9307
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 1.96s
                      Time elapsed: 00:55:08
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 50059 steps/s (collection: 1.870s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 179.3912
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.2066
                       Mean reward: 812.41
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 169.3716
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 1.96s
                      Time elapsed: 00:55:10
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 49271 steps/s (collection: 1.898s, learning 0.098s)
             Mean action noise std: 2.25
          Mean value_function loss: 171.1176
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.2131
                       Mean reward: 872.70
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.0920
     Episode_Reward/lifting_object: 174.0208
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.00s
                      Time elapsed: 00:55:12
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 49108 steps/s (collection: 1.909s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 171.7443
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.2241
                       Mean reward: 861.95
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.0856
     Episode_Reward/lifting_object: 172.7227
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.00s
                      Time elapsed: 00:55:14
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 50340 steps/s (collection: 1.853s, learning 0.099s)
             Mean action noise std: 2.26
          Mean value_function loss: 171.1452
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.2400
                       Mean reward: 875.20
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.0921
     Episode_Reward/lifting_object: 174.3337
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.95s
                      Time elapsed: 00:55:16
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 49283 steps/s (collection: 1.891s, learning 0.104s)
             Mean action noise std: 2.26
          Mean value_function loss: 175.5038
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.2491
                       Mean reward: 875.17
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: 171.7576
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 1.99s
                      Time elapsed: 00:55:18
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 49415 steps/s (collection: 1.881s, learning 0.109s)
             Mean action noise std: 2.26
          Mean value_function loss: 124.5608
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.2581
                       Mean reward: 875.84
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.0990
     Episode_Reward/lifting_object: 175.1977
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 1.99s
                      Time elapsed: 00:55:20
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 50282 steps/s (collection: 1.864s, learning 0.091s)
             Mean action noise std: 2.26
          Mean value_function loss: 160.6471
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 38.2605
                       Mean reward: 870.50
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 1.0947
     Episode_Reward/lifting_object: 175.3606
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 1.96s
                      Time elapsed: 00:55:22
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 49173 steps/s (collection: 1.899s, learning 0.100s)
             Mean action noise std: 2.26
          Mean value_function loss: 147.6518
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.2618
                       Mean reward: 858.71
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 1.0959
     Episode_Reward/lifting_object: 174.6120
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.00s
                      Time elapsed: 00:55:24
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 49466 steps/s (collection: 1.885s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 159.6922
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.2636
                       Mean reward: 864.19
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.0904
     Episode_Reward/lifting_object: 174.6883
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 1.99s
                      Time elapsed: 00:55:26
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 49188 steps/s (collection: 1.887s, learning 0.112s)
             Mean action noise std: 2.26
          Mean value_function loss: 139.3868
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 38.2646
                       Mean reward: 875.86
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.0730
     Episode_Reward/lifting_object: 172.0846
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.00s
                      Time elapsed: 00:55:28
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 50011 steps/s (collection: 1.874s, learning 0.091s)
             Mean action noise std: 2.26
          Mean value_function loss: 153.8265
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.2659
                       Mean reward: 884.02
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.0928
     Episode_Reward/lifting_object: 175.8490
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 1.97s
                      Time elapsed: 00:55:30
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 43107 steps/s (collection: 2.110s, learning 0.171s)
             Mean action noise std: 2.26
          Mean value_function loss: 158.6911
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.2707
                       Mean reward: 891.13
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 172.6242
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.28s
                      Time elapsed: 00:55:32
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 45212 steps/s (collection: 2.077s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 168.7032
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.2828
                       Mean reward: 877.08
               Mean episode length: 237.47
    Episode_Reward/reaching_object: 1.0898
     Episode_Reward/lifting_object: 174.8440
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.17s
                      Time elapsed: 00:55:34
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 48285 steps/s (collection: 1.940s, learning 0.096s)
             Mean action noise std: 2.26
          Mean value_function loss: 164.3033
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.3033
                       Mean reward: 906.01
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 1.0907
     Episode_Reward/lifting_object: 176.1211
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.04s
                      Time elapsed: 00:55:36
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 46214 steps/s (collection: 1.998s, learning 0.129s)
             Mean action noise std: 2.27
          Mean value_function loss: 124.4174
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.3203
                       Mean reward: 910.67
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 1.1029
     Episode_Reward/lifting_object: 177.6800
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.13s
                      Time elapsed: 00:55:38
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 46819 steps/s (collection: 1.924s, learning 0.175s)
             Mean action noise std: 2.27
          Mean value_function loss: 191.3672
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.3319
                       Mean reward: 854.80
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.0797
     Episode_Reward/lifting_object: 173.7700
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.10s
                      Time elapsed: 00:55:40
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 47685 steps/s (collection: 1.910s, learning 0.152s)
             Mean action noise std: 2.27
          Mean value_function loss: 211.2489
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.3479
                       Mean reward: 882.15
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.0848
     Episode_Reward/lifting_object: 174.7810
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.06s
                      Time elapsed: 00:55:42
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 49313 steps/s (collection: 1.888s, learning 0.105s)
             Mean action noise std: 2.27
          Mean value_function loss: 151.1400
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3593
                       Mean reward: 892.58
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 1.0995
     Episode_Reward/lifting_object: 177.4937
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.99s
                      Time elapsed: 00:55:44
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 48997 steps/s (collection: 1.881s, learning 0.125s)
             Mean action noise std: 2.27
          Mean value_function loss: 150.8116
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3702
                       Mean reward: 893.68
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.0928
     Episode_Reward/lifting_object: 176.4691
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.01s
                      Time elapsed: 00:55:46
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.947s, learning 0.137s)
             Mean action noise std: 2.27
          Mean value_function loss: 156.5815
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.3801
                       Mean reward: 911.44
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 1.0917
     Episode_Reward/lifting_object: 176.3191
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.08s
                      Time elapsed: 00:55:49
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 48351 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 127.4808
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.3979
                       Mean reward: 902.77
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.0972
     Episode_Reward/lifting_object: 178.2232
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.03s
                      Time elapsed: 00:55:51
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 49500 steps/s (collection: 1.890s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 123.8784
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.4125
                       Mean reward: 853.31
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.0809
     Episode_Reward/lifting_object: 174.5984
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.99s
                      Time elapsed: 00:55:53
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 49769 steps/s (collection: 1.883s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 112.9403
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 38.4226
                       Mean reward: 918.84
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 1.0804
     Episode_Reward/lifting_object: 175.1352
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 1.98s
                      Time elapsed: 00:55:55
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 48998 steps/s (collection: 1.911s, learning 0.095s)
             Mean action noise std: 2.28
          Mean value_function loss: 126.7026
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.4276
                       Mean reward: 891.97
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 1.0822
     Episode_Reward/lifting_object: 176.0038
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.01s
                      Time elapsed: 00:55:57
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 49196 steps/s (collection: 1.862s, learning 0.136s)
             Mean action noise std: 2.28
          Mean value_function loss: 120.3004
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.4290
                       Mean reward: 885.78
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 1.0749
     Episode_Reward/lifting_object: 175.1901
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.00s
                      Time elapsed: 00:55:59
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 50001 steps/s (collection: 1.869s, learning 0.098s)
             Mean action noise std: 2.28
          Mean value_function loss: 130.7112
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.4304
                       Mean reward: 849.29
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 174.2738
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.97s
                      Time elapsed: 00:56:00
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 46409 steps/s (collection: 1.979s, learning 0.139s)
             Mean action noise std: 2.28
          Mean value_function loss: 131.3498
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.4318
                       Mean reward: 851.05
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 1.0635
     Episode_Reward/lifting_object: 172.4117
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.12s
                      Time elapsed: 00:56:03
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 46236 steps/s (collection: 2.027s, learning 0.100s)
             Mean action noise std: 2.28
          Mean value_function loss: 129.0995
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4350
                       Mean reward: 904.59
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 175.0466
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.13s
                      Time elapsed: 00:56:05
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.074s, learning 0.146s)
             Mean action noise std: 2.28
          Mean value_function loss: 150.8997
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.4412
                       Mean reward: 896.87
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 1.0743
     Episode_Reward/lifting_object: 175.0271
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.22s
                      Time elapsed: 00:56:07
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 48967 steps/s (collection: 1.908s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 126.5923
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.4499
                       Mean reward: 894.41
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 174.9301
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.01s
                      Time elapsed: 00:56:09
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 50154 steps/s (collection: 1.866s, learning 0.095s)
             Mean action noise std: 2.28
          Mean value_function loss: 119.6651
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4561
                       Mean reward: 856.60
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.0794
     Episode_Reward/lifting_object: 175.7675
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 1.96s
                      Time elapsed: 00:56:11
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 48984 steps/s (collection: 1.910s, learning 0.096s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.3284
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.4639
                       Mean reward: 883.22
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 1.0955
     Episode_Reward/lifting_object: 177.5446
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.01s
                      Time elapsed: 00:56:13
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 49224 steps/s (collection: 1.891s, learning 0.106s)
             Mean action noise std: 2.29
          Mean value_function loss: 126.1036
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.4723
                       Mean reward: 876.34
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.0854
     Episode_Reward/lifting_object: 175.7505
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.00s
                      Time elapsed: 00:56:15
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 48648 steps/s (collection: 1.893s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 137.4206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.4863
                       Mean reward: 903.34
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 175.9801
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.02s
                      Time elapsed: 00:56:17
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 49204 steps/s (collection: 1.892s, learning 0.106s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.9328
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.5018
                       Mean reward: 886.75
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 1.0927
     Episode_Reward/lifting_object: 175.2926
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.00s
                      Time elapsed: 00:56:19
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 49594 steps/s (collection: 1.883s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 99.1899
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.5055
                       Mean reward: 922.65
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 1.1091
     Episode_Reward/lifting_object: 179.2663
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 1.98s
                      Time elapsed: 00:56:21
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 49266 steps/s (collection: 1.899s, learning 0.097s)
             Mean action noise std: 2.29
          Mean value_function loss: 119.5041
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.5121
                       Mean reward: 889.79
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.1048
     Episode_Reward/lifting_object: 178.5591
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.00s
                      Time elapsed: 00:56:23
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 49920 steps/s (collection: 1.861s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 134.0787
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.5359
                       Mean reward: 877.17
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.0935
     Episode_Reward/lifting_object: 176.5632
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 1.97s
                      Time elapsed: 00:56:25
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 48767 steps/s (collection: 1.917s, learning 0.099s)
             Mean action noise std: 2.30
          Mean value_function loss: 153.8637
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.5570
                       Mean reward: 857.00
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.0916
     Episode_Reward/lifting_object: 175.8319
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.02s
                      Time elapsed: 00:56:27
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 48280 steps/s (collection: 1.907s, learning 0.130s)
             Mean action noise std: 2.30
          Mean value_function loss: 153.6879
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.5744
                       Mean reward: 882.65
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 176.8505
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.04s
                      Time elapsed: 00:56:29
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 49640 steps/s (collection: 1.877s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 130.7984
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.5887
                       Mean reward: 880.31
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.0986
     Episode_Reward/lifting_object: 178.0916
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 1.98s
                      Time elapsed: 00:56:31
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 47459 steps/s (collection: 1.892s, learning 0.179s)
             Mean action noise std: 2.30
          Mean value_function loss: 118.6591
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.6085
                       Mean reward: 876.64
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.0825
     Episode_Reward/lifting_object: 175.2151
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.07s
                      Time elapsed: 00:56:33
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 47971 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 140.3840
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.6258
                       Mean reward: 870.43
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.0796
     Episode_Reward/lifting_object: 175.4131
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.05s
                      Time elapsed: 00:56:35
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 46591 steps/s (collection: 2.009s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 155.5536
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 38.6352
                       Mean reward: 876.26
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.0888
     Episode_Reward/lifting_object: 176.7124
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.11s
                      Time elapsed: 00:56:37
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 47742 steps/s (collection: 1.914s, learning 0.145s)
             Mean action noise std: 2.31
          Mean value_function loss: 153.4754
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.6424
                       Mean reward: 911.15
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 174.3321
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.06s
                      Time elapsed: 00:56:39
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 48390 steps/s (collection: 1.905s, learning 0.127s)
             Mean action noise std: 2.31
          Mean value_function loss: 167.7831
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.6595
                       Mean reward: 900.06
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 177.7594
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.03s
                      Time elapsed: 00:56:41
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 45525 steps/s (collection: 1.970s, learning 0.189s)
             Mean action noise std: 2.31
          Mean value_function loss: 148.5123
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.6779
                       Mean reward: 881.01
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 1.0734
     Episode_Reward/lifting_object: 174.1331
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.16s
                      Time elapsed: 00:56:43
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 47519 steps/s (collection: 1.936s, learning 0.133s)
             Mean action noise std: 2.31
          Mean value_function loss: 132.9732
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.6910
                       Mean reward: 880.69
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 1.0937
     Episode_Reward/lifting_object: 178.9467
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.07s
                      Time elapsed: 00:56:45
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 49681 steps/s (collection: 1.873s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 153.1397
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.6939
                       Mean reward: 863.46
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.0739
     Episode_Reward/lifting_object: 175.0779
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 1.98s
                      Time elapsed: 00:56:47
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 50411 steps/s (collection: 1.856s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 107.8654
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.6980
                       Mean reward: 920.59
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 1.1025
     Episode_Reward/lifting_object: 181.1334
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 1.95s
                      Time elapsed: 00:56:49
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 46129 steps/s (collection: 2.007s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 114.4989
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.7066
                       Mean reward: 871.89
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.0809
     Episode_Reward/lifting_object: 176.3723
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.13s
                      Time elapsed: 00:56:52
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 46996 steps/s (collection: 1.916s, learning 0.176s)
             Mean action noise std: 2.32
          Mean value_function loss: 109.2203
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.7176
                       Mean reward: 907.69
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.0836
     Episode_Reward/lifting_object: 176.9875
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.09s
                      Time elapsed: 00:56:54
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 46497 steps/s (collection: 2.000s, learning 0.114s)
             Mean action noise std: 2.32
          Mean value_function loss: 106.5665
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.7258
                       Mean reward: 910.35
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 1.0760
     Episode_Reward/lifting_object: 175.6130
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.11s
                      Time elapsed: 00:56:56
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 48183 steps/s (collection: 1.915s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 86.5647
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.7416
                       Mean reward: 909.55
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 1.0940
     Episode_Reward/lifting_object: 179.4991
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.04s
                      Time elapsed: 00:56:58
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 46621 steps/s (collection: 2.009s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 92.9132
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.7610
                       Mean reward: 894.63
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.0858
     Episode_Reward/lifting_object: 177.5423
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.11s
                      Time elapsed: 00:57:00
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 50113 steps/s (collection: 1.866s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 128.0141
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 38.7691
                       Mean reward: 864.68
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.0848
     Episode_Reward/lifting_object: 177.0655
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.96s
                      Time elapsed: 00:57:02
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 48158 steps/s (collection: 1.932s, learning 0.109s)
             Mean action noise std: 2.32
          Mean value_function loss: 96.0952
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.7745
                       Mean reward: 905.61
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 1.0879
     Episode_Reward/lifting_object: 177.3871
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.04s
                      Time elapsed: 00:57:04
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 47982 steps/s (collection: 1.949s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 124.7346
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.7919
                       Mean reward: 881.20
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 1.0662
     Episode_Reward/lifting_object: 173.4541
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.05s
                      Time elapsed: 00:57:06
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 48681 steps/s (collection: 1.897s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 134.7646
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.8106
                       Mean reward: 869.65
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.0796
     Episode_Reward/lifting_object: 175.8685
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.02s
                      Time elapsed: 00:57:08
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 48879 steps/s (collection: 1.895s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 110.5367
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.8276
                       Mean reward: 910.40
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 1.1020
     Episode_Reward/lifting_object: 179.8624
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.01s
                      Time elapsed: 00:57:10
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 49364 steps/s (collection: 1.893s, learning 0.099s)
             Mean action noise std: 2.33
          Mean value_function loss: 130.6797
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.8444
                       Mean reward: 909.49
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 1.0752
     Episode_Reward/lifting_object: 174.4196
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 1.99s
                      Time elapsed: 00:57:12
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 48399 steps/s (collection: 1.931s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 111.8879
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8541
                       Mean reward: 876.29
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 1.0985
     Episode_Reward/lifting_object: 177.9658
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.03s
                      Time elapsed: 00:57:14
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 48550 steps/s (collection: 1.928s, learning 0.097s)
             Mean action noise std: 2.33
          Mean value_function loss: 135.9223
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.8619
                       Mean reward: 860.99
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 173.2551
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.02s
                      Time elapsed: 00:57:16
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.914s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 101.3949
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.8737
                       Mean reward: 891.25
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 1.0810
     Episode_Reward/lifting_object: 174.5546
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.02s
                      Time elapsed: 00:57:18
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.919s, learning 0.144s)
             Mean action noise std: 2.33
          Mean value_function loss: 138.3811
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.8832
                       Mean reward: 877.81
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.1076
     Episode_Reward/lifting_object: 178.9761
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.06s
                      Time elapsed: 00:57:20
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 44318 steps/s (collection: 2.052s, learning 0.166s)
             Mean action noise std: 2.34
          Mean value_function loss: 138.1874
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8891
                       Mean reward: 889.28
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.0896
     Episode_Reward/lifting_object: 175.9998
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.22s
                      Time elapsed: 00:57:22
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 44857 steps/s (collection: 2.016s, learning 0.175s)
             Mean action noise std: 2.34
          Mean value_function loss: 116.1121
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9050
                       Mean reward: 886.98
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.1063
     Episode_Reward/lifting_object: 178.6048
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.19s
                      Time elapsed: 00:57:25
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 48589 steps/s (collection: 1.932s, learning 0.091s)
             Mean action noise std: 2.34
          Mean value_function loss: 164.9832
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.9221
                       Mean reward: 900.24
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.0867
     Episode_Reward/lifting_object: 175.2627
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.02s
                      Time elapsed: 00:57:27
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 46675 steps/s (collection: 1.954s, learning 0.152s)
             Mean action noise std: 2.34
          Mean value_function loss: 195.5816
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.9336
                       Mean reward: 880.32
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.0735
     Episode_Reward/lifting_object: 172.6505
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.11s
                      Time elapsed: 00:57:29
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 47647 steps/s (collection: 1.960s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 171.4367
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.9505
                       Mean reward: 877.95
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.0703
     Episode_Reward/lifting_object: 171.9685
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.06s
                      Time elapsed: 00:57:31
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 46993 steps/s (collection: 1.990s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 122.0909
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.9604
                       Mean reward: 906.93
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 1.0981
     Episode_Reward/lifting_object: 176.2818
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.09s
                      Time elapsed: 00:57:33
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 46980 steps/s (collection: 1.962s, learning 0.131s)
             Mean action noise std: 2.35
          Mean value_function loss: 151.0831
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.9779
                       Mean reward: 885.52
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.0726
     Episode_Reward/lifting_object: 172.5723
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.09s
                      Time elapsed: 00:57:35
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 45725 steps/s (collection: 2.043s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 181.3612
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.9972
                       Mean reward: 871.30
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.0815
     Episode_Reward/lifting_object: 174.5398
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.15s
                      Time elapsed: 00:57:37
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 43728 steps/s (collection: 2.071s, learning 0.177s)
             Mean action noise std: 2.35
          Mean value_function loss: 122.3541
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.0049
                       Mean reward: 844.15
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 175.2242
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.25s
                      Time elapsed: 00:57:39
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 45365 steps/s (collection: 2.001s, learning 0.166s)
             Mean action noise std: 2.35
          Mean value_function loss: 130.3332
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.0178
                       Mean reward: 901.36
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 1.0722
     Episode_Reward/lifting_object: 173.8683
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.17s
                      Time elapsed: 00:57:41
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 46174 steps/s (collection: 1.999s, learning 0.130s)
             Mean action noise std: 2.35
          Mean value_function loss: 152.3571
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 39.0287
                       Mean reward: 846.87
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 1.0824
     Episode_Reward/lifting_object: 175.8667
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.13s
                      Time elapsed: 00:57:44
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 46153 steps/s (collection: 1.998s, learning 0.132s)
             Mean action noise std: 2.35
          Mean value_function loss: 117.9138
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.0344
                       Mean reward: 900.65
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 1.0716
     Episode_Reward/lifting_object: 174.6006
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.13s
                      Time elapsed: 00:57:46
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 47838 steps/s (collection: 1.947s, learning 0.108s)
             Mean action noise std: 2.36
          Mean value_function loss: 113.4428
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.0433
                       Mean reward: 883.98
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.0825
     Episode_Reward/lifting_object: 177.5005
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.05s
                      Time elapsed: 00:57:48
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 48297 steps/s (collection: 1.917s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 115.7665
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.0547
                       Mean reward: 907.03
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 1.0748
     Episode_Reward/lifting_object: 176.9677
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.04s
                      Time elapsed: 00:57:50
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 47874 steps/s (collection: 1.935s, learning 0.118s)
             Mean action noise std: 2.36
          Mean value_function loss: 126.9641
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.0657
                       Mean reward: 879.17
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 179.2061
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.05s
                      Time elapsed: 00:57:52
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 47991 steps/s (collection: 1.921s, learning 0.127s)
             Mean action noise std: 2.36
          Mean value_function loss: 109.5131
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.0745
                       Mean reward: 901.62
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.0761
     Episode_Reward/lifting_object: 178.0226
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.05s
                      Time elapsed: 00:57:54
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.898s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 81.0390
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.0787
                       Mean reward: 910.09
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 179.9459
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 1.99s
                      Time elapsed: 00:57:56
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 47275 steps/s (collection: 1.974s, learning 0.106s)
             Mean action noise std: 2.36
          Mean value_function loss: 125.9099
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.0827
                       Mean reward: 886.96
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.0677
     Episode_Reward/lifting_object: 178.0039
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.08s
                      Time elapsed: 00:57:58
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 49777 steps/s (collection: 1.877s, learning 0.098s)
             Mean action noise std: 2.36
          Mean value_function loss: 142.4073
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0886
                       Mean reward: 865.46
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.0594
     Episode_Reward/lifting_object: 177.4735
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 1.97s
                      Time elapsed: 00:58:00
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 48498 steps/s (collection: 1.908s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 120.7047
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.0949
                       Mean reward: 911.64
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 178.3596
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.03s
                      Time elapsed: 00:58:02
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 48538 steps/s (collection: 1.931s, learning 0.094s)
             Mean action noise std: 2.36
          Mean value_function loss: 108.9258
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.1007
                       Mean reward: 898.60
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 1.0806
     Episode_Reward/lifting_object: 180.9039
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.03s
                      Time elapsed: 00:58:04
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 50019 steps/s (collection: 1.873s, learning 0.092s)
             Mean action noise std: 2.37
          Mean value_function loss: 129.1334
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.1070
                       Mean reward: 904.48
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 177.7536
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 1.97s
                      Time elapsed: 00:58:06
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 48711 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 2.37
          Mean value_function loss: 112.2903
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1169
                       Mean reward: 884.74
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 177.1810
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.02s
                      Time elapsed: 00:58:08
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 48270 steps/s (collection: 1.912s, learning 0.125s)
             Mean action noise std: 2.37
          Mean value_function loss: 140.7151
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.1318
                       Mean reward: 863.87
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: 177.6854
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.04s
                      Time elapsed: 00:58:10
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 48936 steps/s (collection: 1.904s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 123.3073
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.1408
                       Mean reward: 883.51
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.0729
     Episode_Reward/lifting_object: 177.3159
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.01s
                      Time elapsed: 00:58:12
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 50162 steps/s (collection: 1.864s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 146.0202
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.1490
                       Mean reward: 871.79
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 173.0173
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 1.96s
                      Time elapsed: 00:58:14
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 49375 steps/s (collection: 1.891s, learning 0.100s)
             Mean action noise std: 2.37
          Mean value_function loss: 160.4243
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 39.1586
                       Mean reward: 865.22
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 173.7726
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 1.99s
                      Time elapsed: 00:58:16
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 49659 steps/s (collection: 1.888s, learning 0.092s)
             Mean action noise std: 2.37
          Mean value_function loss: 149.8448
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.1638
                       Mean reward: 862.23
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 174.6500
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 1.98s
                      Time elapsed: 00:58:18
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 49319 steps/s (collection: 1.901s, learning 0.093s)
             Mean action noise std: 2.37
          Mean value_function loss: 179.7742
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1735
                       Mean reward: 861.54
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 172.6029
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 1.99s
                      Time elapsed: 00:58:20
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 48777 steps/s (collection: 1.924s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 158.9435
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.1836
                       Mean reward: 840.07
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.0526
     Episode_Reward/lifting_object: 172.7212
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.02s
                      Time elapsed: 00:58:22
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 49501 steps/s (collection: 1.872s, learning 0.114s)
             Mean action noise std: 2.38
          Mean value_function loss: 113.2254
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.1917
                       Mean reward: 891.97
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 1.0793
     Episode_Reward/lifting_object: 177.1237
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 1.99s
                      Time elapsed: 00:58:24
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 48911 steps/s (collection: 1.909s, learning 0.100s)
             Mean action noise std: 2.38
          Mean value_function loss: 118.6177
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1993
                       Mean reward: 911.31
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 1.0857
     Episode_Reward/lifting_object: 178.7588
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.01s
                      Time elapsed: 00:58:26
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 49091 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 94.6081
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.2039
                       Mean reward: 905.52
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 1.0843
     Episode_Reward/lifting_object: 178.7068
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.00s
                      Time elapsed: 00:58:28
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 47374 steps/s (collection: 1.925s, learning 0.151s)
             Mean action noise std: 2.38
          Mean value_function loss: 97.3252
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.2093
                       Mean reward: 904.43
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 1.0749
     Episode_Reward/lifting_object: 177.1567
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.08s
                      Time elapsed: 00:58:30
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 47743 steps/s (collection: 1.940s, learning 0.119s)
             Mean action noise std: 2.38
          Mean value_function loss: 121.1416
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.2144
                       Mean reward: 872.01
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.0759
     Episode_Reward/lifting_object: 177.2508
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.06s
                      Time elapsed: 00:58:32
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 49535 steps/s (collection: 1.883s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 115.6255
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.2206
                       Mean reward: 884.60
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 1.0773
     Episode_Reward/lifting_object: 177.0718
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 1.98s
                      Time elapsed: 00:58:34
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 47579 steps/s (collection: 1.950s, learning 0.116s)
             Mean action noise std: 2.38
          Mean value_function loss: 107.5292
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.2359
                       Mean reward: 853.33
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.0738
     Episode_Reward/lifting_object: 177.4969
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.07s
                      Time elapsed: 00:58:36
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 48539 steps/s (collection: 1.924s, learning 0.101s)
             Mean action noise std: 2.39
          Mean value_function loss: 123.5211
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.2531
                       Mean reward: 908.10
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 1.0721
     Episode_Reward/lifting_object: 177.9736
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.03s
                      Time elapsed: 00:58:38
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 47381 steps/s (collection: 1.977s, learning 0.097s)
             Mean action noise std: 2.39
          Mean value_function loss: 77.6412
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.2701
                       Mean reward: 908.60
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 1.0835
     Episode_Reward/lifting_object: 179.7796
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.07s
                      Time elapsed: 00:58:40
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 49825 steps/s (collection: 1.872s, learning 0.101s)
             Mean action noise std: 2.39
          Mean value_function loss: 139.7011
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.2820
                       Mean reward: 876.90
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 176.0192
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 1.97s
                      Time elapsed: 00:58:42
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 48463 steps/s (collection: 1.903s, learning 0.125s)
             Mean action noise std: 2.39
          Mean value_function loss: 111.6447
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 39.2944
                       Mean reward: 871.19
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 174.3075
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.03s
                      Time elapsed: 00:58:44
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 48956 steps/s (collection: 1.904s, learning 0.104s)
             Mean action noise std: 2.39
          Mean value_function loss: 126.2512
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.3096
                       Mean reward: 859.40
               Mean episode length: 233.06
    Episode_Reward/reaching_object: 1.0536
     Episode_Reward/lifting_object: 174.9316
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.01s
                      Time elapsed: 00:58:46
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 49688 steps/s (collection: 1.879s, learning 0.099s)
             Mean action noise std: 2.40
          Mean value_function loss: 109.1513
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 39.3303
                       Mean reward: 892.57
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 176.9541
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 1.98s
                      Time elapsed: 00:58:48
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 49906 steps/s (collection: 1.873s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 144.1147
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 39.3391
                       Mean reward: 850.83
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 1.0324
     Episode_Reward/lifting_object: 171.1750
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 1.97s
                      Time elapsed: 00:58:50
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 48952 steps/s (collection: 1.871s, learning 0.137s)
             Mean action noise std: 2.40
          Mean value_function loss: 128.9524
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 39.3406
                       Mean reward: 892.00
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 176.1733
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.01s
                      Time elapsed: 00:58:52
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 48552 steps/s (collection: 1.923s, learning 0.102s)
             Mean action noise std: 2.40
          Mean value_function loss: 108.5644
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 39.3426
                       Mean reward: 894.91
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 1.0858
     Episode_Reward/lifting_object: 179.9666
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.02s
                      Time elapsed: 00:58:54
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 49030 steps/s (collection: 1.892s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 120.4201
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.3473
                       Mean reward: 899.29
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 1.0671
     Episode_Reward/lifting_object: 176.4970
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.00s
                      Time elapsed: 00:58:56
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 48234 steps/s (collection: 1.929s, learning 0.109s)
             Mean action noise std: 2.40
          Mean value_function loss: 130.1953
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.3539
                       Mean reward: 868.87
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 172.8256
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.04s
                      Time elapsed: 00:58:58
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 48092 steps/s (collection: 1.916s, learning 0.128s)
             Mean action noise std: 2.40
          Mean value_function loss: 171.3914
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.3683
                       Mean reward: 878.04
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.0511
     Episode_Reward/lifting_object: 172.0580
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.04s
                      Time elapsed: 00:59:00
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 48487 steps/s (collection: 1.908s, learning 0.119s)
             Mean action noise std: 2.40
          Mean value_function loss: 172.6125
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3787
                       Mean reward: 868.69
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: 174.1686
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.03s
                      Time elapsed: 00:59:02
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 48365 steps/s (collection: 1.931s, learning 0.101s)
             Mean action noise std: 2.40
          Mean value_function loss: 145.3175
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3863
                       Mean reward: 864.93
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 172.0655
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.03s
                      Time elapsed: 00:59:04
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 48374 steps/s (collection: 1.936s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 169.4447
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.3966
                       Mean reward: 882.51
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 170.8522
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.03s
                      Time elapsed: 00:59:06
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 49228 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 166.6677
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.4037
                       Mean reward: 869.60
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.0586
     Episode_Reward/lifting_object: 171.4789
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.00s
                      Time elapsed: 00:59:08
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 48486 steps/s (collection: 1.918s, learning 0.110s)
             Mean action noise std: 2.41
          Mean value_function loss: 198.9259
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.4126
                       Mean reward: 852.74
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 169.6831
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.03s
                      Time elapsed: 00:59:10
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 47125 steps/s (collection: 1.971s, learning 0.115s)
             Mean action noise std: 2.41
          Mean value_function loss: 162.9093
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 39.4167
                       Mean reward: 851.63
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 1.0684
     Episode_Reward/lifting_object: 171.9147
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.09s
                      Time elapsed: 00:59:13
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 42676 steps/s (collection: 2.121s, learning 0.182s)
             Mean action noise std: 2.41
          Mean value_function loss: 138.5794
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.4233
                       Mean reward: 876.33
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 1.0760
     Episode_Reward/lifting_object: 171.7034
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.30s
                      Time elapsed: 00:59:15
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 44429 steps/s (collection: 2.041s, learning 0.172s)
             Mean action noise std: 2.41
          Mean value_function loss: 146.2567
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.4336
                       Mean reward: 875.16
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.0801
     Episode_Reward/lifting_object: 173.0402
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.21s
                      Time elapsed: 00:59:17
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.987s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 157.6242
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4452
                       Mean reward: 867.63
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 170.8412
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.08s
                      Time elapsed: 00:59:19
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 48668 steps/s (collection: 1.918s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 150.8549
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 39.4619
                       Mean reward: 886.01
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.0813
     Episode_Reward/lifting_object: 172.9526
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.02s
                      Time elapsed: 00:59:21
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 49086 steps/s (collection: 1.908s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 168.8438
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.4700
                       Mean reward: 883.71
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.0703
     Episode_Reward/lifting_object: 171.6033
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.00s
                      Time elapsed: 00:59:23
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 48898 steps/s (collection: 1.918s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 159.2243
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.4768
                       Mean reward: 885.38
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.0999
     Episode_Reward/lifting_object: 176.3475
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.01s
                      Time elapsed: 00:59:25
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 49571 steps/s (collection: 1.891s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 169.7601
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.4854
                       Mean reward: 880.28
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.1125
     Episode_Reward/lifting_object: 178.2282
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 1.98s
                      Time elapsed: 00:59:27
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 49738 steps/s (collection: 1.880s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 161.1479
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.5004
                       Mean reward: 870.56
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 1.1025
     Episode_Reward/lifting_object: 176.0693
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 1.98s
                      Time elapsed: 00:59:29
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 48669 steps/s (collection: 1.927s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 220.0336
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 39.5156
                       Mean reward: 895.31
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.0992
     Episode_Reward/lifting_object: 175.4404
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.02s
                      Time elapsed: 00:59:31
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 49546 steps/s (collection: 1.882s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 180.7455
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.5205
                       Mean reward: 856.65
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.0946
     Episode_Reward/lifting_object: 175.1275
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 1.98s
                      Time elapsed: 00:59:33
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 49231 steps/s (collection: 1.899s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 258.9382
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 39.5251
                       Mean reward: 847.23
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.0619
     Episode_Reward/lifting_object: 169.4908
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.00s
                      Time elapsed: 00:59:35
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 48494 steps/s (collection: 1.934s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 277.4031
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.5325
                       Mean reward: 849.28
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 166.5335
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.03s
                      Time elapsed: 00:59:37
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 48259 steps/s (collection: 1.923s, learning 0.114s)
             Mean action noise std: 2.42
          Mean value_function loss: 194.1446
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.5419
                       Mean reward: 865.28
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 1.0822
     Episode_Reward/lifting_object: 173.3336
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.04s
                      Time elapsed: 00:59:39
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 45739 steps/s (collection: 1.949s, learning 0.200s)
             Mean action noise std: 2.43
          Mean value_function loss: 211.4844
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 39.5470
                       Mean reward: 896.70
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 172.3804
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.15s
                      Time elapsed: 00:59:41
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 47207 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 2.43
          Mean value_function loss: 242.8337
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.5511
                       Mean reward: 869.94
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.0786
     Episode_Reward/lifting_object: 172.8557
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.08s
                      Time elapsed: 00:59:43
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 47683 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 226.7663
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.5596
                       Mean reward: 850.54
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 169.5062
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.06s
                      Time elapsed: 00:59:45
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 47993 steps/s (collection: 1.933s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 191.6521
               Mean surrogate loss: 0.0297
                 Mean entropy loss: 39.5642
                       Mean reward: 860.39
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 169.4641
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.05s
                      Time elapsed: 00:59:48
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 47876 steps/s (collection: 1.949s, learning 0.105s)
             Mean action noise std: 2.43
          Mean value_function loss: 145.4196
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 39.5647
                       Mean reward: 891.73
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 1.1056
     Episode_Reward/lifting_object: 176.8486
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.05s
                      Time elapsed: 00:59:50
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 47631 steps/s (collection: 1.949s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 155.4750
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.5656
                       Mean reward: 905.00
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 1.1000
     Episode_Reward/lifting_object: 176.8011
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.06s
                      Time elapsed: 00:59:52
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.925s, learning 0.107s)
             Mean action noise std: 2.43
          Mean value_function loss: 155.2508
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.5671
                       Mean reward: 860.29
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.0677
     Episode_Reward/lifting_object: 170.7849
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.03s
                      Time elapsed: 00:59:54
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 47349 steps/s (collection: 1.962s, learning 0.114s)
             Mean action noise std: 2.43
          Mean value_function loss: 140.4208
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.5689
                       Mean reward: 885.77
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 1.0966
     Episode_Reward/lifting_object: 175.6089
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.08s
                      Time elapsed: 00:59:56
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.050s, learning 0.109s)
             Mean action noise std: 2.43
          Mean value_function loss: 115.0172
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.5775
                       Mean reward: 861.38
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 1.0995
     Episode_Reward/lifting_object: 176.7270
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.16s
                      Time elapsed: 00:59:58
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 45044 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 148.7497
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.5980
                       Mean reward: 885.60
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 1.1000
     Episode_Reward/lifting_object: 176.8306
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.18s
                      Time elapsed: 01:00:00
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 49139 steps/s (collection: 1.901s, learning 0.099s)
             Mean action noise std: 2.44
          Mean value_function loss: 163.2605
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.6121
                       Mean reward: 879.18
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.0934
     Episode_Reward/lifting_object: 176.8281
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.00s
                      Time elapsed: 01:00:02
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 48899 steps/s (collection: 1.902s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 150.7221
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.6192
                       Mean reward: 893.21
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.1041
     Episode_Reward/lifting_object: 179.0284
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.01s
                      Time elapsed: 01:00:04
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 50106 steps/s (collection: 1.860s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 144.9421
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.6222
                       Mean reward: 893.83
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.0923
     Episode_Reward/lifting_object: 177.2676
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 1.96s
                      Time elapsed: 01:00:06
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 48582 steps/s (collection: 1.910s, learning 0.114s)
             Mean action noise std: 2.44
          Mean value_function loss: 169.8311
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.6274
                       Mean reward: 881.31
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.0679
     Episode_Reward/lifting_object: 174.0217
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.02s
                      Time elapsed: 01:00:08
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 49121 steps/s (collection: 1.894s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 119.3737
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.6364
                       Mean reward: 905.45
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 1.0922
     Episode_Reward/lifting_object: 178.2929
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.00s
                      Time elapsed: 01:00:10
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 49122 steps/s (collection: 1.896s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 103.5173
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.6450
                       Mean reward: 903.96
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 1.0784
     Episode_Reward/lifting_object: 176.9807
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.00s
                      Time elapsed: 01:00:12
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 48357 steps/s (collection: 1.924s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 137.1168
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.6552
                       Mean reward: 885.16
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.0744
     Episode_Reward/lifting_object: 175.8232
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.03s
                      Time elapsed: 01:00:14
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 48179 steps/s (collection: 1.942s, learning 0.099s)
             Mean action noise std: 2.44
          Mean value_function loss: 159.1313
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.6709
                       Mean reward: 863.04
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 1.0642
     Episode_Reward/lifting_object: 172.8438
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.04s
                      Time elapsed: 01:00:16
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 48705 steps/s (collection: 1.916s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 116.5400
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.6830
                       Mean reward: 885.65
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.0746
     Episode_Reward/lifting_object: 175.7569
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.02s
                      Time elapsed: 01:00:18
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 48528 steps/s (collection: 1.914s, learning 0.112s)
             Mean action noise std: 2.45
          Mean value_function loss: 149.1047
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.6927
                       Mean reward: 872.26
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.0780
     Episode_Reward/lifting_object: 175.1641
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.03s
                      Time elapsed: 01:00:20
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 47851 steps/s (collection: 1.950s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 161.5470
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.7074
                       Mean reward: 855.71
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.0554
     Episode_Reward/lifting_object: 170.8541
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.05s
                      Time elapsed: 01:00:22
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 49030 steps/s (collection: 1.889s, learning 0.116s)
             Mean action noise std: 2.45
          Mean value_function loss: 122.6620
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7233
                       Mean reward: 862.04
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.0986
     Episode_Reward/lifting_object: 178.2428
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.00s
                      Time elapsed: 01:00:24
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 50071 steps/s (collection: 1.869s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 169.3538
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7381
                       Mean reward: 856.18
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 173.9424
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 1.96s
                      Time elapsed: 01:00:26
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 49565 steps/s (collection: 1.881s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 149.2174
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 39.7526
                       Mean reward: 886.26
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.1024
     Episode_Reward/lifting_object: 178.1595
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 1.98s
                      Time elapsed: 01:00:28
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 49628 steps/s (collection: 1.880s, learning 0.101s)
             Mean action noise std: 2.46
          Mean value_function loss: 151.1194
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.7671
                       Mean reward: 878.27
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.0898
     Episode_Reward/lifting_object: 176.1269
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 1.98s
                      Time elapsed: 01:00:30
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 49341 steps/s (collection: 1.882s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 125.6306
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 39.7768
                       Mean reward: 896.49
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.1020
     Episode_Reward/lifting_object: 177.7823
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 1.99s
                      Time elapsed: 01:00:32
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 48478 steps/s (collection: 1.901s, learning 0.127s)
             Mean action noise std: 2.46
          Mean value_function loss: 118.3332
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.7836
                       Mean reward: 854.21
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 173.2116
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.03s
                      Time elapsed: 01:00:34
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 48512 steps/s (collection: 1.921s, learning 0.105s)
             Mean action noise std: 2.46
          Mean value_function loss: 137.1247
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.7998
                       Mean reward: 862.12
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.0795
     Episode_Reward/lifting_object: 173.5555
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.03s
                      Time elapsed: 01:00:36
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 48741 steps/s (collection: 1.920s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 116.8278
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.8053
                       Mean reward: 892.22
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.0938
     Episode_Reward/lifting_object: 175.5766
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.02s
                      Time elapsed: 01:00:38
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 47945 steps/s (collection: 1.948s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 141.3164
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.8129
                       Mean reward: 881.04
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.0774
     Episode_Reward/lifting_object: 172.5688
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.05s
                      Time elapsed: 01:00:40
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 48896 steps/s (collection: 1.919s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 146.1950
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.8273
                       Mean reward: 886.15
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.0862
     Episode_Reward/lifting_object: 174.1501
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.01s
                      Time elapsed: 01:00:42
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 49684 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 2.47
          Mean value_function loss: 160.3464
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.8351
                       Mean reward: 853.91
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 171.2383
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 1.98s
                      Time elapsed: 01:00:44
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 49099 steps/s (collection: 1.903s, learning 0.099s)
             Mean action noise std: 2.47
          Mean value_function loss: 128.2842
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.8421
                       Mean reward: 899.38
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.0982
     Episode_Reward/lifting_object: 175.6654
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.00s
                      Time elapsed: 01:00:46
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 48984 steps/s (collection: 1.906s, learning 0.101s)
             Mean action noise std: 2.47
          Mean value_function loss: 136.4126
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.8527
                       Mean reward: 907.59
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 1.1045
     Episode_Reward/lifting_object: 176.8489
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.01s
                      Time elapsed: 01:00:48
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 48560 steps/s (collection: 1.919s, learning 0.105s)
             Mean action noise std: 2.47
          Mean value_function loss: 179.9537
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.8632
                       Mean reward: 829.26
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.0721
     Episode_Reward/lifting_object: 171.2138
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.02s
                      Time elapsed: 01:00:50
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 49764 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 152.7853
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.8772
                       Mean reward: 889.77
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.0906
     Episode_Reward/lifting_object: 174.5148
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 1.98s
                      Time elapsed: 01:00:52
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 49877 steps/s (collection: 1.863s, learning 0.108s)
             Mean action noise std: 2.47
          Mean value_function loss: 142.0114
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.8870
                       Mean reward: 869.84
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.0940
     Episode_Reward/lifting_object: 174.9677
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 1.97s
                      Time elapsed: 01:00:54
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 48932 steps/s (collection: 1.908s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 158.5959
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.8927
                       Mean reward: 879.51
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.1064
     Episode_Reward/lifting_object: 177.3657
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.01s
                      Time elapsed: 01:00:56
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 48347 steps/s (collection: 1.923s, learning 0.110s)
             Mean action noise std: 2.48
          Mean value_function loss: 149.1340
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.9014
                       Mean reward: 862.25
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.0929
     Episode_Reward/lifting_object: 174.8246
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.03s
                      Time elapsed: 01:00:58
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 47624 steps/s (collection: 1.958s, learning 0.107s)
             Mean action noise std: 2.48
          Mean value_function loss: 126.6682
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.9132
                       Mean reward: 865.02
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.1027
     Episode_Reward/lifting_object: 176.8018
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.06s
                      Time elapsed: 01:01:00
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 49836 steps/s (collection: 1.876s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 134.3579
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.9231
                       Mean reward: 905.36
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 1.1084
     Episode_Reward/lifting_object: 177.7748
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 1.97s
                      Time elapsed: 01:01:02
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 48416 steps/s (collection: 1.933s, learning 0.098s)
             Mean action noise std: 2.48
          Mean value_function loss: 121.7385
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 39.9304
                       Mean reward: 885.35
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 1.0934
     Episode_Reward/lifting_object: 175.7736
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.03s
                      Time elapsed: 01:01:04
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 49158 steps/s (collection: 1.898s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 153.7384
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 39.9326
                       Mean reward: 873.14
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 1.0954
     Episode_Reward/lifting_object: 175.9328
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.00s
                      Time elapsed: 01:01:06
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 50243 steps/s (collection: 1.861s, learning 0.095s)
             Mean action noise std: 2.48
          Mean value_function loss: 156.0409
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.9359
                       Mean reward: 873.74
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.1007
     Episode_Reward/lifting_object: 176.9491
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 1.96s
                      Time elapsed: 01:01:08
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 48924 steps/s (collection: 1.912s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 183.0413
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 39.9414
                       Mean reward: 890.17
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 174.9500
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.01s
                      Time elapsed: 01:01:10
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 49345 steps/s (collection: 1.900s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 132.7930
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9470
                       Mean reward: 906.81
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 1.1053
     Episode_Reward/lifting_object: 177.9188
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 1.99s
                      Time elapsed: 01:01:12
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 49611 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 116.2052
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.9583
                       Mean reward: 884.32
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.1014
     Episode_Reward/lifting_object: 176.3502
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 1.98s
                      Time elapsed: 01:01:14
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 49667 steps/s (collection: 1.862s, learning 0.118s)
             Mean action noise std: 2.49
          Mean value_function loss: 141.4764
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.9718
                       Mean reward: 865.60
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.0949
     Episode_Reward/lifting_object: 175.5240
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 1.98s
                      Time elapsed: 01:01:16
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 48981 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 2.49
          Mean value_function loss: 146.5487
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 39.9777
                       Mean reward: 868.64
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 1.0852
     Episode_Reward/lifting_object: 173.9308
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.01s
                      Time elapsed: 01:01:18
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 47970 steps/s (collection: 1.942s, learning 0.108s)
             Mean action noise std: 2.49
          Mean value_function loss: 191.1670
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.9864
                       Mean reward: 868.67
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.0912
     Episode_Reward/lifting_object: 175.2593
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.05s
                      Time elapsed: 01:01:20
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 49141 steps/s (collection: 1.897s, learning 0.103s)
             Mean action noise std: 2.49
          Mean value_function loss: 145.7920
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 39.9982
                       Mean reward: 903.91
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 1.1046
     Episode_Reward/lifting_object: 178.0490
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.00s
                      Time elapsed: 01:01:22
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 48194 steps/s (collection: 1.932s, learning 0.108s)
             Mean action noise std: 2.49
          Mean value_function loss: 145.6559
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.0028
                       Mean reward: 929.24
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 1.0985
     Episode_Reward/lifting_object: 176.7867
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.04s
                      Time elapsed: 01:01:24
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 28083 steps/s (collection: 3.400s, learning 0.101s)
             Mean action noise std: 2.49
          Mean value_function loss: 124.3868
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.0131
                       Mean reward: 893.71
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 175.3301
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.50s
                      Time elapsed: 01:01:28
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14759 steps/s (collection: 6.522s, learning 0.139s)
             Mean action noise std: 2.49
          Mean value_function loss: 138.6841
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 40.0245
                       Mean reward: 874.02
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.0932
     Episode_Reward/lifting_object: 175.3713
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.66s
                      Time elapsed: 01:01:35
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14637 steps/s (collection: 6.595s, learning 0.121s)
             Mean action noise std: 2.50
          Mean value_function loss: 145.5050
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 40.0383
                       Mean reward: 877.80
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.0904
     Episode_Reward/lifting_object: 175.5638
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.72s
                      Time elapsed: 01:01:41
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14985 steps/s (collection: 6.450s, learning 0.111s)
             Mean action noise std: 2.50
          Mean value_function loss: 160.9868
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0480
                       Mean reward: 872.51
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 1.0986
     Episode_Reward/lifting_object: 176.9319
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.56s
                      Time elapsed: 01:01:48
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14854 steps/s (collection: 6.481s, learning 0.137s)
             Mean action noise std: 2.50
          Mean value_function loss: 133.2029
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.0593
                       Mean reward: 893.14
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 1.0769
     Episode_Reward/lifting_object: 172.6920
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.62s
                      Time elapsed: 01:01:54
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14899 steps/s (collection: 6.483s, learning 0.115s)
             Mean action noise std: 2.50
          Mean value_function loss: 161.9922
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 40.0663
                       Mean reward: 846.51
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 171.4590
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.60s
                      Time elapsed: 01:02:01
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 15073 steps/s (collection: 6.408s, learning 0.114s)
             Mean action noise std: 2.50
          Mean value_function loss: 143.6470
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.0716
                       Mean reward: 885.80
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.0751
     Episode_Reward/lifting_object: 172.4833
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.52s
                      Time elapsed: 01:02:08
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 15135 steps/s (collection: 6.360s, learning 0.135s)
             Mean action noise std: 2.50
          Mean value_function loss: 148.7198
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0746
                       Mean reward: 876.62
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.0796
     Episode_Reward/lifting_object: 173.3672
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.49s
                      Time elapsed: 01:02:14
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14832 steps/s (collection: 6.519s, learning 0.108s)
             Mean action noise std: 2.50
          Mean value_function loss: 143.2664
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.0781
                       Mean reward: 864.57
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.0890
     Episode_Reward/lifting_object: 174.9928
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.63s
                      Time elapsed: 01:02:21
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 24402 steps/s (collection: 3.938s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 174.6354
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 40.0819
                       Mean reward: 834.53
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 1.0905
     Episode_Reward/lifting_object: 175.3893
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.03s
                      Time elapsed: 01:02:25
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 52812 steps/s (collection: 1.768s, learning 0.093s)
             Mean action noise std: 2.50
          Mean value_function loss: 163.3623
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 40.0892
                       Mean reward: 898.84
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.0879
     Episode_Reward/lifting_object: 174.8968
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.86s
                      Time elapsed: 01:02:27
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 52273 steps/s (collection: 1.777s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 173.6515
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0970
                       Mean reward: 872.07
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.0801
     Episode_Reward/lifting_object: 173.1456
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.88s
                      Time elapsed: 01:02:28
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 51108 steps/s (collection: 1.833s, learning 0.091s)
             Mean action noise std: 2.51
          Mean value_function loss: 164.4214
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.1028
                       Mean reward: 888.67
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 1.0863
     Episode_Reward/lifting_object: 174.2745
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 1.92s
                      Time elapsed: 01:02:30
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 51807 steps/s (collection: 1.812s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 161.8202
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.1067
                       Mean reward: 886.48
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.1032
     Episode_Reward/lifting_object: 177.4126
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 1.90s
                      Time elapsed: 01:02:32
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 52359 steps/s (collection: 1.774s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 136.4229
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 40.1129
                       Mean reward: 889.91
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.1045
     Episode_Reward/lifting_object: 178.3152
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 1.88s
                      Time elapsed: 01:02:34
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 51487 steps/s (collection: 1.800s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 148.8266
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 40.1165
                       Mean reward: 881.22
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.0773
     Episode_Reward/lifting_object: 174.3062
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.91s
                      Time elapsed: 01:02:36
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 52670 steps/s (collection: 1.772s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 121.3854
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 40.1182
                       Mean reward: 899.54
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 1.0918
     Episode_Reward/lifting_object: 176.2788
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 1.87s
                      Time elapsed: 01:02:38
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 48223 steps/s (collection: 1.947s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 119.8691
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.1189
                       Mean reward: 888.37
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 1.0890
     Episode_Reward/lifting_object: 175.9961
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.04s
                      Time elapsed: 01:02:40
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 52108 steps/s (collection: 1.798s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 130.0133
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.1221
                       Mean reward: 883.21
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.0842
     Episode_Reward/lifting_object: 175.6049
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.89s
                      Time elapsed: 01:02:42
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 53926 steps/s (collection: 1.730s, learning 0.093s)
             Mean action noise std: 2.51
          Mean value_function loss: 134.9384
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.1278
                       Mean reward: 880.79
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.0885
     Episode_Reward/lifting_object: 177.5491
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 1.82s
                      Time elapsed: 01:02:44
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 53204 steps/s (collection: 1.754s, learning 0.093s)
             Mean action noise std: 2.51
          Mean value_function loss: 122.3292
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.1347
                       Mean reward: 897.57
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 179.1189
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.85s
                      Time elapsed: 01:02:46
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 52809 steps/s (collection: 1.768s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 132.7170
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.1393
                       Mean reward: 883.08
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.0750
     Episode_Reward/lifting_object: 176.4344
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.86s
                      Time elapsed: 01:02:47
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 53098 steps/s (collection: 1.757s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 151.3220
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 40.1435
                       Mean reward: 874.95
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.0730
     Episode_Reward/lifting_object: 175.4437
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 1.85s
                      Time elapsed: 01:02:49
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 52043 steps/s (collection: 1.787s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 140.1959
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.1470
                       Mean reward: 891.53
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.0806
     Episode_Reward/lifting_object: 177.6857
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.89s
                      Time elapsed: 01:02:51
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 52631 steps/s (collection: 1.767s, learning 0.101s)
             Mean action noise std: 2.52
          Mean value_function loss: 119.3210
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 40.1546
                       Mean reward: 876.32
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.0699
     Episode_Reward/lifting_object: 175.8671
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.87s
                      Time elapsed: 01:02:53
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 53316 steps/s (collection: 1.752s, learning 0.092s)
             Mean action noise std: 2.52
          Mean value_function loss: 116.0761
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.1614
                       Mean reward: 898.63
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 1.0836
     Episode_Reward/lifting_object: 178.5911
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.84s
                      Time elapsed: 01:02:55
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 53533 steps/s (collection: 1.747s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 103.8367
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 40.1677
                       Mean reward: 905.00
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 1.0737
     Episode_Reward/lifting_object: 175.8261
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.84s
                      Time elapsed: 01:02:57
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 52352 steps/s (collection: 1.774s, learning 0.104s)
             Mean action noise std: 2.52
          Mean value_function loss: 220.2946
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.1753
                       Mean reward: 892.31
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 1.0759
     Episode_Reward/lifting_object: 176.5352
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.88s
                      Time elapsed: 01:02:59
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 52323 steps/s (collection: 1.781s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 211.7700
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 40.1877
                       Mean reward: 880.88
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.0814
     Episode_Reward/lifting_object: 177.1811
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.88s
                      Time elapsed: 01:03:00
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 52689 steps/s (collection: 1.781s, learning 0.085s)
             Mean action noise std: 2.52
          Mean value_function loss: 154.3013
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.1975
                       Mean reward: 861.67
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 1.0682
     Episode_Reward/lifting_object: 174.7855
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.87s
                      Time elapsed: 01:03:02
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 52983 steps/s (collection: 1.762s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 118.2030
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.2028
                       Mean reward: 869.14
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.0738
     Episode_Reward/lifting_object: 174.9516
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.86s
                      Time elapsed: 01:03:04
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 52491 steps/s (collection: 1.772s, learning 0.101s)
             Mean action noise std: 2.52
          Mean value_function loss: 97.9708
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 40.2106
                       Mean reward: 914.02
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 1.0848
     Episode_Reward/lifting_object: 176.5869
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.87s
                      Time elapsed: 01:03:06
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 52200 steps/s (collection: 1.777s, learning 0.107s)
             Mean action noise std: 2.53
          Mean value_function loss: 132.4787
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.2164
                       Mean reward: 865.21
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.0918
     Episode_Reward/lifting_object: 178.1828
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.88s
                      Time elapsed: 01:03:08
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 52418 steps/s (collection: 1.764s, learning 0.111s)
             Mean action noise std: 2.53
          Mean value_function loss: 130.1147
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.2295
                       Mean reward: 919.28
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 1.0921
     Episode_Reward/lifting_object: 178.3697
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 1.88s
                      Time elapsed: 01:03:10
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 52807 steps/s (collection: 1.768s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 102.3721
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 40.2452
                       Mean reward: 894.07
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 1.0908
     Episode_Reward/lifting_object: 177.4535
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 1.86s
                      Time elapsed: 01:03:12
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 52213 steps/s (collection: 1.780s, learning 0.103s)
             Mean action noise std: 2.53
          Mean value_function loss: 178.6639
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.2538
                       Mean reward: 857.21
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.0737
     Episode_Reward/lifting_object: 174.2779
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 1.88s
                      Time elapsed: 01:03:14
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 52668 steps/s (collection: 1.777s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 118.1192
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.2619
                       Mean reward: 884.83
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.0844
     Episode_Reward/lifting_object: 175.3180
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 1.87s
                      Time elapsed: 01:03:15
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 52122 steps/s (collection: 1.785s, learning 0.101s)
             Mean action noise std: 2.53
          Mean value_function loss: 133.3512
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.2737
                       Mean reward: 849.65
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.0762
     Episode_Reward/lifting_object: 173.9405
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 1.89s
                      Time elapsed: 01:03:17
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 52345 steps/s (collection: 1.787s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 150.1437
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 40.2857
                       Mean reward: 894.91
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 1.0800
     Episode_Reward/lifting_object: 175.0106
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.88s
                      Time elapsed: 01:03:19
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 52687 steps/s (collection: 1.758s, learning 0.108s)
             Mean action noise std: 2.54
          Mean value_function loss: 106.0115
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.2957
                       Mean reward: 908.62
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 1.0952
     Episode_Reward/lifting_object: 177.9499
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.87s
                      Time elapsed: 01:03:21
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 51856 steps/s (collection: 1.806s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 105.2365
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 40.3031
                       Mean reward: 883.76
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 1.0812
     Episode_Reward/lifting_object: 175.9671
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.90s
                      Time elapsed: 01:03:23
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 52185 steps/s (collection: 1.787s, learning 0.097s)
             Mean action noise std: 2.54
          Mean value_function loss: 159.9420
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 40.3062
                       Mean reward: 866.74
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 174.4523
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.88s
                      Time elapsed: 01:03:25
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 53325 steps/s (collection: 1.751s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 148.2827
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 40.3088
                       Mean reward: 890.71
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 1.0889
     Episode_Reward/lifting_object: 176.4950
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.84s
                      Time elapsed: 01:03:27
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 52185 steps/s (collection: 1.791s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 153.4308
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.3133
                       Mean reward: 887.61
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.0814
     Episode_Reward/lifting_object: 175.5938
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.88s
                      Time elapsed: 01:03:29
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 52298 steps/s (collection: 1.786s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 138.1116
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 40.3191
                       Mean reward: 825.54
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 171.2192
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.88s
                      Time elapsed: 01:03:30
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 52925 steps/s (collection: 1.765s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 153.0222
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 40.3210
                       Mean reward: 877.38
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 176.1837
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.86s
                      Time elapsed: 01:03:32
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 51807 steps/s (collection: 1.797s, learning 0.101s)
             Mean action noise std: 2.54
          Mean value_function loss: 132.2777
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 40.3224
                       Mean reward: 896.72
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 1.0898
     Episode_Reward/lifting_object: 177.2057
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.90s
                      Time elapsed: 01:03:34
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 50847 steps/s (collection: 1.843s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 136.6090
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 40.3231
                       Mean reward: 867.38
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.0863
     Episode_Reward/lifting_object: 175.4737
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.93s
                      Time elapsed: 01:03:36
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 51363 steps/s (collection: 1.826s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 123.1917
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.3239
                       Mean reward: 909.73
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 1.1076
     Episode_Reward/lifting_object: 179.2217
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 1.91s
                      Time elapsed: 01:03:38
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 51740 steps/s (collection: 1.812s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 131.7987
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.3277
                       Mean reward: 874.97
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.0931
     Episode_Reward/lifting_object: 176.5877
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 1.90s
                      Time elapsed: 01:03:40
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 52347 steps/s (collection: 1.785s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 142.2201
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.3371
                       Mean reward: 886.04
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 1.0856
     Episode_Reward/lifting_object: 174.2770
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.88s
                      Time elapsed: 01:03:42
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 51418 steps/s (collection: 1.812s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 141.0743
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.3497
                       Mean reward: 890.40
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 1.0906
     Episode_Reward/lifting_object: 176.7433
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.91s
                      Time elapsed: 01:03:44
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 51671 steps/s (collection: 1.808s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 163.4345
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 40.3551
                       Mean reward: 906.03
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 1.0919
     Episode_Reward/lifting_object: 175.9554
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.90s
                      Time elapsed: 01:03:46
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 51044 steps/s (collection: 1.832s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 113.2451
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.3653
                       Mean reward: 877.77
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.0921
     Episode_Reward/lifting_object: 176.3297
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.93s
                      Time elapsed: 01:03:48
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 50689 steps/s (collection: 1.823s, learning 0.116s)
             Mean action noise std: 2.55
          Mean value_function loss: 138.5933
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 40.3762
                       Mean reward: 878.91
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.0835
     Episode_Reward/lifting_object: 174.9314
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 1.94s
                      Time elapsed: 01:03:49
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 51337 steps/s (collection: 1.811s, learning 0.104s)
             Mean action noise std: 2.55
          Mean value_function loss: 136.4277
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.3856
                       Mean reward: 834.87
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 1.0819
     Episode_Reward/lifting_object: 174.5233
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 1.91s
                      Time elapsed: 01:03:51
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 51107 steps/s (collection: 1.839s, learning 0.085s)
             Mean action noise std: 2.55
          Mean value_function loss: 129.5864
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.3955
                       Mean reward: 901.16
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.0892
     Episode_Reward/lifting_object: 176.1138
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 1.92s
                      Time elapsed: 01:03:53
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 52886 steps/s (collection: 1.774s, learning 0.085s)
             Mean action noise std: 2.55
          Mean value_function loss: 132.9560
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.4058
                       Mean reward: 864.76
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 169.6801
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 1.86s
                      Time elapsed: 01:03:55
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 51029 steps/s (collection: 1.835s, learning 0.091s)
             Mean action noise std: 2.56
          Mean value_function loss: 105.9543
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.4138
                       Mean reward: 899.54
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.1064
     Episode_Reward/lifting_object: 178.2836
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 1.93s
                      Time elapsed: 01:03:57
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 51068 steps/s (collection: 1.833s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 120.3471
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 40.4217
                       Mean reward: 898.98
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 1.0969
     Episode_Reward/lifting_object: 177.3375
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 1.92s
                      Time elapsed: 01:03:59
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 51308 steps/s (collection: 1.819s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 126.1369
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.4305
                       Mean reward: 898.35
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 1.0971
     Episode_Reward/lifting_object: 176.5691
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 1.92s
                      Time elapsed: 01:04:01
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 48469 steps/s (collection: 1.936s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 154.8547
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.4393
                       Mean reward: 868.15
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.0907
     Episode_Reward/lifting_object: 174.8021
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.03s
                      Time elapsed: 01:04:03
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 48546 steps/s (collection: 1.898s, learning 0.127s)
             Mean action noise std: 2.56
          Mean value_function loss: 145.7026
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.4535
                       Mean reward: 851.39
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.0928
     Episode_Reward/lifting_object: 175.4002
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.02s
                      Time elapsed: 01:04:05
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 50873 steps/s (collection: 1.811s, learning 0.121s)
             Mean action noise std: 2.56
          Mean value_function loss: 130.4915
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 40.4711
                       Mean reward: 892.32
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 174.6944
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 1.93s
                      Time elapsed: 01:04:07
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 49712 steps/s (collection: 1.862s, learning 0.115s)
             Mean action noise std: 2.57
          Mean value_function loss: 148.3432
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.4859
                       Mean reward: 857.19
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.0857
     Episode_Reward/lifting_object: 173.7795
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.98s
                      Time elapsed: 01:04:09
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 49578 steps/s (collection: 1.858s, learning 0.125s)
             Mean action noise std: 2.57
          Mean value_function loss: 168.8567
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.5065
                       Mean reward: 868.03
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 174.8797
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.98s
                      Time elapsed: 01:04:11
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 50772 steps/s (collection: 1.828s, learning 0.109s)
             Mean action noise std: 2.57
          Mean value_function loss: 173.3120
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 40.5203
                       Mean reward: 879.07
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.0875
     Episode_Reward/lifting_object: 174.4411
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 1.94s
                      Time elapsed: 01:04:13
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 51226 steps/s (collection: 1.816s, learning 0.103s)
             Mean action noise std: 2.57
          Mean value_function loss: 146.2149
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.5246
                       Mean reward: 864.97
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.0945
     Episode_Reward/lifting_object: 174.9801
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 1.92s
                      Time elapsed: 01:04:15
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 49748 steps/s (collection: 1.853s, learning 0.123s)
             Mean action noise std: 2.57
          Mean value_function loss: 118.4817
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.5325
                       Mean reward: 875.28
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.0925
     Episode_Reward/lifting_object: 175.1648
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 1.98s
                      Time elapsed: 01:04:17
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 51410 steps/s (collection: 1.813s, learning 0.099s)
             Mean action noise std: 2.57
          Mean value_function loss: 173.8606
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 40.5406
                       Mean reward: 873.30
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 171.4150
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.91s
                      Time elapsed: 01:04:19
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 51835 steps/s (collection: 1.810s, learning 0.086s)
             Mean action noise std: 2.57
          Mean value_function loss: 168.0304
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.5457
                       Mean reward: 863.59
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.0926
     Episode_Reward/lifting_object: 175.8498
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.90s
                      Time elapsed: 01:04:21
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 51893 steps/s (collection: 1.804s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 148.4620
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 40.5521
                       Mean reward: 875.19
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 1.0842
     Episode_Reward/lifting_object: 173.9419
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 1.89s
                      Time elapsed: 01:04:22
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 50959 steps/s (collection: 1.821s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 165.1794
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 40.5558
                       Mean reward: 847.48
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: 171.5871
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 1.93s
                      Time elapsed: 01:04:24
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 51161 steps/s (collection: 1.806s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 161.2987
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.5618
                       Mean reward: 889.85
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.0924
     Episode_Reward/lifting_object: 176.1490
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.92s
                      Time elapsed: 01:04:26
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 51241 steps/s (collection: 1.824s, learning 0.094s)
             Mean action noise std: 2.58
          Mean value_function loss: 120.4303
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.5711
                       Mean reward: 895.87
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 1.0970
     Episode_Reward/lifting_object: 176.0053
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 1.92s
                      Time elapsed: 01:04:28
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 50934 steps/s (collection: 1.826s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 157.4740
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.5818
                       Mean reward: 895.69
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 174.4640
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 1.93s
                      Time elapsed: 01:04:30
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 52024 steps/s (collection: 1.799s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 133.0575
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.5937
                       Mean reward: 913.01
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 1.1036
     Episode_Reward/lifting_object: 177.8666
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 1.89s
                      Time elapsed: 01:04:32
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 51939 steps/s (collection: 1.795s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 129.8866
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.6039
                       Mean reward: 882.62
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.0941
     Episode_Reward/lifting_object: 176.8811
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.89s
                      Time elapsed: 01:04:34
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 50373 steps/s (collection: 1.804s, learning 0.148s)
             Mean action noise std: 2.58
          Mean value_function loss: 188.4657
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.6088
                       Mean reward: 839.34
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 169.1087
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.95s
                      Time elapsed: 01:04:36
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 48393 steps/s (collection: 1.872s, learning 0.159s)
             Mean action noise std: 2.58
          Mean value_function loss: 139.5876
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.6145
                       Mean reward: 846.85
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.0719
     Episode_Reward/lifting_object: 173.1513
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.03s
                      Time elapsed: 01:04:38
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 50892 steps/s (collection: 1.827s, learning 0.105s)
             Mean action noise std: 2.58
          Mean value_function loss: 175.7380
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.6188
                       Mean reward: 850.46
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 170.1403
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 1.93s
                      Time elapsed: 01:04:40
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 51976 steps/s (collection: 1.788s, learning 0.103s)
             Mean action noise std: 2.58
          Mean value_function loss: 162.9627
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.6234
                       Mean reward: 878.57
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.0841
     Episode_Reward/lifting_object: 175.0395
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 1.89s
                      Time elapsed: 01:04:42
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 52329 steps/s (collection: 1.789s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 156.7285
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.6320
                       Mean reward: 833.33
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.0826
     Episode_Reward/lifting_object: 174.3016
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 1.88s
                      Time elapsed: 01:04:44
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 51509 steps/s (collection: 1.814s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 148.7703
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 40.6418
                       Mean reward: 880.23
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.0744
     Episode_Reward/lifting_object: 172.7494
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 1.91s
                      Time elapsed: 01:04:46
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 51631 steps/s (collection: 1.795s, learning 0.109s)
             Mean action noise std: 2.59
          Mean value_function loss: 109.1351
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.6503
                       Mean reward: 891.06
               Mean episode length: 239.56
    Episode_Reward/reaching_object: 1.0974
     Episode_Reward/lifting_object: 176.9994
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 1.90s
                      Time elapsed: 01:04:47
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 49298 steps/s (collection: 1.889s, learning 0.105s)
             Mean action noise std: 2.59
          Mean value_function loss: 158.3257
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 40.6580
                       Mean reward: 894.74
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 1.0828
     Episode_Reward/lifting_object: 173.7573
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 1.99s
                      Time elapsed: 01:04:49
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 50955 steps/s (collection: 1.819s, learning 0.110s)
             Mean action noise std: 2.59
          Mean value_function loss: 162.1798
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 40.6598
                       Mean reward: 865.47
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.0830
     Episode_Reward/lifting_object: 174.3652
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.93s
                      Time elapsed: 01:04:51
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 51261 steps/s (collection: 1.829s, learning 0.089s)
             Mean action noise std: 2.59
          Mean value_function loss: 115.5753
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 40.6605
                       Mean reward: 884.96
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 1.0951
     Episode_Reward/lifting_object: 176.4283
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.92s
                      Time elapsed: 01:04:53
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 52198 steps/s (collection: 1.796s, learning 0.087s)
             Mean action noise std: 2.59
          Mean value_function loss: 123.0754
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 40.6614
                       Mean reward: 864.67
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.0876
     Episode_Reward/lifting_object: 174.2707
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.88s
                      Time elapsed: 01:04:55
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 50912 steps/s (collection: 1.836s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 136.9072
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 40.6621
                       Mean reward: 873.19
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.0957
     Episode_Reward/lifting_object: 176.4353
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.93s
                      Time elapsed: 01:04:57
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 50985 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 151.9338
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 40.6630
                       Mean reward: 899.97
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 1.0937
     Episode_Reward/lifting_object: 175.8734
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 1.93s
                      Time elapsed: 01:04:59
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 51811 steps/s (collection: 1.803s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 118.6305
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 40.6636
                       Mean reward: 910.62
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 1.0982
     Episode_Reward/lifting_object: 175.6837
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 1.90s
                      Time elapsed: 01:05:01
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 50321 steps/s (collection: 1.847s, learning 0.107s)
             Mean action noise std: 2.59
          Mean value_function loss: 140.4784
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 40.6645
                       Mean reward: 930.49
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 1.1116
     Episode_Reward/lifting_object: 179.0016
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 1.95s
                      Time elapsed: 01:05:03
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 51398 steps/s (collection: 1.828s, learning 0.085s)
             Mean action noise std: 2.59
          Mean value_function loss: 207.2969
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 40.6656
                       Mean reward: 912.96
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 1.1061
     Episode_Reward/lifting_object: 178.2558
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 1.91s
                      Time elapsed: 01:05:05
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 51042 steps/s (collection: 1.822s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 190.8504
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6700
                       Mean reward: 892.01
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 1.1117
     Episode_Reward/lifting_object: 178.6505
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 1.93s
                      Time elapsed: 01:05:07
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 51071 steps/s (collection: 1.827s, learning 0.098s)
             Mean action noise std: 2.59
          Mean value_function loss: 198.4329
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.6838
                       Mean reward: 864.41
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 173.5049
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 1.92s
                      Time elapsed: 01:05:09
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 50319 steps/s (collection: 1.863s, learning 0.091s)
             Mean action noise std: 2.60
          Mean value_function loss: 161.8701
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.6935
                       Mean reward: 915.30
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.0912
     Episode_Reward/lifting_object: 173.8873
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 1.95s
                      Time elapsed: 01:05:11
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 51488 steps/s (collection: 1.810s, learning 0.099s)
             Mean action noise std: 2.60
          Mean value_function loss: 154.2921
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.7033
                       Mean reward: 920.50
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.1214
     Episode_Reward/lifting_object: 180.0038
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 1.91s
                      Time elapsed: 01:05:12
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 51292 steps/s (collection: 1.830s, learning 0.086s)
             Mean action noise std: 2.60
          Mean value_function loss: 149.5830
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.7163
                       Mean reward: 872.13
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.1045
     Episode_Reward/lifting_object: 176.8737
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 1.92s
                      Time elapsed: 01:05:14
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 48140 steps/s (collection: 1.903s, learning 0.139s)
             Mean action noise std: 2.60
          Mean value_function loss: 183.8730
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.7277
                       Mean reward: 892.83
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.0889
     Episode_Reward/lifting_object: 174.4229
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.04s
                      Time elapsed: 01:05:16
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 50206 steps/s (collection: 1.833s, learning 0.125s)
             Mean action noise std: 2.60
          Mean value_function loss: 146.7841
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.7341
                       Mean reward: 899.02
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 1.0935
     Episode_Reward/lifting_object: 175.4884
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.96s
                      Time elapsed: 01:05:18
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.889s, learning 0.102s)
             Mean action noise std: 2.60
          Mean value_function loss: 145.3936
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.7425
                       Mean reward: 888.64
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.1013
     Episode_Reward/lifting_object: 177.2675
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 1.99s
                      Time elapsed: 01:05:20
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 48490 steps/s (collection: 1.917s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 153.4228
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.7541
                       Mean reward: 910.94
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 1.1046
     Episode_Reward/lifting_object: 177.1621
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.03s
                      Time elapsed: 01:05:22
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 50901 steps/s (collection: 1.827s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 121.6858
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 40.7679
                       Mean reward: 888.44
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.0970
     Episode_Reward/lifting_object: 176.0725
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 1.93s
                      Time elapsed: 01:05:24
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 49509 steps/s (collection: 1.879s, learning 0.106s)
             Mean action noise std: 2.61
          Mean value_function loss: 128.5933
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.7731
                       Mean reward: 872.00
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.0743
     Episode_Reward/lifting_object: 172.9562
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.99s
                      Time elapsed: 01:05:26
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 50871 steps/s (collection: 1.834s, learning 0.098s)
             Mean action noise std: 2.61
          Mean value_function loss: 172.6971
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.7770
                       Mean reward: 865.79
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 170.8129
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.93s
                      Time elapsed: 01:05:28
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 50788 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 135.1656
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.7814
                       Mean reward: 876.52
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 1.0906
     Episode_Reward/lifting_object: 176.4454
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.94s
                      Time elapsed: 01:05:30
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 50620 steps/s (collection: 1.824s, learning 0.118s)
             Mean action noise std: 2.61
          Mean value_function loss: 163.2940
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.7871
                       Mean reward: 863.34
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.0766
     Episode_Reward/lifting_object: 173.8884
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.94s
                      Time elapsed: 01:05:32
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 50890 steps/s (collection: 1.842s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 127.4088
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.7989
                       Mean reward: 900.43
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.1064
     Episode_Reward/lifting_object: 178.9894
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 1.93s
                      Time elapsed: 01:05:34
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 49991 steps/s (collection: 1.823s, learning 0.143s)
             Mean action noise std: 2.61
          Mean value_function loss: 126.1236
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.8083
                       Mean reward: 904.38
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 1.0941
     Episode_Reward/lifting_object: 176.7770
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 1.97s
                      Time elapsed: 01:05:36
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 46945 steps/s (collection: 1.926s, learning 0.168s)
             Mean action noise std: 2.62
          Mean value_function loss: 153.3410
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.8276
                       Mean reward: 888.48
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.1162
     Episode_Reward/lifting_object: 180.7080
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.09s
                      Time elapsed: 01:05:38
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 49354 steps/s (collection: 1.866s, learning 0.126s)
             Mean action noise std: 2.62
          Mean value_function loss: 179.9141
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.8450
                       Mean reward: 871.86
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 175.0995
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 1.99s
                      Time elapsed: 01:05:40
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 48373 steps/s (collection: 1.886s, learning 0.147s)
             Mean action noise std: 2.62
          Mean value_function loss: 172.7009
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.8533
                       Mean reward: 891.17
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.0909
     Episode_Reward/lifting_object: 176.2901
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.03s
                      Time elapsed: 01:05:42
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 49643 steps/s (collection: 1.856s, learning 0.124s)
             Mean action noise std: 2.62
          Mean value_function loss: 138.7740
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.8606
                       Mean reward: 866.05
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.0904
     Episode_Reward/lifting_object: 174.7520
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 1.98s
                      Time elapsed: 01:05:44
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 50024 steps/s (collection: 1.845s, learning 0.120s)
             Mean action noise std: 2.62
          Mean value_function loss: 125.1891
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.8670
                       Mean reward: 867.21
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.1001
     Episode_Reward/lifting_object: 175.8456
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 1.97s
                      Time elapsed: 01:05:46
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 49731 steps/s (collection: 1.835s, learning 0.141s)
             Mean action noise std: 2.62
          Mean value_function loss: 137.0170
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.8775
                       Mean reward: 876.53
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.1053
     Episode_Reward/lifting_object: 176.9104
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 1.98s
                      Time elapsed: 01:05:48
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 49528 steps/s (collection: 1.882s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 137.6601
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.8890
                       Mean reward: 894.15
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 1.1145
     Episode_Reward/lifting_object: 178.7253
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.98s
                      Time elapsed: 01:05:50
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 50235 steps/s (collection: 1.867s, learning 0.090s)
             Mean action noise std: 2.63
          Mean value_function loss: 132.2237
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 40.9007
                       Mean reward: 919.49
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 1.1232
     Episode_Reward/lifting_object: 179.8740
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 1.96s
                      Time elapsed: 01:05:52
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 50107 steps/s (collection: 1.850s, learning 0.112s)
             Mean action noise std: 2.63
          Mean value_function loss: 160.4498
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.9134
                       Mean reward: 905.82
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 1.1177
     Episode_Reward/lifting_object: 178.6127
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 1.96s
                      Time elapsed: 01:05:54
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 49458 steps/s (collection: 1.903s, learning 0.085s)
             Mean action noise std: 2.63
          Mean value_function loss: 138.3355
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 40.9274
                       Mean reward: 894.16
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.1130
     Episode_Reward/lifting_object: 178.1715
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 1.99s
                      Time elapsed: 01:05:56
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.917s, learning 0.165s)
             Mean action noise std: 2.63
          Mean value_function loss: 197.8870
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.9400
                       Mean reward: 895.25
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.1005
     Episode_Reward/lifting_object: 175.2247
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.08s
                      Time elapsed: 01:05:58
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 50502 steps/s (collection: 1.845s, learning 0.101s)
             Mean action noise std: 2.63
          Mean value_function loss: 151.2375
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.9546
                       Mean reward: 888.64
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.1025
     Episode_Reward/lifting_object: 175.1363
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 1.95s
                      Time elapsed: 01:06:00
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 45545 steps/s (collection: 1.927s, learning 0.231s)
             Mean action noise std: 2.64
          Mean value_function loss: 109.4049
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 40.9663
                       Mean reward: 898.39
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 1.1218
     Episode_Reward/lifting_object: 178.8896
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.16s
                      Time elapsed: 01:06:02
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 48596 steps/s (collection: 1.876s, learning 0.146s)
             Mean action noise std: 2.64
          Mean value_function loss: 192.9285
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.9814
                       Mean reward: 887.15
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.1223
     Episode_Reward/lifting_object: 179.2476
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.02s
                      Time elapsed: 01:06:04
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 49923 steps/s (collection: 1.825s, learning 0.144s)
             Mean action noise std: 2.64
          Mean value_function loss: 122.0866
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 40.9916
                       Mean reward: 884.60
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.1108
     Episode_Reward/lifting_object: 177.4596
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 1.97s
                      Time elapsed: 01:06:06
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 50505 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 124.2633
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.9957
                       Mean reward: 893.12
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.1038
     Episode_Reward/lifting_object: 176.1811
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 1.95s
                      Time elapsed: 01:06:08
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 50324 steps/s (collection: 1.862s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 93.1834
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.0059
                       Mean reward: 917.31
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 1.1271
     Episode_Reward/lifting_object: 180.9909
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.95s
                      Time elapsed: 01:06:10
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 49837 steps/s (collection: 1.875s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 179.9394
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.0179
                       Mean reward: 867.88
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.1021
     Episode_Reward/lifting_object: 175.9572
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.97s
                      Time elapsed: 01:06:12
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 50533 steps/s (collection: 1.846s, learning 0.100s)
             Mean action noise std: 2.65
          Mean value_function loss: 128.6123
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.0335
                       Mean reward: 896.00
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 1.1018
     Episode_Reward/lifting_object: 176.0181
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 1.95s
                      Time elapsed: 01:06:14
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 50295 steps/s (collection: 1.860s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 153.3548
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.0485
                       Mean reward: 851.31
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.0934
     Episode_Reward/lifting_object: 174.7072
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 1.95s
                      Time elapsed: 01:06:16
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 50224 steps/s (collection: 1.851s, learning 0.106s)
             Mean action noise std: 2.65
          Mean value_function loss: 143.0768
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.0604
                       Mean reward: 913.78
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 1.0998
     Episode_Reward/lifting_object: 175.6659
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 1.96s
                      Time elapsed: 01:06:18
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 50060 steps/s (collection: 1.877s, learning 0.087s)
             Mean action noise std: 2.65
          Mean value_function loss: 148.6140
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.0682
                       Mean reward: 892.70
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.1007
     Episode_Reward/lifting_object: 175.3133
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 1.96s
                      Time elapsed: 01:06:20
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 49969 steps/s (collection: 1.854s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 150.4774
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.0769
                       Mean reward: 887.64
               Mean episode length: 237.80
    Episode_Reward/reaching_object: 1.1211
     Episode_Reward/lifting_object: 178.7415
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 1.97s
                      Time elapsed: 01:06:22
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 50035 steps/s (collection: 1.855s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 114.2967
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 41.0904
                       Mean reward: 880.84
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.1082
     Episode_Reward/lifting_object: 176.5216
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 1.96s
                      Time elapsed: 01:06:24
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 50683 steps/s (collection: 1.854s, learning 0.086s)
             Mean action noise std: 2.66
          Mean value_function loss: 121.9841
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.1050
                       Mean reward: 893.67
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.1175
     Episode_Reward/lifting_object: 178.1515
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.94s
                      Time elapsed: 01:06:26
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.888s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 106.3493
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 41.1175
                       Mean reward: 916.52
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 1.1016
     Episode_Reward/lifting_object: 175.1736
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 1.99s
                      Time elapsed: 01:06:28
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 50601 steps/s (collection: 1.840s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 110.5361
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.1199
                       Mean reward: 891.44
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 1.1035
     Episode_Reward/lifting_object: 175.6359
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 1.94s
                      Time elapsed: 01:06:30
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 50770 steps/s (collection: 1.838s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 161.0623
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.1232
                       Mean reward: 873.58
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 1.1092
     Episode_Reward/lifting_object: 175.9760
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 1.94s
                      Time elapsed: 01:06:32
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 47701 steps/s (collection: 1.974s, learning 0.087s)
             Mean action noise std: 2.66
          Mean value_function loss: 122.7995
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.1271
                       Mean reward: 849.87
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.1038
     Episode_Reward/lifting_object: 175.1462
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.06s
                      Time elapsed: 01:06:34
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 50978 steps/s (collection: 1.832s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 133.2420
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.1318
                       Mean reward: 913.39
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 1.1118
     Episode_Reward/lifting_object: 176.5645
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.93s
                      Time elapsed: 01:06:36
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 49481 steps/s (collection: 1.901s, learning 0.086s)
             Mean action noise std: 2.66
          Mean value_function loss: 122.6778
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1395
                       Mean reward: 873.15
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.1150
     Episode_Reward/lifting_object: 176.4768
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 1.99s
                      Time elapsed: 01:06:38
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 49965 steps/s (collection: 1.816s, learning 0.152s)
             Mean action noise std: 2.67
          Mean value_function loss: 89.6805
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.1447
                       Mean reward: 883.73
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 1.1184
     Episode_Reward/lifting_object: 177.5268
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 1.97s
                      Time elapsed: 01:06:40
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 50661 steps/s (collection: 1.851s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 101.2356
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.1500
                       Mean reward: 903.94
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.1142
     Episode_Reward/lifting_object: 177.7276
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 1.94s
                      Time elapsed: 01:06:41
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 47465 steps/s (collection: 1.937s, learning 0.135s)
             Mean action noise std: 2.67
          Mean value_function loss: 141.1983
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1585
                       Mean reward: 896.58
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.1107
     Episode_Reward/lifting_object: 177.3244
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.07s
                      Time elapsed: 01:06:44
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 48159 steps/s (collection: 1.913s, learning 0.129s)
             Mean action noise std: 2.67
          Mean value_function loss: 157.9684
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.1725
                       Mean reward: 893.47
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.0926
     Episode_Reward/lifting_object: 173.4089
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.04s
                      Time elapsed: 01:06:46
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 47139 steps/s (collection: 1.901s, learning 0.184s)
             Mean action noise std: 2.67
          Mean value_function loss: 140.3569
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.1863
                       Mean reward: 899.44
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 1.0925
     Episode_Reward/lifting_object: 173.7482
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.09s
                      Time elapsed: 01:06:48
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 49239 steps/s (collection: 1.886s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 147.1046
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 41.1924
                       Mean reward: 865.55
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.1030
     Episode_Reward/lifting_object: 176.0195
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.00s
                      Time elapsed: 01:06:50
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 49802 steps/s (collection: 1.843s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 170.3641
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.1962
                       Mean reward: 852.19
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.0742
     Episode_Reward/lifting_object: 170.7571
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 1.97s
                      Time elapsed: 01:06:52
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 48036 steps/s (collection: 1.956s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 118.9285
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.2013
                       Mean reward: 895.16
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 1.1081
     Episode_Reward/lifting_object: 176.2402
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.05s
                      Time elapsed: 01:06:54
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 50023 steps/s (collection: 1.844s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 92.1754
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 41.2086
                       Mean reward: 893.67
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 1.1173
     Episode_Reward/lifting_object: 177.8121
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 1.97s
                      Time elapsed: 01:06:56
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 50110 steps/s (collection: 1.825s, learning 0.137s)
             Mean action noise std: 2.68
          Mean value_function loss: 146.7492
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2128
                       Mean reward: 891.25
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 1.1073
     Episode_Reward/lifting_object: 175.6321
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 1.96s
                      Time elapsed: 01:06:58
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 50114 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 178.6701
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.2194
                       Mean reward: 884.93
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 1.1079
     Episode_Reward/lifting_object: 176.0396
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 1.96s
                      Time elapsed: 01:07:00
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 50011 steps/s (collection: 1.839s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 136.9770
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.2273
                       Mean reward: 893.13
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 1.1047
     Episode_Reward/lifting_object: 174.8888
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.97s
                      Time elapsed: 01:07:02
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 47420 steps/s (collection: 1.916s, learning 0.157s)
             Mean action noise std: 2.68
          Mean value_function loss: 154.8743
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2335
                       Mean reward: 860.59
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.1162
     Episode_Reward/lifting_object: 176.7313
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.07s
                      Time elapsed: 01:07:04
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 51445 steps/s (collection: 1.809s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 127.1111
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.2458
                       Mean reward: 890.90
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.1165
     Episode_Reward/lifting_object: 177.1407
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 1.91s
                      Time elapsed: 01:07:06
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 49403 steps/s (collection: 1.890s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 130.0308
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.2592
                       Mean reward: 904.18
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 1.1138
     Episode_Reward/lifting_object: 176.9577
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 1.99s
                      Time elapsed: 01:07:07
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 50339 steps/s (collection: 1.847s, learning 0.106s)
             Mean action noise std: 2.69
          Mean value_function loss: 147.1427
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 41.2716
                       Mean reward: 885.66
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.0849
     Episode_Reward/lifting_object: 171.7289
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.95s
                      Time elapsed: 01:07:09
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 48699 steps/s (collection: 1.873s, learning 0.146s)
             Mean action noise std: 2.69
          Mean value_function loss: 107.8249
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.2788
                       Mean reward: 874.14
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.1171
     Episode_Reward/lifting_object: 177.6592
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.02s
                      Time elapsed: 01:07:11
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 47073 steps/s (collection: 1.951s, learning 0.138s)
             Mean action noise std: 2.69
          Mean value_function loss: 153.9154
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 41.2826
                       Mean reward: 914.37
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 1.1238
     Episode_Reward/lifting_object: 178.9057
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.09s
                      Time elapsed: 01:07:14
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 48924 steps/s (collection: 1.894s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 125.8003
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 41.2840
                       Mean reward: 882.42
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.1012
     Episode_Reward/lifting_object: 175.6186
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.01s
                      Time elapsed: 01:07:16
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 49915 steps/s (collection: 1.851s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 162.9875
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 41.2854
                       Mean reward: 854.89
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.0791
     Episode_Reward/lifting_object: 171.0000
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 1.97s
                      Time elapsed: 01:07:18
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 49645 steps/s (collection: 1.856s, learning 0.124s)
             Mean action noise std: 2.69
          Mean value_function loss: 118.5975
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.2872
                       Mean reward: 886.56
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.1171
     Episode_Reward/lifting_object: 176.7160
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 1.98s
                      Time elapsed: 01:07:20
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 46633 steps/s (collection: 1.950s, learning 0.159s)
             Mean action noise std: 2.69
          Mean value_function loss: 178.2040
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.2939
                       Mean reward: 884.51
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 1.1201
     Episode_Reward/lifting_object: 176.4923
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.11s
                      Time elapsed: 01:07:22
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 50774 steps/s (collection: 1.842s, learning 0.094s)
             Mean action noise std: 2.69
          Mean value_function loss: 122.8178
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.3016
                       Mean reward: 852.05
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.1133
     Episode_Reward/lifting_object: 176.1891
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 1.94s
                      Time elapsed: 01:07:24
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 49440 steps/s (collection: 1.836s, learning 0.153s)
             Mean action noise std: 2.69
          Mean value_function loss: 108.6270
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 41.3131
                       Mean reward: 892.62
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 1.1205
     Episode_Reward/lifting_object: 177.7901
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 1.99s
                      Time elapsed: 01:07:26
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 47130 steps/s (collection: 1.933s, learning 0.153s)
             Mean action noise std: 2.70
          Mean value_function loss: 81.0449
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.3286
                       Mean reward: 915.54
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 1.1271
     Episode_Reward/lifting_object: 179.1067
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.09s
                      Time elapsed: 01:07:28
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 50970 steps/s (collection: 1.823s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 122.0497
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 41.3344
                       Mean reward: 897.86
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 1.1151
     Episode_Reward/lifting_object: 176.9989
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 1.93s
                      Time elapsed: 01:07:30
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 47272 steps/s (collection: 1.908s, learning 0.172s)
             Mean action noise std: 2.70
          Mean value_function loss: 109.8892
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.3393
                       Mean reward: 913.71
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 1.1149
     Episode_Reward/lifting_object: 176.2516
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.08s
                      Time elapsed: 01:07:32
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 48963 steps/s (collection: 1.885s, learning 0.123s)
             Mean action noise std: 2.70
          Mean value_function loss: 133.7846
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.3507
                       Mean reward: 884.03
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.1106
     Episode_Reward/lifting_object: 175.9194
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.01s
                      Time elapsed: 01:07:34
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 50194 steps/s (collection: 1.865s, learning 0.094s)
             Mean action noise std: 2.70
          Mean value_function loss: 80.4634
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.3555
                       Mean reward: 896.44
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 1.1306
     Episode_Reward/lifting_object: 179.1822
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.96s
                      Time elapsed: 01:07:36
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 46702 steps/s (collection: 1.993s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 116.6413
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.3584
                       Mean reward: 878.42
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.1101
     Episode_Reward/lifting_object: 175.7994
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.10s
                      Time elapsed: 01:07:38
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 49990 steps/s (collection: 1.851s, learning 0.116s)
             Mean action noise std: 2.70
          Mean value_function loss: 79.0246
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.3602
                       Mean reward: 900.03
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 1.1332
     Episode_Reward/lifting_object: 180.1492
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.97s
                      Time elapsed: 01:07:40
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 49913 steps/s (collection: 1.867s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 76.3208
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.3591
                       Mean reward: 906.26
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 1.1419
     Episode_Reward/lifting_object: 181.5199
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 1.97s
                      Time elapsed: 01:07:42
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 49216 steps/s (collection: 1.900s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 95.7465
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.3645
                       Mean reward: 908.38
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 1.1321
     Episode_Reward/lifting_object: 179.9311
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.00s
                      Time elapsed: 01:07:44
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 50458 steps/s (collection: 1.836s, learning 0.113s)
             Mean action noise std: 2.71
          Mean value_function loss: 117.3779
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.3750
                       Mean reward: 896.27
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 1.1166
     Episode_Reward/lifting_object: 176.5178
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 1.95s
                      Time elapsed: 01:07:46
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 50364 steps/s (collection: 1.848s, learning 0.104s)
             Mean action noise std: 2.71
          Mean value_function loss: 96.1074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.3837
                       Mean reward: 883.04
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.1354
     Episode_Reward/lifting_object: 179.5597
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.95s
                      Time elapsed: 01:07:48
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 50749 steps/s (collection: 1.839s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 104.3763
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.3866
                       Mean reward: 912.73
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 1.1333
     Episode_Reward/lifting_object: 178.7422
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 1.94s
                      Time elapsed: 01:07:49
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 47749 steps/s (collection: 1.844s, learning 0.215s)
             Mean action noise std: 2.71
          Mean value_function loss: 123.7904
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 41.3909
                       Mean reward: 875.44
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.1166
     Episode_Reward/lifting_object: 176.1451
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.06s
                      Time elapsed: 01:07:52
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 47024 steps/s (collection: 1.927s, learning 0.163s)
             Mean action noise std: 2.71
          Mean value_function loss: 125.8046
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.3945
                       Mean reward: 896.83
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 1.1278
     Episode_Reward/lifting_object: 178.4352
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.09s
                      Time elapsed: 01:07:54
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 49346 steps/s (collection: 1.866s, learning 0.127s)
             Mean action noise std: 2.71
          Mean value_function loss: 149.2278
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.3992
                       Mean reward: 856.40
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.1104
     Episode_Reward/lifting_object: 175.2073
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.99s
                      Time elapsed: 01:07:56
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 47406 steps/s (collection: 1.955s, learning 0.118s)
             Mean action noise std: 2.71
          Mean value_function loss: 137.9466
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.4056
                       Mean reward: 860.57
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.0907
     Episode_Reward/lifting_object: 171.7505
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.07s
                      Time elapsed: 01:07:58
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 49338 steps/s (collection: 1.882s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 128.5737
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.4164
                       Mean reward: 902.56
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 1.1313
     Episode_Reward/lifting_object: 179.5896
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.99s
                      Time elapsed: 01:08:00
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 49193 steps/s (collection: 1.804s, learning 0.195s)
             Mean action noise std: 2.71
          Mean value_function loss: 119.2828
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 41.4233
                       Mean reward: 890.73
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 1.1134
     Episode_Reward/lifting_object: 175.7670
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.00s
                      Time elapsed: 01:08:02
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 49586 steps/s (collection: 1.875s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 117.4279
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 41.4253
                       Mean reward: 875.25
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.1334
     Episode_Reward/lifting_object: 178.5142
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.98s
                      Time elapsed: 01:08:04
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 50792 steps/s (collection: 1.837s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 134.0581
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.4299
                       Mean reward: 874.22
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.1132
     Episode_Reward/lifting_object: 174.2820
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.94s
                      Time elapsed: 01:08:06
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 48848 steps/s (collection: 1.876s, learning 0.136s)
             Mean action noise std: 2.72
          Mean value_function loss: 168.9065
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.4437
                       Mean reward: 888.17
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 1.1088
     Episode_Reward/lifting_object: 174.6219
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.01s
                      Time elapsed: 01:08:08
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 50894 steps/s (collection: 1.821s, learning 0.111s)
             Mean action noise std: 2.72
          Mean value_function loss: 138.3087
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4550
                       Mean reward: 861.92
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.1097
     Episode_Reward/lifting_object: 174.3714
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.93s
                      Time elapsed: 01:08:10
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 51422 steps/s (collection: 1.823s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 99.9499
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.4623
                       Mean reward: 879.30
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.1282
     Episode_Reward/lifting_object: 177.2958
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.91s
                      Time elapsed: 01:08:11
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 50459 steps/s (collection: 1.856s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 118.9042
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.4685
                       Mean reward: 887.22
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.1182
     Episode_Reward/lifting_object: 175.8252
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.95s
                      Time elapsed: 01:08:13
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 51086 steps/s (collection: 1.831s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 136.4547
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.4786
                       Mean reward: 887.96
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 1.1238
     Episode_Reward/lifting_object: 176.8469
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 1.92s
                      Time elapsed: 01:08:15
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 51067 steps/s (collection: 1.819s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 113.0915
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 41.4954
                       Mean reward: 876.42
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.1260
     Episode_Reward/lifting_object: 177.2789
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 1.92s
                      Time elapsed: 01:08:17
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 50980 steps/s (collection: 1.829s, learning 0.099s)
             Mean action noise std: 2.73
          Mean value_function loss: 137.8694
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.5062
                       Mean reward: 875.64
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 1.1121
     Episode_Reward/lifting_object: 174.5695
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 1.93s
                      Time elapsed: 01:08:19
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 50881 steps/s (collection: 1.844s, learning 0.088s)
             Mean action noise std: 2.73
          Mean value_function loss: 91.4292
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 41.5148
                       Mean reward: 910.00
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.1228
     Episode_Reward/lifting_object: 175.7259
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.93s
                      Time elapsed: 01:08:21
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 50181 steps/s (collection: 1.855s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 132.9127
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.5266
                       Mean reward: 826.99
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 1.1155
     Episode_Reward/lifting_object: 174.7198
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 1.96s
                      Time elapsed: 01:08:23
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 51124 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 2.73
          Mean value_function loss: 83.6876
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.5362
                       Mean reward: 908.58
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.1379
     Episode_Reward/lifting_object: 178.8464
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 1.92s
                      Time elapsed: 01:08:25
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 50328 steps/s (collection: 1.859s, learning 0.094s)
             Mean action noise std: 2.73
          Mean value_function loss: 83.1760
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.5443
                       Mean reward: 915.04
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 1.1470
     Episode_Reward/lifting_object: 180.3165
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 1.95s
                      Time elapsed: 01:08:27
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 49770 steps/s (collection: 1.853s, learning 0.122s)
             Mean action noise std: 2.73
          Mean value_function loss: 118.2272
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.5492
                       Mean reward: 878.64
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.1412
     Episode_Reward/lifting_object: 179.1549
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 1.98s
                      Time elapsed: 01:08:29
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 50286 steps/s (collection: 1.833s, learning 0.122s)
             Mean action noise std: 2.74
          Mean value_function loss: 125.5085
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.5532
                       Mean reward: 899.09
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 1.1395
     Episode_Reward/lifting_object: 179.0562
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 1.95s
                      Time elapsed: 01:08:31
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 46745 steps/s (collection: 1.951s, learning 0.152s)
             Mean action noise std: 2.74
          Mean value_function loss: 135.7720
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.5620
                       Mean reward: 883.55
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.1300
     Episode_Reward/lifting_object: 178.1951
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.10s
                      Time elapsed: 01:08:33
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 51324 steps/s (collection: 1.818s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 158.5986
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 41.5861
                       Mean reward: 879.99
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.1221
     Episode_Reward/lifting_object: 176.5283
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.92s
                      Time elapsed: 01:08:35
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 47762 steps/s (collection: 1.872s, learning 0.186s)
             Mean action noise std: 2.74
          Mean value_function loss: 133.3459
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 41.5988
                       Mean reward: 897.62
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 1.1196
     Episode_Reward/lifting_object: 175.7151
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.06s
                      Time elapsed: 01:08:37
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 47774 steps/s (collection: 1.966s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 170.6244
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 41.6006
                       Mean reward: 868.99
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 168.5669
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.06s
                      Time elapsed: 01:08:39
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 50563 steps/s (collection: 1.844s, learning 0.100s)
             Mean action noise std: 2.74
          Mean value_function loss: 118.0794
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.6032
                       Mean reward: 884.28
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.1356
     Episode_Reward/lifting_object: 178.0032
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 1.94s
                      Time elapsed: 01:08:41
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 48059 steps/s (collection: 1.922s, learning 0.123s)
             Mean action noise std: 2.74
          Mean value_function loss: 119.9645
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6128
                       Mean reward: 866.98
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 1.1042
     Episode_Reward/lifting_object: 173.3351
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.05s
                      Time elapsed: 01:08:43
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 50099 steps/s (collection: 1.869s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 113.6351
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.6260
                       Mean reward: 869.95
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.1115
     Episode_Reward/lifting_object: 174.4406
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.96s
                      Time elapsed: 01:08:45
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 49758 steps/s (collection: 1.841s, learning 0.135s)
             Mean action noise std: 2.75
          Mean value_function loss: 192.9700
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.6369
                       Mean reward: 848.52
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.0774
     Episode_Reward/lifting_object: 169.1823
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.98s
                      Time elapsed: 01:08:47
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 50768 steps/s (collection: 1.823s, learning 0.113s)
             Mean action noise std: 2.75
          Mean value_function loss: 147.5855
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.6413
                       Mean reward: 867.01
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.0974
     Episode_Reward/lifting_object: 172.2773
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 1.94s
                      Time elapsed: 01:08:49
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 50966 steps/s (collection: 1.837s, learning 0.092s)
             Mean action noise std: 2.75
          Mean value_function loss: 129.1131
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.6478
                       Mean reward: 877.64
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.1064
     Episode_Reward/lifting_object: 174.4484
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.93s
                      Time elapsed: 01:08:51
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 50549 steps/s (collection: 1.839s, learning 0.106s)
             Mean action noise std: 2.75
          Mean value_function loss: 70.4332
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.6514
                       Mean reward: 904.37
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 1.1362
     Episode_Reward/lifting_object: 178.9280
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 1.94s
                      Time elapsed: 01:08:53
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 50596 steps/s (collection: 1.845s, learning 0.098s)
             Mean action noise std: 2.75
          Mean value_function loss: 97.1659
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.6551
                       Mean reward: 911.66
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 1.1474
     Episode_Reward/lifting_object: 181.0172
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 1.94s
                      Time elapsed: 01:08:55
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 50616 steps/s (collection: 1.851s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 118.7370
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.6554
                       Mean reward: 896.57
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 1.1529
     Episode_Reward/lifting_object: 181.6685
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.94s
                      Time elapsed: 01:08:57
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 50775 steps/s (collection: 1.850s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 156.1004
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 41.6622
                       Mean reward: 866.16
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.1130
     Episode_Reward/lifting_object: 175.5135
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.94s
                      Time elapsed: 01:08:59
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 51110 steps/s (collection: 1.830s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 144.9018
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.6658
                       Mean reward: 872.45
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.1207
     Episode_Reward/lifting_object: 176.6140
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.92s
                      Time elapsed: 01:09:00
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 51055 steps/s (collection: 1.840s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 152.2035
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.6713
                       Mean reward: 865.51
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.1091
     Episode_Reward/lifting_object: 173.8997
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.93s
                      Time elapsed: 01:09:02
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 50501 steps/s (collection: 1.852s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 142.8794
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.6812
                       Mean reward: 872.24
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.1130
     Episode_Reward/lifting_object: 175.3022
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.95s
                      Time elapsed: 01:09:04
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 51131 steps/s (collection: 1.837s, learning 0.086s)
             Mean action noise std: 2.76
          Mean value_function loss: 134.1970
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.6864
                       Mean reward: 900.89
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 1.1284
     Episode_Reward/lifting_object: 177.4997
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.92s
                      Time elapsed: 01:09:06
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 50147 steps/s (collection: 1.864s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 154.1632
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.6878
                       Mean reward: 852.38
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 1.0890
     Episode_Reward/lifting_object: 171.2092
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.96s
                      Time elapsed: 01:09:08
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 51295 steps/s (collection: 1.819s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 133.0762
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.6899
                       Mean reward: 873.69
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.1115
     Episode_Reward/lifting_object: 174.8800
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.92s
                      Time elapsed: 01:09:10
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 47570 steps/s (collection: 1.941s, learning 0.125s)
             Mean action noise std: 2.76
          Mean value_function loss: 62.0043
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.6955
                       Mean reward: 898.10
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.1401
     Episode_Reward/lifting_object: 179.6551
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.07s
                      Time elapsed: 01:09:12
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 49494 steps/s (collection: 1.897s, learning 0.090s)
             Mean action noise std: 2.76
          Mean value_function loss: 98.5823
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.7028
                       Mean reward: 861.65
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 1.1219
     Episode_Reward/lifting_object: 174.8894
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.99s
                      Time elapsed: 01:09:14
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.809s, learning 0.116s)
             Mean action noise std: 2.76
          Mean value_function loss: 105.1084
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.7107
                       Mean reward: 906.90
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.1293
     Episode_Reward/lifting_object: 177.6943
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 1.93s
                      Time elapsed: 01:09:16
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 50170 steps/s (collection: 1.865s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 78.9845
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.7171
                       Mean reward: 878.52
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.1298
     Episode_Reward/lifting_object: 177.3270
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 1.96s
                      Time elapsed: 01:09:18
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 49985 steps/s (collection: 1.876s, learning 0.091s)
             Mean action noise std: 2.77
          Mean value_function loss: 92.3616
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 41.7286
                       Mean reward: 907.47
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.1329
     Episode_Reward/lifting_object: 177.5818
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.97s
                      Time elapsed: 01:09:20
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 49644 steps/s (collection: 1.861s, learning 0.120s)
             Mean action noise std: 2.77
          Mean value_function loss: 125.2545
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.7337
                       Mean reward: 862.61
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.0999
     Episode_Reward/lifting_object: 172.2903
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 1.98s
                      Time elapsed: 01:09:22
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 50901 steps/s (collection: 1.841s, learning 0.090s)
             Mean action noise std: 2.77
          Mean value_function loss: 77.7713
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.7371
                       Mean reward: 914.60
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.1422
     Episode_Reward/lifting_object: 179.0712
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.93s
                      Time elapsed: 01:09:24
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 48836 steps/s (collection: 1.903s, learning 0.110s)
             Mean action noise std: 2.77
          Mean value_function loss: 89.9314
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.7450
                       Mean reward: 872.75
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 1.1362
     Episode_Reward/lifting_object: 178.1621
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.01s
                      Time elapsed: 01:09:26
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 49805 steps/s (collection: 1.876s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 107.5027
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.7544
                       Mean reward: 919.84
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 1.1537
     Episode_Reward/lifting_object: 181.2918
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 1.97s
                      Time elapsed: 01:09:28
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 49959 steps/s (collection: 1.875s, learning 0.093s)
             Mean action noise std: 2.77
          Mean value_function loss: 114.3517
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 41.7627
                       Mean reward: 907.46
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.1438
     Episode_Reward/lifting_object: 179.3055
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 1.97s
                      Time elapsed: 01:09:30
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 49514 steps/s (collection: 1.878s, learning 0.107s)
             Mean action noise std: 2.77
          Mean value_function loss: 115.6059
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.7692
                       Mean reward: 849.18
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.1199
     Episode_Reward/lifting_object: 175.8789
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 1.99s
                      Time elapsed: 01:09:32
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 49905 steps/s (collection: 1.856s, learning 0.114s)
             Mean action noise std: 2.77
          Mean value_function loss: 109.0855
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 41.7809
                       Mean reward: 839.46
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 1.1235
     Episode_Reward/lifting_object: 175.9794
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 1.97s
                      Time elapsed: 01:09:34
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 47181 steps/s (collection: 1.938s, learning 0.145s)
             Mean action noise std: 2.78
          Mean value_function loss: 139.8151
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 41.7899
                       Mean reward: 855.10
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.1155
     Episode_Reward/lifting_object: 175.0297
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.08s
                      Time elapsed: 01:09:36
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 49316 steps/s (collection: 1.890s, learning 0.104s)
             Mean action noise std: 2.78
          Mean value_function loss: 128.0918
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.7927
                       Mean reward: 908.60
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 1.1295
     Episode_Reward/lifting_object: 177.3890
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 1.99s
                      Time elapsed: 01:09:38
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 49423 steps/s (collection: 1.895s, learning 0.094s)
             Mean action noise std: 2.78
          Mean value_function loss: 121.3049
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.7936
                       Mean reward: 863.70
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.1276
     Episode_Reward/lifting_object: 177.0225
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 1.99s
                      Time elapsed: 01:09:40
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 48704 steps/s (collection: 1.916s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 178.6338
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.7999
                       Mean reward: 906.62
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 1.1277
     Episode_Reward/lifting_object: 177.5128
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.02s
                      Time elapsed: 01:09:42
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 49460 steps/s (collection: 1.878s, learning 0.110s)
             Mean action noise std: 2.78
          Mean value_function loss: 119.9962
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.8069
                       Mean reward: 892.58
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.1386
     Episode_Reward/lifting_object: 178.8796
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 1.99s
                      Time elapsed: 01:09:44
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 48545 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 113.7452
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.8192
                       Mean reward: 909.57
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.1474
     Episode_Reward/lifting_object: 180.7680
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.02s
                      Time elapsed: 01:09:46
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 49286 steps/s (collection: 1.898s, learning 0.096s)
             Mean action noise std: 2.78
          Mean value_function loss: 104.5894
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 41.8341
                       Mean reward: 862.02
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 1.1147
     Episode_Reward/lifting_object: 175.1221
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 1.99s
                      Time elapsed: 01:09:48
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 48728 steps/s (collection: 1.914s, learning 0.104s)
             Mean action noise std: 2.79
          Mean value_function loss: 167.3801
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 41.8471
                       Mean reward: 856.72
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.1298
     Episode_Reward/lifting_object: 177.5649
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.02s
                      Time elapsed: 01:09:50
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 49851 steps/s (collection: 1.874s, learning 0.098s)
             Mean action noise std: 2.79
          Mean value_function loss: 100.2469
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.8645
                       Mean reward: 921.73
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 1.1344
     Episode_Reward/lifting_object: 178.6010
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 1.97s
                      Time elapsed: 01:09:52
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 49452 steps/s (collection: 1.881s, learning 0.107s)
             Mean action noise std: 2.79
          Mean value_function loss: 88.0308
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 41.8761
                       Mean reward: 885.14
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 1.1249
     Episode_Reward/lifting_object: 176.3665
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 1.99s
                      Time elapsed: 01:09:54
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 49229 steps/s (collection: 1.895s, learning 0.102s)
             Mean action noise std: 2.79
          Mean value_function loss: 125.6527
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.8795
                       Mean reward: 858.38
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.1009
     Episode_Reward/lifting_object: 172.2289
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.00s
                      Time elapsed: 01:09:56
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 48002 steps/s (collection: 1.934s, learning 0.114s)
             Mean action noise std: 2.79
          Mean value_function loss: 82.0062
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.8844
                       Mean reward: 915.59
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 1.1570
     Episode_Reward/lifting_object: 182.6329
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.05s
                      Time elapsed: 01:09:58
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 49450 steps/s (collection: 1.892s, learning 0.096s)
             Mean action noise std: 2.79
          Mean value_function loss: 66.0301
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.8895
                       Mean reward: 918.26
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 1.1477
     Episode_Reward/lifting_object: 180.8113
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 1.99s
                      Time elapsed: 01:10:00
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 50645 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 128.1989
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.8925
                       Mean reward: 897.69
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 1.1317
     Episode_Reward/lifting_object: 178.2625
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 1.94s
                      Time elapsed: 01:10:02
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 50096 steps/s (collection: 1.869s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 78.9479
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.9022
                       Mean reward: 905.64
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.1377
     Episode_Reward/lifting_object: 178.0023
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 1.96s
                      Time elapsed: 01:10:04
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 49307 steps/s (collection: 1.902s, learning 0.092s)
             Mean action noise std: 2.80
          Mean value_function loss: 66.3464
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.9110
                       Mean reward: 920.58
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 1.1294
     Episode_Reward/lifting_object: 177.0161
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 1.99s
                      Time elapsed: 01:10:06
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 49913 steps/s (collection: 1.879s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 83.0654
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 41.9185
                       Mean reward: 927.89
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 1.1438
     Episode_Reward/lifting_object: 180.0719
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 1.97s
                      Time elapsed: 01:10:08
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 45876 steps/s (collection: 2.009s, learning 0.134s)
             Mean action noise std: 2.80
          Mean value_function loss: 102.1649
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.9245
                       Mean reward: 884.24
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.1307
     Episode_Reward/lifting_object: 177.3706
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.14s
                      Time elapsed: 01:10:10
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 46694 steps/s (collection: 1.951s, learning 0.155s)
             Mean action noise std: 2.80
          Mean value_function loss: 124.9015
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.9301
                       Mean reward: 898.03
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 1.1153
     Episode_Reward/lifting_object: 174.9104
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.11s
                      Time elapsed: 01:10:12
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 45973 steps/s (collection: 1.967s, learning 0.172s)
             Mean action noise std: 2.80
          Mean value_function loss: 130.8439
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.9434
                       Mean reward: 872.79
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.1175
     Episode_Reward/lifting_object: 175.4924
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.14s
                      Time elapsed: 01:10:14
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 45387 steps/s (collection: 2.036s, learning 0.130s)
             Mean action noise std: 2.80
          Mean value_function loss: 111.4344
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.9533
                       Mean reward: 914.37
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 1.1414
     Episode_Reward/lifting_object: 179.6177
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.17s
                      Time elapsed: 01:10:16
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 46560 steps/s (collection: 1.976s, learning 0.136s)
             Mean action noise std: 2.81
          Mean value_function loss: 94.1172
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 41.9663
                       Mean reward: 891.49
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.1297
     Episode_Reward/lifting_object: 177.1197
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.11s
                      Time elapsed: 01:10:19
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 47198 steps/s (collection: 1.939s, learning 0.144s)
             Mean action noise std: 2.81
          Mean value_function loss: 101.7571
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.9784
                       Mean reward: 888.38
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 179.6194
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.08s
                      Time elapsed: 01:10:21
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 46334 steps/s (collection: 1.981s, learning 0.140s)
             Mean action noise std: 2.81
          Mean value_function loss: 128.3413
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.9886
                       Mean reward: 902.60
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 1.1372
     Episode_Reward/lifting_object: 178.5839
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.12s
                      Time elapsed: 01:10:23
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 48454 steps/s (collection: 1.920s, learning 0.109s)
             Mean action noise std: 2.81
          Mean value_function loss: 126.2157
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.9957
                       Mean reward: 913.50
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 1.1423
     Episode_Reward/lifting_object: 179.1797
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.03s
                      Time elapsed: 01:10:25
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 48784 steps/s (collection: 1.879s, learning 0.136s)
             Mean action noise std: 2.81
          Mean value_function loss: 158.3869
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.0029
                       Mean reward: 877.28
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 1.1281
     Episode_Reward/lifting_object: 176.5413
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.02s
                      Time elapsed: 01:10:27
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 47377 steps/s (collection: 1.938s, learning 0.137s)
             Mean action noise std: 2.81
          Mean value_function loss: 128.2704
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 42.0103
                       Mean reward: 908.10
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 1.1277
     Episode_Reward/lifting_object: 177.0775
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.07s
                      Time elapsed: 01:10:29
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 48156 steps/s (collection: 1.916s, learning 0.125s)
             Mean action noise std: 2.81
          Mean value_function loss: 102.9354
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 42.0115
                       Mean reward: 907.57
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 1.1461
     Episode_Reward/lifting_object: 180.0066
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.04s
                      Time elapsed: 01:10:31
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 48493 steps/s (collection: 1.895s, learning 0.132s)
             Mean action noise std: 2.81
          Mean value_function loss: 128.3205
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 42.0122
                       Mean reward: 848.97
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.1098
     Episode_Reward/lifting_object: 173.5022
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.03s
                      Time elapsed: 01:10:33
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 47882 steps/s (collection: 1.950s, learning 0.104s)
             Mean action noise std: 2.81
          Mean value_function loss: 173.0584
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.0142
                       Mean reward: 829.44
               Mean episode length: 223.37
    Episode_Reward/reaching_object: 1.0887
     Episode_Reward/lifting_object: 170.7415
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.05s
                      Time elapsed: 01:10:35
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 47044 steps/s (collection: 1.984s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 83.4856
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.0179
                       Mean reward: 911.64
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.1234
     Episode_Reward/lifting_object: 175.9410
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.09s
                      Time elapsed: 01:10:37
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 50053 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 2.82
          Mean value_function loss: 100.0352
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.0244
                       Mean reward: 901.41
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 1.1299
     Episode_Reward/lifting_object: 177.1166
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 1.96s
                      Time elapsed: 01:10:39
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 49817 steps/s (collection: 1.880s, learning 0.093s)
             Mean action noise std: 2.82
          Mean value_function loss: 78.0399
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.0348
                       Mean reward: 917.16
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 1.1292
     Episode_Reward/lifting_object: 176.8796
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 1.97s
                      Time elapsed: 01:10:41
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 48613 steps/s (collection: 1.925s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 92.0264
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.0408
                       Mean reward: 898.65
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.1292
     Episode_Reward/lifting_object: 177.2238
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.02s
                      Time elapsed: 01:10:43
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 47682 steps/s (collection: 1.962s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 93.0941
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.0456
                       Mean reward: 919.79
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 1.1435
     Episode_Reward/lifting_object: 178.8399
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.06s
                      Time elapsed: 01:10:45
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 48724 steps/s (collection: 1.917s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 147.0345
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.0514
                       Mean reward: 858.24
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 1.1221
     Episode_Reward/lifting_object: 176.0444
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.02s
                      Time elapsed: 01:10:47
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 49407 steps/s (collection: 1.881s, learning 0.109s)
             Mean action noise std: 2.82
          Mean value_function loss: 81.6232
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.0612
                       Mean reward: 888.54
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 1.1336
     Episode_Reward/lifting_object: 177.4551
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 1.99s
                      Time elapsed: 01:10:49
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 49471 steps/s (collection: 1.886s, learning 0.101s)
             Mean action noise std: 2.83
          Mean value_function loss: 81.7155
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.0713
                       Mean reward: 922.71
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 1.1624
     Episode_Reward/lifting_object: 182.6477
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 1.99s
                      Time elapsed: 01:10:51
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 49123 steps/s (collection: 1.905s, learning 0.096s)
             Mean action noise std: 2.83
          Mean value_function loss: 147.2290
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 42.0764
                       Mean reward: 884.00
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 1.1194
     Episode_Reward/lifting_object: 175.5783
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.00s
                      Time elapsed: 01:10:53
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 47323 steps/s (collection: 1.948s, learning 0.129s)
             Mean action noise std: 2.83
          Mean value_function loss: 181.3307
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.0795
                       Mean reward: 846.56
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 1.0972
     Episode_Reward/lifting_object: 172.2544
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.08s
                      Time elapsed: 01:10:55
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 46579 steps/s (collection: 1.969s, learning 0.142s)
             Mean action noise std: 2.83
          Mean value_function loss: 111.0199
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.0850
                       Mean reward: 899.63
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.1086
     Episode_Reward/lifting_object: 174.5123
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.11s
                      Time elapsed: 01:10:57
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 47736 steps/s (collection: 1.930s, learning 0.129s)
             Mean action noise std: 2.83
          Mean value_function loss: 142.8267
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.0932
                       Mean reward: 901.44
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.1087
     Episode_Reward/lifting_object: 174.5513
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.06s
                      Time elapsed: 01:10:59
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 45070 steps/s (collection: 2.089s, learning 0.092s)
             Mean action noise std: 2.83
          Mean value_function loss: 132.3628
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.1012
                       Mean reward: 877.36
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.1066
     Episode_Reward/lifting_object: 173.6176
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.18s
                      Time elapsed: 01:11:02
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 49053 steps/s (collection: 1.907s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 162.4489
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.1122
                       Mean reward: 862.22
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 1.1215
     Episode_Reward/lifting_object: 176.0608
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.00s
                      Time elapsed: 01:11:04
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 48772 steps/s (collection: 1.912s, learning 0.103s)
             Mean action noise std: 2.83
          Mean value_function loss: 123.6338
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.1184
                       Mean reward: 892.31
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 1.1170
     Episode_Reward/lifting_object: 175.0748
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.02s
                      Time elapsed: 01:11:06
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 48188 steps/s (collection: 1.921s, learning 0.119s)
             Mean action noise std: 2.84
          Mean value_function loss: 110.8236
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.1291
                       Mean reward: 918.72
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 1.1303
     Episode_Reward/lifting_object: 177.6764
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.04s
                      Time elapsed: 01:11:08
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.962s, learning 0.102s)
             Mean action noise std: 2.84
          Mean value_function loss: 100.6826
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 42.1346
                       Mean reward: 919.83
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.1353
     Episode_Reward/lifting_object: 178.3006
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.06s
                      Time elapsed: 01:11:10
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 48722 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 2.84
          Mean value_function loss: 134.9999
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.1356
                       Mean reward: 874.43
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.1137
     Episode_Reward/lifting_object: 174.8182
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.02s
                      Time elapsed: 01:11:12
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 49128 steps/s (collection: 1.890s, learning 0.111s)
             Mean action noise std: 2.84
          Mean value_function loss: 166.8515
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.1396
                       Mean reward: 847.43
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.0888
     Episode_Reward/lifting_object: 170.3612
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.00s
                      Time elapsed: 01:11:14
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 49084 steps/s (collection: 1.901s, learning 0.102s)
             Mean action noise std: 2.84
          Mean value_function loss: 111.6659
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.1454
                       Mean reward: 897.30
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.1192
     Episode_Reward/lifting_object: 175.0837
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.00s
                      Time elapsed: 01:11:16
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 49114 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 2.84
          Mean value_function loss: 75.9243
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 42.1560
                       Mean reward: 923.81
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 1.1596
     Episode_Reward/lifting_object: 181.7134
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.00s
                      Time elapsed: 01:11:18
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 47457 steps/s (collection: 1.936s, learning 0.136s)
             Mean action noise std: 2.84
          Mean value_function loss: 157.6266
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.1594
                       Mean reward: 897.10
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.1356
     Episode_Reward/lifting_object: 178.4085
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.07s
                      Time elapsed: 01:11:20
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 48464 steps/s (collection: 1.898s, learning 0.130s)
             Mean action noise std: 2.84
          Mean value_function loss: 118.2039
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.1647
                       Mean reward: 905.10
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 1.1274
     Episode_Reward/lifting_object: 176.9245
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.03s
                      Time elapsed: 01:11:22
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 49026 steps/s (collection: 1.906s, learning 0.099s)
             Mean action noise std: 2.84
          Mean value_function loss: 118.3359
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.1694
                       Mean reward: 882.32
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.1172
     Episode_Reward/lifting_object: 174.4874
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.01s
                      Time elapsed: 01:11:24
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 47141 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.85
          Mean value_function loss: 170.7201
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.1786
                       Mean reward: 896.36
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 1.1279
     Episode_Reward/lifting_object: 176.8837
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.09s
                      Time elapsed: 01:11:26
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 45473 steps/s (collection: 2.042s, learning 0.120s)
             Mean action noise std: 2.85
          Mean value_function loss: 103.5473
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.1907
                       Mean reward: 879.00
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.1225
     Episode_Reward/lifting_object: 174.8973
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.16s
                      Time elapsed: 01:11:28
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 48329 steps/s (collection: 1.927s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 102.0648
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 42.1971
                       Mean reward: 894.17
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.1353
     Episode_Reward/lifting_object: 178.7961
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.03s
                      Time elapsed: 01:11:30
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.065s, learning 0.094s)
             Mean action noise std: 2.85
          Mean value_function loss: 88.9383
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.2033
                       Mean reward: 884.21
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.1304
     Episode_Reward/lifting_object: 177.1389
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.16s
                      Time elapsed: 01:11:32
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 47266 steps/s (collection: 1.974s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 90.8330
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 42.2081
                       Mean reward: 885.55
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 1.1258
     Episode_Reward/lifting_object: 176.3065
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.08s
                      Time elapsed: 01:11:34
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 46354 steps/s (collection: 2.025s, learning 0.095s)
             Mean action noise std: 2.85
          Mean value_function loss: 78.7292
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 42.2093
                       Mean reward: 902.92
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 1.1451
     Episode_Reward/lifting_object: 179.7186
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.12s
                      Time elapsed: 01:11:36
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 47444 steps/s (collection: 1.974s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 81.4189
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.2110
                       Mean reward: 865.84
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.1438
     Episode_Reward/lifting_object: 179.4547
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.07s
                      Time elapsed: 01:11:38
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 48859 steps/s (collection: 1.893s, learning 0.119s)
             Mean action noise std: 2.85
          Mean value_function loss: 165.4267
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.2171
                       Mean reward: 856.78
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.1133
     Episode_Reward/lifting_object: 174.6511
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.01s
                      Time elapsed: 01:11:40
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 49377 steps/s (collection: 1.899s, learning 0.092s)
             Mean action noise std: 2.86
          Mean value_function loss: 79.6118
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.2270
                       Mean reward: 905.01
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 1.1353
     Episode_Reward/lifting_object: 177.9938
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 1.99s
                      Time elapsed: 01:11:42
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 48507 steps/s (collection: 1.914s, learning 0.113s)
             Mean action noise std: 2.86
          Mean value_function loss: 100.1377
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.2339
                       Mean reward: 899.69
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 1.1294
     Episode_Reward/lifting_object: 176.7654
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.03s
                      Time elapsed: 01:11:44
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 48644 steps/s (collection: 1.903s, learning 0.118s)
             Mean action noise std: 2.86
          Mean value_function loss: 227.9278
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.2368
                       Mean reward: 916.50
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 1.1259
     Episode_Reward/lifting_object: 176.2735
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.02s
                      Time elapsed: 01:11:47
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 48806 steps/s (collection: 1.908s, learning 0.106s)
             Mean action noise std: 2.86
          Mean value_function loss: 189.4661
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.2458
                       Mean reward: 900.37
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.1470
     Episode_Reward/lifting_object: 180.1629
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.01s
                      Time elapsed: 01:11:49
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 48320 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 2.86
          Mean value_function loss: 128.5467
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 42.2629
                       Mean reward: 886.64
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.1204
     Episode_Reward/lifting_object: 175.3084
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.03s
                      Time elapsed: 01:11:51
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 49255 steps/s (collection: 1.899s, learning 0.097s)
             Mean action noise std: 2.86
          Mean value_function loss: 130.8786
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.2689
                       Mean reward: 881.59
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.1162
     Episode_Reward/lifting_object: 174.8077
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.00s
                      Time elapsed: 01:11:53
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 49276 steps/s (collection: 1.897s, learning 0.097s)
             Mean action noise std: 2.86
          Mean value_function loss: 67.6851
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.2725
                       Mean reward: 916.45
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 1.1435
     Episode_Reward/lifting_object: 179.9278
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 1.99s
                      Time elapsed: 01:11:55
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 48539 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 2.87
          Mean value_function loss: 93.0326
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.2821
                       Mean reward: 894.73
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.1501
     Episode_Reward/lifting_object: 181.0134
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.03s
                      Time elapsed: 01:11:57
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 48788 steps/s (collection: 1.917s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 115.6407
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 42.2921
                       Mean reward: 902.03
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 1.1409
     Episode_Reward/lifting_object: 179.1487
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.01s
                      Time elapsed: 01:11:59
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 49221 steps/s (collection: 1.898s, learning 0.099s)
             Mean action noise std: 2.87
          Mean value_function loss: 117.9458
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 42.2935
                       Mean reward: 859.37
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.1270
     Episode_Reward/lifting_object: 176.6720
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.00s
                      Time elapsed: 01:12:01
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 49405 steps/s (collection: 1.890s, learning 0.100s)
             Mean action noise std: 2.87
          Mean value_function loss: 99.3587
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.2949
                       Mean reward: 884.55
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 178.7890
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 1.99s
                      Time elapsed: 01:12:03
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 49671 steps/s (collection: 1.882s, learning 0.097s)
             Mean action noise std: 2.87
          Mean value_function loss: 98.6653
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.3001
                       Mean reward: 889.85
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.1509
     Episode_Reward/lifting_object: 181.0558
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 1.98s
                      Time elapsed: 01:12:05
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 49770 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.87
          Mean value_function loss: 153.3399
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 42.3119
                       Mean reward: 890.37
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.1318
     Episode_Reward/lifting_object: 177.7669
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 1.98s
                      Time elapsed: 01:12:07
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 49524 steps/s (collection: 1.880s, learning 0.105s)
             Mean action noise std: 2.87
          Mean value_function loss: 106.5370
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 42.3234
                       Mean reward: 902.87
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.1476
     Episode_Reward/lifting_object: 180.3454
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 1.98s
                      Time elapsed: 01:12:09
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 48911 steps/s (collection: 1.900s, learning 0.110s)
             Mean action noise std: 2.87
          Mean value_function loss: 129.1602
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 42.3302
                       Mean reward: 893.41
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 179.0503
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.01s
                      Time elapsed: 01:12:11
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 47785 steps/s (collection: 1.956s, learning 0.101s)
             Mean action noise std: 2.87
          Mean value_function loss: 143.3562
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.3338
                       Mean reward: 886.51
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.1208
     Episode_Reward/lifting_object: 176.1188
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.06s
                      Time elapsed: 01:12:13
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 49281 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 2.87
          Mean value_function loss: 143.9280
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.3405
                       Mean reward: 869.99
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 1.1467
     Episode_Reward/lifting_object: 179.8113
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 1.99s
                      Time elapsed: 01:12:15
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 49622 steps/s (collection: 1.879s, learning 0.102s)
             Mean action noise std: 2.88
          Mean value_function loss: 115.2627
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 42.3500
                       Mean reward: 895.62
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 1.1323
     Episode_Reward/lifting_object: 177.7044
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 1.98s
                      Time elapsed: 01:12:17
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 49198 steps/s (collection: 1.885s, learning 0.114s)
             Mean action noise std: 2.88
          Mean value_function loss: 100.6134
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.3576
                       Mean reward: 885.91
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 1.1496
     Episode_Reward/lifting_object: 178.7645
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.00s
                      Time elapsed: 01:12:19
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 49132 steps/s (collection: 1.892s, learning 0.109s)
             Mean action noise std: 2.88
          Mean value_function loss: 71.3441
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.3636
                       Mean reward: 922.98
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 1.1472
     Episode_Reward/lifting_object: 180.3206
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.00s
                      Time elapsed: 01:12:21
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 48849 steps/s (collection: 1.910s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 105.2872
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.3682
                       Mean reward: 895.85
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.1320
     Episode_Reward/lifting_object: 177.4301
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.01s
                      Time elapsed: 01:12:23
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.872s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 121.7113
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.3749
                       Mean reward: 880.84
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.1244
     Episode_Reward/lifting_object: 176.1869
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 1.97s
                      Time elapsed: 01:12:25
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 49442 steps/s (collection: 1.891s, learning 0.097s)
             Mean action noise std: 2.88
          Mean value_function loss: 76.6315
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.3885
                       Mean reward: 898.55
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 1.1502
     Episode_Reward/lifting_object: 180.6399
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 1.99s
                      Time elapsed: 01:12:27
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 48354 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 2.88
          Mean value_function loss: 67.0912
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.3971
                       Mean reward: 908.96
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 1.1604
     Episode_Reward/lifting_object: 181.9409
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.03s
                      Time elapsed: 01:12:29
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 49264 steps/s (collection: 1.902s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 99.3619
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.4072
                       Mean reward: 894.69
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.1331
     Episode_Reward/lifting_object: 177.5132
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.00s
                      Time elapsed: 01:12:31
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 47735 steps/s (collection: 1.962s, learning 0.097s)
             Mean action noise std: 2.89
          Mean value_function loss: 126.6769
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 42.4171
                       Mean reward: 859.38
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.1497
     Episode_Reward/lifting_object: 180.3705
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.06s
                      Time elapsed: 01:12:33
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 48413 steps/s (collection: 1.925s, learning 0.105s)
             Mean action noise std: 2.89
          Mean value_function loss: 113.9915
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 42.4256
                       Mean reward: 906.20
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 1.1492
     Episode_Reward/lifting_object: 179.9600
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.03s
                      Time elapsed: 01:12:35
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 47647 steps/s (collection: 1.952s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 113.1497
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 42.4289
                       Mean reward: 888.75
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.1372
     Episode_Reward/lifting_object: 178.3789
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.06s
                      Time elapsed: 01:12:37
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 47547 steps/s (collection: 1.967s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 98.4165
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.4318
                       Mean reward: 896.42
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.1497
     Episode_Reward/lifting_object: 180.6329
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.07s
                      Time elapsed: 01:12:39
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 48656 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 122.4777
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.4395
                       Mean reward: 906.11
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 1.1439
     Episode_Reward/lifting_object: 179.5916
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.02s
                      Time elapsed: 01:12:41
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 47741 steps/s (collection: 1.956s, learning 0.104s)
             Mean action noise std: 2.89
          Mean value_function loss: 131.5870
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 42.4445
                       Mean reward: 872.91
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.1380
     Episode_Reward/lifting_object: 178.2139
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.06s
                      Time elapsed: 01:12:43
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 48671 steps/s (collection: 1.908s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 100.3509
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.4466
                       Mean reward: 875.78
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.1323
     Episode_Reward/lifting_object: 177.3125
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.02s
                      Time elapsed: 01:12:45
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 48681 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 89.2443
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.4538
                       Mean reward: 897.37
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.1426
     Episode_Reward/lifting_object: 178.6000
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.02s
                      Time elapsed: 01:12:47
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 48832 steps/s (collection: 1.893s, learning 0.121s)
             Mean action noise std: 2.89
          Mean value_function loss: 105.3276
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.4621
                       Mean reward: 885.93
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.1287
     Episode_Reward/lifting_object: 177.1067
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.01s
                      Time elapsed: 01:12:49
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 47756 steps/s (collection: 1.949s, learning 0.110s)
             Mean action noise std: 2.90
          Mean value_function loss: 87.3241
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.4660
                       Mean reward: 909.58
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 1.1568
     Episode_Reward/lifting_object: 181.2145
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.06s
                      Time elapsed: 01:12:51
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 44787 steps/s (collection: 2.090s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 81.0243
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.4685
                       Mean reward: 888.13
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.1467
     Episode_Reward/lifting_object: 179.4048
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.19s
                      Time elapsed: 01:12:53
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 48406 steps/s (collection: 1.928s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 74.7627
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.4785
                       Mean reward: 907.84
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 1.1565
     Episode_Reward/lifting_object: 179.8364
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.03s
                      Time elapsed: 01:12:55
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 47484 steps/s (collection: 1.967s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 130.9029
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.4893
                       Mean reward: 869.26
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.1214
     Episode_Reward/lifting_object: 175.6907
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.07s
                      Time elapsed: 01:12:57
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 47835 steps/s (collection: 1.954s, learning 0.101s)
             Mean action noise std: 2.90
          Mean value_function loss: 111.6278
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 42.4956
                       Mean reward: 869.33
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.1227
     Episode_Reward/lifting_object: 175.2899
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.06s
                      Time elapsed: 01:12:59
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 48693 steps/s (collection: 1.916s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 103.4656
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.4974
                       Mean reward: 877.69
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 1.1327
     Episode_Reward/lifting_object: 177.9348
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.02s
                      Time elapsed: 01:13:01
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 48051 steps/s (collection: 1.950s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 195.5229
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.5026
                       Mean reward: 878.25
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.1429
     Episode_Reward/lifting_object: 178.5336
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.05s
                      Time elapsed: 01:13:03
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 49222 steps/s (collection: 1.904s, learning 0.093s)
             Mean action noise std: 2.90
          Mean value_function loss: 167.8807
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 42.5092
                       Mean reward: 915.49
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 1.1535
     Episode_Reward/lifting_object: 180.0380
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.00s
                      Time elapsed: 01:13:05
                               ETA: 00:00:02

