################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10900 steps/s (collection: 8.766s, learning 0.252s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 25.5714
                       Mean reward: 0.00
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0005
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0002
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.02s
                      Time elapsed: 00:00:09
                               ETA: 05:00:35

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 15697 steps/s (collection: 6.127s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 25.6611
                       Mean reward: 0.00
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0013
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.26s
                      Time elapsed: 00:00:15
                               ETA: 04:14:32

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14930 steps/s (collection: 6.456s, learning 0.129s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.6866
                       Mean reward: 0.00
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0023
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.58s
                      Time elapsed: 00:00:21
                               ETA: 04:02:41

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14933 steps/s (collection: 6.434s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.6963
                       Mean reward: 0.01
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.58s
                      Time elapsed: 00:00:28
                               ETA: 03:56:42

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14371 steps/s (collection: 6.693s, learning 0.148s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.7644
                       Mean reward: 0.02
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0050
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.84s
                      Time elapsed: 00:00:35
                               ETA: 03:54:46

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14850 steps/s (collection: 6.478s, learning 0.141s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 25.8261
                       Mean reward: 0.02
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0063
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.62s
                      Time elapsed: 00:00:41
                               ETA: 03:52:14

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14668 steps/s (collection: 6.505s, learning 0.197s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 25.8555
                       Mean reward: 0.02
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0080
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.70s
                      Time elapsed: 00:00:48
                               ETA: 03:50:46

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14168 steps/s (collection: 6.785s, learning 0.153s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 25.8378
                       Mean reward: 0.03
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0107
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0032
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.94s
                      Time elapsed: 00:00:55
                               ETA: 03:50:38

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 16207 steps/s (collection: 5.959s, learning 0.106s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 25.8425
                       Mean reward: 0.06
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0141
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 6.07s
                      Time elapsed: 00:01:01
                               ETA: 03:47:16

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 58210 steps/s (collection: 1.563s, learning 0.126s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 25.8526
                       Mean reward: 0.07
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0163
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.69s
                      Time elapsed: 00:01:03
                               ETA: 03:30:03

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 61237 steps/s (collection: 1.482s, learning 0.123s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 25.8730
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0212
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.61s
                      Time elapsed: 00:01:04
                               ETA: 03:15:42

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 57723 steps/s (collection: 1.576s, learning 0.127s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 25.9131
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0238
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.70s
                      Time elapsed: 00:01:06
                               ETA: 03:04:00

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 60307 steps/s (collection: 1.522s, learning 0.108s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 25.9472
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0300
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.63s
                      Time elapsed: 00:01:08
                               ETA: 02:53:55

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 60890 steps/s (collection: 1.514s, learning 0.100s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 25.9843
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0351
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.61s
                      Time elapsed: 00:01:09
                               ETA: 02:45:14

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 59086 steps/s (collection: 1.576s, learning 0.088s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 26.0183
                       Mean reward: 0.20
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.0417
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.66s
                      Time elapsed: 00:01:11
                               ETA: 02:37:48

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 58656 steps/s (collection: 1.570s, learning 0.106s)
             Mean action noise std: 1.04
          Mean value_function loss: 2.9471
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.1648
                       Mean reward: -1.35
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.0517
     Episode_Reward/lifting_object: -0.2625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.68s
                      Time elapsed: 00:01:13
                               ETA: 02:31:20

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 57469 steps/s (collection: 1.590s, learning 0.120s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.2287
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.2645
                       Mean reward: -0.21
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.0700
     Episode_Reward/lifting_object: -0.1731
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.71s
                      Time elapsed: 00:01:14
                               ETA: 02:25:41

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 56167 steps/s (collection: 1.648s, learning 0.102s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.5141
                       Mean reward: 0.37
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.0780
     Episode_Reward/lifting_object: -0.0221
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.75s
                      Time elapsed: 00:01:16
                               ETA: 02:20:44

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 57347 steps/s (collection: 1.627s, learning 0.087s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0582
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.5945
                       Mean reward: 0.43
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.0903
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.71s
                      Time elapsed: 00:01:18
                               ETA: 02:16:15

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 55301 steps/s (collection: 1.667s, learning 0.111s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.1998
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.6414
                       Mean reward: 0.52
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.1083
     Episode_Reward/lifting_object: -0.0282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.78s
                      Time elapsed: 00:01:20
                               ETA: 02:12:18

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 54942 steps/s (collection: 1.686s, learning 0.104s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.8065
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.8419
                       Mean reward: -0.79
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.1201
     Episode_Reward/lifting_object: -0.1388
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.79s
                      Time elapsed: 00:01:21
                               ETA: 02:08:45

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 55368 steps/s (collection: 1.683s, learning 0.093s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0300
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.9777
                       Mean reward: 0.64
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.1246
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.78s
                      Time elapsed: 00:01:23
                               ETA: 02:05:30

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 55103 steps/s (collection: 1.694s, learning 0.090s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 27.1314
                       Mean reward: 0.62
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.1290
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.78s
                      Time elapsed: 00:01:25
                               ETA: 02:02:32

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 56424 steps/s (collection: 1.642s, learning 0.100s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 27.1860
                       Mean reward: 0.65
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.1344
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.74s
                      Time elapsed: 00:01:27
                               ETA: 01:59:46

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 54151 steps/s (collection: 1.711s, learning 0.104s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0006
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.2314
                       Mean reward: 0.75
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.1411
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.82s
                      Time elapsed: 00:01:29
                               ETA: 01:57:18

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 52476 steps/s (collection: 1.720s, learning 0.153s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0112
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 27.2465
                       Mean reward: 0.63
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.1361
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.87s
                      Time elapsed: 00:01:30
                               ETA: 01:55:06

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 55500 steps/s (collection: 1.675s, learning 0.096s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 27.2883
                       Mean reward: 0.60
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.1291
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.77s
                      Time elapsed: 00:01:32
                               ETA: 01:52:57

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 54146 steps/s (collection: 1.718s, learning 0.098s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0890
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.3937
                       Mean reward: 0.63
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.1314
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.82s
                      Time elapsed: 00:01:34
                               ETA: 01:50:59

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 53927 steps/s (collection: 1.733s, learning 0.090s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0510
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.4469
                       Mean reward: 0.42
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.1360
     Episode_Reward/lifting_object: -0.0197
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.82s
                      Time elapsed: 00:01:36
                               ETA: 01:49:10

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 54372 steps/s (collection: 1.697s, learning 0.111s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 27.5692
                       Mean reward: 0.64
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.1371
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.81s
                      Time elapsed: 00:01:38
                               ETA: 01:47:28

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 53047 steps/s (collection: 1.698s, learning 0.155s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.7400
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.6097
                       Mean reward: -0.54
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.1342
     Episode_Reward/lifting_object: -0.0965
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.85s
                      Time elapsed: 00:01:39
                               ETA: 01:45:54

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 54810 steps/s (collection: 1.703s, learning 0.091s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.4665
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.6578
                       Mean reward: -0.10
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.1442
     Episode_Reward/lifting_object: -0.1395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.79s
                      Time elapsed: 00:01:41
                               ETA: 01:44:23

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 54424 steps/s (collection: 1.705s, learning 0.102s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0484
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.7950
                       Mean reward: 0.55
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 0.1513
     Episode_Reward/lifting_object: -0.0231
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.81s
                      Time elapsed: 00:01:43
                               ETA: 01:42:58

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 53174 steps/s (collection: 1.764s, learning 0.085s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 27.8877
                       Mean reward: 0.82
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.1589
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.85s
                      Time elapsed: 00:01:45
                               ETA: 01:41:40

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 52682 steps/s (collection: 1.764s, learning 0.102s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1127
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.0050
                       Mean reward: 0.77
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 0.1556
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.87s
                      Time elapsed: 00:01:47
                               ETA: 01:40:27

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 51543 steps/s (collection: 1.736s, learning 0.171s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.4566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.0572
                       Mean reward: -0.25
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 0.1604
     Episode_Reward/lifting_object: -0.0687
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.91s
                      Time elapsed: 00:01:49
                               ETA: 01:39:21

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 53972 steps/s (collection: 1.718s, learning 0.104s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 28.1636
                       Mean reward: 0.77
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 0.1635
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.82s
                      Time elapsed: 00:01:51
                               ETA: 01:38:14

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 53752 steps/s (collection: 1.740s, learning 0.089s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 28.2600
                       Mean reward: 0.78
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 0.1591
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.83s
                      Time elapsed: 00:01:52
                               ETA: 01:37:10

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 52345 steps/s (collection: 1.779s, learning 0.099s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 28.3199
                       Mean reward: 0.72
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 0.1548
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.88s
                      Time elapsed: 00:01:54
                               ETA: 01:36:12

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 53532 steps/s (collection: 1.744s, learning 0.093s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.1049
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 28.3624
                       Mean reward: 0.78
               Mean episode length: 230.80
    Episode_Reward/reaching_object: 0.1588
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.84s
                      Time elapsed: 00:01:56
                               ETA: 01:35:15

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 52196 steps/s (collection: 1.799s, learning 0.085s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0298
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.3962
                       Mean reward: 0.79
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 0.1714
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.88s
                      Time elapsed: 00:01:58
                               ETA: 01:34:23

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 51216 steps/s (collection: 1.795s, learning 0.125s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5015
                       Mean reward: 0.85
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 0.1714
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.92s
                      Time elapsed: 00:02:00
                               ETA: 01:33:35

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 49810 steps/s (collection: 1.803s, learning 0.171s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.4095
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.7103
                       Mean reward: 0.74
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 0.1614
     Episode_Reward/lifting_object: -0.0736
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.97s
                      Time elapsed: 00:02:02
                               ETA: 01:32:51

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 52350 steps/s (collection: 1.768s, learning 0.110s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1673
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.7609
                       Mean reward: 0.65
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 0.1757
     Episode_Reward/lifting_object: -0.0247
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.88s
                      Time elapsed: 00:02:04
                               ETA: 01:32:05

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 51933 steps/s (collection: 1.779s, learning 0.114s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1326
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.8321
                       Mean reward: 0.87
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 0.1772
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.89s
                      Time elapsed: 00:02:06
                               ETA: 01:31:22

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 51755 steps/s (collection: 1.772s, learning 0.127s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1056
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.9519
                       Mean reward: 0.99
               Mean episode length: 225.20
    Episode_Reward/reaching_object: 0.1922
     Episode_Reward/lifting_object: -0.0486
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.90s
                      Time elapsed: 00:02:08
                               ETA: 01:30:41

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 53382 steps/s (collection: 1.748s, learning 0.093s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1306
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.1347
                       Mean reward: 0.64
               Mean episode length: 224.54
    Episode_Reward/reaching_object: 0.2030
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.84s
                      Time elapsed: 00:02:09
                               ETA: 01:29:59

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 53010 steps/s (collection: 1.761s, learning 0.094s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 29.1600
                       Mean reward: 1.05
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 0.2150
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.85s
                      Time elapsed: 00:02:11
                               ETA: 01:29:19

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 52294 steps/s (collection: 1.785s, learning 0.095s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0394
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.1898
                       Mean reward: 0.96
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 0.2308
     Episode_Reward/lifting_object: -0.0230
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.88s
                      Time elapsed: 00:02:13
                               ETA: 01:28:42

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52670 steps/s (collection: 1.751s, learning 0.115s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 29.2042
                       Mean reward: 1.24
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 0.2520
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.87s
                      Time elapsed: 00:02:15
                               ETA: 01:28:06

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52871 steps/s (collection: 1.761s, learning 0.098s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 29.2372
                       Mean reward: 1.25
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.2620
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.86s
                      Time elapsed: 00:02:17
                               ETA: 01:27:30

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 52704 steps/s (collection: 1.739s, learning 0.126s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.2642
                       Mean reward: 1.33
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.2838
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.87s
                      Time elapsed: 00:02:19
                               ETA: 01:26:57

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52676 steps/s (collection: 1.769s, learning 0.098s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0843
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 29.2743
                       Mean reward: 1.41
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: -0.0243
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.87s
                      Time elapsed: 00:02:21
                               ETA: 01:26:24

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 50114 steps/s (collection: 1.843s, learning 0.119s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0681
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.2931
                       Mean reward: 1.21
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: -0.0433
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.96s
                      Time elapsed: 00:02:23
                               ETA: 01:25:56

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 47922 steps/s (collection: 1.929s, learning 0.123s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2929
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.3652
                       Mean reward: 1.06
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 0.2985
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.05s
                      Time elapsed: 00:02:25
                               ETA: 01:25:33

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 50648 steps/s (collection: 1.840s, learning 0.101s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.4593
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.3923
                       Mean reward: 0.67
               Mean episode length: 216.12
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: -0.1043
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.94s
                      Time elapsed: 00:02:27
                               ETA: 01:25:06

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 51298 steps/s (collection: 1.831s, learning 0.086s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7229
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.4238
                       Mean reward: 1.06
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.3387
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.92s
                      Time elapsed: 00:02:28
                               ETA: 01:24:39

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 53339 steps/s (collection: 1.746s, learning 0.097s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0499
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.4594
                       Mean reward: 1.64
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 0.3643
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.84s
                      Time elapsed: 00:02:30
                               ETA: 01:24:10

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 52565 steps/s (collection: 1.773s, learning 0.097s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.5907
                       Mean reward: 1.70
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.3964
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.87s
                      Time elapsed: 00:02:32
                               ETA: 01:23:44

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 52415 steps/s (collection: 1.779s, learning 0.097s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1453
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.7506
                       Mean reward: 1.95
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.3827
     Episode_Reward/lifting_object: -0.0317
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.88s
                      Time elapsed: 00:02:34
                               ETA: 01:23:18

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 53290 steps/s (collection: 1.752s, learning 0.093s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 29.7739
                       Mean reward: 1.83
               Mean episode length: 221.51
    Episode_Reward/reaching_object: 0.3856
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.84s
                      Time elapsed: 00:02:36
                               ETA: 01:22:52

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52414 steps/s (collection: 1.770s, learning 0.105s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 29.7905
                       Mean reward: 2.03
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.4181
     Episode_Reward/lifting_object: -0.0753
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.88s
                      Time elapsed: 00:02:38
                               ETA: 01:22:28

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 48855 steps/s (collection: 1.882s, learning 0.130s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.8132
                       Mean reward: 1.89
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 0.4232
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.01s
                      Time elapsed: 00:02:40
                               ETA: 01:22:09

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 50237 steps/s (collection: 1.853s, learning 0.104s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0581
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.8666
                       Mean reward: 2.16
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.4238
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.96s
                      Time elapsed: 00:02:42
                               ETA: 01:21:49

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 50027 steps/s (collection: 1.873s, learning 0.092s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.5508
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.9173
                       Mean reward: 2.06
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 0.4303
     Episode_Reward/lifting_object: -0.0027
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.96s
                      Time elapsed: 00:02:44
                               ETA: 01:21:29

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 50993 steps/s (collection: 1.813s, learning 0.115s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2221
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.9349
                       Mean reward: 2.28
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 0.4480
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.93s
                      Time elapsed: 00:02:46
                               ETA: 01:21:09

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 48799 steps/s (collection: 1.913s, learning 0.102s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.9567
                       Mean reward: 2.24
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.4555
     Episode_Reward/lifting_object: -0.0107
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.01s
                      Time elapsed: 00:02:48
                               ETA: 01:20:52

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 51268 steps/s (collection: 1.812s, learning 0.106s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 29.9798
                       Mean reward: 2.33
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 0.4699
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.92s
                      Time elapsed: 00:02:50
                               ETA: 01:20:33

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 50281 steps/s (collection: 1.861s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.9993
                       Mean reward: 1.09
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.4816
     Episode_Reward/lifting_object: -0.0671
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.96s
                      Time elapsed: 00:02:51
                               ETA: 01:20:15

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 50955 steps/s (collection: 1.823s, learning 0.106s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1624
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 30.0261
                       Mean reward: 2.11
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.4831
     Episode_Reward/lifting_object: -0.0287
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.93s
                      Time elapsed: 00:02:53
                               ETA: 01:19:57

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 51729 steps/s (collection: 1.801s, learning 0.100s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.2705
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.0420
                       Mean reward: 2.56
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.5155
     Episode_Reward/lifting_object: -0.0761
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.90s
                      Time elapsed: 00:02:55
                               ETA: 01:19:39

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 51841 steps/s (collection: 1.780s, learning 0.116s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0404
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.1236
                       Mean reward: 2.65
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.5352
     Episode_Reward/lifting_object: -0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.90s
                      Time elapsed: 00:02:57
                               ETA: 01:19:21

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 51817 steps/s (collection: 1.794s, learning 0.103s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0573
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 30.2157
                       Mean reward: 2.44
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.90s
                      Time elapsed: 00:02:59
                               ETA: 01:19:03

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 50850 steps/s (collection: 1.835s, learning 0.098s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0337
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.2402
                       Mean reward: 2.38
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.5255
     Episode_Reward/lifting_object: -0.0012
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.93s
                      Time elapsed: 00:03:01
                               ETA: 01:18:47

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 52198 steps/s (collection: 1.784s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0426
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.3099
                       Mean reward: 2.71
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.5473
     Episode_Reward/lifting_object: -0.0206
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 20.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.88s
                      Time elapsed: 00:03:03
                               ETA: 01:18:30

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 52057 steps/s (collection: 1.783s, learning 0.105s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.4184
                       Mean reward: 2.79
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.5393
     Episode_Reward/lifting_object: 0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.89s
                      Time elapsed: 00:03:05
                               ETA: 01:18:13

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 46050 steps/s (collection: 2.029s, learning 0.106s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1237
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.4546
                       Mean reward: 2.55
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 0.5381
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.13s
                      Time elapsed: 00:03:07
                               ETA: 01:18:03

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 51674 steps/s (collection: 1.787s, learning 0.116s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0335
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.4672
                       Mean reward: 2.98
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.5602
     Episode_Reward/lifting_object: 0.0036
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.90s
                      Time elapsed: 00:03:09
                               ETA: 01:17:48

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 49392 steps/s (collection: 1.865s, learning 0.125s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 30.4969
                       Mean reward: 2.84
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.5708
     Episode_Reward/lifting_object: -0.0036
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.99s
                      Time elapsed: 00:03:11
                               ETA: 01:17:35

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 46413 steps/s (collection: 1.985s, learning 0.133s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.5205
                       Mean reward: 2.73
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 0.0143
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.12s
                      Time elapsed: 00:03:13
                               ETA: 01:17:25

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 48177 steps/s (collection: 1.936s, learning 0.105s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1106
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.5741
                       Mean reward: 2.93
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.04s
                      Time elapsed: 00:03:15
                               ETA: 01:17:14

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 47342 steps/s (collection: 1.984s, learning 0.093s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.4804
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.6542
                       Mean reward: 2.83
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.5821
     Episode_Reward/lifting_object: 0.0179
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.08s
                      Time elapsed: 00:03:17
                               ETA: 01:17:03

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 44133 steps/s (collection: 2.027s, learning 0.201s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.6683
                       Mean reward: 2.96
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.5879
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.23s
                      Time elapsed: 00:03:19
                               ETA: 01:16:57

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 40181 steps/s (collection: 2.272s, learning 0.175s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0309
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.7160
                       Mean reward: 3.10
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 0.0118
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.45s
                      Time elapsed: 00:03:22
                               ETA: 01:16:55

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 48178 steps/s (collection: 1.915s, learning 0.125s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0555
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 30.7716
                       Mean reward: 2.88
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 19.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.04s
                      Time elapsed: 00:03:24
                               ETA: 01:16:45

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 49767 steps/s (collection: 1.888s, learning 0.087s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0556
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.7818
                       Mean reward: 3.10
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 20.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.98s
                      Time elapsed: 00:03:26
                               ETA: 01:16:33

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 50433 steps/s (collection: 1.815s, learning 0.134s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1665
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.8046
                       Mean reward: 2.28
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.6107
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.95s
                      Time elapsed: 00:03:28
                               ETA: 01:16:20

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 48232 steps/s (collection: 1.904s, learning 0.134s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0678
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.8242
                       Mean reward: 2.38
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: -0.0366
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.04s
                      Time elapsed: 00:03:30
                               ETA: 01:16:10

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 50095 steps/s (collection: 1.838s, learning 0.125s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.8761
                       Mean reward: 2.66
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.6088
     Episode_Reward/lifting_object: 0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.96s
                      Time elapsed: 00:03:32
                               ETA: 01:15:59

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 47918 steps/s (collection: 1.888s, learning 0.163s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1725
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.9084
                       Mean reward: 2.99
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 0.6010
     Episode_Reward/lifting_object: -0.0191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.05s
                      Time elapsed: 00:03:34
                               ETA: 01:15:49

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 49329 steps/s (collection: 1.830s, learning 0.163s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0783
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.9188
                       Mean reward: 3.14
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.6300
     Episode_Reward/lifting_object: -0.0249
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.99s
                      Time elapsed: 00:03:36
                               ETA: 01:15:39

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 47030 steps/s (collection: 1.938s, learning 0.153s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0415
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.9615
                       Mean reward: 2.92
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.6326
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.09s
                      Time elapsed: 00:03:38
                               ETA: 01:15:30

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 38042 steps/s (collection: 2.442s, learning 0.143s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0875
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.0217
                       Mean reward: 2.61
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.6235
     Episode_Reward/lifting_object: -0.0322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.58s
                      Time elapsed: 00:03:40
                               ETA: 01:15:32

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 44231 steps/s (collection: 2.034s, learning 0.188s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3165
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.0448
                       Mean reward: 3.68
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: 0.0233
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.22s
                      Time elapsed: 00:03:43
                               ETA: 01:15:27

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 48295 steps/s (collection: 1.921s, learning 0.115s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0613
                       Mean reward: 3.14
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.6451
     Episode_Reward/lifting_object: 0.0362
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.04s
                      Time elapsed: 00:03:45
                               ETA: 01:15:18

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 48042 steps/s (collection: 1.929s, learning 0.117s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1038
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.0893
                       Mean reward: 3.19
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 0.6445
     Episode_Reward/lifting_object: 0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 19.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.05s
                      Time elapsed: 00:03:47
                               ETA: 01:15:09

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 47867 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.8683
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.1180
                       Mean reward: 2.96
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.6437
     Episode_Reward/lifting_object: 0.0052
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.05s
                      Time elapsed: 00:03:49
                               ETA: 01:15:00

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 51115 steps/s (collection: 1.830s, learning 0.094s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1127
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.1293
                       Mean reward: 3.29
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.6559
     Episode_Reward/lifting_object: -0.0378
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.92s
                      Time elapsed: 00:03:51
                               ETA: 01:14:49

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 49323 steps/s (collection: 1.877s, learning 0.116s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.1702
                       Mean reward: 3.16
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 0.6414
     Episode_Reward/lifting_object: -0.0115
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.99s
                      Time elapsed: 00:03:53
                               ETA: 01:14:40

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 50532 steps/s (collection: 1.849s, learning 0.096s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.8274
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.2233
                       Mean reward: 3.30
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.6714
     Episode_Reward/lifting_object: 0.0044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.95s
                      Time elapsed: 00:03:55
                               ETA: 01:14:30

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 51629 steps/s (collection: 1.811s, learning 0.093s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1229
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.2423
                       Mean reward: 2.98
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 0.6548
     Episode_Reward/lifting_object: -0.0214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.90s
                      Time elapsed: 00:03:57
                               ETA: 01:14:19

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 42276 steps/s (collection: 2.036s, learning 0.290s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.3028
                       Mean reward: 3.31
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 0.6744
     Episode_Reward/lifting_object: 0.0134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.33s
                      Time elapsed: 00:03:59
                               ETA: 01:14:16

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 40647 steps/s (collection: 2.237s, learning 0.181s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1612
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.3590
                       Mean reward: 2.29
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: -0.0769
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.42s
                      Time elapsed: 00:04:01
                               ETA: 01:14:15

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 42503 steps/s (collection: 2.215s, learning 0.098s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1215
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.3814
                       Mean reward: 3.32
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.6738
     Episode_Reward/lifting_object: -0.0469
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.31s
                      Time elapsed: 00:04:04
                               ETA: 01:14:12

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 44658 steps/s (collection: 2.088s, learning 0.113s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0738
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.4345
                       Mean reward: 3.41
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.6770
     Episode_Reward/lifting_object: -0.0100
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.20s
                      Time elapsed: 00:04:06
                               ETA: 01:14:07

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 50115 steps/s (collection: 1.854s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0583
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.5045
                       Mean reward: 3.64
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.6718
     Episode_Reward/lifting_object: 0.0143
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.96s
                      Time elapsed: 00:04:08
                               ETA: 01:13:58

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 45507 steps/s (collection: 2.043s, learning 0.117s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1012
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.5339
                       Mean reward: 3.06
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 0.6819
     Episode_Reward/lifting_object: -0.0298
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.16s
                      Time elapsed: 00:04:10
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 46495 steps/s (collection: 2.002s, learning 0.112s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0992
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.5455
                       Mean reward: 3.44
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.6837
     Episode_Reward/lifting_object: 0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.11s
                      Time elapsed: 00:04:12
                               ETA: 01:13:46

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 47206 steps/s (collection: 1.982s, learning 0.101s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.3155
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 31.5688
                       Mean reward: 3.12
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 0.6719
     Episode_Reward/lifting_object: 0.0101
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.08s
                      Time elapsed: 00:04:14
                               ETA: 01:13:39

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 49801 steps/s (collection: 1.881s, learning 0.093s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0647
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.5774
                       Mean reward: 3.17
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.6713
     Episode_Reward/lifting_object: -0.0070
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.97s
                      Time elapsed: 00:04:16
                               ETA: 01:13:31

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 49278 steps/s (collection: 1.896s, learning 0.099s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0745
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.6073
                       Mean reward: 3.42
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.6557
     Episode_Reward/lifting_object: 0.0094
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.99s
                      Time elapsed: 00:04:18
                               ETA: 01:13:23

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 49069 steps/s (collection: 1.912s, learning 0.092s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2618
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.6551
                       Mean reward: 3.38
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 0.0008
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.00s
                      Time elapsed: 00:04:20
                               ETA: 01:13:15

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 51853 steps/s (collection: 1.806s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0639
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.6661
                       Mean reward: 3.52
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.6679
     Episode_Reward/lifting_object: 0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.90s
                      Time elapsed: 00:04:22
                               ETA: 01:13:05

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 52380 steps/s (collection: 1.778s, learning 0.099s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 31.6812
                       Mean reward: 3.45
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 0.0226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.88s
                      Time elapsed: 00:04:24
                               ETA: 01:12:56

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 50093 steps/s (collection: 1.860s, learning 0.103s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0588
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.6975
                       Mean reward: 3.37
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 0.0088
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.96s
                      Time elapsed: 00:04:26
                               ETA: 01:12:48

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 51654 steps/s (collection: 1.812s, learning 0.092s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0425
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 31.7691
                       Mean reward: 3.09
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: -0.0252
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 18.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.90s
                      Time elapsed: 00:04:28
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 51185 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0703
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 31.8492
                       Mean reward: 3.34
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.7072
     Episode_Reward/lifting_object: -0.0042
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 18.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.92s
                      Time elapsed: 00:04:30
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 50136 steps/s (collection: 1.856s, learning 0.105s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0647
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.8856
                       Mean reward: 3.49
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 0.7045
     Episode_Reward/lifting_object: 0.0009
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.96s
                      Time elapsed: 00:04:32
                               ETA: 01:12:22

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 50788 steps/s (collection: 1.848s, learning 0.088s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0564
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.9424
                       Mean reward: 3.42
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 0.6961
     Episode_Reward/lifting_object: 0.0357
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.94s
                      Time elapsed: 00:04:34
                               ETA: 01:12:14

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 50725 steps/s (collection: 1.840s, learning 0.098s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.6043
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.9768
                       Mean reward: 3.32
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.6984
     Episode_Reward/lifting_object: 0.0197
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.94s
                      Time elapsed: 00:04:36
                               ETA: 01:12:06

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 42475 steps/s (collection: 2.166s, learning 0.148s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1382
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.9955
                       Mean reward: 3.47
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 0.0160
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.31s
                      Time elapsed: 00:04:38
                               ETA: 01:12:04

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 40535 steps/s (collection: 2.269s, learning 0.157s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0372
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 32.0471
                       Mean reward: 3.24
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 0.6916
     Episode_Reward/lifting_object: 0.0058
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.43s
                      Time elapsed: 00:04:40
                               ETA: 01:12:03

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 49672 steps/s (collection: 1.871s, learning 0.108s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1296
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 32.1081
                       Mean reward: 3.01
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.7016
     Episode_Reward/lifting_object: -0.0080
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.98s
                      Time elapsed: 00:04:42
                               ETA: 01:11:56

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 45732 steps/s (collection: 2.015s, learning 0.135s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1378
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.1551
                       Mean reward: 3.67
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.7270
     Episode_Reward/lifting_object: -0.0136
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.15s
                      Time elapsed: 00:04:44
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 51874 steps/s (collection: 1.804s, learning 0.091s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1286
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.1971
                       Mean reward: 3.58
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.7001
     Episode_Reward/lifting_object: 0.0524
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.90s
                      Time elapsed: 00:04:46
                               ETA: 01:11:43

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 51636 steps/s (collection: 1.808s, learning 0.096s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.5886
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 32.2172
                       Mean reward: 3.89
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 0.0552
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.90s
                      Time elapsed: 00:04:48
                               ETA: 01:11:35

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 51240 steps/s (collection: 1.826s, learning 0.092s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0751
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.2257
                       Mean reward: 3.98
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.7335
     Episode_Reward/lifting_object: 0.0128
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.92s
                      Time elapsed: 00:04:50
                               ETA: 01:11:27

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 52043 steps/s (collection: 1.788s, learning 0.101s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0099
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.2522
                       Mean reward: 3.63
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7298
     Episode_Reward/lifting_object: -0.0026
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.89s
                      Time elapsed: 00:04:52
                               ETA: 01:11:19

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 53362 steps/s (collection: 1.755s, learning 0.087s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1110
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.2764
                       Mean reward: 3.68
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 0.0068
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.84s
                      Time elapsed: 00:04:54
                               ETA: 01:11:11

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 52302 steps/s (collection: 1.792s, learning 0.087s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1210
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.3030
                       Mean reward: 3.72
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 0.7240
     Episode_Reward/lifting_object: -0.0505
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.88s
                      Time elapsed: 00:04:56
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 52720 steps/s (collection: 1.772s, learning 0.093s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1402
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.3578
                       Mean reward: 3.91
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.7214
     Episode_Reward/lifting_object: 0.0425
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.86s
                      Time elapsed: 00:04:58
                               ETA: 01:10:54

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51387 steps/s (collection: 1.821s, learning 0.092s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0828
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.4019
                       Mean reward: 3.45
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 0.7313
     Episode_Reward/lifting_object: 0.0363
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.91s
                      Time elapsed: 00:04:59
                               ETA: 01:10:47

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51690 steps/s (collection: 1.812s, learning 0.089s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1433
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 32.4447
                       Mean reward: 3.59
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 0.0254
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.90s
                      Time elapsed: 00:05:01
                               ETA: 01:10:39

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 50959 steps/s (collection: 1.822s, learning 0.107s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.6922
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.4942
                       Mean reward: 4.13
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 0.7353
     Episode_Reward/lifting_object: 0.0276
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.93s
                      Time elapsed: 00:05:03
                               ETA: 01:10:32

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 51450 steps/s (collection: 1.821s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.2921
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 32.5401
                       Mean reward: 2.97
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.7199
     Episode_Reward/lifting_object: 0.0263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.91s
                      Time elapsed: 00:05:05
                               ETA: 01:10:25

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 50686 steps/s (collection: 1.828s, learning 0.111s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4078
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.5760
                       Mean reward: 3.78
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.7219
     Episode_Reward/lifting_object: 0.0347
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.94s
                      Time elapsed: 00:05:07
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 51949 steps/s (collection: 1.788s, learning 0.104s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1782
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.6142
                       Mean reward: 3.90
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 0.0503
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.89s
                      Time elapsed: 00:05:09
                               ETA: 01:10:11

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 51195 steps/s (collection: 1.816s, learning 0.104s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2288
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.6576
                       Mean reward: 2.79
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 0.0421
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.92s
                      Time elapsed: 00:05:11
                               ETA: 01:10:04

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 50164 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.5504
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.6757
                       Mean reward: 3.59
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 0.0713
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.96s
                      Time elapsed: 00:05:13
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 51564 steps/s (collection: 1.808s, learning 0.098s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.6512
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 32.6954
                       Mean reward: 4.27
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 0.1385
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.91s
                      Time elapsed: 00:05:15
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 51396 steps/s (collection: 1.822s, learning 0.091s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2417
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 32.7468
                       Mean reward: 4.26
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.7120
     Episode_Reward/lifting_object: 0.1318
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.91s
                      Time elapsed: 00:05:17
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 51248 steps/s (collection: 1.830s, learning 0.088s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3544
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 32.7727
                       Mean reward: 4.06
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 0.0077
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.92s
                      Time elapsed: 00:05:19
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 51767 steps/s (collection: 1.810s, learning 0.089s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2838
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.7845
                       Mean reward: 4.01
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.7156
     Episode_Reward/lifting_object: 0.0896
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.90s
                      Time elapsed: 00:05:21
                               ETA: 01:09:31

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 50424 steps/s (collection: 1.855s, learning 0.095s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2724
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 32.8088
                       Mean reward: 4.70
               Mean episode length: 237.41
    Episode_Reward/reaching_object: 0.7268
     Episode_Reward/lifting_object: 0.1637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.95s
                      Time elapsed: 00:05:23
                               ETA: 01:09:25

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 51040 steps/s (collection: 1.830s, learning 0.096s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.5538
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 32.8426
                       Mean reward: 3.97
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.7346
     Episode_Reward/lifting_object: 0.0576
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.93s
                      Time elapsed: 00:05:24
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49665 steps/s (collection: 1.879s, learning 0.101s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.5419
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.8887
                       Mean reward: 4.85
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: 0.1889
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.98s
                      Time elapsed: 00:05:26
                               ETA: 01:09:13

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 50038 steps/s (collection: 1.872s, learning 0.093s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4641
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 32.9211
                       Mean reward: 3.88
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 0.7322
     Episode_Reward/lifting_object: 0.1523
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.96s
                      Time elapsed: 00:05:28
                               ETA: 01:09:07

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49634 steps/s (collection: 1.880s, learning 0.101s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.6762
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 32.9572
                       Mean reward: 4.31
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 0.7061
     Episode_Reward/lifting_object: 0.1263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.98s
                      Time elapsed: 00:05:30
                               ETA: 01:09:02

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 50740 steps/s (collection: 1.846s, learning 0.091s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.5636
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.0011
                       Mean reward: 4.90
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 0.7082
     Episode_Reward/lifting_object: 0.1340
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.94s
                      Time elapsed: 00:05:32
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 50953 steps/s (collection: 1.833s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.9767
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.0425
                       Mean reward: 5.07
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: 0.2310
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.93s
                      Time elapsed: 00:05:34
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 49291 steps/s (collection: 1.869s, learning 0.125s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.7801
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.1057
                       Mean reward: 5.55
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 0.2700
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.99s
                      Time elapsed: 00:05:36
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 45810 steps/s (collection: 1.998s, learning 0.148s)
             Mean action noise std: 1.53
          Mean value_function loss: 1.1686
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 33.1429
                       Mean reward: 4.11
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.6846
     Episode_Reward/lifting_object: 0.1983
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.15s
                      Time elapsed: 00:05:38
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 46592 steps/s (collection: 1.998s, learning 0.111s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.9438
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.1661
                       Mean reward: 4.27
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 0.7096
     Episode_Reward/lifting_object: 0.2232
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.11s
                      Time elapsed: 00:05:40
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 45072 steps/s (collection: 2.092s, learning 0.089s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.8057
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.1982
                       Mean reward: 5.73
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 0.7018
     Episode_Reward/lifting_object: 0.3463
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.18s
                      Time elapsed: 00:05:43
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 48586 steps/s (collection: 1.935s, learning 0.089s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.0019
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 33.2518
                       Mean reward: 5.28
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 0.7163
     Episode_Reward/lifting_object: 0.3079
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.02s
                      Time elapsed: 00:05:45
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 49613 steps/s (collection: 1.876s, learning 0.105s)
             Mean action noise std: 1.55
          Mean value_function loss: 1.9745
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.3053
                       Mean reward: 4.88
               Mean episode length: 219.46
    Episode_Reward/reaching_object: 0.6806
     Episode_Reward/lifting_object: 0.3960
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.98s
                      Time elapsed: 00:05:47
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49829 steps/s (collection: 1.876s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 2.3687
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.3461
                       Mean reward: 5.32
               Mean episode length: 223.83
    Episode_Reward/reaching_object: 0.6750
     Episode_Reward/lifting_object: 0.4382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.97s
                      Time elapsed: 00:05:49
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 51600 steps/s (collection: 1.810s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.3856
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 33.3798
                       Mean reward: 4.56
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 0.6448
     Episode_Reward/lifting_object: 0.3364
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.91s
                      Time elapsed: 00:05:51
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 50990 steps/s (collection: 1.838s, learning 0.090s)
             Mean action noise std: 1.56
          Mean value_function loss: 2.1267
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.4410
                       Mean reward: 4.81
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 0.6513
     Episode_Reward/lifting_object: 0.5552
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.93s
                      Time elapsed: 00:05:52
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 51175 steps/s (collection: 1.831s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.4680
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.4962
                       Mean reward: 5.24
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 0.4631
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.92s
                      Time elapsed: 00:05:54
                               ETA: 01:08:03

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 50583 steps/s (collection: 1.840s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.2626
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.5538
                       Mean reward: 4.66
               Mean episode length: 199.83
    Episode_Reward/reaching_object: 0.5863
     Episode_Reward/lifting_object: 0.5263
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.94s
                      Time elapsed: 00:05:56
                               ETA: 01:07:58

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 48474 steps/s (collection: 1.929s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.6343
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.6000
                       Mean reward: 5.47
               Mean episode length: 209.06
    Episode_Reward/reaching_object: 0.6344
     Episode_Reward/lifting_object: 0.5731
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.03s
                      Time elapsed: 00:05:58
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49248 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 3.2850
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.6385
                       Mean reward: 6.22
               Mean episode length: 204.08
    Episode_Reward/reaching_object: 0.5804
     Episode_Reward/lifting_object: 0.6130
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.00s
                      Time elapsed: 00:06:00
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 45401 steps/s (collection: 2.022s, learning 0.143s)
             Mean action noise std: 1.58
          Mean value_function loss: 2.0526
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 33.6872
                       Mean reward: 4.40
               Mean episode length: 213.22
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 0.6335
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.17s
                      Time elapsed: 00:06:03
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 47924 steps/s (collection: 1.957s, learning 0.095s)
             Mean action noise std: 1.59
          Mean value_function loss: 3.4182
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.7300
                       Mean reward: 6.40
               Mean episode length: 211.56
    Episode_Reward/reaching_object: 0.6070
     Episode_Reward/lifting_object: 0.7028
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.05s
                      Time elapsed: 00:06:05
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 47801 steps/s (collection: 1.954s, learning 0.102s)
             Mean action noise std: 1.59
          Mean value_function loss: 4.4838
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 33.7777
                       Mean reward: 5.70
               Mean episode length: 214.10
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 0.5264
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.06s
                      Time elapsed: 00:06:07
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 49688 steps/s (collection: 1.883s, learning 0.096s)
             Mean action noise std: 1.59
          Mean value_function loss: 3.2155
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.8161
                       Mean reward: 6.67
               Mean episode length: 219.51
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 0.7642
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.98s
                      Time elapsed: 00:06:09
                               ETA: 01:07:33

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 48874 steps/s (collection: 1.899s, learning 0.112s)
             Mean action noise std: 1.59
          Mean value_function loss: 3.1921
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.8422
                       Mean reward: 7.22
               Mean episode length: 224.68
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 0.9221
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.01s
                      Time elapsed: 00:06:11
                               ETA: 01:07:29

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 48474 steps/s (collection: 1.940s, learning 0.088s)
             Mean action noise std: 1.60
          Mean value_function loss: 3.1253
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.8837
                       Mean reward: 6.86
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 0.6135
     Episode_Reward/lifting_object: 0.9910
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.03s
                      Time elapsed: 00:06:13
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 47979 steps/s (collection: 1.947s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 2.9762
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.9327
                       Mean reward: 8.14
               Mean episode length: 202.88
    Episode_Reward/reaching_object: 0.5871
     Episode_Reward/lifting_object: 1.1214
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.05s
                      Time elapsed: 00:06:15
                               ETA: 01:07:21

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 44989 steps/s (collection: 2.089s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 4.5659
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.9674
                       Mean reward: 7.98
               Mean episode length: 203.55
    Episode_Reward/reaching_object: 0.5753
     Episode_Reward/lifting_object: 1.1427
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.19s
                      Time elapsed: 00:06:17
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 39065 steps/s (collection: 2.282s, learning 0.234s)
             Mean action noise std: 1.61
          Mean value_function loss: 5.3393
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.0130
                       Mean reward: 8.19
               Mean episode length: 193.17
    Episode_Reward/reaching_object: 0.5380
     Episode_Reward/lifting_object: 0.9570
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.52s
                      Time elapsed: 00:06:19
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 46037 steps/s (collection: 2.023s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 3.9255
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.0481
                       Mean reward: 8.08
               Mean episode length: 193.45
    Episode_Reward/reaching_object: 0.5150
     Episode_Reward/lifting_object: 0.9857
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.14s
                      Time elapsed: 00:06:22
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 39459 steps/s (collection: 2.391s, learning 0.100s)
             Mean action noise std: 1.62
          Mean value_function loss: 5.5381
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.0695
                       Mean reward: 8.63
               Mean episode length: 198.77
    Episode_Reward/reaching_object: 0.5125
     Episode_Reward/lifting_object: 1.0244
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.49s
                      Time elapsed: 00:06:24
                               ETA: 01:07:17

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 42546 steps/s (collection: 2.183s, learning 0.128s)
             Mean action noise std: 1.62
          Mean value_function loss: 4.3759
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.1022
                       Mean reward: 7.02
               Mean episode length: 181.70
    Episode_Reward/reaching_object: 0.4964
     Episode_Reward/lifting_object: 1.0435
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.31s
                      Time elapsed: 00:06:26
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 47429 steps/s (collection: 1.978s, learning 0.094s)
             Mean action noise std: 1.62
          Mean value_function loss: 4.4207
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.1317
                       Mean reward: 9.34
               Mean episode length: 194.09
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: 1.3070
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.07s
                      Time elapsed: 00:06:28
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 47792 steps/s (collection: 1.950s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 4.6826
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.1550
                       Mean reward: 7.72
               Mean episode length: 185.97
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: 1.3069
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.06s
                      Time elapsed: 00:06:30
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 49621 steps/s (collection: 1.882s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.7083
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.1942
                       Mean reward: 7.15
               Mean episode length: 190.25
    Episode_Reward/reaching_object: 0.4962
     Episode_Reward/lifting_object: 1.1832
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.98s
                      Time elapsed: 00:06:32
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 49965 steps/s (collection: 1.866s, learning 0.101s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.9037
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.2156
                       Mean reward: 10.97
               Mean episode length: 201.76
    Episode_Reward/reaching_object: 0.5053
     Episode_Reward/lifting_object: 1.3433
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.97s
                      Time elapsed: 00:06:34
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 48851 steps/s (collection: 1.908s, learning 0.105s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.6976
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 34.2310
                       Mean reward: 9.70
               Mean episode length: 193.33
    Episode_Reward/reaching_object: 0.5138
     Episode_Reward/lifting_object: 1.4838
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.01s
                      Time elapsed: 00:06:36
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 49059 steps/s (collection: 1.899s, learning 0.105s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.9397
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.2584
                       Mean reward: 12.04
               Mean episode length: 201.86
    Episode_Reward/reaching_object: 0.5063
     Episode_Reward/lifting_object: 1.6497
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.00s
                      Time elapsed: 00:06:38
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 48518 steps/s (collection: 1.914s, learning 0.112s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.5784
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.2886
                       Mean reward: 12.55
               Mean episode length: 201.02
    Episode_Reward/reaching_object: 0.5136
     Episode_Reward/lifting_object: 1.8419
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.03s
                      Time elapsed: 00:06:40
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 47978 steps/s (collection: 1.946s, learning 0.103s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.1341
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.3015
                       Mean reward: 9.91
               Mean episode length: 186.78
    Episode_Reward/reaching_object: 0.4792
     Episode_Reward/lifting_object: 1.5856
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.05s
                      Time elapsed: 00:06:43
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 48143 steps/s (collection: 1.943s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.5027
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.3128
                       Mean reward: 11.81
               Mean episode length: 178.97
    Episode_Reward/reaching_object: 0.4626
     Episode_Reward/lifting_object: 1.6188
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.04s
                      Time elapsed: 00:06:45
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 48138 steps/s (collection: 1.947s, learning 0.095s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.3938
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.3299
                       Mean reward: 8.63
               Mean episode length: 171.68
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 1.5201
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.04s
                      Time elapsed: 00:06:47
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 47597 steps/s (collection: 1.973s, learning 0.093s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.7697
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.3526
                       Mean reward: 10.48
               Mean episode length: 157.69
    Episode_Reward/reaching_object: 0.4141
     Episode_Reward/lifting_object: 1.7160
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.07s
                      Time elapsed: 00:06:49
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 48252 steps/s (collection: 1.940s, learning 0.097s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.3665
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.3765
                       Mean reward: 8.17
               Mean episode length: 169.59
    Episode_Reward/reaching_object: 0.3990
     Episode_Reward/lifting_object: 1.4362
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.04s
                      Time elapsed: 00:06:51
                               ETA: 01:06:28

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 49208 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 8.9069
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.3884
                       Mean reward: 10.74
               Mean episode length: 161.41
    Episode_Reward/reaching_object: 0.3919
     Episode_Reward/lifting_object: 1.6844
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.00s
                      Time elapsed: 00:06:53
                               ETA: 01:06:24

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 48632 steps/s (collection: 1.926s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 6.2034
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.4036
                       Mean reward: 12.59
               Mean episode length: 162.57
    Episode_Reward/reaching_object: 0.3834
     Episode_Reward/lifting_object: 1.7668
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.02s
                      Time elapsed: 00:06:55
                               ETA: 01:06:20

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 48511 steps/s (collection: 1.933s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 7.5393
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.4193
                       Mean reward: 10.31
               Mean episode length: 161.34
    Episode_Reward/reaching_object: 0.3753
     Episode_Reward/lifting_object: 1.7609
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.03s
                      Time elapsed: 00:06:57
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 47618 steps/s (collection: 1.966s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 10.4893
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.4355
                       Mean reward: 10.64
               Mean episode length: 155.44
    Episode_Reward/reaching_object: 0.3605
     Episode_Reward/lifting_object: 1.5864
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.06s
                      Time elapsed: 00:06:59
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 47623 steps/s (collection: 1.975s, learning 0.089s)
             Mean action noise std: 1.65
          Mean value_function loss: 8.0123
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.4580
                       Mean reward: 10.40
               Mean episode length: 146.23
    Episode_Reward/reaching_object: 0.3531
     Episode_Reward/lifting_object: 1.7999
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.06s
                      Time elapsed: 00:07:01
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 47352 steps/s (collection: 1.965s, learning 0.111s)
             Mean action noise std: 1.65
          Mean value_function loss: 8.9006
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.4821
                       Mean reward: 9.30
               Mean episode length: 160.53
    Episode_Reward/reaching_object: 0.3552
     Episode_Reward/lifting_object: 1.4489
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.08s
                      Time elapsed: 00:07:03
                               ETA: 01:06:06

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 47446 steps/s (collection: 1.981s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 7.4844
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.5025
                       Mean reward: 8.94
               Mean episode length: 154.66
    Episode_Reward/reaching_object: 0.3596
     Episode_Reward/lifting_object: 1.6100
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.07s
                      Time elapsed: 00:07:05
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 49199 steps/s (collection: 1.911s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 8.7259
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.5199
                       Mean reward: 10.35
               Mean episode length: 128.80
    Episode_Reward/reaching_object: 0.3459
     Episode_Reward/lifting_object: 1.9055
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.00s
                      Time elapsed: 00:07:07
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 48653 steps/s (collection: 1.929s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 8.2387
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.5324
                       Mean reward: 12.20
               Mean episode length: 140.08
    Episode_Reward/reaching_object: 0.3454
     Episode_Reward/lifting_object: 1.9054
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.02s
                      Time elapsed: 00:07:09
                               ETA: 01:05:55

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 48324 steps/s (collection: 1.928s, learning 0.107s)
             Mean action noise std: 1.66
          Mean value_function loss: 8.2610
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.5483
                       Mean reward: 9.16
               Mean episode length: 134.66
    Episode_Reward/reaching_object: 0.3157
     Episode_Reward/lifting_object: 1.7285
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.03s
                      Time elapsed: 00:07:11
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 48469 steps/s (collection: 1.918s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 7.8528
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.5611
                       Mean reward: 11.39
               Mean episode length: 134.50
    Episode_Reward/reaching_object: 0.3421
     Episode_Reward/lifting_object: 1.9537
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.03s
                      Time elapsed: 00:07:13
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 44858 steps/s (collection: 2.070s, learning 0.122s)
             Mean action noise std: 1.66
          Mean value_function loss: 8.8727
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.5753
                       Mean reward: 15.56
               Mean episode length: 143.03
    Episode_Reward/reaching_object: 0.3566
     Episode_Reward/lifting_object: 2.4227
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.19s
                      Time elapsed: 00:07:15
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 45916 steps/s (collection: 2.028s, learning 0.113s)
             Mean action noise std: 1.66
          Mean value_function loss: 7.5507
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.5922
                       Mean reward: 10.94
               Mean episode length: 153.93
    Episode_Reward/reaching_object: 0.3595
     Episode_Reward/lifting_object: 2.1815
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.14s
                      Time elapsed: 00:07:17
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 41071 steps/s (collection: 2.131s, learning 0.262s)
             Mean action noise std: 1.67
          Mean value_function loss: 8.8791
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.6128
                       Mean reward: 13.84
               Mean episode length: 162.89
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 2.2955
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.39s
                      Time elapsed: 00:07:20
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 42854 steps/s (collection: 2.197s, learning 0.097s)
             Mean action noise std: 1.67
          Mean value_function loss: 10.6013
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.6366
                       Mean reward: 14.47
               Mean episode length: 160.89
    Episode_Reward/reaching_object: 0.3759
     Episode_Reward/lifting_object: 2.2363
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.29s
                      Time elapsed: 00:07:22
                               ETA: 01:05:41

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 47367 steps/s (collection: 1.976s, learning 0.099s)
             Mean action noise std: 1.67
          Mean value_function loss: 8.9976
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.6551
                       Mean reward: 16.38
               Mean episode length: 168.92
    Episode_Reward/reaching_object: 0.3700
     Episode_Reward/lifting_object: 2.1681
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.08s
                      Time elapsed: 00:07:24
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 46342 steps/s (collection: 2.008s, learning 0.113s)
             Mean action noise std: 1.67
          Mean value_function loss: 9.3016
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.6816
                       Mean reward: 12.93
               Mean episode length: 173.59
    Episode_Reward/reaching_object: 0.3921
     Episode_Reward/lifting_object: 2.5311
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.12s
                      Time elapsed: 00:07:26
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 40181 steps/s (collection: 2.330s, learning 0.116s)
             Mean action noise std: 1.67
          Mean value_function loss: 8.6427
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.7070
                       Mean reward: 14.50
               Mean episode length: 162.97
    Episode_Reward/reaching_object: 0.3628
     Episode_Reward/lifting_object: 2.1510
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.45s
                      Time elapsed: 00:07:29
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 39785 steps/s (collection: 2.335s, learning 0.136s)
             Mean action noise std: 1.68
          Mean value_function loss: 7.8819
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.7274
                       Mean reward: 14.97
               Mean episode length: 180.37
    Episode_Reward/reaching_object: 0.3715
     Episode_Reward/lifting_object: 2.4546
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.47s
                      Time elapsed: 00:07:31
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 39152 steps/s (collection: 2.377s, learning 0.134s)
             Mean action noise std: 1.68
          Mean value_function loss: 7.3698
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.7418
                       Mean reward: 15.72
               Mean episode length: 174.03
    Episode_Reward/reaching_object: 0.3601
     Episode_Reward/lifting_object: 2.6611
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.51s
                      Time elapsed: 00:07:34
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 38267 steps/s (collection: 2.457s, learning 0.112s)
             Mean action noise std: 1.68
          Mean value_function loss: 9.3679
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7522
                       Mean reward: 9.68
               Mean episode length: 176.11
    Episode_Reward/reaching_object: 0.3608
     Episode_Reward/lifting_object: 2.2901
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.57s
                      Time elapsed: 00:07:36
                               ETA: 01:05:37

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 34357 steps/s (collection: 2.626s, learning 0.236s)
             Mean action noise std: 1.68
          Mean value_function loss: 7.8036
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.7654
                       Mean reward: 13.17
               Mean episode length: 164.68
    Episode_Reward/reaching_object: 0.3476
     Episode_Reward/lifting_object: 2.4410
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.86s
                      Time elapsed: 00:07:39
                               ETA: 01:05:41

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 36396 steps/s (collection: 2.594s, learning 0.107s)
             Mean action noise std: 1.68
          Mean value_function loss: 8.4277
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.7846
                       Mean reward: 12.45
               Mean episode length: 173.69
    Episode_Reward/reaching_object: 0.3377
     Episode_Reward/lifting_object: 2.3570
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.70s
                      Time elapsed: 00:07:42
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 42522 steps/s (collection: 2.120s, learning 0.192s)
             Mean action noise std: 1.68
          Mean value_function loss: 9.6774
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8046
                       Mean reward: 13.61
               Mean episode length: 154.10
    Episode_Reward/reaching_object: 0.3342
     Episode_Reward/lifting_object: 2.7209
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.31s
                      Time elapsed: 00:07:44
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 34295 steps/s (collection: 2.617s, learning 0.250s)
             Mean action noise std: 1.69
          Mean value_function loss: 10.1141
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.8236
                       Mean reward: 13.63
               Mean episode length: 159.94
    Episode_Reward/reaching_object: 0.3363
     Episode_Reward/lifting_object: 2.4926
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.87s
                      Time elapsed: 00:07:47
                               ETA: 01:05:45

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 32324 steps/s (collection: 2.845s, learning 0.196s)
             Mean action noise std: 1.69
          Mean value_function loss: 9.2033
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8459
                       Mean reward: 15.04
               Mean episode length: 166.06
    Episode_Reward/reaching_object: 0.3299
     Episode_Reward/lifting_object: 2.3763
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 3.04s
                      Time elapsed: 00:07:50
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 35975 steps/s (collection: 2.542s, learning 0.191s)
             Mean action noise std: 1.69
          Mean value_function loss: 7.6093
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8664
                       Mean reward: 14.08
               Mean episode length: 159.91
    Episode_Reward/reaching_object: 0.3222
     Episode_Reward/lifting_object: 2.4294
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.73s
                      Time elapsed: 00:07:53
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 37829 steps/s (collection: 2.424s, learning 0.175s)
             Mean action noise std: 1.69
          Mean value_function loss: 7.9183
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.8792
                       Mean reward: 15.09
               Mean episode length: 158.95
    Episode_Reward/reaching_object: 0.3458
     Episode_Reward/lifting_object: 2.4366
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.60s
                      Time elapsed: 00:07:55
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 38847 steps/s (collection: 2.330s, learning 0.200s)
             Mean action noise std: 1.69
          Mean value_function loss: 7.5740
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8921
                       Mean reward: 15.83
               Mean episode length: 154.55
    Episode_Reward/reaching_object: 0.3219
     Episode_Reward/lifting_object: 2.5625
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.53s
                      Time elapsed: 00:07:58
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 40869 steps/s (collection: 2.297s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 10.0317
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.9097
                       Mean reward: 14.38
               Mean episode length: 150.08
    Episode_Reward/reaching_object: 0.3250
     Episode_Reward/lifting_object: 2.4350
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.41s
                      Time elapsed: 00:08:00
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 43316 steps/s (collection: 2.151s, learning 0.118s)
             Mean action noise std: 1.70
          Mean value_function loss: 9.7577
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9306
                       Mean reward: 12.60
               Mean episode length: 157.29
    Episode_Reward/reaching_object: 0.3212
     Episode_Reward/lifting_object: 2.5834
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.27s
                      Time elapsed: 00:08:03
                               ETA: 01:05:51

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 44803 steps/s (collection: 2.103s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 8.1999
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 34.9410
                       Mean reward: 13.69
               Mean episode length: 154.36
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: 2.4526
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.19s
                      Time elapsed: 00:08:05
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 45231 steps/s (collection: 2.046s, learning 0.127s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.7845
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 34.9426
                       Mean reward: 13.69
               Mean episode length: 166.61
    Episode_Reward/reaching_object: 0.3150
     Episode_Reward/lifting_object: 2.5059
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.17s
                      Time elapsed: 00:08:07
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 44745 steps/s (collection: 2.052s, learning 0.145s)
             Mean action noise std: 1.70
          Mean value_function loss: 9.2136
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.9454
                       Mean reward: 14.84
               Mean episode length: 158.65
    Episode_Reward/reaching_object: 0.2943
     Episode_Reward/lifting_object: 2.4788
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.20s
                      Time elapsed: 00:08:09
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 44842 steps/s (collection: 2.093s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.3969
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.9556
                       Mean reward: 13.44
               Mean episode length: 143.53
    Episode_Reward/reaching_object: 0.2822
     Episode_Reward/lifting_object: 2.4727
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.19s
                      Time elapsed: 00:08:11
                               ETA: 01:05:41

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 47800 steps/s (collection: 1.962s, learning 0.094s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.2330
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.9679
                       Mean reward: 14.17
               Mean episode length: 127.02
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 2.3678
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.06s
                      Time elapsed: 00:08:13
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 43550 steps/s (collection: 2.166s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.5440
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.9772
                       Mean reward: 14.49
               Mean episode length: 121.33
    Episode_Reward/reaching_object: 0.2698
     Episode_Reward/lifting_object: 2.4540
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.26s
                      Time elapsed: 00:08:16
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 48742 steps/s (collection: 1.921s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.4872
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.9819
                       Mean reward: 14.07
               Mean episode length: 136.10
    Episode_Reward/reaching_object: 0.2736
     Episode_Reward/lifting_object: 2.5468
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.02s
                      Time elapsed: 00:08:18
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 49068 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.4173
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9932
                       Mean reward: 14.44
               Mean episode length: 118.22
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: 2.7001
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.00s
                      Time elapsed: 00:08:20
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 48296 steps/s (collection: 1.925s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 12.2589
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.0105
                       Mean reward: 16.04
               Mean episode length: 130.64
    Episode_Reward/reaching_object: 0.2617
     Episode_Reward/lifting_object: 2.8197
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.04s
                      Time elapsed: 00:08:22
                               ETA: 01:05:25

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 48640 steps/s (collection: 1.899s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 11.5888
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.0289
                       Mean reward: 16.27
               Mean episode length: 141.00
    Episode_Reward/reaching_object: 0.2815
     Episode_Reward/lifting_object: 2.8771
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.02s
                      Time elapsed: 00:08:24
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 48147 steps/s (collection: 1.948s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 11.0242
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.0372
                       Mean reward: 18.05
               Mean episode length: 149.53
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 3.3612
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.04s
                      Time elapsed: 00:08:26
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 46460 steps/s (collection: 2.012s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 11.1839
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.0402
                       Mean reward: 19.54
               Mean episode length: 142.38
    Episode_Reward/reaching_object: 0.3055
     Episode_Reward/lifting_object: 3.5031
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.12s
                      Time elapsed: 00:08:28
                               ETA: 01:05:14

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 46548 steps/s (collection: 2.009s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 12.2945
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.0446
                       Mean reward: 18.33
               Mean episode length: 155.39
    Episode_Reward/reaching_object: 0.3158
     Episode_Reward/lifting_object: 3.5862
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.11s
                      Time elapsed: 00:08:30
                               ETA: 01:05:11

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 47340 steps/s (collection: 1.973s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 10.8210
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.0489
                       Mean reward: 20.04
               Mean episode length: 156.46
    Episode_Reward/reaching_object: 0.3344
     Episode_Reward/lifting_object: 3.5901
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.08s
                      Time elapsed: 00:08:32
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 47334 steps/s (collection: 1.959s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 10.0994
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.0515
                       Mean reward: 23.55
               Mean episode length: 157.51
    Episode_Reward/reaching_object: 0.3295
     Episode_Reward/lifting_object: 3.7172
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.08s
                      Time elapsed: 00:08:34
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 48012 steps/s (collection: 1.931s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 15.1815
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 35.0529
                       Mean reward: 20.03
               Mean episode length: 166.03
    Episode_Reward/reaching_object: 0.3326
     Episode_Reward/lifting_object: 3.5749
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.05s
                      Time elapsed: 00:08:36
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 47412 steps/s (collection: 1.967s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 13.4578
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.0533
                       Mean reward: 20.48
               Mean episode length: 152.47
    Episode_Reward/reaching_object: 0.3305
     Episode_Reward/lifting_object: 3.5729
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.07s
                      Time elapsed: 00:08:38
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 47078 steps/s (collection: 1.971s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 12.9106
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.0575
                       Mean reward: 19.52
               Mean episode length: 140.51
    Episode_Reward/reaching_object: 0.3060
     Episode_Reward/lifting_object: 3.6427
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.09s
                      Time elapsed: 00:08:40
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 40404 steps/s (collection: 2.226s, learning 0.207s)
             Mean action noise std: 1.71
          Mean value_function loss: 14.3988
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.0679
                       Mean reward: 18.99
               Mean episode length: 147.34
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 3.8132
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.43s
                      Time elapsed: 00:08:43
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 37728 steps/s (collection: 2.507s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 17.6828
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 35.0784
                       Mean reward: 21.84
               Mean episode length: 149.11
    Episode_Reward/reaching_object: 0.3106
     Episode_Reward/lifting_object: 3.8887
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.61s
                      Time elapsed: 00:08:45
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 38933 steps/s (collection: 2.393s, learning 0.132s)
             Mean action noise std: 1.71
          Mean value_function loss: 17.2444
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.0889
                       Mean reward: 20.83
               Mean episode length: 134.95
    Episode_Reward/reaching_object: 0.2849
     Episode_Reward/lifting_object: 3.7421
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.52s
                      Time elapsed: 00:08:48
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 39576 steps/s (collection: 2.300s, learning 0.184s)
             Mean action noise std: 1.71
          Mean value_function loss: 17.9774
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.0995
                       Mean reward: 22.01
               Mean episode length: 138.85
    Episode_Reward/reaching_object: 0.2938
     Episode_Reward/lifting_object: 4.1444
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.48s
                      Time elapsed: 00:08:50
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 37468 steps/s (collection: 2.435s, learning 0.188s)
             Mean action noise std: 1.71
          Mean value_function loss: 15.9837
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.1077
                       Mean reward: 21.50
               Mean episode length: 139.44
    Episode_Reward/reaching_object: 0.2913
     Episode_Reward/lifting_object: 4.1112
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.62s
                      Time elapsed: 00:08:53
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 40603 steps/s (collection: 2.292s, learning 0.129s)
             Mean action noise std: 1.71
          Mean value_function loss: 18.8278
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.1156
                       Mean reward: 25.82
               Mean episode length: 125.84
    Episode_Reward/reaching_object: 0.2888
     Episode_Reward/lifting_object: 4.4434
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.42s
                      Time elapsed: 00:08:56
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 41371 steps/s (collection: 2.229s, learning 0.147s)
             Mean action noise std: 1.71
          Mean value_function loss: 17.7655
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.1230
                       Mean reward: 21.90
               Mean episode length: 136.26
    Episode_Reward/reaching_object: 0.2936
     Episode_Reward/lifting_object: 4.4591
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.38s
                      Time elapsed: 00:08:58
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 34529 steps/s (collection: 2.663s, learning 0.184s)
             Mean action noise std: 1.71
          Mean value_function loss: 15.6244
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.1250
                       Mean reward: 24.70
               Mean episode length: 133.64
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 5.0390
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.85s
                      Time elapsed: 00:09:01
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 36788 steps/s (collection: 2.530s, learning 0.142s)
             Mean action noise std: 1.71
          Mean value_function loss: 16.3261
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 35.1258
                       Mean reward: 26.77
               Mean episode length: 126.86
    Episode_Reward/reaching_object: 0.2971
     Episode_Reward/lifting_object: 4.8393
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.67s
                      Time elapsed: 00:09:03
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 37377 steps/s (collection: 2.448s, learning 0.182s)
             Mean action noise std: 1.71
          Mean value_function loss: 15.5379
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.1271
                       Mean reward: 28.40
               Mean episode length: 144.98
    Episode_Reward/reaching_object: 0.3042
     Episode_Reward/lifting_object: 5.0947
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.63s
                      Time elapsed: 00:09:06
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 38069 steps/s (collection: 2.464s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 14.4974
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.1307
                       Mean reward: 23.53
               Mean episode length: 137.71
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: 4.8045
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.58s
                      Time elapsed: 00:09:09
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 45484 steps/s (collection: 2.043s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 16.5713
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.1358
                       Mean reward: 23.05
               Mean episode length: 123.49
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 4.2998
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.16s
                      Time elapsed: 00:09:11
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 45765 steps/s (collection: 2.032s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 15.7186
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.1425
                       Mean reward: 25.59
               Mean episode length: 128.27
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 4.6847
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.15s
                      Time elapsed: 00:09:13
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 46213 steps/s (collection: 1.999s, learning 0.128s)
             Mean action noise std: 1.72
          Mean value_function loss: 20.3039
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.1494
                       Mean reward: 23.66
               Mean episode length: 123.21
    Episode_Reward/reaching_object: 0.2894
     Episode_Reward/lifting_object: 5.0584
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.13s
                      Time elapsed: 00:09:15
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 45290 steps/s (collection: 2.060s, learning 0.110s)
             Mean action noise std: 1.72
          Mean value_function loss: 19.1067
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.1565
                       Mean reward: 26.72
               Mean episode length: 123.93
    Episode_Reward/reaching_object: 0.2731
     Episode_Reward/lifting_object: 4.7575
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.17s
                      Time elapsed: 00:09:17
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 46309 steps/s (collection: 2.023s, learning 0.100s)
             Mean action noise std: 1.72
          Mean value_function loss: 16.4330
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.1599
                       Mean reward: 24.07
               Mean episode length: 126.45
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 5.0900
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.12s
                      Time elapsed: 00:09:19
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 46305 steps/s (collection: 2.008s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 16.7961
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.1627
                       Mean reward: 31.08
               Mean episode length: 130.65
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 5.1818
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.12s
                      Time elapsed: 00:09:21
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 47168 steps/s (collection: 1.981s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 16.4897
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.1691
                       Mean reward: 31.57
               Mean episode length: 121.90
    Episode_Reward/reaching_object: 0.2846
     Episode_Reward/lifting_object: 5.3539
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.08s
                      Time elapsed: 00:09:24
                               ETA: 01:04:39

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 44028 steps/s (collection: 2.102s, learning 0.131s)
             Mean action noise std: 1.72
          Mean value_function loss: 17.3419
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 35.1753
                       Mean reward: 24.95
               Mean episode length: 115.53
    Episode_Reward/reaching_object: 0.2777
     Episode_Reward/lifting_object: 5.0436
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.23s
                      Time elapsed: 00:09:26
                               ETA: 01:04:37

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 42618 steps/s (collection: 2.179s, learning 0.128s)
             Mean action noise std: 1.72
          Mean value_function loss: 29.1956
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.1780
                       Mean reward: 27.04
               Mean episode length: 129.14
    Episode_Reward/reaching_object: 0.2737
     Episode_Reward/lifting_object: 5.2259
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.31s
                      Time elapsed: 00:09:28
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 45265 steps/s (collection: 2.069s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 19.8240
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.1808
                       Mean reward: 30.55
               Mean episode length: 119.14
    Episode_Reward/reaching_object: 0.2648
     Episode_Reward/lifting_object: 4.9502
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.17s
                      Time elapsed: 00:09:30
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 44896 steps/s (collection: 2.080s, learning 0.110s)
             Mean action noise std: 1.72
          Mean value_function loss: 18.8882
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.1849
                       Mean reward: 25.09
               Mean episode length: 115.57
    Episode_Reward/reaching_object: 0.2675
     Episode_Reward/lifting_object: 5.0185
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.19s
                      Time elapsed: 00:09:32
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 44905 steps/s (collection: 2.081s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 24.6374
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.1896
                       Mean reward: 30.66
               Mean episode length: 127.65
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 5.4885
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.19s
                      Time elapsed: 00:09:35
                               ETA: 01:04:28

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 45537 steps/s (collection: 2.061s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 24.6079
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2001
                       Mean reward: 27.65
               Mean episode length: 107.36
    Episode_Reward/reaching_object: 0.2598
     Episode_Reward/lifting_object: 5.2582
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.16s
                      Time elapsed: 00:09:37
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 47419 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 27.3325
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.2090
                       Mean reward: 27.67
               Mean episode length: 108.62
    Episode_Reward/reaching_object: 0.2501
     Episode_Reward/lifting_object: 5.0928
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.07s
                      Time elapsed: 00:09:39
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 46313 steps/s (collection: 2.020s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 21.6241
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.2135
                       Mean reward: 22.33
               Mean episode length: 103.08
    Episode_Reward/reaching_object: 0.2517
     Episode_Reward/lifting_object: 5.4981
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.12s
                      Time elapsed: 00:09:41
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 45781 steps/s (collection: 2.035s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 22.0662
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 35.2149
                       Mean reward: 25.66
               Mean episode length: 115.82
    Episode_Reward/reaching_object: 0.2575
     Episode_Reward/lifting_object: 5.6325
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.15s
                      Time elapsed: 00:09:43
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 46736 steps/s (collection: 1.986s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 20.8790
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 35.2162
                       Mean reward: 29.80
               Mean episode length: 119.97
    Episode_Reward/reaching_object: 0.2598
     Episode_Reward/lifting_object: 5.7053
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 32.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.10s
                      Time elapsed: 00:09:45
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 46369 steps/s (collection: 2.004s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 24.7145
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 35.2169
                       Mean reward: 30.92
               Mean episode length: 116.86
    Episode_Reward/reaching_object: 0.2647
     Episode_Reward/lifting_object: 5.8456
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.12s
                      Time elapsed: 00:09:47
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 45188 steps/s (collection: 2.080s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 22.9456
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.2171
                       Mean reward: 33.07
               Mean episode length: 117.99
    Episode_Reward/reaching_object: 0.2654
     Episode_Reward/lifting_object: 5.8995
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.18s
                      Time elapsed: 00:09:50
                               ETA: 01:04:08

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 44887 steps/s (collection: 2.087s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 25.0100
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.2193
                       Mean reward: 30.61
               Mean episode length: 108.27
    Episode_Reward/reaching_object: 0.2593
     Episode_Reward/lifting_object: 5.7254
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.19s
                      Time elapsed: 00:09:52
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 43110 steps/s (collection: 2.163s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 26.1059
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.2241
                       Mean reward: 27.19
               Mean episode length: 109.29
    Episode_Reward/reaching_object: 0.2585
     Episode_Reward/lifting_object: 6.0787
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.28s
                      Time elapsed: 00:09:54
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 41467 steps/s (collection: 2.253s, learning 0.118s)
             Mean action noise std: 1.73
          Mean value_function loss: 30.1195
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.2279
                       Mean reward: 31.14
               Mean episode length: 110.85
    Episode_Reward/reaching_object: 0.2490
     Episode_Reward/lifting_object: 5.7787
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.37s
                      Time elapsed: 00:09:56
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 41968 steps/s (collection: 2.230s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 24.8250
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.2336
                       Mean reward: 31.60
               Mean episode length: 107.45
    Episode_Reward/reaching_object: 0.2456
     Episode_Reward/lifting_object: 5.9759
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.34s
                      Time elapsed: 00:09:59
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 43079 steps/s (collection: 2.169s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 28.8365
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 35.2357
                       Mean reward: 37.01
               Mean episode length: 107.26
    Episode_Reward/reaching_object: 0.2503
     Episode_Reward/lifting_object: 6.4304
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.28s
                      Time elapsed: 00:10:01
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 42490 steps/s (collection: 2.190s, learning 0.124s)
             Mean action noise std: 1.73
          Mean value_function loss: 23.5592
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.2370
                       Mean reward: 29.84
               Mean episode length: 105.06
    Episode_Reward/reaching_object: 0.2472
     Episode_Reward/lifting_object: 6.1505
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.31s
                      Time elapsed: 00:10:03
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 43221 steps/s (collection: 2.161s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 26.8402
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.2409
                       Mean reward: 30.08
               Mean episode length: 102.11
    Episode_Reward/reaching_object: 0.2467
     Episode_Reward/lifting_object: 5.9959
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.27s
                      Time elapsed: 00:10:06
                               ETA: 01:03:56

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 40883 steps/s (collection: 2.275s, learning 0.130s)
             Mean action noise std: 1.73
          Mean value_function loss: 25.7832
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2455
                       Mean reward: 30.99
               Mean episode length: 106.56
    Episode_Reward/reaching_object: 0.2377
     Episode_Reward/lifting_object: 5.8469
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.40s
                      Time elapsed: 00:10:08
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 42253 steps/s (collection: 2.198s, learning 0.128s)
             Mean action noise std: 1.73
          Mean value_function loss: 26.8975
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.2510
                       Mean reward: 29.00
               Mean episode length: 99.77
    Episode_Reward/reaching_object: 0.2396
     Episode_Reward/lifting_object: 6.1797
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.33s
                      Time elapsed: 00:10:10
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 41821 steps/s (collection: 2.225s, learning 0.125s)
             Mean action noise std: 1.73
          Mean value_function loss: 26.2098
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2608
                       Mean reward: 29.54
               Mean episode length: 100.23
    Episode_Reward/reaching_object: 0.2326
     Episode_Reward/lifting_object: 6.0136
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.35s
                      Time elapsed: 00:10:13
                               ETA: 01:03:52

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 42668 steps/s (collection: 2.168s, learning 0.136s)
             Mean action noise std: 1.73
          Mean value_function loss: 29.8587
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.2708
                       Mean reward: 31.92
               Mean episode length: 102.75
    Episode_Reward/reaching_object: 0.2345
     Episode_Reward/lifting_object: 6.2563
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.30s
                      Time elapsed: 00:10:15
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 42404 steps/s (collection: 2.177s, learning 0.141s)
             Mean action noise std: 1.73
          Mean value_function loss: 25.1699
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.2780
                       Mean reward: 27.54
               Mean episode length: 98.38
    Episode_Reward/reaching_object: 0.2354
     Episode_Reward/lifting_object: 5.9576
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 41.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.32s
                      Time elapsed: 00:10:17
                               ETA: 01:03:48

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 41948 steps/s (collection: 2.235s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 25.2122
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.2856
                       Mean reward: 31.44
               Mean episode length: 97.02
    Episode_Reward/reaching_object: 0.2318
     Episode_Reward/lifting_object: 6.1157
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.34s
                      Time elapsed: 00:10:20
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 43020 steps/s (collection: 2.161s, learning 0.124s)
             Mean action noise std: 1.73
          Mean value_function loss: 29.5256
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.2913
                       Mean reward: 27.16
               Mean episode length: 90.53
    Episode_Reward/reaching_object: 0.2243
     Episode_Reward/lifting_object: 6.1243
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 39.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.29s
                      Time elapsed: 00:10:22
                               ETA: 01:03:45

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 41891 steps/s (collection: 2.230s, learning 0.116s)
             Mean action noise std: 1.73
          Mean value_function loss: 27.4616
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2953
                       Mean reward: 28.46
               Mean episode length: 95.40
    Episode_Reward/reaching_object: 0.2225
     Episode_Reward/lifting_object: 6.0586
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.35s
                      Time elapsed: 00:10:24
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 42326 steps/s (collection: 2.209s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 32.7445
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.3026
                       Mean reward: 33.92
               Mean episode length: 103.68
    Episode_Reward/reaching_object: 0.2271
     Episode_Reward/lifting_object: 6.1148
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 41.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.32s
                      Time elapsed: 00:10:27
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 42826 steps/s (collection: 2.181s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 33.6663
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.3071
                       Mean reward: 29.80
               Mean episode length: 95.84
    Episode_Reward/reaching_object: 0.2231
     Episode_Reward/lifting_object: 6.0052
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.30s
                      Time elapsed: 00:10:29
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 42901 steps/s (collection: 2.175s, learning 0.116s)
             Mean action noise std: 1.73
          Mean value_function loss: 27.4165
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.3086
                       Mean reward: 30.33
               Mean episode length: 97.16
    Episode_Reward/reaching_object: 0.2280
     Episode_Reward/lifting_object: 6.3744
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.29s
                      Time elapsed: 00:10:31
                               ETA: 01:03:38

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 42624 steps/s (collection: 2.201s, learning 0.105s)
             Mean action noise std: 1.73
          Mean value_function loss: 31.6653
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 35.3124
                       Mean reward: 29.44
               Mean episode length: 92.63
    Episode_Reward/reaching_object: 0.2249
     Episode_Reward/lifting_object: 6.6264
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.31s
                      Time elapsed: 00:10:33
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 43111 steps/s (collection: 2.164s, learning 0.116s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.0389
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.3174
                       Mean reward: 31.39
               Mean episode length: 93.13
    Episode_Reward/reaching_object: 0.2249
     Episode_Reward/lifting_object: 6.5348
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.28s
                      Time elapsed: 00:10:36
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 43089 steps/s (collection: 2.167s, learning 0.115s)
             Mean action noise std: 1.73
          Mean value_function loss: 37.5860
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.3194
                       Mean reward: 32.27
               Mean episode length: 92.48
    Episode_Reward/reaching_object: 0.2261
     Episode_Reward/lifting_object: 6.5134
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.28s
                      Time elapsed: 00:10:38
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 43388 steps/s (collection: 2.149s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 29.8595
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.3207
                       Mean reward: 28.58
               Mean episode length: 98.23
    Episode_Reward/reaching_object: 0.2301
     Episode_Reward/lifting_object: 6.9671
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.27s
                      Time elapsed: 00:10:40
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 41667 steps/s (collection: 2.246s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 33.8880
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.3229
                       Mean reward: 36.94
               Mean episode length: 94.68
    Episode_Reward/reaching_object: 0.2264
     Episode_Reward/lifting_object: 6.5845
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.36s
                      Time elapsed: 00:10:43
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 35768 steps/s (collection: 2.431s, learning 0.318s)
             Mean action noise std: 1.74
          Mean value_function loss: 31.7754
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.3300
                       Mean reward: 37.90
               Mean episode length: 93.53
    Episode_Reward/reaching_object: 0.2278
     Episode_Reward/lifting_object: 6.9076
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 41.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.75s
                      Time elapsed: 00:10:45
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 40034 steps/s (collection: 2.327s, learning 0.129s)
             Mean action noise std: 1.74
          Mean value_function loss: 31.5831
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.3378
                       Mean reward: 35.40
               Mean episode length: 91.89
    Episode_Reward/reaching_object: 0.2268
     Episode_Reward/lifting_object: 7.0525
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 42.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.46s
                      Time elapsed: 00:10:48
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 38224 steps/s (collection: 2.431s, learning 0.141s)
             Mean action noise std: 1.74
          Mean value_function loss: 28.5929
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 35.3431
                       Mean reward: 34.80
               Mean episode length: 97.13
    Episode_Reward/reaching_object: 0.2224
     Episode_Reward/lifting_object: 6.7293
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 42.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.57s
                      Time elapsed: 00:10:50
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 36634 steps/s (collection: 2.551s, learning 0.133s)
             Mean action noise std: 1.74
          Mean value_function loss: 30.6807
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.3453
                       Mean reward: 33.57
               Mean episode length: 97.12
    Episode_Reward/reaching_object: 0.2239
     Episode_Reward/lifting_object: 6.7712
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 42.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.68s
                      Time elapsed: 00:10:53
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 34433 steps/s (collection: 2.705s, learning 0.150s)
             Mean action noise std: 1.74
          Mean value_function loss: 31.7630
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.3478
                       Mean reward: 37.35
               Mean episode length: 105.54
    Episode_Reward/reaching_object: 0.2254
     Episode_Reward/lifting_object: 6.8812
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.85s
                      Time elapsed: 00:10:56
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 36310 steps/s (collection: 2.556s, learning 0.152s)
             Mean action noise std: 1.74
          Mean value_function loss: 30.6607
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 35.3487
                       Mean reward: 33.28
               Mean episode length: 93.37
    Episode_Reward/reaching_object: 0.2210
     Episode_Reward/lifting_object: 6.8897
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.71s
                      Time elapsed: 00:10:59
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 34632 steps/s (collection: 2.677s, learning 0.161s)
             Mean action noise std: 1.74
          Mean value_function loss: 30.6571
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 35.3491
                       Mean reward: 43.27
               Mean episode length: 102.53
    Episode_Reward/reaching_object: 0.2232
     Episode_Reward/lifting_object: 7.0310
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.84s
                      Time elapsed: 00:11:02
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 37243 steps/s (collection: 2.510s, learning 0.129s)
             Mean action noise std: 1.74
          Mean value_function loss: 31.2744
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.3492
                       Mean reward: 40.92
               Mean episode length: 99.22
    Episode_Reward/reaching_object: 0.2276
     Episode_Reward/lifting_object: 7.4334
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 41.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.64s
                      Time elapsed: 00:11:04
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 36700 steps/s (collection: 2.548s, learning 0.130s)
             Mean action noise std: 1.74
          Mean value_function loss: 29.8181
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.3494
                       Mean reward: 38.43
               Mean episode length: 99.43
    Episode_Reward/reaching_object: 0.2247
     Episode_Reward/lifting_object: 7.5052
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.68s
                      Time elapsed: 00:11:07
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 35611 steps/s (collection: 2.610s, learning 0.150s)
             Mean action noise std: 1.74
          Mean value_function loss: 32.5183
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.3508
                       Mean reward: 42.40
               Mean episode length: 96.45
    Episode_Reward/reaching_object: 0.2241
     Episode_Reward/lifting_object: 7.2063
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.76s
                      Time elapsed: 00:11:10
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 36222 steps/s (collection: 2.570s, learning 0.144s)
             Mean action noise std: 1.74
          Mean value_function loss: 32.0800
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.3561
                       Mean reward: 37.68
               Mean episode length: 100.17
    Episode_Reward/reaching_object: 0.2289
     Episode_Reward/lifting_object: 7.0741
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 43.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.71s
                      Time elapsed: 00:11:12
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 37907 steps/s (collection: 2.464s, learning 0.129s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.3113
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.3632
                       Mean reward: 40.55
               Mean episode length: 101.89
    Episode_Reward/reaching_object: 0.2290
     Episode_Reward/lifting_object: 7.4953
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.59s
                      Time elapsed: 00:11:15
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 37453 steps/s (collection: 2.474s, learning 0.150s)
             Mean action noise std: 1.74
          Mean value_function loss: 40.8848
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.3676
                       Mean reward: 33.36
               Mean episode length: 99.75
    Episode_Reward/reaching_object: 0.2269
     Episode_Reward/lifting_object: 6.9873
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 40.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.62s
                      Time elapsed: 00:11:18
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 36892 steps/s (collection: 2.530s, learning 0.135s)
             Mean action noise std: 1.74
          Mean value_function loss: 32.5726
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 35.3711
                       Mean reward: 38.43
               Mean episode length: 96.48
    Episode_Reward/reaching_object: 0.2307
     Episode_Reward/lifting_object: 7.4546
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.66s
                      Time elapsed: 00:11:20
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 35957 steps/s (collection: 2.596s, learning 0.138s)
             Mean action noise std: 1.74
          Mean value_function loss: 34.9906
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 35.3722
                       Mean reward: 33.36
               Mean episode length: 95.20
    Episode_Reward/reaching_object: 0.2311
     Episode_Reward/lifting_object: 7.2546
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.73s
                      Time elapsed: 00:11:23
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 37520 steps/s (collection: 2.480s, learning 0.140s)
             Mean action noise std: 1.74
          Mean value_function loss: 33.8111
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.3730
                       Mean reward: 32.79
               Mean episode length: 92.16
    Episode_Reward/reaching_object: 0.2349
     Episode_Reward/lifting_object: 7.4083
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.62s
                      Time elapsed: 00:11:26
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 37951 steps/s (collection: 2.457s, learning 0.133s)
             Mean action noise std: 1.74
          Mean value_function loss: 36.3187
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.3748
                       Mean reward: 38.74
               Mean episode length: 95.03
    Episode_Reward/reaching_object: 0.2473
     Episode_Reward/lifting_object: 7.5758
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 39.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.59s
                      Time elapsed: 00:11:28
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 38324 steps/s (collection: 2.421s, learning 0.144s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.1784
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.3761
                       Mean reward: 41.68
               Mean episode length: 103.27
    Episode_Reward/reaching_object: 0.2514
     Episode_Reward/lifting_object: 7.5217
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.57s
                      Time elapsed: 00:11:31
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 38714 steps/s (collection: 2.403s, learning 0.137s)
             Mean action noise std: 1.74
          Mean value_function loss: 37.1935
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.3803
                       Mean reward: 35.69
               Mean episode length: 97.17
    Episode_Reward/reaching_object: 0.2486
     Episode_Reward/lifting_object: 7.3888
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.54s
                      Time elapsed: 00:11:33
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 36407 steps/s (collection: 2.566s, learning 0.134s)
             Mean action noise std: 1.74
          Mean value_function loss: 34.1449
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 35.3838
                       Mean reward: 38.51
               Mean episode length: 99.99
    Episode_Reward/reaching_object: 0.2571
     Episode_Reward/lifting_object: 7.4172
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.70s
                      Time elapsed: 00:11:36
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 35961 steps/s (collection: 2.558s, learning 0.176s)
             Mean action noise std: 1.74
          Mean value_function loss: 36.8825
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 35.3850
                       Mean reward: 41.33
               Mean episode length: 98.88
    Episode_Reward/reaching_object: 0.2518
     Episode_Reward/lifting_object: 7.3591
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.73s
                      Time elapsed: 00:11:39
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 36347 steps/s (collection: 2.569s, learning 0.136s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.3259
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 35.3854
                       Mean reward: 44.90
               Mean episode length: 112.38
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: 7.6067
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.70s
                      Time elapsed: 00:11:41
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 37258 steps/s (collection: 2.508s, learning 0.130s)
             Mean action noise std: 1.74
          Mean value_function loss: 35.7956
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.3857
                       Mean reward: 34.86
               Mean episode length: 100.59
    Episode_Reward/reaching_object: 0.2616
     Episode_Reward/lifting_object: 7.5206
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.64s
                      Time elapsed: 00:11:44
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 38243 steps/s (collection: 2.426s, learning 0.144s)
             Mean action noise std: 1.74
          Mean value_function loss: 40.2660
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.3868
                       Mean reward: 43.69
               Mean episode length: 103.72
    Episode_Reward/reaching_object: 0.2640
     Episode_Reward/lifting_object: 7.7321
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.57s
                      Time elapsed: 00:11:47
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 38052 steps/s (collection: 2.444s, learning 0.139s)
             Mean action noise std: 1.74
          Mean value_function loss: 34.7317
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.3887
                       Mean reward: 40.88
               Mean episode length: 98.33
    Episode_Reward/reaching_object: 0.2612
     Episode_Reward/lifting_object: 7.6861
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.58s
                      Time elapsed: 00:11:49
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 37858 steps/s (collection: 2.441s, learning 0.155s)
             Mean action noise std: 1.74
          Mean value_function loss: 37.2025
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3934
                       Mean reward: 41.49
               Mean episode length: 97.66
    Episode_Reward/reaching_object: 0.2616
     Episode_Reward/lifting_object: 7.8211
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 40.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.60s
                      Time elapsed: 00:11:52
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 38693 steps/s (collection: 2.419s, learning 0.121s)
             Mean action noise std: 1.74
          Mean value_function loss: 40.4267
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 35.4016
                       Mean reward: 42.33
               Mean episode length: 98.00
    Episode_Reward/reaching_object: 0.2679
     Episode_Reward/lifting_object: 8.1211
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.54s
                      Time elapsed: 00:11:54
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 42024 steps/s (collection: 2.223s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 45.0598
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.4032
                       Mean reward: 40.18
               Mean episode length: 101.86
    Episode_Reward/reaching_object: 0.2632
     Episode_Reward/lifting_object: 8.1158
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.34s
                      Time elapsed: 00:11:57
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 40658 steps/s (collection: 2.295s, learning 0.123s)
             Mean action noise std: 1.74
          Mean value_function loss: 39.0438
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 35.4043
                       Mean reward: 44.47
               Mean episode length: 101.01
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 8.0300
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.42s
                      Time elapsed: 00:11:59
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 42570 steps/s (collection: 2.192s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 37.4396
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.4046
                       Mean reward: 40.17
               Mean episode length: 91.94
    Episode_Reward/reaching_object: 0.2465
     Episode_Reward/lifting_object: 7.6094
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.31s
                      Time elapsed: 00:12:01
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 43369 steps/s (collection: 2.143s, learning 0.124s)
             Mean action noise std: 1.74
          Mean value_function loss: 43.8672
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.4047
                       Mean reward: 37.82
               Mean episode length: 92.39
    Episode_Reward/reaching_object: 0.2480
     Episode_Reward/lifting_object: 8.0077
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.27s
                      Time elapsed: 00:12:04
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 39975 steps/s (collection: 2.374s, learning 0.085s)
             Mean action noise std: 1.74
          Mean value_function loss: 47.5192
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.4069
                       Mean reward: 42.56
               Mean episode length: 93.25
    Episode_Reward/reaching_object: 0.2482
     Episode_Reward/lifting_object: 8.0659
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.46s
                      Time elapsed: 00:12:06
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 49028 steps/s (collection: 1.903s, learning 0.102s)
             Mean action noise std: 1.74
          Mean value_function loss: 41.8444
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.4110
                       Mean reward: 46.66
               Mean episode length: 98.42
    Episode_Reward/reaching_object: 0.2520
     Episode_Reward/lifting_object: 8.4107
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.01s
                      Time elapsed: 00:12:08
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 49073 steps/s (collection: 1.898s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 40.2027
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.4129
                       Mean reward: 39.75
               Mean episode length: 89.54
    Episode_Reward/reaching_object: 0.2436
     Episode_Reward/lifting_object: 8.0146
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.00s
                      Time elapsed: 00:12:10
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 47140 steps/s (collection: 1.976s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 39.4774
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.4141
                       Mean reward: 42.99
               Mean episode length: 97.74
    Episode_Reward/reaching_object: 0.2427
     Episode_Reward/lifting_object: 7.9974
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 42.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.09s
                      Time elapsed: 00:12:12
                               ETA: 01:03:12

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 47908 steps/s (collection: 1.948s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 42.8648
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.4160
                       Mean reward: 45.25
               Mean episode length: 97.51
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: 8.2234
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.05s
                      Time elapsed: 00:12:14
                               ETA: 01:03:09

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46936 steps/s (collection: 1.988s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 39.6951
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.4187
                       Mean reward: 44.16
               Mean episode length: 101.50
    Episode_Reward/reaching_object: 0.2573
     Episode_Reward/lifting_object: 8.8096
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.09s
                      Time elapsed: 00:12:16
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 47983 steps/s (collection: 1.938s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 51.2625
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.4217
                       Mean reward: 46.25
               Mean episode length: 103.55
    Episode_Reward/reaching_object: 0.2608
     Episode_Reward/lifting_object: 8.4807
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.05s
                      Time elapsed: 00:12:18
                               ETA: 01:03:02

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 49107 steps/s (collection: 1.899s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 41.9668
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.4264
                       Mean reward: 38.48
               Mean episode length: 91.64
    Episode_Reward/reaching_object: 0.2548
     Episode_Reward/lifting_object: 8.6600
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 40.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.00s
                      Time elapsed: 00:12:20
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 48637 steps/s (collection: 1.892s, learning 0.130s)
             Mean action noise std: 1.75
          Mean value_function loss: 43.3883
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.4309
                       Mean reward: 40.05
               Mean episode length: 91.71
    Episode_Reward/reaching_object: 0.2549
     Episode_Reward/lifting_object: 8.5611
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.02s
                      Time elapsed: 00:12:22
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 44036 steps/s (collection: 2.103s, learning 0.129s)
             Mean action noise std: 1.75
          Mean value_function loss: 44.3169
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.4374
                       Mean reward: 46.85
               Mean episode length: 100.56
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 8.9572
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.23s
                      Time elapsed: 00:12:25
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 41537 steps/s (collection: 2.258s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 50.9705
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4428
                       Mean reward: 46.39
               Mean episode length: 99.84
    Episode_Reward/reaching_object: 0.2665
     Episode_Reward/lifting_object: 9.0687
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.37s
                      Time elapsed: 00:12:27
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 37389 steps/s (collection: 2.384s, learning 0.245s)
             Mean action noise std: 1.75
          Mean value_function loss: 44.1649
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 35.4472
                       Mean reward: 51.42
               Mean episode length: 109.15
    Episode_Reward/reaching_object: 0.2697
     Episode_Reward/lifting_object: 9.3781
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.63s
                      Time elapsed: 00:12:30
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 28365 steps/s (collection: 3.252s, learning 0.214s)
             Mean action noise std: 1.75
          Mean value_function loss: 50.1798
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.4513
                       Mean reward: 43.16
               Mean episode length: 97.47
    Episode_Reward/reaching_object: 0.2630
     Episode_Reward/lifting_object: 8.4210
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 3.47s
                      Time elapsed: 00:12:33
                               ETA: 01:02:54

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 13786 steps/s (collection: 6.881s, learning 0.250s)
             Mean action noise std: 1.75
          Mean value_function loss: 38.9730
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.4555
                       Mean reward: 48.61
               Mean episode length: 95.26
    Episode_Reward/reaching_object: 0.2509
     Episode_Reward/lifting_object: 8.4792
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 7.13s
                      Time elapsed: 00:12:40
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 10564 steps/s (collection: 9.136s, learning 0.169s)
             Mean action noise std: 1.75
          Mean value_function loss: 48.9386
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.4608
                       Mean reward: 38.04
               Mean episode length: 93.87
    Episode_Reward/reaching_object: 0.2574
     Episode_Reward/lifting_object: 8.7564
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 38.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 9.31s
                      Time elapsed: 00:12:50
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 11055 steps/s (collection: 8.674s, learning 0.218s)
             Mean action noise std: 1.75
          Mean value_function loss: 52.0125
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.4639
                       Mean reward: 49.59
               Mean episode length: 102.68
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: 9.1070
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 41.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.89s
                      Time elapsed: 00:12:58
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 10806 steps/s (collection: 8.849s, learning 0.248s)
             Mean action noise std: 1.75
          Mean value_function loss: 51.5029
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.4707
                       Mean reward: 44.81
               Mean episode length: 97.97
    Episode_Reward/reaching_object: 0.2576
     Episode_Reward/lifting_object: 8.8065
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 41.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 9.10s
                      Time elapsed: 00:13:08
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 10503 steps/s (collection: 9.185s, learning 0.175s)
             Mean action noise std: 1.75
          Mean value_function loss: 45.8105
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.4771
                       Mean reward: 44.69
               Mean episode length: 97.45
    Episode_Reward/reaching_object: 0.2619
     Episode_Reward/lifting_object: 8.9860
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 9.36s
                      Time elapsed: 00:13:17
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 9982 steps/s (collection: 9.448s, learning 0.400s)
             Mean action noise std: 1.75
          Mean value_function loss: 45.8571
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.4805
                       Mean reward: 49.23
               Mean episode length: 90.57
    Episode_Reward/reaching_object: 0.2584
     Episode_Reward/lifting_object: 9.4331
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 42.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 9.85s
                      Time elapsed: 00:13:27
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 9989 steps/s (collection: 9.562s, learning 0.279s)
             Mean action noise std: 1.75
          Mean value_function loss: 44.9796
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4852
                       Mean reward: 45.81
               Mean episode length: 90.28
    Episode_Reward/reaching_object: 0.2515
     Episode_Reward/lifting_object: 9.3458
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 9.84s
                      Time elapsed: 00:13:37
                               ETA: 01:06:31

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 10316 steps/s (collection: 9.334s, learning 0.194s)
             Mean action noise std: 1.75
          Mean value_function loss: 45.1844
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.4899
                       Mean reward: 46.12
               Mean episode length: 94.20
    Episode_Reward/reaching_object: 0.2485
     Episode_Reward/lifting_object: 8.9765
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 42.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 9.53s
                      Time elapsed: 00:13:46
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 10033 steps/s (collection: 9.542s, learning 0.256s)
             Mean action noise std: 1.75
          Mean value_function loss: 50.1606
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.4934
                       Mean reward: 45.66
               Mean episode length: 92.98
    Episode_Reward/reaching_object: 0.2537
     Episode_Reward/lifting_object: 9.2485
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 41.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 9.80s
                      Time elapsed: 00:13:56
                               ETA: 01:07:37

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 40342 steps/s (collection: 2.318s, learning 0.119s)
             Mean action noise std: 1.75
          Mean value_function loss: 52.6830
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.4952
                       Mean reward: 53.59
               Mean episode length: 96.19
    Episode_Reward/reaching_object: 0.2557
     Episode_Reward/lifting_object: 9.5540
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.44s
                      Time elapsed: 00:13:58
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 39752 steps/s (collection: 2.281s, learning 0.192s)
             Mean action noise std: 1.75
          Mean value_function loss: 52.8347
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 35.4964
                       Mean reward: 52.36
               Mean episode length: 96.64
    Episode_Reward/reaching_object: 0.2587
     Episode_Reward/lifting_object: 9.6946
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.47s
                      Time elapsed: 00:14:01
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 36737 steps/s (collection: 2.532s, learning 0.144s)
             Mean action noise std: 1.75
          Mean value_function loss: 50.3434
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.4967
                       Mean reward: 43.29
               Mean episode length: 93.51
    Episode_Reward/reaching_object: 0.2605
     Episode_Reward/lifting_object: 9.6135
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.68s
                      Time elapsed: 00:14:04
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 32095 steps/s (collection: 2.837s, learning 0.226s)
             Mean action noise std: 1.75
          Mean value_function loss: 46.5003
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.4980
                       Mean reward: 52.27
               Mean episode length: 103.86
    Episode_Reward/reaching_object: 0.2671
     Episode_Reward/lifting_object: 9.9439
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 3.06s
                      Time elapsed: 00:14:07
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 31323 steps/s (collection: 3.000s, learning 0.139s)
             Mean action noise std: 1.75
          Mean value_function loss: 46.9105
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.5018
                       Mean reward: 51.40
               Mean episode length: 95.82
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 9.8364
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 3.14s
                      Time elapsed: 00:14:10
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 38560 steps/s (collection: 2.427s, learning 0.122s)
             Mean action noise std: 1.75
          Mean value_function loss: 49.2248
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.5063
                       Mean reward: 51.82
               Mean episode length: 102.20
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: 10.1206
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.55s
                      Time elapsed: 00:14:12
                               ETA: 01:07:30

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 38873 steps/s (collection: 2.387s, learning 0.141s)
             Mean action noise std: 1.75
          Mean value_function loss: 53.3680
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.5125
                       Mean reward: 55.14
               Mean episode length: 99.07
    Episode_Reward/reaching_object: 0.2705
     Episode_Reward/lifting_object: 10.0121
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 40.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.53s
                      Time elapsed: 00:14:15
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 32462 steps/s (collection: 2.810s, learning 0.218s)
             Mean action noise std: 1.76
          Mean value_function loss: 49.1528
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.5160
                       Mean reward: 63.00
               Mean episode length: 107.22
    Episode_Reward/reaching_object: 0.2774
     Episode_Reward/lifting_object: 10.6874
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 3.03s
                      Time elapsed: 00:14:18
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 29997 steps/s (collection: 3.046s, learning 0.231s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.7783
               Mean surrogate loss: 0.0166
                 Mean entropy loss: 35.5174
                       Mean reward: 48.47
               Mean episode length: 99.00
    Episode_Reward/reaching_object: 0.2752
     Episode_Reward/lifting_object: 9.9605
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 3.28s
                      Time elapsed: 00:14:21
                               ETA: 01:07:30

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 25746 steps/s (collection: 3.537s, learning 0.281s)
             Mean action noise std: 1.76
          Mean value_function loss: 47.6348
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.5176
                       Mean reward: 49.78
               Mean episode length: 105.40
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 10.3638
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 3.82s
                      Time elapsed: 00:14:25
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 26331 steps/s (collection: 3.521s, learning 0.213s)
             Mean action noise std: 1.76
          Mean value_function loss: 51.7564
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.5187
                       Mean reward: 49.58
               Mean episode length: 97.40
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 10.4565
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 3.73s
                      Time elapsed: 00:14:29
                               ETA: 01:07:37

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 29393 steps/s (collection: 3.097s, learning 0.248s)
             Mean action noise std: 1.76
          Mean value_function loss: 48.5169
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.5225
                       Mean reward: 47.44
               Mean episode length: 99.79
    Episode_Reward/reaching_object: 0.2836
     Episode_Reward/lifting_object: 10.5517
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 3.34s
                      Time elapsed: 00:14:32
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 28558 steps/s (collection: 3.218s, learning 0.225s)
             Mean action noise std: 1.76
          Mean value_function loss: 50.9772
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.5266
                       Mean reward: 55.95
               Mean episode length: 100.71
    Episode_Reward/reaching_object: 0.2772
     Episode_Reward/lifting_object: 10.5230
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 3.44s
                      Time elapsed: 00:14:35
                               ETA: 01:07:41

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 27939 steps/s (collection: 3.285s, learning 0.233s)
             Mean action noise std: 1.76
          Mean value_function loss: 56.2826
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.5300
                       Mean reward: 58.45
               Mean episode length: 107.70
    Episode_Reward/reaching_object: 0.2810
     Episode_Reward/lifting_object: 10.7600
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 3.52s
                      Time elapsed: 00:14:39
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 27906 steps/s (collection: 3.246s, learning 0.276s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.7581
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5324
                       Mean reward: 56.08
               Mean episode length: 110.08
    Episode_Reward/reaching_object: 0.2772
     Episode_Reward/lifting_object: 10.3952
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 3.52s
                      Time elapsed: 00:14:42
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 27892 steps/s (collection: 3.287s, learning 0.238s)
             Mean action noise std: 1.76
          Mean value_function loss: 55.5478
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.5349
                       Mean reward: 51.35
               Mean episode length: 99.29
    Episode_Reward/reaching_object: 0.2819
     Episode_Reward/lifting_object: 10.8748
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 3.52s
                      Time elapsed: 00:14:46
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 29435 steps/s (collection: 3.127s, learning 0.213s)
             Mean action noise std: 1.76
          Mean value_function loss: 58.7420
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.5407
                       Mean reward: 55.50
               Mean episode length: 104.72
    Episode_Reward/reaching_object: 0.2775
     Episode_Reward/lifting_object: 10.3977
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 3.34s
                      Time elapsed: 00:14:49
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 30921 steps/s (collection: 2.994s, learning 0.185s)
             Mean action noise std: 1.76
          Mean value_function loss: 55.3996
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.5469
                       Mean reward: 53.13
               Mean episode length: 96.33
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 10.6961
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 3.18s
                      Time elapsed: 00:14:53
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 30707 steps/s (collection: 3.041s, learning 0.160s)
             Mean action noise std: 1.76
          Mean value_function loss: 57.2264
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.5523
                       Mean reward: 56.99
               Mean episode length: 103.21
    Episode_Reward/reaching_object: 0.2738
     Episode_Reward/lifting_object: 10.5086
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 3.20s
                      Time elapsed: 00:14:56
                               ETA: 01:07:51

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 31632 steps/s (collection: 2.888s, learning 0.220s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.1503
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.5581
                       Mean reward: 54.85
               Mean episode length: 99.81
    Episode_Reward/reaching_object: 0.2826
     Episode_Reward/lifting_object: 10.8062
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 3.11s
                      Time elapsed: 00:14:59
                               ETA: 01:07:51

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 29263 steps/s (collection: 3.111s, learning 0.248s)
             Mean action noise std: 1.76
          Mean value_function loss: 61.4656
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.5633
                       Mean reward: 53.75
               Mean episode length: 97.31
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 10.9106
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 38.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 3.36s
                      Time elapsed: 00:15:02
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 31185 steps/s (collection: 2.905s, learning 0.247s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.0568
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.5671
                       Mean reward: 53.81
               Mean episode length: 103.34
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 11.3466
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 3.15s
                      Time elapsed: 00:15:05
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 27615 steps/s (collection: 3.345s, learning 0.215s)
             Mean action noise std: 1.76
          Mean value_function loss: 56.6498
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.5703
                       Mean reward: 59.01
               Mean episode length: 101.88
    Episode_Reward/reaching_object: 0.2872
     Episode_Reward/lifting_object: 11.3557
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 3.56s
                      Time elapsed: 00:15:09
                               ETA: 01:07:56

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 27841 steps/s (collection: 3.328s, learning 0.202s)
             Mean action noise std: 1.76
          Mean value_function loss: 57.9816
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.5725
                       Mean reward: 58.69
               Mean episode length: 106.29
    Episode_Reward/reaching_object: 0.2823
     Episode_Reward/lifting_object: 11.3829
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 3.53s
                      Time elapsed: 00:15:12
                               ETA: 01:07:58

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 27545 steps/s (collection: 3.325s, learning 0.244s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.3875
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.5758
                       Mean reward: 60.16
               Mean episode length: 103.49
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 11.3860
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 3.57s
                      Time elapsed: 00:15:16
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 28524 steps/s (collection: 3.240s, learning 0.206s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.6254
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.5785
                       Mean reward: 58.22
               Mean episode length: 105.40
    Episode_Reward/reaching_object: 0.2896
     Episode_Reward/lifting_object: 11.6809
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 3.45s
                      Time elapsed: 00:15:19
                               ETA: 01:08:02

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 27355 steps/s (collection: 3.367s, learning 0.226s)
             Mean action noise std: 1.76
          Mean value_function loss: 55.2637
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 35.5800
                       Mean reward: 63.93
               Mean episode length: 106.51
    Episode_Reward/reaching_object: 0.2958
     Episode_Reward/lifting_object: 12.0069
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 3.59s
                      Time elapsed: 00:15:23
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 28404 steps/s (collection: 3.220s, learning 0.241s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.0695
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.5806
                       Mean reward: 64.74
               Mean episode length: 103.42
    Episode_Reward/reaching_object: 0.2954
     Episode_Reward/lifting_object: 12.5494
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 40.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 3.46s
                      Time elapsed: 00:15:26
                               ETA: 01:08:06

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 28026 steps/s (collection: 3.287s, learning 0.221s)
             Mean action noise std: 1.76
          Mean value_function loss: 56.3934
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 35.5811
                       Mean reward: 59.01
               Mean episode length: 98.43
    Episode_Reward/reaching_object: 0.2968
     Episode_Reward/lifting_object: 12.4403
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 3.51s
                      Time elapsed: 00:15:30
                               ETA: 01:08:08

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 29366 steps/s (collection: 3.115s, learning 0.232s)
             Mean action noise std: 1.76
          Mean value_function loss: 56.0000
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.5819
                       Mean reward: 69.26
               Mean episode length: 106.82
    Episode_Reward/reaching_object: 0.3058
     Episode_Reward/lifting_object: 12.6966
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 3.35s
                      Time elapsed: 00:15:33
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 30860 steps/s (collection: 2.977s, learning 0.209s)
             Mean action noise std: 1.76
          Mean value_function loss: 61.0970
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.5843
                       Mean reward: 58.46
               Mean episode length: 100.59
    Episode_Reward/reaching_object: 0.3001
     Episode_Reward/lifting_object: 12.5090
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 3.19s
                      Time elapsed: 00:15:37
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 29528 steps/s (collection: 3.138s, learning 0.191s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.2415
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.5865
                       Mean reward: 61.94
               Mean episode length: 105.25
    Episode_Reward/reaching_object: 0.2997
     Episode_Reward/lifting_object: 12.4775
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 3.33s
                      Time elapsed: 00:15:40
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 26811 steps/s (collection: 3.426s, learning 0.241s)
             Mean action noise std: 1.76
          Mean value_function loss: 58.7927
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.5890
                       Mean reward: 72.13
               Mean episode length: 119.33
    Episode_Reward/reaching_object: 0.3139
     Episode_Reward/lifting_object: 13.3396
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 3.67s
                      Time elapsed: 00:15:44
                               ETA: 01:08:13

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 23763 steps/s (collection: 3.903s, learning 0.234s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.3725
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.5929
                       Mean reward: 65.58
               Mean episode length: 104.96
    Episode_Reward/reaching_object: 0.3051
     Episode_Reward/lifting_object: 12.9906
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 4.14s
                      Time elapsed: 00:15:48
                               ETA: 01:08:17

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 24939 steps/s (collection: 3.606s, learning 0.336s)
             Mean action noise std: 1.76
          Mean value_function loss: 69.3046
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.5960
                       Mean reward: 72.82
               Mean episode length: 109.85
    Episode_Reward/reaching_object: 0.3075
     Episode_Reward/lifting_object: 13.0313
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 3.94s
                      Time elapsed: 00:15:52
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 30151 steps/s (collection: 3.097s, learning 0.163s)
             Mean action noise std: 1.76
          Mean value_function loss: 61.8619
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.5971
                       Mean reward: 65.21
               Mean episode length: 111.02
    Episode_Reward/reaching_object: 0.3169
     Episode_Reward/lifting_object: 13.8694
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 34.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 3.26s
                      Time elapsed: 00:15:55
                               ETA: 01:08:22

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 41464 steps/s (collection: 2.177s, learning 0.194s)
             Mean action noise std: 1.76
          Mean value_function loss: 74.9601
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.5974
                       Mean reward: 70.32
               Mean episode length: 111.29
    Episode_Reward/reaching_object: 0.3155
     Episode_Reward/lifting_object: 13.7221
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.37s
                      Time elapsed: 00:15:57
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 44897 steps/s (collection: 2.081s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 70.1439
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.5986
                       Mean reward: 62.64
               Mean episode length: 108.11
    Episode_Reward/reaching_object: 0.3083
     Episode_Reward/lifting_object: 12.8428
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.19s
                      Time elapsed: 00:15:59
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 46313 steps/s (collection: 2.023s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 73.4591
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.5991
                       Mean reward: 66.87
               Mean episode length: 107.73
    Episode_Reward/reaching_object: 0.3208
     Episode_Reward/lifting_object: 13.6917
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.12s
                      Time elapsed: 00:16:02
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 47336 steps/s (collection: 1.974s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 70.9501
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.5998
                       Mean reward: 69.37
               Mean episode length: 109.88
    Episode_Reward/reaching_object: 0.3318
     Episode_Reward/lifting_object: 14.2340
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.08s
                      Time elapsed: 00:16:04
                               ETA: 01:08:06

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 46526 steps/s (collection: 2.006s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 94.8116
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.6024
                       Mean reward: 63.20
               Mean episode length: 108.67
    Episode_Reward/reaching_object: 0.3176
     Episode_Reward/lifting_object: 13.3672
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.11s
                      Time elapsed: 00:16:06
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 48603 steps/s (collection: 1.935s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 62.8446
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.6057
                       Mean reward: 68.28
               Mean episode length: 114.03
    Episode_Reward/reaching_object: 0.3127
     Episode_Reward/lifting_object: 12.8794
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.02s
                      Time elapsed: 00:16:08
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 46248 steps/s (collection: 2.038s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 70.0464
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 35.6075
                       Mean reward: 69.56
               Mean episode length: 120.39
    Episode_Reward/reaching_object: 0.3191
     Episode_Reward/lifting_object: 13.4986
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.13s
                      Time elapsed: 00:16:10
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 48230 steps/s (collection: 1.951s, learning 0.087s)
             Mean action noise std: 1.77
          Mean value_function loss: 55.5689
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 35.6084
                       Mean reward: 73.13
               Mean episode length: 116.71
    Episode_Reward/reaching_object: 0.3237
     Episode_Reward/lifting_object: 13.7960
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.04s
                      Time elapsed: 00:16:12
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 47660 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 67.4970
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.6095
                       Mean reward: 70.63
               Mean episode length: 110.07
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 13.1638
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.06s
                      Time elapsed: 00:16:14
                               ETA: 01:07:44

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 46795 steps/s (collection: 1.988s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 64.2411
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 35.6111
                       Mean reward: 69.54
               Mean episode length: 108.31
    Episode_Reward/reaching_object: 0.3086
     Episode_Reward/lifting_object: 13.3998
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.10s
                      Time elapsed: 00:16:16
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 47228 steps/s (collection: 1.969s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 68.1798
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.6096
                       Mean reward: 80.88
               Mean episode length: 115.16
    Episode_Reward/reaching_object: 0.3236
     Episode_Reward/lifting_object: 14.3726
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.08s
                      Time elapsed: 00:16:18
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 46192 steps/s (collection: 1.983s, learning 0.146s)
             Mean action noise std: 1.77
          Mean value_function loss: 79.7713
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.6096
                       Mean reward: 75.75
               Mean episode length: 115.98
    Episode_Reward/reaching_object: 0.3283
     Episode_Reward/lifting_object: 14.5364
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.13s
                      Time elapsed: 00:16:20
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 48237 steps/s (collection: 1.943s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 82.5146
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.6115
                       Mean reward: 75.95
               Mean episode length: 118.01
    Episode_Reward/reaching_object: 0.3270
     Episode_Reward/lifting_object: 14.5321
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.04s
                      Time elapsed: 00:16:22
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 49657 steps/s (collection: 1.866s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 71.7357
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.6159
                       Mean reward: 63.41
               Mean episode length: 101.86
    Episode_Reward/reaching_object: 0.3155
     Episode_Reward/lifting_object: 14.3546
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 1.98s
                      Time elapsed: 00:16:24
                               ETA: 01:07:22

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 45532 steps/s (collection: 2.001s, learning 0.158s)
             Mean action noise std: 1.77
          Mean value_function loss: 85.1950
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.6195
                       Mean reward: 68.86
               Mean episode length: 103.29
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: 14.2823
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.16s
                      Time elapsed: 00:16:26
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 36491 steps/s (collection: 2.495s, learning 0.199s)
             Mean action noise std: 1.77
          Mean value_function loss: 77.2097
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.6232
                       Mean reward: 76.54
               Mean episode length: 103.26
    Episode_Reward/reaching_object: 0.3128
     Episode_Reward/lifting_object: 14.1727
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.69s
                      Time elapsed: 00:16:29
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 28022 steps/s (collection: 3.195s, learning 0.313s)
             Mean action noise std: 1.77
          Mean value_function loss: 68.0498
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.6282
                       Mean reward: 76.87
               Mean episode length: 102.95
    Episode_Reward/reaching_object: 0.3139
     Episode_Reward/lifting_object: 14.4632
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 3.51s
                      Time elapsed: 00:16:33
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 36948 steps/s (collection: 2.519s, learning 0.142s)
             Mean action noise std: 1.77
          Mean value_function loss: 74.4640
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.6307
                       Mean reward: 73.23
               Mean episode length: 105.82
    Episode_Reward/reaching_object: 0.3074
     Episode_Reward/lifting_object: 14.6735
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.66s
                      Time elapsed: 00:16:35
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 41452 steps/s (collection: 2.258s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 87.7872
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6339
                       Mean reward: 80.69
               Mean episode length: 109.59
    Episode_Reward/reaching_object: 0.3155
     Episode_Reward/lifting_object: 14.9328
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.37s
                      Time elapsed: 00:16:38
                               ETA: 01:07:13

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 40491 steps/s (collection: 2.315s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 84.8993
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.6388
                       Mean reward: 74.27
               Mean episode length: 108.49
    Episode_Reward/reaching_object: 0.3134
     Episode_Reward/lifting_object: 14.3856
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.43s
                      Time elapsed: 00:16:40
                               ETA: 01:07:10

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 40095 steps/s (collection: 2.335s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 75.5119
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.6458
                       Mean reward: 76.55
               Mean episode length: 111.49
    Episode_Reward/reaching_object: 0.3156
     Episode_Reward/lifting_object: 14.5873
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.45s
                      Time elapsed: 00:16:43
                               ETA: 01:07:07

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 39728 steps/s (collection: 2.280s, learning 0.195s)
             Mean action noise std: 1.77
          Mean value_function loss: 74.0217
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.6506
                       Mean reward: 79.55
               Mean episode length: 107.97
    Episode_Reward/reaching_object: 0.3122
     Episode_Reward/lifting_object: 15.1393
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.47s
                      Time elapsed: 00:16:45
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 42920 steps/s (collection: 2.203s, learning 0.087s)
             Mean action noise std: 1.77
          Mean value_function loss: 82.5176
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6515
                       Mean reward: 77.65
               Mean episode length: 108.93
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: 15.3916
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.29s
                      Time elapsed: 00:16:47
                               ETA: 01:07:01

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 46484 steps/s (collection: 1.981s, learning 0.134s)
             Mean action noise std: 1.77
          Mean value_function loss: 76.2568
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6537
                       Mean reward: 75.78
               Mean episode length: 105.20
    Episode_Reward/reaching_object: 0.3299
     Episode_Reward/lifting_object: 15.6230
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.11s
                      Time elapsed: 00:16:49
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 45240 steps/s (collection: 1.999s, learning 0.174s)
             Mean action noise std: 1.77
          Mean value_function loss: 83.5586
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.6560
                       Mean reward: 77.54
               Mean episode length: 110.90
    Episode_Reward/reaching_object: 0.3313
     Episode_Reward/lifting_object: 15.7431
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.17s
                      Time elapsed: 00:16:52
                               ETA: 01:06:53

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 45655 steps/s (collection: 2.057s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 86.8260
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.6578
                       Mean reward: 75.41
               Mean episode length: 105.52
    Episode_Reward/reaching_object: 0.3331
     Episode_Reward/lifting_object: 16.0761
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.15s
                      Time elapsed: 00:16:54
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 47802 steps/s (collection: 1.944s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 75.3507
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.6583
                       Mean reward: 86.27
               Mean episode length: 119.34
    Episode_Reward/reaching_object: 0.3422
     Episode_Reward/lifting_object: 16.3373
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.06s
                      Time elapsed: 00:16:56
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 46165 steps/s (collection: 2.004s, learning 0.126s)
             Mean action noise std: 1.77
          Mean value_function loss: 86.8054
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.6585
                       Mean reward: 79.90
               Mean episode length: 104.73
    Episode_Reward/reaching_object: 0.3380
     Episode_Reward/lifting_object: 16.5106
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.13s
                      Time elapsed: 00:16:58
                               ETA: 01:06:41

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 42348 steps/s (collection: 2.227s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 83.2950
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 35.6586
                       Mean reward: 76.50
               Mean episode length: 106.58
    Episode_Reward/reaching_object: 0.3251
     Episode_Reward/lifting_object: 15.9692
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.32s
                      Time elapsed: 00:17:00
                               ETA: 01:06:37

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 43664 steps/s (collection: 2.122s, learning 0.129s)
             Mean action noise std: 1.77
          Mean value_function loss: 85.3975
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 35.6586
                       Mean reward: 80.30
               Mean episode length: 113.69
    Episode_Reward/reaching_object: 0.3422
     Episode_Reward/lifting_object: 16.6939
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.25s
                      Time elapsed: 00:17:03
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 45520 steps/s (collection: 2.008s, learning 0.152s)
             Mean action noise std: 1.77
          Mean value_function loss: 76.1450
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.6587
                       Mean reward: 77.07
               Mean episode length: 109.43
    Episode_Reward/reaching_object: 0.3288
     Episode_Reward/lifting_object: 16.6146
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.16s
                      Time elapsed: 00:17:05
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 45688 steps/s (collection: 2.040s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 88.7107
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.6587
                       Mean reward: 75.38
               Mean episode length: 113.92
    Episode_Reward/reaching_object: 0.3427
     Episode_Reward/lifting_object: 16.9979
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.15s
                      Time elapsed: 00:17:07
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 34248 steps/s (collection: 2.700s, learning 0.170s)
             Mean action noise std: 1.77
          Mean value_function loss: 86.8445
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.6589
                       Mean reward: 97.00
               Mean episode length: 121.19
    Episode_Reward/reaching_object: 0.3442
     Episode_Reward/lifting_object: 17.1469
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.87s
                      Time elapsed: 00:17:10
                               ETA: 01:06:25

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 46492 steps/s (collection: 1.980s, learning 0.134s)
             Mean action noise std: 1.77
          Mean value_function loss: 79.7634
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6598
                       Mean reward: 88.70
               Mean episode length: 121.94
    Episode_Reward/reaching_object: 0.3492
     Episode_Reward/lifting_object: 17.7951
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.11s
                      Time elapsed: 00:17:12
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 46290 steps/s (collection: 2.023s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 83.0256
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.6613
                       Mean reward: 92.89
               Mean episode length: 117.09
    Episode_Reward/reaching_object: 0.3424
     Episode_Reward/lifting_object: 17.7996
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.12s
                      Time elapsed: 00:17:14
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 48289 steps/s (collection: 1.939s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 82.6844
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.6646
                       Mean reward: 89.77
               Mean episode length: 113.16
    Episode_Reward/reaching_object: 0.3437
     Episode_Reward/lifting_object: 18.0765
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.04s
                      Time elapsed: 00:17:16
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 47988 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 77.0039
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.6658
                       Mean reward: 96.73
               Mean episode length: 119.83
    Episode_Reward/reaching_object: 0.3505
     Episode_Reward/lifting_object: 18.5225
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.05s
                      Time elapsed: 00:17:18
                               ETA: 01:06:09

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 43902 steps/s (collection: 2.147s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 78.8365
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 35.6664
                       Mean reward: 82.97
               Mean episode length: 112.12
    Episode_Reward/reaching_object: 0.3381
     Episode_Reward/lifting_object: 17.5802
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 34.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.24s
                      Time elapsed: 00:17:20
                               ETA: 01:06:05

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 45979 steps/s (collection: 2.037s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 77.7397
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6676
                       Mean reward: 95.62
               Mean episode length: 114.79
    Episode_Reward/reaching_object: 0.3336
     Episode_Reward/lifting_object: 17.7178
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.14s
                      Time elapsed: 00:17:22
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 47776 steps/s (collection: 1.951s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 79.9666
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.6693
                       Mean reward: 85.35
               Mean episode length: 106.97
    Episode_Reward/reaching_object: 0.3403
     Episode_Reward/lifting_object: 18.0786
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.06s
                      Time elapsed: 00:17:24
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 49015 steps/s (collection: 1.915s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 84.3924
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.6713
                       Mean reward: 96.01
               Mean episode length: 116.92
    Episode_Reward/reaching_object: 0.3399
     Episode_Reward/lifting_object: 17.8057
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.01s
                      Time elapsed: 00:17:26
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 48733 steps/s (collection: 1.911s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 84.4324
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.6708
                       Mean reward: 95.26
               Mean episode length: 114.27
    Episode_Reward/reaching_object: 0.3280
     Episode_Reward/lifting_object: 17.0620
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.02s
                      Time elapsed: 00:17:29
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 48230 steps/s (collection: 1.953s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 89.5626
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.6696
                       Mean reward: 99.14
               Mean episode length: 119.21
    Episode_Reward/reaching_object: 0.3507
     Episode_Reward/lifting_object: 18.9959
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.04s
                      Time elapsed: 00:17:31
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 46820 steps/s (collection: 1.994s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 89.2763
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.6702
                       Mean reward: 98.65
               Mean episode length: 122.65
    Episode_Reward/reaching_object: 0.3412
     Episode_Reward/lifting_object: 18.0952
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.10s
                      Time elapsed: 00:17:33
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 48062 steps/s (collection: 1.939s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 79.7901
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6713
                       Mean reward: 102.09
               Mean episode length: 119.10
    Episode_Reward/reaching_object: 0.3480
     Episode_Reward/lifting_object: 18.2295
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.05s
                      Time elapsed: 00:17:35
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 47435 steps/s (collection: 1.965s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 87.8139
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.6726
                       Mean reward: 95.96
               Mean episode length: 117.96
    Episode_Reward/reaching_object: 0.3582
     Episode_Reward/lifting_object: 19.4571
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.07s
                      Time elapsed: 00:17:37
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 49008 steps/s (collection: 1.913s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 82.7304
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.6732
                       Mean reward: 104.69
               Mean episode length: 122.96
    Episode_Reward/reaching_object: 0.3580
     Episode_Reward/lifting_object: 19.4226
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.01s
                      Time elapsed: 00:17:39
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 47489 steps/s (collection: 1.937s, learning 0.133s)
             Mean action noise std: 1.77
          Mean value_function loss: 80.1812
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.6728
                       Mean reward: 102.08
               Mean episode length: 118.80
    Episode_Reward/reaching_object: 0.3534
     Episode_Reward/lifting_object: 19.6808
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.07s
                      Time elapsed: 00:17:41
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 46715 steps/s (collection: 1.984s, learning 0.120s)
             Mean action noise std: 1.77
          Mean value_function loss: 81.4576
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.6721
                       Mean reward: 99.11
               Mean episode length: 118.40
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: 19.0624
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.10s
                      Time elapsed: 00:17:43
                               ETA: 01:05:20

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 43065 steps/s (collection: 2.137s, learning 0.146s)
             Mean action noise std: 1.77
          Mean value_function loss: 85.1252
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 35.6717
                       Mean reward: 100.83
               Mean episode length: 119.56
    Episode_Reward/reaching_object: 0.3669
     Episode_Reward/lifting_object: 20.7040
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.28s
                      Time elapsed: 00:17:45
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 39209 steps/s (collection: 2.392s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 91.3582
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.6719
                       Mean reward: 105.96
               Mean episode length: 123.75
    Episode_Reward/reaching_object: 0.3655
     Episode_Reward/lifting_object: 20.3354
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.51s
                      Time elapsed: 00:17:48
                               ETA: 01:05:14

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 44274 steps/s (collection: 2.129s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 87.1249
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 35.6732
                       Mean reward: 99.24
               Mean episode length: 114.31
    Episode_Reward/reaching_object: 0.3571
     Episode_Reward/lifting_object: 20.1272
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.22s
                      Time elapsed: 00:17:50
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 43135 steps/s (collection: 2.118s, learning 0.161s)
             Mean action noise std: 1.77
          Mean value_function loss: 95.3135
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.6742
                       Mean reward: 106.07
               Mean episode length: 121.13
    Episode_Reward/reaching_object: 0.3722
     Episode_Reward/lifting_object: 20.9183
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.28s
                      Time elapsed: 00:17:52
                               ETA: 01:05:07

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 40694 steps/s (collection: 2.265s, learning 0.150s)
             Mean action noise std: 1.77
          Mean value_function loss: 86.6696
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.6762
                       Mean reward: 103.39
               Mean episode length: 117.86
    Episode_Reward/reaching_object: 0.3663
     Episode_Reward/lifting_object: 21.4791
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.42s
                      Time elapsed: 00:17:55
                               ETA: 01:05:04

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 36050 steps/s (collection: 2.549s, learning 0.178s)
             Mean action noise std: 1.77
          Mean value_function loss: 91.8911
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.6787
                       Mean reward: 98.37
               Mean episode length: 122.69
    Episode_Reward/reaching_object: 0.3660
     Episode_Reward/lifting_object: 20.7406
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.73s
                      Time elapsed: 00:17:57
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 37239 steps/s (collection: 2.394s, learning 0.245s)
             Mean action noise std: 1.77
          Mean value_function loss: 104.3622
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6807
                       Mean reward: 113.69
               Mean episode length: 131.36
    Episode_Reward/reaching_object: 0.3632
     Episode_Reward/lifting_object: 20.9222
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.64s
                      Time elapsed: 00:18:00
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 41721 steps/s (collection: 2.264s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 130.7953
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6832
                       Mean reward: 109.57
               Mean episode length: 127.68
    Episode_Reward/reaching_object: 0.3640
     Episode_Reward/lifting_object: 20.3537
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.36s
                      Time elapsed: 00:18:02
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 45919 steps/s (collection: 2.041s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 102.9634
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 35.6863
                       Mean reward: 114.36
               Mean episode length: 127.78
    Episode_Reward/reaching_object: 0.3737
     Episode_Reward/lifting_object: 21.1213
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.14s
                      Time elapsed: 00:18:05
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 40065 steps/s (collection: 2.300s, learning 0.153s)
             Mean action noise std: 1.77
          Mean value_function loss: 91.9836
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.6870
                       Mean reward: 94.84
               Mean episode length: 118.41
    Episode_Reward/reaching_object: 0.3506
     Episode_Reward/lifting_object: 20.1110
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.45s
                      Time elapsed: 00:18:07
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 41616 steps/s (collection: 2.264s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 94.1845
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.6878
                       Mean reward: 95.89
               Mean episode length: 116.85
    Episode_Reward/reaching_object: 0.3612
     Episode_Reward/lifting_object: 20.6618
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.36s
                      Time elapsed: 00:18:09
                               ETA: 01:04:49

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 44844 steps/s (collection: 2.099s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.2099
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.6888
                       Mean reward: 111.23
               Mean episode length: 122.69
    Episode_Reward/reaching_object: 0.3579
     Episode_Reward/lifting_object: 20.5816
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.19s
                      Time elapsed: 00:18:12
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 44829 steps/s (collection: 2.058s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 98.6183
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6894
                       Mean reward: 113.87
               Mean episode length: 128.46
    Episode_Reward/reaching_object: 0.3570
     Episode_Reward/lifting_object: 20.7255
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.19s
                      Time elapsed: 00:18:14
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 44141 steps/s (collection: 2.074s, learning 0.154s)
             Mean action noise std: 1.78
          Mean value_function loss: 93.3139
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.6916
                       Mean reward: 104.93
               Mean episode length: 116.82
    Episode_Reward/reaching_object: 0.3829
     Episode_Reward/lifting_object: 22.3484
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.23s
                      Time elapsed: 00:18:16
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 43478 steps/s (collection: 2.108s, learning 0.153s)
             Mean action noise std: 1.78
          Mean value_function loss: 97.9820
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.6931
                       Mean reward: 119.29
               Mean episode length: 126.97
    Episode_Reward/reaching_object: 0.3643
     Episode_Reward/lifting_object: 21.6988
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.26s
                      Time elapsed: 00:18:18
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 37468 steps/s (collection: 2.503s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 102.1695
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.6943
                       Mean reward: 106.35
               Mean episode length: 110.61
    Episode_Reward/reaching_object: 0.3738
     Episode_Reward/lifting_object: 21.7604
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.62s
                      Time elapsed: 00:18:21
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 34085 steps/s (collection: 2.656s, learning 0.228s)
             Mean action noise std: 1.78
          Mean value_function loss: 97.2743
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.6946
                       Mean reward: 115.93
               Mean episode length: 125.78
    Episode_Reward/reaching_object: 0.3672
     Episode_Reward/lifting_object: 21.9039
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.88s
                      Time elapsed: 00:18:24
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 39328 steps/s (collection: 2.390s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.6654
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.6960
                       Mean reward: 105.52
               Mean episode length: 118.18
    Episode_Reward/reaching_object: 0.3726
     Episode_Reward/lifting_object: 22.2263
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.50s
                      Time elapsed: 00:18:26
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 39170 steps/s (collection: 2.394s, learning 0.116s)
             Mean action noise std: 1.78
          Mean value_function loss: 85.9992
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 35.6966
                       Mean reward: 119.06
               Mean episode length: 129.25
    Episode_Reward/reaching_object: 0.3695
     Episode_Reward/lifting_object: 21.9523
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.51s
                      Time elapsed: 00:18:29
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 36018 steps/s (collection: 2.499s, learning 0.230s)
             Mean action noise std: 1.78
          Mean value_function loss: 97.7732
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6970
                       Mean reward: 109.10
               Mean episode length: 119.80
    Episode_Reward/reaching_object: 0.3671
     Episode_Reward/lifting_object: 21.9300
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.73s
                      Time elapsed: 00:18:31
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 31408 steps/s (collection: 2.891s, learning 0.239s)
             Mean action noise std: 1.78
          Mean value_function loss: 103.4674
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6983
                       Mean reward: 127.10
               Mean episode length: 135.22
    Episode_Reward/reaching_object: 0.3716
     Episode_Reward/lifting_object: 22.3378
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 3.13s
                      Time elapsed: 00:18:35
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 31517 steps/s (collection: 2.970s, learning 0.149s)
             Mean action noise std: 1.78
          Mean value_function loss: 93.6259
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.7000
                       Mean reward: 126.84
               Mean episode length: 129.96
    Episode_Reward/reaching_object: 0.3865
     Episode_Reward/lifting_object: 23.4029
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 3.12s
                      Time elapsed: 00:18:38
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 32392 steps/s (collection: 2.841s, learning 0.194s)
             Mean action noise std: 1.78
          Mean value_function loss: 101.4036
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.7005
                       Mean reward: 108.84
               Mean episode length: 116.97
    Episode_Reward/reaching_object: 0.3765
     Episode_Reward/lifting_object: 22.7862
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 3.03s
                      Time elapsed: 00:18:41
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 34322 steps/s (collection: 2.739s, learning 0.125s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.5938
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.7011
                       Mean reward: 115.63
               Mean episode length: 125.10
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: 22.9172
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.86s
                      Time elapsed: 00:18:44
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 30218 steps/s (collection: 3.111s, learning 0.142s)
             Mean action noise std: 1.78
          Mean value_function loss: 110.2405
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.7008
                       Mean reward: 125.20
               Mean episode length: 131.24
    Episode_Reward/reaching_object: 0.3944
     Episode_Reward/lifting_object: 23.6838
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 3.25s
                      Time elapsed: 00:18:47
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 35318 steps/s (collection: 2.660s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.4302
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.7026
                       Mean reward: 111.64
               Mean episode length: 124.16
    Episode_Reward/reaching_object: 0.3961
     Episode_Reward/lifting_object: 23.5137
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.78s
                      Time elapsed: 00:18:50
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 38588 steps/s (collection: 2.353s, learning 0.195s)
             Mean action noise std: 1.78
          Mean value_function loss: 90.5495
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.7043
                       Mean reward: 122.80
               Mean episode length: 127.48
    Episode_Reward/reaching_object: 0.3908
     Episode_Reward/lifting_object: 23.4465
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.55s
                      Time elapsed: 00:18:52
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 33507 steps/s (collection: 2.646s, learning 0.287s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.7118
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.7056
                       Mean reward: 128.63
               Mean episode length: 134.51
    Episode_Reward/reaching_object: 0.4107
     Episode_Reward/lifting_object: 25.2255
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.93s
                      Time elapsed: 00:18:55
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 30250 steps/s (collection: 3.018s, learning 0.231s)
             Mean action noise std: 1.78
          Mean value_function loss: 108.4654
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.7062
                       Mean reward: 120.02
               Mean episode length: 124.53
    Episode_Reward/reaching_object: 0.3999
     Episode_Reward/lifting_object: 24.6576
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 3.25s
                      Time elapsed: 00:18:58
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 28027 steps/s (collection: 3.297s, learning 0.211s)
             Mean action noise std: 1.78
          Mean value_function loss: 99.0766
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.7060
                       Mean reward: 124.58
               Mean episode length: 131.71
    Episode_Reward/reaching_object: 0.3995
     Episode_Reward/lifting_object: 24.4413
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 3.51s
                      Time elapsed: 00:19:02
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 30795 steps/s (collection: 2.993s, learning 0.199s)
             Mean action noise std: 1.78
          Mean value_function loss: 116.8919
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.7064
                       Mean reward: 132.51
               Mean episode length: 133.33
    Episode_Reward/reaching_object: 0.4235
     Episode_Reward/lifting_object: 26.1562
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 3.19s
                      Time elapsed: 00:19:05
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 32453 steps/s (collection: 2.859s, learning 0.170s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.1065
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.7078
                       Mean reward: 129.55
               Mean episode length: 134.29
    Episode_Reward/reaching_object: 0.4073
     Episode_Reward/lifting_object: 24.5420
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 3.03s
                      Time elapsed: 00:19:08
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 34173 steps/s (collection: 2.611s, learning 0.265s)
             Mean action noise std: 1.78
          Mean value_function loss: 111.6560
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7093
                       Mean reward: 121.00
               Mean episode length: 128.08
    Episode_Reward/reaching_object: 0.4036
     Episode_Reward/lifting_object: 24.9056
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.88s
                      Time elapsed: 00:19:11
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 33868 steps/s (collection: 2.755s, learning 0.148s)
             Mean action noise std: 1.78
          Mean value_function loss: 119.2952
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.7099
                       Mean reward: 118.78
               Mean episode length: 121.51
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: 24.5384
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.90s
                      Time elapsed: 00:19:14
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 31333 steps/s (collection: 3.002s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 105.7980
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.7115
                       Mean reward: 123.93
               Mean episode length: 119.14
    Episode_Reward/reaching_object: 0.3929
     Episode_Reward/lifting_object: 24.9724
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 3.14s
                      Time elapsed: 00:19:17
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 37396 steps/s (collection: 2.449s, learning 0.180s)
             Mean action noise std: 1.78
          Mean value_function loss: 107.2471
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.7137
                       Mean reward: 124.87
               Mean episode length: 128.79
    Episode_Reward/reaching_object: 0.3792
     Episode_Reward/lifting_object: 23.8602
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.63s
                      Time elapsed: 00:19:20
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 31272 steps/s (collection: 2.986s, learning 0.157s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.7066
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.7150
                       Mean reward: 130.01
               Mean episode length: 128.20
    Episode_Reward/reaching_object: 0.3942
     Episode_Reward/lifting_object: 25.4008
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 3.14s
                      Time elapsed: 00:19:23
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 33987 steps/s (collection: 2.701s, learning 0.191s)
             Mean action noise std: 1.78
          Mean value_function loss: 106.4386
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 35.7164
                       Mean reward: 125.45
               Mean episode length: 125.65
    Episode_Reward/reaching_object: 0.3949
     Episode_Reward/lifting_object: 25.3149
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.89s
                      Time elapsed: 00:19:26
                               ETA: 01:04:12

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 37571 steps/s (collection: 2.475s, learning 0.141s)
             Mean action noise std: 1.78
          Mean value_function loss: 101.4222
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 35.7169
                       Mean reward: 135.36
               Mean episode length: 129.21
    Episode_Reward/reaching_object: 0.3926
     Episode_Reward/lifting_object: 25.1547
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.62s
                      Time elapsed: 00:19:28
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 35760 steps/s (collection: 2.609s, learning 0.140s)
             Mean action noise std: 1.78
          Mean value_function loss: 86.7209
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.7172
                       Mean reward: 122.82
               Mean episode length: 123.27
    Episode_Reward/reaching_object: 0.3905
     Episode_Reward/lifting_object: 25.0648
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.75s
                      Time elapsed: 00:19:31
                               ETA: 01:04:08

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 39596 steps/s (collection: 2.349s, learning 0.134s)
             Mean action noise std: 1.78
          Mean value_function loss: 100.3585
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7172
                       Mean reward: 104.56
               Mean episode length: 114.77
    Episode_Reward/reaching_object: 0.3971
     Episode_Reward/lifting_object: 25.1099
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.48s
                      Time elapsed: 00:19:34
                               ETA: 01:04:05

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 41857 steps/s (collection: 2.239s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 98.2198
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7162
                       Mean reward: 122.37
               Mean episode length: 124.74
    Episode_Reward/reaching_object: 0.4014
     Episode_Reward/lifting_object: 25.7593
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.35s
                      Time elapsed: 00:19:36
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 44864 steps/s (collection: 2.089s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.9019
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.7162
                       Mean reward: 138.77
               Mean episode length: 131.72
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: 26.9845
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.19s
                      Time elapsed: 00:19:38
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 44412 steps/s (collection: 2.113s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 103.7981
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 35.7170
                       Mean reward: 132.16
               Mean episode length: 138.47
    Episode_Reward/reaching_object: 0.4185
     Episode_Reward/lifting_object: 27.2645
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.21s
                      Time elapsed: 00:19:40
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 47491 steps/s (collection: 1.944s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 105.1300
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7171
                       Mean reward: 136.04
               Mean episode length: 133.98
    Episode_Reward/reaching_object: 0.4349
     Episode_Reward/lifting_object: 28.1721
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.07s
                      Time elapsed: 00:19:42
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47360 steps/s (collection: 1.974s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 119.3061
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.7168
                       Mean reward: 134.87
               Mean episode length: 133.12
    Episode_Reward/reaching_object: 0.4145
     Episode_Reward/lifting_object: 26.8532
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.08s
                      Time elapsed: 00:19:44
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 46704 steps/s (collection: 1.981s, learning 0.124s)
             Mean action noise std: 1.78
          Mean value_function loss: 105.5547
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 35.7162
                       Mean reward: 149.90
               Mean episode length: 143.05
    Episode_Reward/reaching_object: 0.4364
     Episode_Reward/lifting_object: 28.3568
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.10s
                      Time elapsed: 00:19:47
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 45698 steps/s (collection: 2.016s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.4050
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.7167
                       Mean reward: 150.09
               Mean episode length: 145.10
    Episode_Reward/reaching_object: 0.4473
     Episode_Reward/lifting_object: 28.6678
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.15s
                      Time elapsed: 00:19:49
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 45183 steps/s (collection: 2.063s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 94.7433
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7173
                       Mean reward: 118.30
               Mean episode length: 120.79
    Episode_Reward/reaching_object: 0.4188
     Episode_Reward/lifting_object: 26.9226
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.18s
                      Time elapsed: 00:19:51
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 47905 steps/s (collection: 1.961s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 99.3512
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.7197
                       Mean reward: 141.53
               Mean episode length: 133.07
    Episode_Reward/reaching_object: 0.4272
     Episode_Reward/lifting_object: 28.1591
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.05s
                      Time elapsed: 00:19:53
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 46919 steps/s (collection: 1.988s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 124.4510
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.7238
                       Mean reward: 148.49
               Mean episode length: 140.23
    Episode_Reward/reaching_object: 0.4138
     Episode_Reward/lifting_object: 27.5513
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.10s
                      Time elapsed: 00:19:55
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 44132 steps/s (collection: 2.127s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 103.5987
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7262
                       Mean reward: 118.36
               Mean episode length: 122.69
    Episode_Reward/reaching_object: 0.4105
     Episode_Reward/lifting_object: 26.7267
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.23s
                      Time elapsed: 00:19:57
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 47239 steps/s (collection: 1.984s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.2869
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.7278
                       Mean reward: 136.57
               Mean episode length: 130.66
    Episode_Reward/reaching_object: 0.4123
     Episode_Reward/lifting_object: 27.0725
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.08s
                      Time elapsed: 00:19:59
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 45685 steps/s (collection: 2.047s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 110.8305
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.7296
                       Mean reward: 129.59
               Mean episode length: 127.61
    Episode_Reward/reaching_object: 0.4175
     Episode_Reward/lifting_object: 27.5142
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.15s
                      Time elapsed: 00:20:01
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 48428 steps/s (collection: 1.932s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 111.7989
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.7302
                       Mean reward: 137.03
               Mean episode length: 129.16
    Episode_Reward/reaching_object: 0.4224
     Episode_Reward/lifting_object: 28.2202
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.03s
                      Time elapsed: 00:20:03
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 46784 steps/s (collection: 2.008s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 107.8452
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 35.7307
                       Mean reward: 142.25
               Mean episode length: 135.18
    Episode_Reward/reaching_object: 0.4289
     Episode_Reward/lifting_object: 28.7956
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.10s
                      Time elapsed: 00:20:06
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 49009 steps/s (collection: 1.919s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 99.1485
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 35.7312
                       Mean reward: 141.30
               Mean episode length: 131.67
    Episode_Reward/reaching_object: 0.4090
     Episode_Reward/lifting_object: 26.8587
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.01s
                      Time elapsed: 00:20:08
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 46068 steps/s (collection: 2.045s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 100.9414
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.7313
                       Mean reward: 132.23
               Mean episode length: 127.49
    Episode_Reward/reaching_object: 0.4098
     Episode_Reward/lifting_object: 27.5780
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.13s
                      Time elapsed: 00:20:10
                               ETA: 01:03:02

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 38261 steps/s (collection: 2.379s, learning 0.190s)
             Mean action noise std: 1.78
          Mean value_function loss: 110.0306
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.7317
                       Mean reward: 143.77
               Mean episode length: 135.46
    Episode_Reward/reaching_object: 0.4099
     Episode_Reward/lifting_object: 27.4558
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.57s
                      Time elapsed: 00:20:12
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 35486 steps/s (collection: 2.392s, learning 0.378s)
             Mean action noise std: 1.78
          Mean value_function loss: 109.8340
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.7317
                       Mean reward: 165.44
               Mean episode length: 147.95
    Episode_Reward/reaching_object: 0.4418
     Episode_Reward/lifting_object: 29.8847
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.77s
                      Time elapsed: 00:20:15
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 36596 steps/s (collection: 2.564s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 118.2932
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.7303
                       Mean reward: 151.06
               Mean episode length: 137.72
    Episode_Reward/reaching_object: 0.4215
     Episode_Reward/lifting_object: 28.1544
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.69s
                      Time elapsed: 00:20:18
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 39391 steps/s (collection: 2.332s, learning 0.163s)
             Mean action noise std: 1.78
          Mean value_function loss: 117.1462
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.7303
                       Mean reward: 139.85
               Mean episode length: 130.46
    Episode_Reward/reaching_object: 0.4252
     Episode_Reward/lifting_object: 28.6942
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.50s
                      Time elapsed: 00:20:20
                               ETA: 01:02:54

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 35746 steps/s (collection: 2.607s, learning 0.143s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.1408
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7327
                       Mean reward: 151.09
               Mean episode length: 142.34
    Episode_Reward/reaching_object: 0.4344
     Episode_Reward/lifting_object: 29.1401
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.75s
                      Time elapsed: 00:20:23
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 39750 steps/s (collection: 2.337s, learning 0.136s)
             Mean action noise std: 1.78
          Mean value_function loss: 119.1419
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7358
                       Mean reward: 150.43
               Mean episode length: 135.22
    Episode_Reward/reaching_object: 0.4255
     Episode_Reward/lifting_object: 28.4331
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.47s
                      Time elapsed: 00:20:25
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 34439 steps/s (collection: 2.704s, learning 0.151s)
             Mean action noise std: 1.78
          Mean value_function loss: 115.4861
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.7374
                       Mean reward: 148.41
               Mean episode length: 137.40
    Episode_Reward/reaching_object: 0.4199
     Episode_Reward/lifting_object: 27.9982
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.85s
                      Time elapsed: 00:20:28
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 37091 steps/s (collection: 2.549s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.7813
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.7383
                       Mean reward: 149.37
               Mean episode length: 134.82
    Episode_Reward/reaching_object: 0.4199
     Episode_Reward/lifting_object: 28.6858
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.65s
                      Time elapsed: 00:20:31
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 48453 steps/s (collection: 1.915s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 118.3612
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 35.7395
                       Mean reward: 146.82
               Mean episode length: 138.01
    Episode_Reward/reaching_object: 0.4216
     Episode_Reward/lifting_object: 28.1484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.03s
                      Time elapsed: 00:20:33
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 48567 steps/s (collection: 1.925s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 108.6919
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.7400
                       Mean reward: 140.64
               Mean episode length: 124.09
    Episode_Reward/reaching_object: 0.4112
     Episode_Reward/lifting_object: 28.0369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.02s
                      Time elapsed: 00:20:35
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 45874 steps/s (collection: 2.044s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 118.1055
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.7409
                       Mean reward: 146.75
               Mean episode length: 130.89
    Episode_Reward/reaching_object: 0.4117
     Episode_Reward/lifting_object: 28.0326
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 29.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.14s
                      Time elapsed: 00:20:37
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 48195 steps/s (collection: 1.950s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 132.4449
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7437
                       Mean reward: 139.67
               Mean episode length: 136.38
    Episode_Reward/reaching_object: 0.4325
     Episode_Reward/lifting_object: 29.5681
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.04s
                      Time elapsed: 00:20:39
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 47767 steps/s (collection: 1.949s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 117.6863
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.7464
                       Mean reward: 160.03
               Mean episode length: 139.35
    Episode_Reward/reaching_object: 0.4330
     Episode_Reward/lifting_object: 29.5476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.06s
                      Time elapsed: 00:20:41
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 47783 steps/s (collection: 1.942s, learning 0.116s)
             Mean action noise std: 1.78
          Mean value_function loss: 113.3080
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.7469
                       Mean reward: 148.51
               Mean episode length: 129.17
    Episode_Reward/reaching_object: 0.4201
     Episode_Reward/lifting_object: 29.3589
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.06s
                      Time elapsed: 00:20:43
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 45563 steps/s (collection: 2.051s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 113.9627
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.7469
                       Mean reward: 147.12
               Mean episode length: 132.39
    Episode_Reward/reaching_object: 0.4135
     Episode_Reward/lifting_object: 28.6629
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.16s
                      Time elapsed: 00:20:45
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 48218 steps/s (collection: 1.949s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 128.5137
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 35.7469
                       Mean reward: 163.06
               Mean episode length: 140.84
    Episode_Reward/reaching_object: 0.4246
     Episode_Reward/lifting_object: 29.4417
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.04s
                      Time elapsed: 00:20:48
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 47620 steps/s (collection: 1.951s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 127.0361
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7474
                       Mean reward: 132.40
               Mean episode length: 128.74
    Episode_Reward/reaching_object: 0.4241
     Episode_Reward/lifting_object: 29.5567
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.06s
                      Time elapsed: 00:20:50
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 39626 steps/s (collection: 2.255s, learning 0.226s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.7019
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.7483
                       Mean reward: 149.93
               Mean episode length: 134.19
    Episode_Reward/reaching_object: 0.4173
     Episode_Reward/lifting_object: 29.1096
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.48s
                      Time elapsed: 00:20:52
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 36321 steps/s (collection: 2.509s, learning 0.198s)
             Mean action noise std: 1.78
          Mean value_function loss: 122.6856
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.7501
                       Mean reward: 148.90
               Mean episode length: 136.09
    Episode_Reward/reaching_object: 0.4167
     Episode_Reward/lifting_object: 28.9391
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.71s
                      Time elapsed: 00:20:55
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 33698 steps/s (collection: 2.772s, learning 0.145s)
             Mean action noise std: 1.78
          Mean value_function loss: 126.4593
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.7513
                       Mean reward: 151.08
               Mean episode length: 136.16
    Episode_Reward/reaching_object: 0.4154
     Episode_Reward/lifting_object: 29.2290
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.92s
                      Time elapsed: 00:20:58
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 38507 steps/s (collection: 2.413s, learning 0.140s)
             Mean action noise std: 1.78
          Mean value_function loss: 122.8946
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.7520
                       Mean reward: 148.51
               Mean episode length: 133.04
    Episode_Reward/reaching_object: 0.4206
     Episode_Reward/lifting_object: 29.8612
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.55s
                      Time elapsed: 00:21:00
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 42480 steps/s (collection: 2.199s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 118.6955
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 35.7528
                       Mean reward: 151.38
               Mean episode length: 133.09
    Episode_Reward/reaching_object: 0.4036
     Episode_Reward/lifting_object: 28.3518
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.31s
                      Time elapsed: 00:21:03
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 39814 steps/s (collection: 2.330s, learning 0.139s)
             Mean action noise std: 1.78
          Mean value_function loss: 117.3713
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 35.7532
                       Mean reward: 148.31
               Mean episode length: 129.48
    Episode_Reward/reaching_object: 0.4201
     Episode_Reward/lifting_object: 29.7577
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.47s
                      Time elapsed: 00:21:05
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 39725 steps/s (collection: 2.346s, learning 0.128s)
             Mean action noise std: 1.78
          Mean value_function loss: 130.2686
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.7534
                       Mean reward: 144.25
               Mean episode length: 126.64
    Episode_Reward/reaching_object: 0.4129
     Episode_Reward/lifting_object: 29.3366
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.47s
                      Time elapsed: 00:21:08
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 39881 steps/s (collection: 2.354s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 166.9414
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 35.7541
                       Mean reward: 139.49
               Mean episode length: 122.30
    Episode_Reward/reaching_object: 0.4111
     Episode_Reward/lifting_object: 29.9088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.46s
                      Time elapsed: 00:21:10
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 46501 steps/s (collection: 1.986s, learning 0.128s)
             Mean action noise std: 1.78
          Mean value_function loss: 162.9063
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.7548
                       Mean reward: 156.18
               Mean episode length: 137.69
    Episode_Reward/reaching_object: 0.4145
     Episode_Reward/lifting_object: 29.1294
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.11s
                      Time elapsed: 00:21:12
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 44189 steps/s (collection: 2.097s, learning 0.128s)
             Mean action noise std: 1.78
          Mean value_function loss: 128.2807
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7556
                       Mean reward: 151.88
               Mean episode length: 123.61
    Episode_Reward/reaching_object: 0.4151
     Episode_Reward/lifting_object: 29.5112
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.22s
                      Time elapsed: 00:21:14
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 45803 steps/s (collection: 2.025s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 126.8010
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.7573
                       Mean reward: 172.27
               Mean episode length: 144.67
    Episode_Reward/reaching_object: 0.4303
     Episode_Reward/lifting_object: 30.9396
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.15s
                      Time elapsed: 00:21:16
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 45929 steps/s (collection: 2.035s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 147.4198
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.7596
                       Mean reward: 160.38
               Mean episode length: 136.25
    Episode_Reward/reaching_object: 0.4238
     Episode_Reward/lifting_object: 30.4821
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.14s
                      Time elapsed: 00:21:19
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 47992 steps/s (collection: 1.957s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.7389
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.7617
                       Mean reward: 159.22
               Mean episode length: 134.76
    Episode_Reward/reaching_object: 0.4222
     Episode_Reward/lifting_object: 29.5418
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.05s
                      Time elapsed: 00:21:21
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 48220 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 141.8532
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7643
                       Mean reward: 143.51
               Mean episode length: 129.14
    Episode_Reward/reaching_object: 0.4262
     Episode_Reward/lifting_object: 30.6493
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.04s
                      Time elapsed: 00:21:23
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.964s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 150.1784
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7673
                       Mean reward: 155.35
               Mean episode length: 130.66
    Episode_Reward/reaching_object: 0.4345
     Episode_Reward/lifting_object: 31.4771
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.06s
                      Time elapsed: 00:21:25
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 48324 steps/s (collection: 1.944s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 126.3994
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.7687
                       Mean reward: 148.19
               Mean episode length: 125.92
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: 30.1555
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.03s
                      Time elapsed: 00:21:27
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 48162 steps/s (collection: 1.929s, learning 0.112s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.9968
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7712
                       Mean reward: 149.25
               Mean episode length: 133.56
    Episode_Reward/reaching_object: 0.4195
     Episode_Reward/lifting_object: 30.3376
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.04s
                      Time elapsed: 00:21:29
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 47861 steps/s (collection: 1.954s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 122.8915
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.7738
                       Mean reward: 159.87
               Mean episode length: 132.92
    Episode_Reward/reaching_object: 0.4212
     Episode_Reward/lifting_object: 30.8686
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.05s
                      Time elapsed: 00:21:31
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 48562 steps/s (collection: 1.914s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 130.2543
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 35.7763
                       Mean reward: 149.68
               Mean episode length: 127.33
    Episode_Reward/reaching_object: 0.4332
     Episode_Reward/lifting_object: 31.5268
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.02s
                      Time elapsed: 00:21:33
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 47450 steps/s (collection: 1.963s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.3313
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.7765
                       Mean reward: 183.83
               Mean episode length: 151.45
    Episode_Reward/reaching_object: 0.4381
     Episode_Reward/lifting_object: 32.0014
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.07s
                      Time elapsed: 00:21:35
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 45001 steps/s (collection: 2.064s, learning 0.121s)
             Mean action noise std: 1.79
          Mean value_function loss: 154.8496
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.7771
                       Mean reward: 167.89
               Mean episode length: 139.51
    Episode_Reward/reaching_object: 0.4290
     Episode_Reward/lifting_object: 30.9418
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.18s
                      Time elapsed: 00:21:37
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 44817 steps/s (collection: 2.090s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 117.5526
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.7781
                       Mean reward: 160.11
               Mean episode length: 141.15
    Episode_Reward/reaching_object: 0.4463
     Episode_Reward/lifting_object: 32.8182
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.19s
                      Time elapsed: 00:21:39
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 42777 steps/s (collection: 2.098s, learning 0.200s)
             Mean action noise std: 1.79
          Mean value_function loss: 118.5221
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.7785
                       Mean reward: 170.89
               Mean episode length: 141.85
    Episode_Reward/reaching_object: 0.4477
     Episode_Reward/lifting_object: 33.1382
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.30s
                      Time elapsed: 00:21:42
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 35033 steps/s (collection: 2.645s, learning 0.161s)
             Mean action noise std: 1.79
          Mean value_function loss: 140.7976
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7791
                       Mean reward: 157.95
               Mean episode length: 135.69
    Episode_Reward/reaching_object: 0.4375
     Episode_Reward/lifting_object: 31.8822
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.81s
                      Time elapsed: 00:21:44
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 35606 steps/s (collection: 2.444s, learning 0.317s)
             Mean action noise std: 1.79
          Mean value_function loss: 132.7791
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7806
                       Mean reward: 172.12
               Mean episode length: 143.14
    Episode_Reward/reaching_object: 0.4544
     Episode_Reward/lifting_object: 33.3174
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.76s
                      Time elapsed: 00:21:47
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 35591 steps/s (collection: 2.627s, learning 0.135s)
             Mean action noise std: 1.79
          Mean value_function loss: 141.0742
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.7820
                       Mean reward: 186.37
               Mean episode length: 151.75
    Episode_Reward/reaching_object: 0.4709
     Episode_Reward/lifting_object: 34.5820
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.76s
                      Time elapsed: 00:21:50
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 37452 steps/s (collection: 2.516s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 138.4698
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 35.7827
                       Mean reward: 170.94
               Mean episode length: 140.92
    Episode_Reward/reaching_object: 0.4455
     Episode_Reward/lifting_object: 33.1026
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.62s
                      Time elapsed: 00:21:53
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 43111 steps/s (collection: 2.160s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 123.7928
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.7842
                       Mean reward: 179.96
               Mean episode length: 148.43
    Episode_Reward/reaching_object: 0.4495
     Episode_Reward/lifting_object: 33.3480
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.28s
                      Time elapsed: 00:21:55
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 48265 steps/s (collection: 1.937s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 147.1105
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.7860
                       Mean reward: 164.86
               Mean episode length: 146.97
    Episode_Reward/reaching_object: 0.4556
     Episode_Reward/lifting_object: 34.0297
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.04s
                      Time elapsed: 00:21:57
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 49377 steps/s (collection: 1.899s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 120.1924
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7880
                       Mean reward: 169.10
               Mean episode length: 137.33
    Episode_Reward/reaching_object: 0.4686
     Episode_Reward/lifting_object: 33.7239
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.99s
                      Time elapsed: 00:21:59
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 48679 steps/s (collection: 1.922s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 125.5486
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 35.7885
                       Mean reward: 186.80
               Mean episode length: 154.18
    Episode_Reward/reaching_object: 0.4752
     Episode_Reward/lifting_object: 35.2008
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.02s
                      Time elapsed: 00:22:01
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 48560 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 193.3722
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7898
                       Mean reward: 166.97
               Mean episode length: 138.42
    Episode_Reward/reaching_object: 0.4694
     Episode_Reward/lifting_object: 34.4129
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.02s
                      Time elapsed: 00:22:03
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 47844 steps/s (collection: 1.962s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 173.4839
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7909
                       Mean reward: 164.88
               Mean episode length: 139.65
    Episode_Reward/reaching_object: 0.4543
     Episode_Reward/lifting_object: 33.3378
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.05s
                      Time elapsed: 00:22:05
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.945s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 147.8558
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.7916
                       Mean reward: 167.95
               Mean episode length: 131.46
    Episode_Reward/reaching_object: 0.4464
     Episode_Reward/lifting_object: 32.9787
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.04s
                      Time elapsed: 00:22:07
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 46227 steps/s (collection: 2.024s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 136.1669
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.7941
                       Mean reward: 160.92
               Mean episode length: 135.34
    Episode_Reward/reaching_object: 0.4341
     Episode_Reward/lifting_object: 31.8836
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.13s
                      Time elapsed: 00:22:09
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 45237 steps/s (collection: 2.072s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 144.1994
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.7965
                       Mean reward: 176.74
               Mean episode length: 143.03
    Episode_Reward/reaching_object: 0.4465
     Episode_Reward/lifting_object: 33.3395
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.17s
                      Time elapsed: 00:22:11
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 44003 steps/s (collection: 2.094s, learning 0.140s)
             Mean action noise std: 1.79
          Mean value_function loss: 134.1037
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.7968
                       Mean reward: 174.99
               Mean episode length: 144.63
    Episode_Reward/reaching_object: 0.4510
     Episode_Reward/lifting_object: 33.3790
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.23s
                      Time elapsed: 00:22:14
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 45776 steps/s (collection: 2.049s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 124.1328
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 35.7971
                       Mean reward: 167.07
               Mean episode length: 142.13
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 33.2964
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.15s
                      Time elapsed: 00:22:16
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 46877 steps/s (collection: 1.993s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.4204
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 35.7974
                       Mean reward: 167.82
               Mean episode length: 139.91
    Episode_Reward/reaching_object: 0.4503
     Episode_Reward/lifting_object: 33.7095
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.10s
                      Time elapsed: 00:22:18
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 45923 steps/s (collection: 2.040s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 129.6689
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 35.7980
                       Mean reward: 185.71
               Mean episode length: 143.01
    Episode_Reward/reaching_object: 0.4291
     Episode_Reward/lifting_object: 32.3638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.14s
                      Time elapsed: 00:22:20
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 46641 steps/s (collection: 2.015s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 149.2999
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.7982
                       Mean reward: 170.64
               Mean episode length: 139.55
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 34.3840
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.11s
                      Time elapsed: 00:22:22
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 44772 steps/s (collection: 2.074s, learning 0.122s)
             Mean action noise std: 1.79
          Mean value_function loss: 127.3195
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.7986
                       Mean reward: 166.41
               Mean episode length: 134.94
    Episode_Reward/reaching_object: 0.4485
     Episode_Reward/lifting_object: 33.8996
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.20s
                      Time elapsed: 00:22:24
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 47118 steps/s (collection: 1.967s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 132.4699
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.7995
                       Mean reward: 184.79
               Mean episode length: 146.32
    Episode_Reward/reaching_object: 0.4579
     Episode_Reward/lifting_object: 34.3613
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.09s
                      Time elapsed: 00:22:26
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 47615 steps/s (collection: 1.942s, learning 0.123s)
             Mean action noise std: 1.79
          Mean value_function loss: 125.2491
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.8005
                       Mean reward: 163.24
               Mean episode length: 132.80
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 33.7754
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.06s
                      Time elapsed: 00:22:28
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 48870 steps/s (collection: 1.921s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 129.4711
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.8011
                       Mean reward: 178.34
               Mean episode length: 143.57
    Episode_Reward/reaching_object: 0.4402
     Episode_Reward/lifting_object: 32.9759
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.01s
                      Time elapsed: 00:22:30
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 47106 steps/s (collection: 1.950s, learning 0.137s)
             Mean action noise std: 1.79
          Mean value_function loss: 123.4101
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.8021
                       Mean reward: 164.87
               Mean episode length: 141.45
    Episode_Reward/reaching_object: 0.4529
     Episode_Reward/lifting_object: 34.0244
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.09s
                      Time elapsed: 00:22:33
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 49329 steps/s (collection: 1.900s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 130.9414
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.8032
                       Mean reward: 200.06
               Mean episode length: 151.01
    Episode_Reward/reaching_object: 0.4550
     Episode_Reward/lifting_object: 35.0208
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 1.99s
                      Time elapsed: 00:22:35
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 48125 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 133.9611
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.8055
                       Mean reward: 177.69
               Mean episode length: 140.58
    Episode_Reward/reaching_object: 0.4532
     Episode_Reward/lifting_object: 34.6697
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.04s
                      Time elapsed: 00:22:37
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 46820 steps/s (collection: 2.002s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 127.1413
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.8058
                       Mean reward: 178.62
               Mean episode length: 143.51
    Episode_Reward/reaching_object: 0.4616
     Episode_Reward/lifting_object: 34.8538
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.10s
                      Time elapsed: 00:22:39
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 49507 steps/s (collection: 1.894s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 134.7446
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 35.8059
                       Mean reward: 179.29
               Mean episode length: 140.31
    Episode_Reward/reaching_object: 0.4396
     Episode_Reward/lifting_object: 33.9409
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 1.99s
                      Time elapsed: 00:22:41
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 49796 steps/s (collection: 1.874s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 125.7019
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.8049
                       Mean reward: 176.29
               Mean episode length: 144.55
    Episode_Reward/reaching_object: 0.4687
     Episode_Reward/lifting_object: 35.4510
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 1.97s
                      Time elapsed: 00:22:43
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 47837 steps/s (collection: 1.963s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 148.8702
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8051
                       Mean reward: 186.26
               Mean episode length: 145.98
    Episode_Reward/reaching_object: 0.4623
     Episode_Reward/lifting_object: 35.6830
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.05s
                      Time elapsed: 00:22:45
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 49584 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 143.1159
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.8083
                       Mean reward: 174.63
               Mean episode length: 140.28
    Episode_Reward/reaching_object: 0.4632
     Episode_Reward/lifting_object: 35.3437
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 1.98s
                      Time elapsed: 00:22:47
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 48165 steps/s (collection: 1.950s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 134.0132
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8092
                       Mean reward: 171.80
               Mean episode length: 143.90
    Episode_Reward/reaching_object: 0.4716
     Episode_Reward/lifting_object: 35.7528
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.04s
                      Time elapsed: 00:22:49
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 49478 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.5174
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 35.8091
                       Mean reward: 179.63
               Mean episode length: 141.89
    Episode_Reward/reaching_object: 0.4791
     Episode_Reward/lifting_object: 36.1265
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 1.99s
                      Time elapsed: 00:22:51
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.894s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 119.5620
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8092
                       Mean reward: 190.93
               Mean episode length: 156.45
    Episode_Reward/reaching_object: 0.4889
     Episode_Reward/lifting_object: 37.1820
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 1.98s
                      Time elapsed: 00:22:53
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 49411 steps/s (collection: 1.874s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.6528
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.8101
                       Mean reward: 184.82
               Mean episode length: 153.44
    Episode_Reward/reaching_object: 0.4917
     Episode_Reward/lifting_object: 36.8316
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.99s
                      Time elapsed: 00:22:55
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 49241 steps/s (collection: 1.888s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 115.3958
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 35.8104
                       Mean reward: 186.73
               Mean episode length: 149.37
    Episode_Reward/reaching_object: 0.4991
     Episode_Reward/lifting_object: 38.2713
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.00s
                      Time elapsed: 00:22:57
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 49270 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.9909
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.8104
                       Mean reward: 183.67
               Mean episode length: 144.85
    Episode_Reward/reaching_object: 0.4887
     Episode_Reward/lifting_object: 37.7144
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.00s
                      Time elapsed: 00:22:59
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 48756 steps/s (collection: 1.916s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 124.0928
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8105
                       Mean reward: 202.82
               Mean episode length: 156.24
    Episode_Reward/reaching_object: 0.5080
     Episode_Reward/lifting_object: 38.2528
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.02s
                      Time elapsed: 00:23:01
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 49176 steps/s (collection: 1.902s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.8195
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.8105
                       Mean reward: 210.30
               Mean episode length: 166.64
    Episode_Reward/reaching_object: 0.5187
     Episode_Reward/lifting_object: 39.8374
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.00s
                      Time elapsed: 00:23:03
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 50063 steps/s (collection: 1.873s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 123.6618
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.8106
                       Mean reward: 179.89
               Mean episode length: 146.67
    Episode_Reward/reaching_object: 0.4931
     Episode_Reward/lifting_object: 37.6724
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 1.96s
                      Time elapsed: 00:23:05
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 49946 steps/s (collection: 1.874s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 131.0572
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8105
                       Mean reward: 199.64
               Mean episode length: 161.08
    Episode_Reward/reaching_object: 0.4968
     Episode_Reward/lifting_object: 37.7723
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.97s
                      Time elapsed: 00:23:07
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 49301 steps/s (collection: 1.892s, learning 0.102s)
             Mean action noise std: 1.79
          Mean value_function loss: 138.6153
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8111
                       Mean reward: 196.06
               Mean episode length: 155.24
    Episode_Reward/reaching_object: 0.4998
     Episode_Reward/lifting_object: 38.0654
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 1.99s
                      Time elapsed: 00:23:09
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 49508 steps/s (collection: 1.899s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 120.8722
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8139
                       Mean reward: 189.24
               Mean episode length: 149.97
    Episode_Reward/reaching_object: 0.4784
     Episode_Reward/lifting_object: 36.3053
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 1.99s
                      Time elapsed: 00:23:11
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 49660 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 129.5517
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8155
                       Mean reward: 193.02
               Mean episode length: 149.68
    Episode_Reward/reaching_object: 0.5051
     Episode_Reward/lifting_object: 38.8338
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.98s
                      Time elapsed: 00:23:13
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 48980 steps/s (collection: 1.913s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 131.0952
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.8154
                       Mean reward: 195.09
               Mean episode length: 149.79
    Episode_Reward/reaching_object: 0.5048
     Episode_Reward/lifting_object: 38.5546
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.01s
                      Time elapsed: 00:23:15
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 49264 steps/s (collection: 1.899s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.5363
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8170
                       Mean reward: 184.16
               Mean episode length: 144.95
    Episode_Reward/reaching_object: 0.4911
     Episode_Reward/lifting_object: 37.8554
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.00s
                      Time elapsed: 00:23:17
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 48975 steps/s (collection: 1.912s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 125.2075
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.8198
                       Mean reward: 187.08
               Mean episode length: 148.22
    Episode_Reward/reaching_object: 0.4950
     Episode_Reward/lifting_object: 38.7013
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.01s
                      Time elapsed: 00:23:19
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 49728 steps/s (collection: 1.877s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 135.6650
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.8207
                       Mean reward: 195.81
               Mean episode length: 158.23
    Episode_Reward/reaching_object: 0.4979
     Episode_Reward/lifting_object: 38.2260
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 1.98s
                      Time elapsed: 00:23:21
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 49370 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 127.1919
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8208
                       Mean reward: 175.43
               Mean episode length: 142.44
    Episode_Reward/reaching_object: 0.5095
     Episode_Reward/lifting_object: 40.0446
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 1.99s
                      Time elapsed: 00:23:23
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 46793 steps/s (collection: 1.989s, learning 0.112s)
             Mean action noise std: 1.79
          Mean value_function loss: 127.6113
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8221
                       Mean reward: 190.81
               Mean episode length: 154.68
    Episode_Reward/reaching_object: 0.5132
     Episode_Reward/lifting_object: 40.1678
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.10s
                      Time elapsed: 00:23:25
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 47139 steps/s (collection: 1.964s, learning 0.122s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.1242
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.8231
                       Mean reward: 211.80
               Mean episode length: 155.36
    Episode_Reward/reaching_object: 0.5116
     Episode_Reward/lifting_object: 40.8065
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.09s
                      Time elapsed: 00:23:27
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 43633 steps/s (collection: 2.101s, learning 0.152s)
             Mean action noise std: 1.79
          Mean value_function loss: 119.3286
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.8231
                       Mean reward: 191.98
               Mean episode length: 149.74
    Episode_Reward/reaching_object: 0.5063
     Episode_Reward/lifting_object: 39.8844
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.25s
                      Time elapsed: 00:23:29
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 43828 steps/s (collection: 2.155s, learning 0.088s)
             Mean action noise std: 1.79
          Mean value_function loss: 130.4762
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.8222
                       Mean reward: 217.38
               Mean episode length: 163.56
    Episode_Reward/reaching_object: 0.5402
     Episode_Reward/lifting_object: 43.0679
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.24s
                      Time elapsed: 00:23:31
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 47608 steps/s (collection: 1.968s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.2866
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.8223
                       Mean reward: 211.96
               Mean episode length: 157.34
    Episode_Reward/reaching_object: 0.5325
     Episode_Reward/lifting_object: 42.2705
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.06s
                      Time elapsed: 00:23:33
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 47588 steps/s (collection: 1.975s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.0826
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.8233
                       Mean reward: 192.59
               Mean episode length: 153.54
    Episode_Reward/reaching_object: 0.5085
     Episode_Reward/lifting_object: 40.3280
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.07s
                      Time elapsed: 00:23:35
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 47913 steps/s (collection: 1.949s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.7670
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.8233
                       Mean reward: 196.51
               Mean episode length: 157.49
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 42.7597
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.05s
                      Time elapsed: 00:23:37
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 42663 steps/s (collection: 2.173s, learning 0.131s)
             Mean action noise std: 1.79
          Mean value_function loss: 127.8609
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8241
                       Mean reward: 208.39
               Mean episode length: 152.03
    Episode_Reward/reaching_object: 0.5278
     Episode_Reward/lifting_object: 42.7147
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.30s
                      Time elapsed: 00:23:40
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 46988 steps/s (collection: 1.939s, learning 0.154s)
             Mean action noise std: 1.79
          Mean value_function loss: 136.9425
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.8251
                       Mean reward: 217.79
               Mean episode length: 163.27
    Episode_Reward/reaching_object: 0.5090
     Episode_Reward/lifting_object: 40.9701
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.09s
                      Time elapsed: 00:23:42
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 43877 steps/s (collection: 2.126s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 133.7154
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8260
                       Mean reward: 218.25
               Mean episode length: 160.69
    Episode_Reward/reaching_object: 0.5159
     Episode_Reward/lifting_object: 41.7586
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.24s
                      Time elapsed: 00:23:44
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 43728 steps/s (collection: 2.139s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 160.2416
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 35.8271
                       Mean reward: 201.06
               Mean episode length: 155.34
    Episode_Reward/reaching_object: 0.5137
     Episode_Reward/lifting_object: 41.0875
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.25s
                      Time elapsed: 00:23:46
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 46464 steps/s (collection: 2.022s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 146.3258
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.8273
                       Mean reward: 202.25
               Mean episode length: 154.28
    Episode_Reward/reaching_object: 0.5169
     Episode_Reward/lifting_object: 42.2925
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.12s
                      Time elapsed: 00:23:48
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 43736 steps/s (collection: 2.091s, learning 0.157s)
             Mean action noise std: 1.79
          Mean value_function loss: 139.2428
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.8279
                       Mean reward: 208.77
               Mean episode length: 151.10
    Episode_Reward/reaching_object: 0.5079
     Episode_Reward/lifting_object: 41.7450
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.25s
                      Time elapsed: 00:23:51
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 42488 steps/s (collection: 2.194s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 150.3570
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.8283
                       Mean reward: 215.88
               Mean episode length: 153.85
    Episode_Reward/reaching_object: 0.5005
     Episode_Reward/lifting_object: 41.4516
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.31s
                      Time elapsed: 00:23:53
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 44805 steps/s (collection: 2.104s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 148.5672
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.8282
                       Mean reward: 221.04
               Mean episode length: 160.67
    Episode_Reward/reaching_object: 0.4957
     Episode_Reward/lifting_object: 40.7237
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.19s
                      Time elapsed: 00:23:55
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 48899 steps/s (collection: 1.919s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 148.9581
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.8282
                       Mean reward: 205.06
               Mean episode length: 150.65
    Episode_Reward/reaching_object: 0.5038
     Episode_Reward/lifting_object: 41.1786
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.01s
                      Time elapsed: 00:23:57
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 45029 steps/s (collection: 2.079s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 154.5349
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.8274
                       Mean reward: 204.02
               Mean episode length: 148.02
    Episode_Reward/reaching_object: 0.4919
     Episode_Reward/lifting_object: 40.5646
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.18s
                      Time elapsed: 00:23:59
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 46697 steps/s (collection: 1.997s, learning 0.108s)
             Mean action noise std: 1.79
          Mean value_function loss: 160.2472
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.8270
                       Mean reward: 205.14
               Mean episode length: 148.79
    Episode_Reward/reaching_object: 0.4883
     Episode_Reward/lifting_object: 40.3705
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.11s
                      Time elapsed: 00:24:01
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 39948 steps/s (collection: 2.191s, learning 0.270s)
             Mean action noise std: 1.79
          Mean value_function loss: 140.3116
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8262
                       Mean reward: 166.77
               Mean episode length: 124.88
    Episode_Reward/reaching_object: 0.4643
     Episode_Reward/lifting_object: 38.5412
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.46s
                      Time elapsed: 00:24:04
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 29441 steps/s (collection: 3.069s, learning 0.270s)
             Mean action noise std: 1.79
          Mean value_function loss: 150.1973
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.8247
                       Mean reward: 195.55
               Mean episode length: 143.93
    Episode_Reward/reaching_object: 0.4859
     Episode_Reward/lifting_object: 40.6720
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 3.34s
                      Time elapsed: 00:24:07
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 35343 steps/s (collection: 2.673s, learning 0.108s)
             Mean action noise std: 1.79
          Mean value_function loss: 161.9910
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8257
                       Mean reward: 214.94
               Mean episode length: 151.49
    Episode_Reward/reaching_object: 0.4897
     Episode_Reward/lifting_object: 40.9996
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.78s
                      Time elapsed: 00:24:10
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 48421 steps/s (collection: 1.931s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 163.8695
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8274
                       Mean reward: 205.86
               Mean episode length: 146.88
    Episode_Reward/reaching_object: 0.4817
     Episode_Reward/lifting_object: 40.8412
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.03s
                      Time elapsed: 00:24:12
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 45381 steps/s (collection: 2.043s, learning 0.123s)
             Mean action noise std: 1.79
          Mean value_function loss: 146.6491
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8282
                       Mean reward: 211.05
               Mean episode length: 149.42
    Episode_Reward/reaching_object: 0.4821
     Episode_Reward/lifting_object: 40.3641
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.17s
                      Time elapsed: 00:24:14
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 47664 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 172.1212
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.8287
                       Mean reward: 217.67
               Mean episode length: 156.34
    Episode_Reward/reaching_object: 0.4662
     Episode_Reward/lifting_object: 39.3065
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.06s
                      Time elapsed: 00:24:16
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 49464 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 153.8659
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.8289
                       Mean reward: 203.42
               Mean episode length: 142.81
    Episode_Reward/reaching_object: 0.4697
     Episode_Reward/lifting_object: 38.9497
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 1.99s
                      Time elapsed: 00:24:18
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 48943 steps/s (collection: 1.911s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 172.5505
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.8278
                       Mean reward: 209.86
               Mean episode length: 147.41
    Episode_Reward/reaching_object: 0.4766
     Episode_Reward/lifting_object: 39.9696
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.01s
                      Time elapsed: 00:24:20
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 48315 steps/s (collection: 1.942s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 156.7729
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8280
                       Mean reward: 188.70
               Mean episode length: 139.43
    Episode_Reward/reaching_object: 0.4587
     Episode_Reward/lifting_object: 38.2816
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.03s
                      Time elapsed: 00:24:22
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 49056 steps/s (collection: 1.917s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 161.7240
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.8276
                       Mean reward: 199.26
               Mean episode length: 145.80
    Episode_Reward/reaching_object: 0.4846
     Episode_Reward/lifting_object: 40.9287
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.00s
                      Time elapsed: 00:24:24
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 47565 steps/s (collection: 1.960s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 154.2796
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8271
                       Mean reward: 213.04
               Mean episode length: 143.52
    Episode_Reward/reaching_object: 0.4701
     Episode_Reward/lifting_object: 40.1236
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.07s
                      Time elapsed: 00:24:26
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 47908 steps/s (collection: 1.938s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 164.1379
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.8279
                       Mean reward: 220.50
               Mean episode length: 151.72
    Episode_Reward/reaching_object: 0.4905
     Episode_Reward/lifting_object: 42.4783
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.05s
                      Time elapsed: 00:24:28
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 49195 steps/s (collection: 1.888s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.6896
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8297
                       Mean reward: 213.53
               Mean episode length: 152.96
    Episode_Reward/reaching_object: 0.4860
     Episode_Reward/lifting_object: 40.8920
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.00s
                      Time elapsed: 00:24:30
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 49920 steps/s (collection: 1.878s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 147.2354
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8305
                       Mean reward: 218.26
               Mean episode length: 151.12
    Episode_Reward/reaching_object: 0.4938
     Episode_Reward/lifting_object: 42.5190
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 1.97s
                      Time elapsed: 00:24:32
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 47598 steps/s (collection: 1.938s, learning 0.127s)
             Mean action noise std: 1.80
          Mean value_function loss: 145.5810
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.8319
                       Mean reward: 239.19
               Mean episode length: 158.75
    Episode_Reward/reaching_object: 0.5227
     Episode_Reward/lifting_object: 45.6085
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.07s
                      Time elapsed: 00:24:34
                               ETA: 00:56:35

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45187 steps/s (collection: 2.067s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 152.5861
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.8338
                       Mean reward: 213.53
               Mean episode length: 151.76
    Episode_Reward/reaching_object: 0.4981
     Episode_Reward/lifting_object: 42.7089
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.18s
                      Time elapsed: 00:24:37
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 47855 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 157.2290
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8357
                       Mean reward: 214.92
               Mean episode length: 148.52
    Episode_Reward/reaching_object: 0.5038
     Episode_Reward/lifting_object: 43.5321
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.05s
                      Time elapsed: 00:24:39
                               ETA: 00:56:29

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 49145 steps/s (collection: 1.909s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 185.5923
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.8374
                       Mean reward: 230.25
               Mean episode length: 155.02
    Episode_Reward/reaching_object: 0.5143
     Episode_Reward/lifting_object: 45.3272
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.00s
                      Time elapsed: 00:24:41
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 48509 steps/s (collection: 1.923s, learning 0.104s)
             Mean action noise std: 1.80
          Mean value_function loss: 151.4942
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8392
                       Mean reward: 218.71
               Mean episode length: 148.16
    Episode_Reward/reaching_object: 0.5002
     Episode_Reward/lifting_object: 43.5736
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.03s
                      Time elapsed: 00:24:43
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 46845 steps/s (collection: 2.000s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 171.5025
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.8400
                       Mean reward: 242.77
               Mean episode length: 159.94
    Episode_Reward/reaching_object: 0.5150
     Episode_Reward/lifting_object: 45.7209
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.10s
                      Time elapsed: 00:24:45
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 48501 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 166.2968
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8403
                       Mean reward: 224.27
               Mean episode length: 151.43
    Episode_Reward/reaching_object: 0.5010
     Episode_Reward/lifting_object: 43.7813
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.03s
                      Time elapsed: 00:24:47
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 45546 steps/s (collection: 2.061s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.6578
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.8416
                       Mean reward: 224.43
               Mean episode length: 158.14
    Episode_Reward/reaching_object: 0.5083
     Episode_Reward/lifting_object: 45.2314
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.16s
                      Time elapsed: 00:24:49
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 47839 steps/s (collection: 1.959s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 149.9681
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8427
                       Mean reward: 230.12
               Mean episode length: 156.80
    Episode_Reward/reaching_object: 0.5055
     Episode_Reward/lifting_object: 44.1587
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.05s
                      Time elapsed: 00:24:51
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 43289 steps/s (collection: 2.173s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 172.1524
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8425
                       Mean reward: 217.11
               Mean episode length: 153.61
    Episode_Reward/reaching_object: 0.5045
     Episode_Reward/lifting_object: 44.7193
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.27s
                      Time elapsed: 00:24:53
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 48611 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 196.9887
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8434
                       Mean reward: 216.75
               Mean episode length: 152.24
    Episode_Reward/reaching_object: 0.5200
     Episode_Reward/lifting_object: 45.7102
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.02s
                      Time elapsed: 00:24:55
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 46718 steps/s (collection: 1.998s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 161.0616
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8449
                       Mean reward: 222.60
               Mean episode length: 155.19
    Episode_Reward/reaching_object: 0.5088
     Episode_Reward/lifting_object: 44.3623
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.10s
                      Time elapsed: 00:24:57
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 45740 steps/s (collection: 2.037s, learning 0.113s)
             Mean action noise std: 1.80
          Mean value_function loss: 168.4499
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8468
                       Mean reward: 245.37
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 0.5005
     Episode_Reward/lifting_object: 44.3580
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.15s
                      Time elapsed: 00:25:00
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 45957 steps/s (collection: 2.015s, learning 0.124s)
             Mean action noise std: 1.80
          Mean value_function loss: 167.7242
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8490
                       Mean reward: 226.77
               Mean episode length: 149.58
    Episode_Reward/reaching_object: 0.5041
     Episode_Reward/lifting_object: 45.1004
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.14s
                      Time elapsed: 00:25:02
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 45462 steps/s (collection: 2.054s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 144.7637
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8493
                       Mean reward: 237.57
               Mean episode length: 159.19
    Episode_Reward/reaching_object: 0.5328
     Episode_Reward/lifting_object: 47.0333
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.16s
                      Time elapsed: 00:25:04
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 44446 steps/s (collection: 2.123s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 162.9714
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8472
                       Mean reward: 241.31
               Mean episode length: 158.18
    Episode_Reward/reaching_object: 0.5249
     Episode_Reward/lifting_object: 47.0091
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.21s
                      Time elapsed: 00:25:06
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 41794 steps/s (collection: 2.240s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 148.5384
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.8455
                       Mean reward: 234.06
               Mean episode length: 150.85
    Episode_Reward/reaching_object: 0.5212
     Episode_Reward/lifting_object: 46.8926
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.35s
                      Time elapsed: 00:25:08
                               ETA: 00:55:45

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 33447 steps/s (collection: 2.703s, learning 0.236s)
             Mean action noise std: 1.80
          Mean value_function loss: 165.8765
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.8426
                       Mean reward: 232.92
               Mean episode length: 159.16
    Episode_Reward/reaching_object: 0.5185
     Episode_Reward/lifting_object: 45.9972
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.94s
                      Time elapsed: 00:25:11
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 40525 steps/s (collection: 2.215s, learning 0.211s)
             Mean action noise std: 1.80
          Mean value_function loss: 165.1801
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8422
                       Mean reward: 230.94
               Mean episode length: 154.67
    Episode_Reward/reaching_object: 0.5255
     Episode_Reward/lifting_object: 46.5509
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.43s
                      Time elapsed: 00:25:14
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 48699 steps/s (collection: 1.924s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 229.0117
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.8411
                       Mean reward: 244.44
               Mean episode length: 160.52
    Episode_Reward/reaching_object: 0.5251
     Episode_Reward/lifting_object: 46.9173
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.02s
                      Time elapsed: 00:25:16
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 49569 steps/s (collection: 1.892s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 228.6399
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8410
                       Mean reward: 247.64
               Mean episode length: 159.90
    Episode_Reward/reaching_object: 0.5346
     Episode_Reward/lifting_object: 48.1687
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 1.98s
                      Time elapsed: 00:25:18
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.910s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 153.3938
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.8422
                       Mean reward: 231.95
               Mean episode length: 156.02
    Episode_Reward/reaching_object: 0.5478
     Episode_Reward/lifting_object: 48.9877
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.01s
                      Time elapsed: 00:25:20
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 49079 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.80
          Mean value_function loss: 160.6490
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8446
                       Mean reward: 235.51
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 0.5243
     Episode_Reward/lifting_object: 46.8361
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.00s
                      Time elapsed: 00:25:22
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 44791 steps/s (collection: 2.085s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 175.1473
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8461
                       Mean reward: 227.23
               Mean episode length: 147.73
    Episode_Reward/reaching_object: 0.5170
     Episode_Reward/lifting_object: 46.6684
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.19s
                      Time elapsed: 00:25:24
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 40172 steps/s (collection: 2.314s, learning 0.133s)
             Mean action noise std: 1.80
          Mean value_function loss: 158.3456
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8483
                       Mean reward: 238.03
               Mean episode length: 156.64
    Episode_Reward/reaching_object: 0.5481
     Episode_Reward/lifting_object: 50.2259
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.45s
                      Time elapsed: 00:25:26
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 35660 steps/s (collection: 2.562s, learning 0.194s)
             Mean action noise std: 1.80
          Mean value_function loss: 163.6186
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.8480
                       Mean reward: 252.40
               Mean episode length: 165.94
    Episode_Reward/reaching_object: 0.5324
     Episode_Reward/lifting_object: 48.8301
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.76s
                      Time elapsed: 00:25:29
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 36269 steps/s (collection: 2.545s, learning 0.166s)
             Mean action noise std: 1.80
          Mean value_function loss: 162.5476
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.8476
                       Mean reward: 269.04
               Mean episode length: 172.15
    Episode_Reward/reaching_object: 0.5453
     Episode_Reward/lifting_object: 48.9212
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.71s
                      Time elapsed: 00:25:32
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 39653 steps/s (collection: 2.346s, learning 0.133s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.1821
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.8460
                       Mean reward: 249.52
               Mean episode length: 165.15
    Episode_Reward/reaching_object: 0.5461
     Episode_Reward/lifting_object: 49.5729
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.48s
                      Time elapsed: 00:25:34
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 38542 steps/s (collection: 2.424s, learning 0.126s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.0972
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8457
                       Mean reward: 247.21
               Mean episode length: 158.15
    Episode_Reward/reaching_object: 0.5350
     Episode_Reward/lifting_object: 48.6619
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.55s
                      Time elapsed: 00:25:37
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 33252 steps/s (collection: 2.809s, learning 0.147s)
             Mean action noise std: 1.80
          Mean value_function loss: 158.2845
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8454
                       Mean reward: 266.93
               Mean episode length: 169.62
    Episode_Reward/reaching_object: 0.5517
     Episode_Reward/lifting_object: 50.9777
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.96s
                      Time elapsed: 00:25:40
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 36908 steps/s (collection: 2.526s, learning 0.138s)
             Mean action noise std: 1.80
          Mean value_function loss: 164.0527
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8449
                       Mean reward: 252.56
               Mean episode length: 165.17
    Episode_Reward/reaching_object: 0.5316
     Episode_Reward/lifting_object: 49.0823
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.66s
                      Time elapsed: 00:25:43
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 38985 steps/s (collection: 2.375s, learning 0.147s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.1802
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.8438
                       Mean reward: 242.56
               Mean episode length: 155.01
    Episode_Reward/reaching_object: 0.5136
     Episode_Reward/lifting_object: 47.5476
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.52s
                      Time elapsed: 00:25:45
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 43327 steps/s (collection: 2.161s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 156.2921
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8447
                       Mean reward: 237.92
               Mean episode length: 154.86
    Episode_Reward/reaching_object: 0.5240
     Episode_Reward/lifting_object: 48.7244
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.27s
                      Time elapsed: 00:25:47
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 41165 steps/s (collection: 2.266s, learning 0.122s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.3544
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8435
                       Mean reward: 248.07
               Mean episode length: 158.42
    Episode_Reward/reaching_object: 0.5491
     Episode_Reward/lifting_object: 50.1524
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.39s
                      Time elapsed: 00:25:50
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 33987 steps/s (collection: 2.693s, learning 0.199s)
             Mean action noise std: 1.80
          Mean value_function loss: 160.0553
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.8426
                       Mean reward: 249.41
               Mean episode length: 157.98
    Episode_Reward/reaching_object: 0.5183
     Episode_Reward/lifting_object: 48.1292
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.89s
                      Time elapsed: 00:25:53
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 33688 steps/s (collection: 2.743s, learning 0.175s)
             Mean action noise std: 1.80
          Mean value_function loss: 159.6904
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.8436
                       Mean reward: 253.19
               Mean episode length: 161.93
    Episode_Reward/reaching_object: 0.5258
     Episode_Reward/lifting_object: 49.4390
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.92s
                      Time elapsed: 00:25:56
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 39458 steps/s (collection: 2.357s, learning 0.135s)
             Mean action noise std: 1.80
          Mean value_function loss: 166.1406
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8464
                       Mean reward: 256.73
               Mean episode length: 160.52
    Episode_Reward/reaching_object: 0.5396
     Episode_Reward/lifting_object: 51.0151
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.49s
                      Time elapsed: 00:25:58
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 42396 steps/s (collection: 2.199s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 168.8583
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8475
                       Mean reward: 265.76
               Mean episode length: 166.93
    Episode_Reward/reaching_object: 0.5318
     Episode_Reward/lifting_object: 50.2836
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.32s
                      Time elapsed: 00:26:00
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 41007 steps/s (collection: 2.243s, learning 0.154s)
             Mean action noise std: 1.80
          Mean value_function loss: 167.3624
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.8486
                       Mean reward: 254.87
               Mean episode length: 160.78
    Episode_Reward/reaching_object: 0.5536
     Episode_Reward/lifting_object: 52.6861
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.40s
                      Time elapsed: 00:26:03
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 40327 steps/s (collection: 2.311s, learning 0.126s)
             Mean action noise std: 1.80
          Mean value_function loss: 189.9030
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8507
                       Mean reward: 238.28
               Mean episode length: 151.38
    Episode_Reward/reaching_object: 0.5336
     Episode_Reward/lifting_object: 50.0531
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.44s
                      Time elapsed: 00:26:05
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 39630 steps/s (collection: 2.338s, learning 0.142s)
             Mean action noise std: 1.80
          Mean value_function loss: 170.7193
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8535
                       Mean reward: 243.08
               Mean episode length: 154.62
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: 52.0160
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.48s
                      Time elapsed: 00:26:08
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 33497 steps/s (collection: 2.806s, learning 0.129s)
             Mean action noise std: 1.80
          Mean value_function loss: 162.4057
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8554
                       Mean reward: 274.68
               Mean episode length: 166.71
    Episode_Reward/reaching_object: 0.5454
     Episode_Reward/lifting_object: 51.9340
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.93s
                      Time elapsed: 00:26:11
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 35511 steps/s (collection: 2.617s, learning 0.152s)
             Mean action noise std: 1.80
          Mean value_function loss: 190.8659
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8563
                       Mean reward: 258.73
               Mean episode length: 163.39
    Episode_Reward/reaching_object: 0.5297
     Episode_Reward/lifting_object: 50.4132
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.77s
                      Time elapsed: 00:26:13
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 34675 steps/s (collection: 2.659s, learning 0.176s)
             Mean action noise std: 1.80
          Mean value_function loss: 185.2930
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8549
                       Mean reward: 250.09
               Mean episode length: 155.51
    Episode_Reward/reaching_object: 0.5130
     Episode_Reward/lifting_object: 49.2056
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.84s
                      Time elapsed: 00:26:16
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 32698 steps/s (collection: 2.772s, learning 0.234s)
             Mean action noise std: 1.80
          Mean value_function loss: 184.6860
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8578
                       Mean reward: 273.79
               Mean episode length: 169.64
    Episode_Reward/reaching_object: 0.5400
     Episode_Reward/lifting_object: 51.6112
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 3.01s
                      Time elapsed: 00:26:19
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 35287 steps/s (collection: 2.646s, learning 0.140s)
             Mean action noise std: 1.80
          Mean value_function loss: 200.7958
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 35.8596
                       Mean reward: 276.58
               Mean episode length: 169.30
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 51.9341
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.79s
                      Time elapsed: 00:26:22
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 34153 steps/s (collection: 2.729s, learning 0.150s)
             Mean action noise std: 1.80
          Mean value_function loss: 150.8553
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8606
                       Mean reward: 239.77
               Mean episode length: 150.11
    Episode_Reward/reaching_object: 0.5212
     Episode_Reward/lifting_object: 49.6912
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.88s
                      Time elapsed: 00:26:25
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 34070 steps/s (collection: 2.739s, learning 0.146s)
             Mean action noise std: 1.80
          Mean value_function loss: 172.7246
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.8607
                       Mean reward: 265.70
               Mean episode length: 165.54
    Episode_Reward/reaching_object: 0.5530
     Episode_Reward/lifting_object: 53.3988
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.89s
                      Time elapsed: 00:26:28
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 32727 steps/s (collection: 2.852s, learning 0.151s)
             Mean action noise std: 1.80
          Mean value_function loss: 168.0277
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8605
                       Mean reward: 284.53
               Mean episode length: 175.77
    Episode_Reward/reaching_object: 0.5723
     Episode_Reward/lifting_object: 54.7982
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 3.00s
                      Time elapsed: 00:26:31
                               ETA: 00:54:37

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 38103 steps/s (collection: 2.439s, learning 0.141s)
             Mean action noise std: 1.80
          Mean value_function loss: 173.7846
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8602
                       Mean reward: 288.01
               Mean episode length: 173.88
    Episode_Reward/reaching_object: 0.5650
     Episode_Reward/lifting_object: 55.0589
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.58s
                      Time elapsed: 00:26:33
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 29375 steps/s (collection: 3.102s, learning 0.244s)
             Mean action noise std: 1.80
          Mean value_function loss: 158.2271
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 35.8606
                       Mean reward: 245.51
               Mean episode length: 154.34
    Episode_Reward/reaching_object: 0.5537
     Episode_Reward/lifting_object: 53.2800
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 3.35s
                      Time elapsed: 00:26:37
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 27673 steps/s (collection: 3.275s, learning 0.277s)
             Mean action noise std: 1.80
          Mean value_function loss: 170.7882
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.8609
                       Mean reward: 275.77
               Mean episode length: 169.75
    Episode_Reward/reaching_object: 0.5579
     Episode_Reward/lifting_object: 53.5963
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 3.55s
                      Time elapsed: 00:26:40
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 26143 steps/s (collection: 3.540s, learning 0.220s)
             Mean action noise std: 1.80
          Mean value_function loss: 153.0083
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.8611
                       Mean reward: 284.34
               Mean episode length: 173.21
    Episode_Reward/reaching_object: 0.5427
     Episode_Reward/lifting_object: 52.3062
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 3.76s
                      Time elapsed: 00:26:44
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 25246 steps/s (collection: 3.587s, learning 0.307s)
             Mean action noise std: 1.80
          Mean value_function loss: 159.8841
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8608
                       Mean reward: 277.53
               Mean episode length: 173.45
    Episode_Reward/reaching_object: 0.5720
     Episode_Reward/lifting_object: 55.3083
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 3.89s
                      Time elapsed: 00:26:48
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 24371 steps/s (collection: 3.728s, learning 0.305s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.1962
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.8600
                       Mean reward: 234.18
               Mean episode length: 152.78
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 51.2202
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 4.03s
                      Time elapsed: 00:26:52
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 16000 steps/s (collection: 5.734s, learning 0.410s)
             Mean action noise std: 1.80
          Mean value_function loss: 154.6144
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.8605
                       Mean reward: 257.41
               Mean episode length: 154.19
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: 56.1735
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 6.14s
                      Time elapsed: 00:26:58
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 16982 steps/s (collection: 5.501s, learning 0.288s)
             Mean action noise std: 1.80
          Mean value_function loss: 159.3735
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.8619
                       Mean reward: 289.51
               Mean episode length: 174.87
    Episode_Reward/reaching_object: 0.5675
     Episode_Reward/lifting_object: 54.3000
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 5.79s
                      Time elapsed: 00:27:04
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 21430 steps/s (collection: 4.309s, learning 0.278s)
             Mean action noise std: 1.80
          Mean value_function loss: 162.6799
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.8621
                       Mean reward: 269.46
               Mean episode length: 162.06
    Episode_Reward/reaching_object: 0.5596
     Episode_Reward/lifting_object: 54.8075
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 4.59s
                      Time elapsed: 00:27:09
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 16775 steps/s (collection: 5.517s, learning 0.343s)
             Mean action noise std: 1.80
          Mean value_function loss: 147.4090
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.8622
                       Mean reward: 271.35
               Mean episode length: 163.20
    Episode_Reward/reaching_object: 0.5618
     Episode_Reward/lifting_object: 54.5629
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 5.86s
                      Time elapsed: 00:27:14
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 14432 steps/s (collection: 6.474s, learning 0.337s)
             Mean action noise std: 1.80
          Mean value_function loss: 149.4689
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8618
                       Mean reward: 264.35
               Mean episode length: 163.02
    Episode_Reward/reaching_object: 0.5766
     Episode_Reward/lifting_object: 56.0665
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 6.81s
                      Time elapsed: 00:27:21
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 19825 steps/s (collection: 4.856s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 155.9141
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.8608
                       Mean reward: 288.16
               Mean episode length: 176.47
    Episode_Reward/reaching_object: 0.5839
     Episode_Reward/lifting_object: 56.6367
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 4.96s
                      Time elapsed: 00:27:26
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 23566 steps/s (collection: 3.998s, learning 0.173s)
             Mean action noise std: 1.80
          Mean value_function loss: 153.7200
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.8612
                       Mean reward: 280.28
               Mean episode length: 172.17
    Episode_Reward/reaching_object: 0.5584
     Episode_Reward/lifting_object: 54.3032
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 4.17s
                      Time elapsed: 00:27:30
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 11459 steps/s (collection: 8.410s, learning 0.168s)
             Mean action noise std: 1.80
          Mean value_function loss: 188.5823
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.8612
                       Mean reward: 282.23
               Mean episode length: 170.11
    Episode_Reward/reaching_object: 0.5585
     Episode_Reward/lifting_object: 55.0285
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 8.58s
                      Time elapsed: 00:27:39
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 10880 steps/s (collection: 8.860s, learning 0.175s)
             Mean action noise std: 1.80
          Mean value_function loss: 164.6066
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.8591
                       Mean reward: 252.72
               Mean episode length: 158.21
    Episode_Reward/reaching_object: 0.5571
     Episode_Reward/lifting_object: 53.8769
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 9.04s
                      Time elapsed: 00:27:48
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 11663 steps/s (collection: 8.265s, learning 0.163s)
             Mean action noise std: 1.80
          Mean value_function loss: 156.1290
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8577
                       Mean reward: 253.93
               Mean episode length: 157.84
    Episode_Reward/reaching_object: 0.5457
     Episode_Reward/lifting_object: 53.0510
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 8.43s
                      Time elapsed: 00:27:56
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 11496 steps/s (collection: 8.363s, learning 0.188s)
             Mean action noise std: 1.80
          Mean value_function loss: 177.7848
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8579
                       Mean reward: 240.27
               Mean episode length: 150.16
    Episode_Reward/reaching_object: 0.5459
     Episode_Reward/lifting_object: 54.1862
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 8.55s
                      Time elapsed: 00:28:05
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 12518 steps/s (collection: 7.736s, learning 0.117s)
             Mean action noise std: 1.80
          Mean value_function loss: 176.5288
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8588
                       Mean reward: 278.30
               Mean episode length: 173.03
    Episode_Reward/reaching_object: 0.5644
     Episode_Reward/lifting_object: 55.6004
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.85s
                      Time elapsed: 00:28:13
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13939 steps/s (collection: 6.904s, learning 0.148s)
             Mean action noise std: 1.80
          Mean value_function loss: 145.6781
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8592
                       Mean reward: 256.73
               Mean episode length: 153.12
    Episode_Reward/reaching_object: 0.5437
     Episode_Reward/lifting_object: 53.8491
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.05s
                      Time elapsed: 00:28:20
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13846 steps/s (collection: 6.970s, learning 0.129s)
             Mean action noise std: 1.80
          Mean value_function loss: 157.0129
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8583
                       Mean reward: 280.10
               Mean episode length: 166.54
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 54.3672
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.10s
                      Time elapsed: 00:28:27
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13901 steps/s (collection: 6.930s, learning 0.141s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.1449
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8583
                       Mean reward: 287.28
               Mean episode length: 169.52
    Episode_Reward/reaching_object: 0.5486
     Episode_Reward/lifting_object: 55.2580
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.07s
                      Time elapsed: 00:28:34
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 17452 steps/s (collection: 5.450s, learning 0.183s)
             Mean action noise std: 1.80
          Mean value_function loss: 161.6099
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.8583
                       Mean reward: 277.86
               Mean episode length: 167.86
    Episode_Reward/reaching_object: 0.5633
     Episode_Reward/lifting_object: 57.0272
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 5.63s
                      Time elapsed: 00:28:40
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 30733 steps/s (collection: 2.972s, learning 0.227s)
             Mean action noise std: 1.80
          Mean value_function loss: 161.4310
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8579
                       Mean reward: 294.12
               Mean episode length: 171.94
    Episode_Reward/reaching_object: 0.5828
     Episode_Reward/lifting_object: 59.1917
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 3.20s
                      Time elapsed: 00:28:43
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 26096 steps/s (collection: 3.489s, learning 0.278s)
             Mean action noise std: 1.80
          Mean value_function loss: 158.6348
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.8563
                       Mean reward: 270.69
               Mean episode length: 161.33
    Episode_Reward/reaching_object: 0.5675
     Episode_Reward/lifting_object: 57.4093
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 3.77s
                      Time elapsed: 00:28:47
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 27267 steps/s (collection: 3.409s, learning 0.196s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.3260
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8549
                       Mean reward: 288.61
               Mean episode length: 169.72
    Episode_Reward/reaching_object: 0.5698
     Episode_Reward/lifting_object: 57.8050
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 3.61s
                      Time elapsed: 00:28:50
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 33175 steps/s (collection: 2.768s, learning 0.195s)
             Mean action noise std: 1.80
          Mean value_function loss: 163.6022
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 35.8564
                       Mean reward: 297.37
               Mean episode length: 174.66
    Episode_Reward/reaching_object: 0.5675
     Episode_Reward/lifting_object: 58.3328
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.96s
                      Time elapsed: 00:28:53
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 46132 steps/s (collection: 2.028s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 169.3673
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8579
                       Mean reward: 273.30
               Mean episode length: 163.07
    Episode_Reward/reaching_object: 0.5659
     Episode_Reward/lifting_object: 57.8255
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.13s
                      Time elapsed: 00:28:55
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 45124 steps/s (collection: 1.992s, learning 0.186s)
             Mean action noise std: 1.80
          Mean value_function loss: 168.5375
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.8605
                       Mean reward: 275.86
               Mean episode length: 162.70
    Episode_Reward/reaching_object: 0.5786
     Episode_Reward/lifting_object: 59.2501
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.18s
                      Time elapsed: 00:28:57
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 46870 steps/s (collection: 2.007s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 166.5301
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8617
                       Mean reward: 269.94
               Mean episode length: 156.59
    Episode_Reward/reaching_object: 0.5561
     Episode_Reward/lifting_object: 56.8719
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.10s
                      Time elapsed: 00:29:00
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48241 steps/s (collection: 1.950s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 165.6471
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.8617
                       Mean reward: 289.36
               Mean episode length: 170.23
    Episode_Reward/reaching_object: 0.5615
     Episode_Reward/lifting_object: 57.6057
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.04s
                      Time elapsed: 00:29:02
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 45131 steps/s (collection: 2.072s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 151.9238
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8615
                       Mean reward: 330.90
               Mean episode length: 186.76
    Episode_Reward/reaching_object: 0.5839
     Episode_Reward/lifting_object: 60.6393
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.18s
                      Time elapsed: 00:29:04
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 39731 steps/s (collection: 2.314s, learning 0.160s)
             Mean action noise std: 1.80
          Mean value_function loss: 182.9390
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.8624
                       Mean reward: 310.11
               Mean episode length: 176.33
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 59.9778
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.47s
                      Time elapsed: 00:29:06
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 40464 steps/s (collection: 2.317s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.0290
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.8664
                       Mean reward: 305.69
               Mean episode length: 173.94
    Episode_Reward/reaching_object: 0.5771
     Episode_Reward/lifting_object: 59.9645
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.43s
                      Time elapsed: 00:29:09
                               ETA: 00:55:45

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 41972 steps/s (collection: 2.125s, learning 0.217s)
             Mean action noise std: 1.80
          Mean value_function loss: 190.6596
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.8673
                       Mean reward: 290.37
               Mean episode length: 166.76
    Episode_Reward/reaching_object: 0.5748
     Episode_Reward/lifting_object: 59.7671
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.34s
                      Time elapsed: 00:29:11
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 47756 steps/s (collection: 1.930s, learning 0.128s)
             Mean action noise std: 1.80
          Mean value_function loss: 172.0004
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.8670
                       Mean reward: 295.43
               Mean episode length: 167.84
    Episode_Reward/reaching_object: 0.5709
     Episode_Reward/lifting_object: 59.4333
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.06s
                      Time elapsed: 00:29:13
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 46678 steps/s (collection: 1.929s, learning 0.177s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.9432
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.8644
                       Mean reward: 293.35
               Mean episode length: 165.74
    Episode_Reward/reaching_object: 0.5787
     Episode_Reward/lifting_object: 61.0643
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.11s
                      Time elapsed: 00:29:15
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 44512 steps/s (collection: 1.998s, learning 0.211s)
             Mean action noise std: 1.80
          Mean value_function loss: 165.0604
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8627
                       Mean reward: 321.05
               Mean episode length: 179.19
    Episode_Reward/reaching_object: 0.5879
     Episode_Reward/lifting_object: 61.9843
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.21s
                      Time elapsed: 00:29:17
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47646 steps/s (collection: 1.962s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 168.3582
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.8615
                       Mean reward: 309.67
               Mean episode length: 174.74
    Episode_Reward/reaching_object: 0.5662
     Episode_Reward/lifting_object: 59.8041
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.06s
                      Time elapsed: 00:29:19
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 48337 steps/s (collection: 1.946s, learning 0.088s)
             Mean action noise std: 1.80
          Mean value_function loss: 223.6701
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.8609
                       Mean reward: 303.20
               Mean episode length: 173.74
    Episode_Reward/reaching_object: 0.5750
     Episode_Reward/lifting_object: 60.2571
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.03s
                      Time elapsed: 00:29:21
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 49314 steps/s (collection: 1.904s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 230.6594
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8624
                       Mean reward: 279.40
               Mean episode length: 160.29
    Episode_Reward/reaching_object: 0.5351
     Episode_Reward/lifting_object: 56.3659
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 1.99s
                      Time elapsed: 00:29:23
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 48632 steps/s (collection: 1.921s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 193.4757
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.8637
                       Mean reward: 293.92
               Mean episode length: 165.11
    Episode_Reward/reaching_object: 0.5616
     Episode_Reward/lifting_object: 59.9585
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.02s
                      Time elapsed: 00:29:26
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 47518 steps/s (collection: 1.966s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.4454
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8634
                       Mean reward: 297.77
               Mean episode length: 168.77
    Episode_Reward/reaching_object: 0.5573
     Episode_Reward/lifting_object: 58.4336
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.07s
                      Time elapsed: 00:29:28
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 47954 steps/s (collection: 1.937s, learning 0.113s)
             Mean action noise std: 1.81
          Mean value_function loss: 195.9898
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.8665
                       Mean reward: 297.53
               Mean episode length: 169.15
    Episode_Reward/reaching_object: 0.5630
     Episode_Reward/lifting_object: 60.3798
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.05s
                      Time elapsed: 00:29:30
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 49067 steps/s (collection: 1.915s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 201.4739
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8699
                       Mean reward: 272.89
               Mean episode length: 154.96
    Episode_Reward/reaching_object: 0.5374
     Episode_Reward/lifting_object: 57.0990
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.00s
                      Time elapsed: 00:29:32
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 48307 steps/s (collection: 1.937s, learning 0.098s)
             Mean action noise std: 1.81
          Mean value_function loss: 178.6759
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.8708
                       Mean reward: 294.63
               Mean episode length: 162.39
    Episode_Reward/reaching_object: 0.5804
     Episode_Reward/lifting_object: 61.8199
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.03s
                      Time elapsed: 00:29:34
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47632 steps/s (collection: 1.964s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 183.7030
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 35.8697
                       Mean reward: 325.31
               Mean episode length: 177.69
    Episode_Reward/reaching_object: 0.5511
     Episode_Reward/lifting_object: 59.6089
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.06s
                      Time elapsed: 00:29:36
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 48652 steps/s (collection: 1.933s, learning 0.087s)
             Mean action noise std: 1.81
          Mean value_function loss: 174.7906
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8695
                       Mean reward: 318.73
               Mean episode length: 176.44
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 63.0075
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.02s
                      Time elapsed: 00:29:38
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 46607 steps/s (collection: 2.010s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 191.0633
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.8697
                       Mean reward: 277.69
               Mean episode length: 154.06
    Episode_Reward/reaching_object: 0.5469
     Episode_Reward/lifting_object: 58.5712
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.11s
                      Time elapsed: 00:29:40
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 48645 steps/s (collection: 1.927s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.2428
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.8698
                       Mean reward: 323.00
               Mean episode length: 178.47
    Episode_Reward/reaching_object: 0.5632
     Episode_Reward/lifting_object: 60.5414
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.02s
                      Time elapsed: 00:29:42
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 45872 steps/s (collection: 2.028s, learning 0.115s)
             Mean action noise std: 1.81
          Mean value_function loss: 168.5893
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8697
                       Mean reward: 331.17
               Mean episode length: 178.02
    Episode_Reward/reaching_object: 0.5749
     Episode_Reward/lifting_object: 61.6178
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.14s
                      Time elapsed: 00:29:44
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 49168 steps/s (collection: 1.909s, learning 0.090s)
             Mean action noise std: 1.81
          Mean value_function loss: 185.1291
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8696
                       Mean reward: 302.51
               Mean episode length: 167.74
    Episode_Reward/reaching_object: 0.5785
     Episode_Reward/lifting_object: 61.8378
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.00s
                      Time elapsed: 00:29:46
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 50145 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 1.81
          Mean value_function loss: 191.2451
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.8701
                       Mean reward: 322.75
               Mean episode length: 179.17
    Episode_Reward/reaching_object: 0.5867
     Episode_Reward/lifting_object: 62.9970
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 1.96s
                      Time elapsed: 00:29:48
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 49968 steps/s (collection: 1.881s, learning 0.087s)
             Mean action noise std: 1.81
          Mean value_function loss: 196.8627
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.8701
                       Mean reward: 333.55
               Mean episode length: 190.51
    Episode_Reward/reaching_object: 0.5940
     Episode_Reward/lifting_object: 63.7705
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 1.97s
                      Time elapsed: 00:29:50
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 49073 steps/s (collection: 1.886s, learning 0.117s)
             Mean action noise std: 1.81
          Mean value_function loss: 184.3961
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8703
                       Mean reward: 341.07
               Mean episode length: 180.56
    Episode_Reward/reaching_object: 0.5851
     Episode_Reward/lifting_object: 63.2137
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.00s
                      Time elapsed: 00:29:52
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 49037 steps/s (collection: 1.901s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 179.0798
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.8731
                       Mean reward: 297.57
               Mean episode length: 166.46
    Episode_Reward/reaching_object: 0.5815
     Episode_Reward/lifting_object: 63.0529
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.00s
                      Time elapsed: 00:29:54
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 50112 steps/s (collection: 1.876s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 196.7382
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.8782
                       Mean reward: 321.61
               Mean episode length: 176.26
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 63.7855
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 1.96s
                      Time elapsed: 00:29:56
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 49852 steps/s (collection: 1.879s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 189.6417
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8796
                       Mean reward: 304.25
               Mean episode length: 167.21
    Episode_Reward/reaching_object: 0.5618
     Episode_Reward/lifting_object: 60.7273
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 1.97s
                      Time elapsed: 00:29:58
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 50440 steps/s (collection: 1.857s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 179.5184
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8795
                       Mean reward: 313.58
               Mean episode length: 173.08
    Episode_Reward/reaching_object: 0.5866
     Episode_Reward/lifting_object: 63.0088
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 1.95s
                      Time elapsed: 00:30:00
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 50265 steps/s (collection: 1.861s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 187.7577
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8790
                       Mean reward: 317.52
               Mean episode length: 170.40
    Episode_Reward/reaching_object: 0.5799
     Episode_Reward/lifting_object: 63.3522
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 1.96s
                      Time elapsed: 00:30:02
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 48592 steps/s (collection: 1.920s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 197.2210
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.8803
                       Mean reward: 339.44
               Mean episode length: 186.28
    Episode_Reward/reaching_object: 0.5800
     Episode_Reward/lifting_object: 63.0029
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.02s
                      Time elapsed: 00:30:04
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 49439 steps/s (collection: 1.894s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 185.1030
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.8819
                       Mean reward: 329.18
               Mean episode length: 181.96
    Episode_Reward/reaching_object: 0.5713
     Episode_Reward/lifting_object: 61.5092
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.99s
                      Time elapsed: 00:30:06
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 48708 steps/s (collection: 1.932s, learning 0.087s)
             Mean action noise std: 1.81
          Mean value_function loss: 192.1795
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8830
                       Mean reward: 307.71
               Mean episode length: 170.51
    Episode_Reward/reaching_object: 0.5681
     Episode_Reward/lifting_object: 61.3627
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.02s
                      Time elapsed: 00:30:08
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 49969 steps/s (collection: 1.880s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 170.7813
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.8825
                       Mean reward: 337.37
               Mean episode length: 181.79
    Episode_Reward/reaching_object: 0.5888
     Episode_Reward/lifting_object: 63.8153
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.97s
                      Time elapsed: 00:30:10
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 50049 steps/s (collection: 1.871s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 203.0504
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8836
                       Mean reward: 332.10
               Mean episode length: 184.81
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 63.2717
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.96s
                      Time elapsed: 00:30:12
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 50427 steps/s (collection: 1.858s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 206.4269
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.8857
                       Mean reward: 308.43
               Mean episode length: 167.75
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 64.5227
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.95s
                      Time elapsed: 00:30:14
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 49785 steps/s (collection: 1.878s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.6142
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8865
                       Mean reward: 279.94
               Mean episode length: 155.78
    Episode_Reward/reaching_object: 0.5596
     Episode_Reward/lifting_object: 60.5729
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.97s
                      Time elapsed: 00:30:16
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 49939 steps/s (collection: 1.869s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 178.3348
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.8877
                       Mean reward: 336.48
               Mean episode length: 178.88
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 63.8730
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.97s
                      Time elapsed: 00:30:18
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 50521 steps/s (collection: 1.857s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 197.2635
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8865
                       Mean reward: 304.60
               Mean episode length: 165.53
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 62.2174
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.95s
                      Time elapsed: 00:30:20
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 48942 steps/s (collection: 1.900s, learning 0.108s)
             Mean action noise std: 1.81
          Mean value_function loss: 224.5484
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8860
                       Mean reward: 312.90
               Mean episode length: 172.53
    Episode_Reward/reaching_object: 0.5806
     Episode_Reward/lifting_object: 63.2620
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.01s
                      Time elapsed: 00:30:22
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 49421 steps/s (collection: 1.872s, learning 0.117s)
             Mean action noise std: 1.81
          Mean value_function loss: 192.7820
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.8862
                       Mean reward: 319.45
               Mean episode length: 170.83
    Episode_Reward/reaching_object: 0.5840
     Episode_Reward/lifting_object: 63.9204
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 1.99s
                      Time elapsed: 00:30:24
                               ETA: 00:53:37

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 49330 steps/s (collection: 1.888s, learning 0.105s)
             Mean action noise std: 1.81
          Mean value_function loss: 192.6950
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.8850
                       Mean reward: 308.38
               Mean episode length: 163.84
    Episode_Reward/reaching_object: 0.5719
     Episode_Reward/lifting_object: 63.1410
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.99s
                      Time elapsed: 00:30:26
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 47859 steps/s (collection: 1.952s, learning 0.102s)
             Mean action noise std: 1.81
          Mean value_function loss: 181.3289
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.8845
                       Mean reward: 319.99
               Mean episode length: 171.77
    Episode_Reward/reaching_object: 0.5754
     Episode_Reward/lifting_object: 63.4553
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.05s
                      Time elapsed: 00:30:28
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 49545 steps/s (collection: 1.886s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 197.4498
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.8844
                       Mean reward: 309.66
               Mean episode length: 166.31
    Episode_Reward/reaching_object: 0.5609
     Episode_Reward/lifting_object: 62.1740
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.98s
                      Time elapsed: 00:30:30
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 47599 steps/s (collection: 1.957s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 201.0148
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8851
                       Mean reward: 283.39
               Mean episode length: 154.84
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 62.6233
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.07s
                      Time elapsed: 00:30:32
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 48325 steps/s (collection: 1.935s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 202.9357
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.8872
                       Mean reward: 326.01
               Mean episode length: 173.41
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 65.1826
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.03s
                      Time elapsed: 00:30:34
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 47457 steps/s (collection: 1.950s, learning 0.121s)
             Mean action noise std: 1.81
          Mean value_function loss: 183.1640
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.8905
                       Mean reward: 327.32
               Mean episode length: 174.78
    Episode_Reward/reaching_object: 0.5935
     Episode_Reward/lifting_object: 65.5382
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.07s
                      Time elapsed: 00:30:36
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 42091 steps/s (collection: 2.224s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 190.2224
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.8931
                       Mean reward: 321.88
               Mean episode length: 175.43
    Episode_Reward/reaching_object: 0.5771
     Episode_Reward/lifting_object: 63.8986
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.34s
                      Time elapsed: 00:30:38
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 43083 steps/s (collection: 2.066s, learning 0.216s)
             Mean action noise std: 1.81
          Mean value_function loss: 211.1913
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.8953
                       Mean reward: 330.09
               Mean episode length: 178.39
    Episode_Reward/reaching_object: 0.5881
     Episode_Reward/lifting_object: 65.4577
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.28s
                      Time elapsed: 00:30:40
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 43213 steps/s (collection: 2.099s, learning 0.176s)
             Mean action noise std: 1.81
          Mean value_function loss: 195.9704
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.8960
                       Mean reward: 344.91
               Mean episode length: 180.54
    Episode_Reward/reaching_object: 0.6017
     Episode_Reward/lifting_object: 66.7098
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.27s
                      Time elapsed: 00:30:43
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 35672 steps/s (collection: 2.537s, learning 0.218s)
             Mean action noise std: 1.81
          Mean value_function loss: 176.5634
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8965
                       Mean reward: 325.55
               Mean episode length: 170.38
    Episode_Reward/reaching_object: 0.5915
     Episode_Reward/lifting_object: 66.1601
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.76s
                      Time elapsed: 00:30:45
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 37361 steps/s (collection: 2.452s, learning 0.179s)
             Mean action noise std: 1.81
          Mean value_function loss: 230.4409
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8968
                       Mean reward: 338.07
               Mean episode length: 176.51
    Episode_Reward/reaching_object: 0.5853
     Episode_Reward/lifting_object: 65.6417
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.63s
                      Time elapsed: 00:30:48
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 42858 steps/s (collection: 2.170s, learning 0.123s)
             Mean action noise std: 1.81
          Mean value_function loss: 227.4025
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.8974
                       Mean reward: 340.34
               Mean episode length: 181.15
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 65.7803
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.29s
                      Time elapsed: 00:30:50
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 37837 steps/s (collection: 2.446s, learning 0.152s)
             Mean action noise std: 1.81
          Mean value_function loss: 225.6367
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8975
                       Mean reward: 334.85
               Mean episode length: 178.02
    Episode_Reward/reaching_object: 0.5822
     Episode_Reward/lifting_object: 64.4788
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.60s
                      Time elapsed: 00:30:53
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 40831 steps/s (collection: 2.249s, learning 0.159s)
             Mean action noise std: 1.81
          Mean value_function loss: 238.4118
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8987
                       Mean reward: 332.53
               Mean episode length: 178.56
    Episode_Reward/reaching_object: 0.5789
     Episode_Reward/lifting_object: 64.3505
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.41s
                      Time elapsed: 00:30:55
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 46684 steps/s (collection: 1.999s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 197.7432
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8987
                       Mean reward: 317.52
               Mean episode length: 166.67
    Episode_Reward/reaching_object: 0.5725
     Episode_Reward/lifting_object: 63.2736
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.11s
                      Time elapsed: 00:30:57
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 47347 steps/s (collection: 1.973s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 187.0806
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8999
                       Mean reward: 332.02
               Mean episode length: 172.95
    Episode_Reward/reaching_object: 0.5954
     Episode_Reward/lifting_object: 67.1616
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.08s
                      Time elapsed: 00:31:00
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 48746 steps/s (collection: 1.921s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 228.0743
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.9016
                       Mean reward: 320.64
               Mean episode length: 164.51
    Episode_Reward/reaching_object: 0.5645
     Episode_Reward/lifting_object: 63.9966
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.02s
                      Time elapsed: 00:31:02
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 49634 steps/s (collection: 1.886s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 204.4897
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.9019
                       Mean reward: 297.63
               Mean episode length: 158.25
    Episode_Reward/reaching_object: 0.5727
     Episode_Reward/lifting_object: 63.7304
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 1.98s
                      Time elapsed: 00:31:04
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 49652 steps/s (collection: 1.894s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 198.1692
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.9034
                       Mean reward: 304.61
               Mean episode length: 161.69
    Episode_Reward/reaching_object: 0.5681
     Episode_Reward/lifting_object: 63.7732
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 1.98s
                      Time elapsed: 00:31:06
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 49044 steps/s (collection: 1.913s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 211.2519
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.9053
                       Mean reward: 327.17
               Mean episode length: 169.85
    Episode_Reward/reaching_object: 0.5794
     Episode_Reward/lifting_object: 65.8076
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.00s
                      Time elapsed: 00:31:08
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 49514 steps/s (collection: 1.894s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.2301
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.9074
                       Mean reward: 325.58
               Mean episode length: 167.99
    Episode_Reward/reaching_object: 0.5844
     Episode_Reward/lifting_object: 65.9676
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 1.99s
                      Time elapsed: 00:31:10
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 49583 steps/s (collection: 1.894s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 196.3836
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.9087
                       Mean reward: 341.29
               Mean episode length: 175.23
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 64.4670
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 1.98s
                      Time elapsed: 00:31:11
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 49996 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 206.8198
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.9076
                       Mean reward: 333.64
               Mean episode length: 174.94
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 66.5322
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 1.97s
                      Time elapsed: 00:31:13
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 48842 steps/s (collection: 1.917s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 194.3096
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.9087
                       Mean reward: 354.47
               Mean episode length: 181.45
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 69.1072
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.01s
                      Time elapsed: 00:31:15
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 48971 steps/s (collection: 1.911s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 195.0667
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.9128
                       Mean reward: 283.80
               Mean episode length: 150.71
    Episode_Reward/reaching_object: 0.5669
     Episode_Reward/lifting_object: 64.0269
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.01s
                      Time elapsed: 00:31:17
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 49095 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.82
          Mean value_function loss: 215.2939
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.9146
                       Mean reward: 315.63
               Mean episode length: 169.39
    Episode_Reward/reaching_object: 0.6093
     Episode_Reward/lifting_object: 69.4429
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.00s
                      Time elapsed: 00:31:19
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 49216 steps/s (collection: 1.886s, learning 0.111s)
             Mean action noise std: 1.82
          Mean value_function loss: 212.8748
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.9152
                       Mean reward: 318.04
               Mean episode length: 164.91
    Episode_Reward/reaching_object: 0.5692
     Episode_Reward/lifting_object: 63.8843
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.00s
                      Time elapsed: 00:31:21
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 47991 steps/s (collection: 1.942s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 224.7943
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.9177
                       Mean reward: 341.40
               Mean episode length: 176.72
    Episode_Reward/reaching_object: 0.5936
     Episode_Reward/lifting_object: 67.6393
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.05s
                      Time elapsed: 00:31:24
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 49707 steps/s (collection: 1.880s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 228.3543
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.9212
                       Mean reward: 323.19
               Mean episode length: 166.26
    Episode_Reward/reaching_object: 0.5789
     Episode_Reward/lifting_object: 65.2407
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 1.98s
                      Time elapsed: 00:31:26
                               ETA: 00:52:05

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 47883 steps/s (collection: 1.955s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 206.1746
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.9269
                       Mean reward: 323.95
               Mean episode length: 174.18
    Episode_Reward/reaching_object: 0.5891
     Episode_Reward/lifting_object: 66.6573
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.05s
                      Time elapsed: 00:31:28
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 48992 steps/s (collection: 1.911s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 197.6764
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.9298
                       Mean reward: 344.63
               Mean episode length: 175.24
    Episode_Reward/reaching_object: 0.5923
     Episode_Reward/lifting_object: 67.7836
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.01s
                      Time elapsed: 00:31:30
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 49157 steps/s (collection: 1.904s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 185.0681
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.9284
                       Mean reward: 323.47
               Mean episode length: 166.68
    Episode_Reward/reaching_object: 0.5785
     Episode_Reward/lifting_object: 65.4816
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.00s
                      Time elapsed: 00:31:32
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 48693 steps/s (collection: 1.929s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 193.0481
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.9283
                       Mean reward: 348.96
               Mean episode length: 176.53
    Episode_Reward/reaching_object: 0.5715
     Episode_Reward/lifting_object: 65.5231
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.02s
                      Time elapsed: 00:31:34
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 49154 steps/s (collection: 1.903s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 207.6506
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.9304
                       Mean reward: 343.14
               Mean episode length: 182.04
    Episode_Reward/reaching_object: 0.5804
     Episode_Reward/lifting_object: 66.4818
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.00s
                      Time elapsed: 00:31:36
                               ETA: 00:51:49

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 48797 steps/s (collection: 1.901s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 219.8265
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.9329
                       Mean reward: 337.80
               Mean episode length: 172.65
    Episode_Reward/reaching_object: 0.5922
     Episode_Reward/lifting_object: 68.3830
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.01s
                      Time elapsed: 00:31:38
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 49589 steps/s (collection: 1.879s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 206.3647
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.9346
                       Mean reward: 371.59
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 66.0918
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 1.98s
                      Time elapsed: 00:31:40
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 48919 steps/s (collection: 1.918s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 205.9313
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.9332
                       Mean reward: 347.32
               Mean episode length: 178.37
    Episode_Reward/reaching_object: 0.5957
     Episode_Reward/lifting_object: 68.0600
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.01s
                      Time elapsed: 00:31:42
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.921s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 208.7040
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.9311
                       Mean reward: 332.36
               Mean episode length: 171.52
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 66.4405
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.01s
                      Time elapsed: 00:31:44
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 47375 steps/s (collection: 1.969s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 203.6340
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.9306
                       Mean reward: 343.79
               Mean episode length: 174.37
    Episode_Reward/reaching_object: 0.5914
     Episode_Reward/lifting_object: 68.4897
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.07s
                      Time elapsed: 00:31:46
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 46428 steps/s (collection: 2.002s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 198.6962
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.9305
                       Mean reward: 352.75
               Mean episode length: 182.53
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 67.3721
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.12s
                      Time elapsed: 00:31:48
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 46635 steps/s (collection: 2.011s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 253.9314
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.9316
                       Mean reward: 320.64
               Mean episode length: 165.66
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 67.2837
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.11s
                      Time elapsed: 00:31:50
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 40303 steps/s (collection: 2.315s, learning 0.124s)
             Mean action noise std: 1.82
          Mean value_function loss: 251.4923
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.9330
                       Mean reward: 294.78
               Mean episode length: 155.00
    Episode_Reward/reaching_object: 0.5775
     Episode_Reward/lifting_object: 65.7888
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.44s
                      Time elapsed: 00:31:52
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 47744 steps/s (collection: 1.953s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 207.5659
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.9346
                       Mean reward: 342.07
               Mean episode length: 176.52
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 66.3455
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.06s
                      Time elapsed: 00:31:54
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 47617 steps/s (collection: 1.971s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 205.4186
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.9357
                       Mean reward: 332.85
               Mean episode length: 172.85
    Episode_Reward/reaching_object: 0.5737
     Episode_Reward/lifting_object: 66.1111
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.06s
                      Time elapsed: 00:31:56
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 47411 steps/s (collection: 1.969s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 261.3002
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.9365
                       Mean reward: 342.94
               Mean episode length: 176.05
    Episode_Reward/reaching_object: 0.5611
     Episode_Reward/lifting_object: 65.6548
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.07s
                      Time elapsed: 00:31:59
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 48011 steps/s (collection: 1.933s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 232.5276
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.9407
                       Mean reward: 290.11
               Mean episode length: 155.00
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 63.2265
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.05s
                      Time elapsed: 00:32:01
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 48475 steps/s (collection: 1.920s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 227.5805
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.9454
                       Mean reward: 310.87
               Mean episode length: 159.79
    Episode_Reward/reaching_object: 0.5490
     Episode_Reward/lifting_object: 63.1162
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.03s
                      Time elapsed: 00:32:03
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 49051 steps/s (collection: 1.915s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 223.3775
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.9449
                       Mean reward: 336.51
               Mean episode length: 169.43
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 66.0928
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.00s
                      Time elapsed: 00:32:05
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 49250 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 211.2159
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.9447
                       Mean reward: 351.71
               Mean episode length: 176.97
    Episode_Reward/reaching_object: 0.5699
     Episode_Reward/lifting_object: 67.0349
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.00s
                      Time elapsed: 00:32:07
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 48992 steps/s (collection: 1.912s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 241.8028
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.9465
                       Mean reward: 327.69
               Mean episode length: 165.86
    Episode_Reward/reaching_object: 0.5776
     Episode_Reward/lifting_object: 67.8287
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.01s
                      Time elapsed: 00:32:09
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.946s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 226.0089
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9466
                       Mean reward: 329.00
               Mean episode length: 167.33
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 66.9320
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.03s
                      Time elapsed: 00:32:11
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 48921 steps/s (collection: 1.918s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 216.3652
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.9470
                       Mean reward: 386.12
               Mean episode length: 194.16
    Episode_Reward/reaching_object: 0.5831
     Episode_Reward/lifting_object: 68.4679
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.01s
                      Time elapsed: 00:32:13
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 48696 steps/s (collection: 1.929s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 222.0970
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9485
                       Mean reward: 317.47
               Mean episode length: 156.54
    Episode_Reward/reaching_object: 0.5845
     Episode_Reward/lifting_object: 69.0010
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.02s
                      Time elapsed: 00:32:15
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 49611 steps/s (collection: 1.890s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 234.3189
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.9518
                       Mean reward: 327.22
               Mean episode length: 166.29
    Episode_Reward/reaching_object: 0.5729
     Episode_Reward/lifting_object: 67.2422
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 1.98s
                      Time elapsed: 00:32:17
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 49542 steps/s (collection: 1.888s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 197.2417
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.9544
                       Mean reward: 362.69
               Mean episode length: 185.64
    Episode_Reward/reaching_object: 0.5980
     Episode_Reward/lifting_object: 69.8385
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 1.98s
                      Time elapsed: 00:32:19
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 48444 steps/s (collection: 1.922s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 249.6294
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.9562
                       Mean reward: 326.65
               Mean episode length: 166.07
    Episode_Reward/reaching_object: 0.5644
     Episode_Reward/lifting_object: 65.9223
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.03s
                      Time elapsed: 00:32:21
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 49139 steps/s (collection: 1.889s, learning 0.112s)
             Mean action noise std: 1.82
          Mean value_function loss: 208.0443
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.9614
                       Mean reward: 346.33
               Mean episode length: 175.12
    Episode_Reward/reaching_object: 0.5855
     Episode_Reward/lifting_object: 68.0378
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.00s
                      Time elapsed: 00:32:23
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 48604 steps/s (collection: 1.911s, learning 0.112s)
             Mean action noise std: 1.82
          Mean value_function loss: 216.6423
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.9646
                       Mean reward: 325.52
               Mean episode length: 166.31
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 68.3432
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.02s
                      Time elapsed: 00:32:25
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 48323 steps/s (collection: 1.928s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 209.3564
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.9679
                       Mean reward: 332.43
               Mean episode length: 169.59
    Episode_Reward/reaching_object: 0.5742
     Episode_Reward/lifting_object: 67.4377
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.03s
                      Time elapsed: 00:32:27
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 48202 steps/s (collection: 1.933s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 198.7785
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.9697
                       Mean reward: 361.95
               Mean episode length: 182.07
    Episode_Reward/reaching_object: 0.5804
     Episode_Reward/lifting_object: 68.2371
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.04s
                      Time elapsed: 00:32:29
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 48395 steps/s (collection: 1.933s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 252.6813
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.9696
                       Mean reward: 356.04
               Mean episode length: 177.14
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 69.4377
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.03s
                      Time elapsed: 00:32:31
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 49088 steps/s (collection: 1.906s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 215.7735
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.9688
                       Mean reward: 358.98
               Mean episode length: 181.26
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 70.4566
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.00s
                      Time elapsed: 00:32:33
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 49047 steps/s (collection: 1.915s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 207.8074
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.9685
                       Mean reward: 350.77
               Mean episode length: 172.23
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 71.0602
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.00s
                      Time elapsed: 00:32:35
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 49311 steps/s (collection: 1.905s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 236.9612
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.9680
                       Mean reward: 344.24
               Mean episode length: 172.43
    Episode_Reward/reaching_object: 0.6151
     Episode_Reward/lifting_object: 73.0756
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 1.99s
                      Time elapsed: 00:32:37
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 48429 steps/s (collection: 1.932s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 207.0353
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9676
                       Mean reward: 398.26
               Mean episode length: 192.66
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: 71.0786
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.03s
                      Time elapsed: 00:32:39
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 48969 steps/s (collection: 1.916s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 203.6337
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.9694
                       Mean reward: 363.58
               Mean episode length: 179.45
    Episode_Reward/reaching_object: 0.5978
     Episode_Reward/lifting_object: 70.4062
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.01s
                      Time elapsed: 00:32:41
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 49093 steps/s (collection: 1.912s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 235.3189
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.9707
                       Mean reward: 322.94
               Mean episode length: 163.43
    Episode_Reward/reaching_object: 0.5942
     Episode_Reward/lifting_object: 70.0675
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.00s
                      Time elapsed: 00:32:43
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 48455 steps/s (collection: 1.938s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 256.0864
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.9713
                       Mean reward: 352.31
               Mean episode length: 173.14
    Episode_Reward/reaching_object: 0.5763
     Episode_Reward/lifting_object: 67.2002
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.03s
                      Time elapsed: 00:32:45
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 48405 steps/s (collection: 1.937s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 218.2704
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.9716
                       Mean reward: 343.93
               Mean episode length: 177.03
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 70.8809
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.03s
                      Time elapsed: 00:32:47
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 49006 steps/s (collection: 1.912s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 193.3138
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.9700
                       Mean reward: 351.28
               Mean episode length: 173.24
    Episode_Reward/reaching_object: 0.5910
     Episode_Reward/lifting_object: 69.6904
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.01s
                      Time elapsed: 00:32:49
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 48587 steps/s (collection: 1.929s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 228.0804
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.9686
                       Mean reward: 361.93
               Mean episode length: 175.45
    Episode_Reward/reaching_object: 0.5868
     Episode_Reward/lifting_object: 69.6222
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.02s
                      Time elapsed: 00:32:51
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 48113 steps/s (collection: 1.934s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 196.0811
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.9692
                       Mean reward: 361.15
               Mean episode length: 178.21
    Episode_Reward/reaching_object: 0.6153
     Episode_Reward/lifting_object: 72.6747
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.04s
                      Time elapsed: 00:32:53
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 48838 steps/s (collection: 1.902s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 204.3640
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.9702
                       Mean reward: 369.09
               Mean episode length: 184.93
    Episode_Reward/reaching_object: 0.6228
     Episode_Reward/lifting_object: 73.9551
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.01s
                      Time elapsed: 00:32:55
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 47865 steps/s (collection: 1.937s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 244.2400
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.9711
                       Mean reward: 348.94
               Mean episode length: 172.28
    Episode_Reward/reaching_object: 0.6121
     Episode_Reward/lifting_object: 72.7649
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.05s
                      Time elapsed: 00:32:57
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 48618 steps/s (collection: 1.918s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 247.7338
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.9740
                       Mean reward: 344.05
               Mean episode length: 167.45
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 68.7417
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.02s
                      Time elapsed: 00:32:59
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 47710 steps/s (collection: 1.966s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 204.5604
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.9763
                       Mean reward: 350.37
               Mean episode length: 170.60
    Episode_Reward/reaching_object: 0.6117
     Episode_Reward/lifting_object: 72.3864
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.06s
                      Time elapsed: 00:33:01
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 48548 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 203.0377
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.9772
                       Mean reward: 358.69
               Mean episode length: 173.73
    Episode_Reward/reaching_object: 0.5838
     Episode_Reward/lifting_object: 68.7988
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.02s
                      Time elapsed: 00:33:03
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 46878 steps/s (collection: 1.991s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 239.0514
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.9771
                       Mean reward: 348.38
               Mean episode length: 171.51
    Episode_Reward/reaching_object: 0.5907
     Episode_Reward/lifting_object: 70.1232
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.10s
                      Time elapsed: 00:33:05
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 48840 steps/s (collection: 1.920s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 232.7232
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.9801
                       Mean reward: 367.66
               Mean episode length: 181.57
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 74.5613
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.01s
                      Time elapsed: 00:33:07
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 49436 steps/s (collection: 1.901s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 221.6615
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.9822
                       Mean reward: 349.92
               Mean episode length: 170.18
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 69.6473
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 1.99s
                      Time elapsed: 00:33:09
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 48626 steps/s (collection: 1.932s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 216.5111
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.9835
                       Mean reward: 373.83
               Mean episode length: 181.65
    Episode_Reward/reaching_object: 0.5961
     Episode_Reward/lifting_object: 71.1229
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.02s
                      Time elapsed: 00:33:11
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 48463 steps/s (collection: 1.937s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 231.9380
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 35.9843
                       Mean reward: 356.54
               Mean episode length: 173.01
    Episode_Reward/reaching_object: 0.5893
     Episode_Reward/lifting_object: 69.4277
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.03s
                      Time elapsed: 00:33:13
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 49021 steps/s (collection: 1.910s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 193.5488
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.9838
                       Mean reward: 367.44
               Mean episode length: 180.83
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 72.2624
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.01s
                      Time elapsed: 00:33:15
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 48956 steps/s (collection: 1.919s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 218.4406
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.9819
                       Mean reward: 374.51
               Mean episode length: 178.43
    Episode_Reward/reaching_object: 0.6085
     Episode_Reward/lifting_object: 72.8260
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.01s
                      Time elapsed: 00:33:17
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 49035 steps/s (collection: 1.918s, learning 0.087s)
             Mean action noise std: 1.83
          Mean value_function loss: 216.1340
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.9798
                       Mean reward: 346.50
               Mean episode length: 171.69
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 75.4925
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.00s
                      Time elapsed: 00:33:19
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 48709 steps/s (collection: 1.927s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 223.0531
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.9792
                       Mean reward: 363.82
               Mean episode length: 178.91
    Episode_Reward/reaching_object: 0.6015
     Episode_Reward/lifting_object: 71.8003
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.02s
                      Time elapsed: 00:33:21
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 48881 steps/s (collection: 1.910s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 211.7094
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.9776
                       Mean reward: 374.87
               Mean episode length: 181.88
    Episode_Reward/reaching_object: 0.6269
     Episode_Reward/lifting_object: 74.8409
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.01s
                      Time elapsed: 00:33:23
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 48667 steps/s (collection: 1.922s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 226.3731
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.9770
                       Mean reward: 379.65
               Mean episode length: 180.68
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 73.5220
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.02s
                      Time elapsed: 00:33:25
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 44696 steps/s (collection: 2.107s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 208.9243
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.9793
                       Mean reward: 328.24
               Mean episode length: 162.02
    Episode_Reward/reaching_object: 0.6018
     Episode_Reward/lifting_object: 72.0196
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.20s
                      Time elapsed: 00:33:28
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 48000 steps/s (collection: 1.947s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 235.6312
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.9828
                       Mean reward: 390.71
               Mean episode length: 183.25
    Episode_Reward/reaching_object: 0.5961
     Episode_Reward/lifting_object: 71.3291
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.05s
                      Time elapsed: 00:33:30
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 47145 steps/s (collection: 1.991s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 224.4749
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.9868
                       Mean reward: 371.60
               Mean episode length: 177.06
    Episode_Reward/reaching_object: 0.6127
     Episode_Reward/lifting_object: 73.4923
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.09s
                      Time elapsed: 00:33:32
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 48883 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 219.9873
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.9918
                       Mean reward: 378.09
               Mean episode length: 181.46
    Episode_Reward/reaching_object: 0.6178
     Episode_Reward/lifting_object: 74.2632
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.01s
                      Time elapsed: 00:33:34
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 47492 steps/s (collection: 1.970s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 216.9907
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.9941
                       Mean reward: 348.15
               Mean episode length: 169.81
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: 72.0772
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.07s
                      Time elapsed: 00:33:36
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 48696 steps/s (collection: 1.924s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 224.6091
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.9940
                       Mean reward: 382.43
               Mean episode length: 187.60
    Episode_Reward/reaching_object: 0.6121
     Episode_Reward/lifting_object: 72.9215
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.02s
                      Time elapsed: 00:33:38
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 48684 steps/s (collection: 1.922s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 211.4376
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.9952
                       Mean reward: 387.90
               Mean episode length: 183.18
    Episode_Reward/reaching_object: 0.6097
     Episode_Reward/lifting_object: 73.6604
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.02s
                      Time elapsed: 00:33:40
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 48892 steps/s (collection: 1.907s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 199.2351
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.9980
                       Mean reward: 381.47
               Mean episode length: 186.98
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 77.2794
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.01s
                      Time elapsed: 00:33:42
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 48341 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 234.9763
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 35.9984
                       Mean reward: 362.16
               Mean episode length: 172.12
    Episode_Reward/reaching_object: 0.6045
     Episode_Reward/lifting_object: 72.8356
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.03s
                      Time elapsed: 00:33:44
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 48057 steps/s (collection: 1.926s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 223.6962
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.9974
                       Mean reward: 387.01
               Mean episode length: 186.70
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 71.4444
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.05s
                      Time elapsed: 00:33:46
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 49052 steps/s (collection: 1.915s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 221.4153
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.9964
                       Mean reward: 416.23
               Mean episode length: 194.30
    Episode_Reward/reaching_object: 0.6477
     Episode_Reward/lifting_object: 78.9783
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.00s
                      Time elapsed: 00:33:48
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 48368 steps/s (collection: 1.939s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 217.3425
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.9973
                       Mean reward: 370.63
               Mean episode length: 181.08
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 72.6018
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.03s
                      Time elapsed: 00:33:50
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 48523 steps/s (collection: 1.927s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 201.0340
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.0018
                       Mean reward: 372.65
               Mean episode length: 180.71
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: 77.1173
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.03s
                      Time elapsed: 00:33:52
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 48089 steps/s (collection: 1.939s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 222.2338
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.0027
                       Mean reward: 372.02
               Mean episode length: 179.21
    Episode_Reward/reaching_object: 0.6149
     Episode_Reward/lifting_object: 74.4694
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.04s
                      Time elapsed: 00:33:54
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 47979 steps/s (collection: 1.941s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 221.7326
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.0034
                       Mean reward: 408.39
               Mean episode length: 191.58
    Episode_Reward/reaching_object: 0.6226
     Episode_Reward/lifting_object: 75.6664
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.05s
                      Time elapsed: 00:33:56
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 47902 steps/s (collection: 1.958s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 283.7917
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.0073
                       Mean reward: 345.09
               Mean episode length: 166.04
    Episode_Reward/reaching_object: 0.6166
     Episode_Reward/lifting_object: 74.9246
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.05s
                      Time elapsed: 00:33:58
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 47945 steps/s (collection: 1.944s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 258.6985
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.0097
                       Mean reward: 361.32
               Mean episode length: 175.28
    Episode_Reward/reaching_object: 0.6238
     Episode_Reward/lifting_object: 75.6504
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.05s
                      Time elapsed: 00:34:00
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 48206 steps/s (collection: 1.934s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 242.9542
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.0112
                       Mean reward: 418.57
               Mean episode length: 192.75
    Episode_Reward/reaching_object: 0.6098
     Episode_Reward/lifting_object: 74.3583
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.04s
                      Time elapsed: 00:34:02
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 48515 steps/s (collection: 1.918s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 269.9612
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.0127
                       Mean reward: 365.63
               Mean episode length: 172.62
    Episode_Reward/reaching_object: 0.6062
     Episode_Reward/lifting_object: 74.2392
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.03s
                      Time elapsed: 00:34:04
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 48433 steps/s (collection: 1.926s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 229.5773
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.0143
                       Mean reward: 359.30
               Mean episode length: 172.76
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 74.5554
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.03s
                      Time elapsed: 00:34:06
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 48263 steps/s (collection: 1.937s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 231.0957
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.0146
                       Mean reward: 359.45
               Mean episode length: 172.03
    Episode_Reward/reaching_object: 0.6120
     Episode_Reward/lifting_object: 74.8658
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.04s
                      Time elapsed: 00:34:08
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 48597 steps/s (collection: 1.923s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 237.0332
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.0125
                       Mean reward: 355.41
               Mean episode length: 176.22
    Episode_Reward/reaching_object: 0.6134
     Episode_Reward/lifting_object: 73.8742
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.02s
                      Time elapsed: 00:34:10
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 49429 steps/s (collection: 1.891s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 205.1290
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.0164
                       Mean reward: 376.69
               Mean episode length: 179.61
    Episode_Reward/reaching_object: 0.6129
     Episode_Reward/lifting_object: 74.8588
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 1.99s
                      Time elapsed: 00:34:12
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 49248 steps/s (collection: 1.905s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 217.9196
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.0230
                       Mean reward: 368.04
               Mean episode length: 174.76
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 75.8086
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.00s
                      Time elapsed: 00:34:14
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 48764 steps/s (collection: 1.926s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 257.4647
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.0275
                       Mean reward: 391.64
               Mean episode length: 181.15
    Episode_Reward/reaching_object: 0.6271
     Episode_Reward/lifting_object: 77.4732
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.02s
                      Time elapsed: 00:34:16
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 47761 steps/s (collection: 1.956s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 211.8018
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.0299
                       Mean reward: 383.65
               Mean episode length: 180.16
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 73.2458
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.06s
                      Time elapsed: 00:34:18
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 48635 steps/s (collection: 1.927s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 246.4674
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.0306
                       Mean reward: 391.78
               Mean episode length: 182.43
    Episode_Reward/reaching_object: 0.6253
     Episode_Reward/lifting_object: 77.4007
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.02s
                      Time elapsed: 00:34:20
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 47960 steps/s (collection: 1.944s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 239.5112
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.0322
                       Mean reward: 378.55
               Mean episode length: 178.87
    Episode_Reward/reaching_object: 0.6101
     Episode_Reward/lifting_object: 74.6834
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.05s
                      Time elapsed: 00:34:22
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 48687 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 248.7910
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.0365
                       Mean reward: 360.11
               Mean episode length: 174.41
    Episode_Reward/reaching_object: 0.6137
     Episode_Reward/lifting_object: 74.8144
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.02s
                      Time elapsed: 00:34:24
                               ETA: 00:47:28

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 48213 steps/s (collection: 1.943s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 233.4892
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.0428
                       Mean reward: 374.54
               Mean episode length: 177.74
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 78.1443
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.04s
                      Time elapsed: 00:34:27
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 49017 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 222.6178
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.0480
                       Mean reward: 362.45
               Mean episode length: 171.97
    Episode_Reward/reaching_object: 0.6096
     Episode_Reward/lifting_object: 74.9656
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.01s
                      Time elapsed: 00:34:29
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 48562 steps/s (collection: 1.927s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 214.5702
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.0480
                       Mean reward: 367.36
               Mean episode length: 176.35
    Episode_Reward/reaching_object: 0.6135
     Episode_Reward/lifting_object: 76.0672
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.02s
                      Time elapsed: 00:34:31
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 47993 steps/s (collection: 1.945s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 224.7638
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.0480
                       Mean reward: 363.49
               Mean episode length: 172.11
    Episode_Reward/reaching_object: 0.6043
     Episode_Reward/lifting_object: 75.1111
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.05s
                      Time elapsed: 00:34:33
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 48423 steps/s (collection: 1.933s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 202.2166
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.0484
                       Mean reward: 400.81
               Mean episode length: 185.87
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 78.9964
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.03s
                      Time elapsed: 00:34:35
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 49162 steps/s (collection: 1.908s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 246.9473
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.0496
                       Mean reward: 375.44
               Mean episode length: 179.35
    Episode_Reward/reaching_object: 0.6208
     Episode_Reward/lifting_object: 76.4316
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.00s
                      Time elapsed: 00:34:37
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 48638 steps/s (collection: 1.926s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 226.7239
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.0517
                       Mean reward: 378.74
               Mean episode length: 180.50
    Episode_Reward/reaching_object: 0.6512
     Episode_Reward/lifting_object: 80.8529
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.02s
                      Time elapsed: 00:34:39
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 48631 steps/s (collection: 1.926s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 210.5056
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.0560
                       Mean reward: 404.22
               Mean episode length: 188.98
    Episode_Reward/reaching_object: 0.6376
     Episode_Reward/lifting_object: 78.6025
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.02s
                      Time elapsed: 00:34:41
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 48642 steps/s (collection: 1.934s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 220.6595
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.0643
                       Mean reward: 367.92
               Mean episode length: 172.94
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 77.0973
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.02s
                      Time elapsed: 00:34:43
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 49175 steps/s (collection: 1.913s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 232.4193
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.0694
                       Mean reward: 380.46
               Mean episode length: 182.59
    Episode_Reward/reaching_object: 0.6287
     Episode_Reward/lifting_object: 77.7537
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.00s
                      Time elapsed: 00:34:45
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 47559 steps/s (collection: 1.972s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 247.1616
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.0715
                       Mean reward: 395.16
               Mean episode length: 183.54
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 74.0909
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.07s
                      Time elapsed: 00:34:47
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 49302 steps/s (collection: 1.905s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 251.7277
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.0761
                       Mean reward: 408.68
               Mean episode length: 189.42
    Episode_Reward/reaching_object: 0.6390
     Episode_Reward/lifting_object: 79.8926
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 1.99s
                      Time elapsed: 00:34:49
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 49001 steps/s (collection: 1.920s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 242.4710
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.0827
                       Mean reward: 379.20
               Mean episode length: 178.62
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 75.0902
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.01s
                      Time elapsed: 00:34:51
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 48348 steps/s (collection: 1.917s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 230.4574
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.0805
                       Mean reward: 372.13
               Mean episode length: 176.71
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: 78.2339
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.03s
                      Time elapsed: 00:34:53
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 48544 steps/s (collection: 1.919s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 259.1217
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.0816
                       Mean reward: 348.78
               Mean episode length: 165.68
    Episode_Reward/reaching_object: 0.6138
     Episode_Reward/lifting_object: 76.5782
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.03s
                      Time elapsed: 00:34:55
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 48832 steps/s (collection: 1.917s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 255.1501
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.0835
                       Mean reward: 370.05
               Mean episode length: 176.88
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: 75.2333
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.01s
                      Time elapsed: 00:34:57
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 47878 steps/s (collection: 1.958s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 263.1144
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.0831
                       Mean reward: 372.10
               Mean episode length: 172.90
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 74.1539
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.05s
                      Time elapsed: 00:34:59
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 48628 steps/s (collection: 1.927s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 214.5830
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.0879
                       Mean reward: 386.28
               Mean episode length: 180.08
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 75.8736
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.02s
                      Time elapsed: 00:35:01
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 48193 steps/s (collection: 1.936s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 244.2673
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.0934
                       Mean reward: 370.59
               Mean episode length: 173.11
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 75.3610
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.04s
                      Time elapsed: 00:35:03
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 48803 steps/s (collection: 1.926s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 248.5191
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.0982
                       Mean reward: 352.59
               Mean episode length: 163.97
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 74.7482
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.01s
                      Time elapsed: 00:35:05
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 48638 steps/s (collection: 1.934s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 242.9437
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.0988
                       Mean reward: 407.11
               Mean episode length: 188.06
    Episode_Reward/reaching_object: 0.6260
     Episode_Reward/lifting_object: 78.8253
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.02s
                      Time elapsed: 00:35:07
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 48804 steps/s (collection: 1.920s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 250.4193
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.0987
                       Mean reward: 391.19
               Mean episode length: 179.17
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 75.6892
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.01s
                      Time elapsed: 00:35:09
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 48523 steps/s (collection: 1.931s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 247.7446
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.0976
                       Mean reward: 383.38
               Mean episode length: 181.91
    Episode_Reward/reaching_object: 0.5949
     Episode_Reward/lifting_object: 74.5309
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.03s
                      Time elapsed: 00:35:11
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 49405 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 298.6264
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.0993
                       Mean reward: 361.52
               Mean episode length: 166.69
    Episode_Reward/reaching_object: 0.5947
     Episode_Reward/lifting_object: 75.2858
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 1.99s
                      Time elapsed: 00:35:13
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 48968 steps/s (collection: 1.915s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 251.6888
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.1061
                       Mean reward: 403.80
               Mean episode length: 184.88
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 76.8075
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.01s
                      Time elapsed: 00:35:15
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 47994 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 230.9180
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.1123
                       Mean reward: 362.08
               Mean episode length: 169.22
    Episode_Reward/reaching_object: 0.6154
     Episode_Reward/lifting_object: 77.9644
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.05s
                      Time elapsed: 00:35:17
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 48868 steps/s (collection: 1.909s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 237.0011
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.1163
                       Mean reward: 399.78
               Mean episode length: 181.42
    Episode_Reward/reaching_object: 0.6085
     Episode_Reward/lifting_object: 77.9539
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.01s
                      Time elapsed: 00:35:19
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 48707 steps/s (collection: 1.913s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 260.3438
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.1166
                       Mean reward: 382.05
               Mean episode length: 177.97
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: 77.4873
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.02s
                      Time elapsed: 00:35:21
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.925s, learning 0.108s)
             Mean action noise std: 1.85
          Mean value_function loss: 223.7008
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.1204
                       Mean reward: 381.01
               Mean episode length: 176.31
    Episode_Reward/reaching_object: 0.6154
     Episode_Reward/lifting_object: 77.8994
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.03s
                      Time elapsed: 00:35:23
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 48248 steps/s (collection: 1.929s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 235.9148
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.1252
                       Mean reward: 370.58
               Mean episode length: 170.70
    Episode_Reward/reaching_object: 0.5839
     Episode_Reward/lifting_object: 73.7746
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.04s
                      Time elapsed: 00:35:25
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 48930 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 249.8954
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.1304
                       Mean reward: 392.65
               Mean episode length: 181.25
    Episode_Reward/reaching_object: 0.6167
     Episode_Reward/lifting_object: 79.0036
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.01s
                      Time elapsed: 00:35:27
                               ETA: 00:45:54

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.934s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 237.4685
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1377
                       Mean reward: 394.70
               Mean episode length: 184.49
    Episode_Reward/reaching_object: 0.6253
     Episode_Reward/lifting_object: 79.4976
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.03s
                      Time elapsed: 00:35:29
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 48309 steps/s (collection: 1.943s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 269.7142
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.1454
                       Mean reward: 426.16
               Mean episode length: 195.34
    Episode_Reward/reaching_object: 0.6161
     Episode_Reward/lifting_object: 78.9117
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.03s
                      Time elapsed: 00:35:31
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 48653 steps/s (collection: 1.924s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 261.4466
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.1472
                       Mean reward: 379.64
               Mean episode length: 181.85
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 74.6397
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.02s
                      Time elapsed: 00:35:33
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 48877 steps/s (collection: 1.922s, learning 0.089s)
             Mean action noise std: 1.86
          Mean value_function loss: 272.8016
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.1504
                       Mean reward: 374.54
               Mean episode length: 173.22
    Episode_Reward/reaching_object: 0.6120
     Episode_Reward/lifting_object: 79.0827
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.01s
                      Time elapsed: 00:35:35
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 49561 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 250.7247
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.1556
                       Mean reward: 393.86
               Mean episode length: 181.98
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 77.1909
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 1.98s
                      Time elapsed: 00:35:37
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 48821 steps/s (collection: 1.919s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 260.6445
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.1576
                       Mean reward: 392.67
               Mean episode length: 178.05
    Episode_Reward/reaching_object: 0.6168
     Episode_Reward/lifting_object: 79.5732
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.01s
                      Time elapsed: 00:35:39
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 48048 steps/s (collection: 1.946s, learning 0.100s)
             Mean action noise std: 1.86
          Mean value_function loss: 285.0295
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.1583
                       Mean reward: 385.23
               Mean episode length: 175.66
    Episode_Reward/reaching_object: 0.5931
     Episode_Reward/lifting_object: 77.0120
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.05s
                      Time elapsed: 00:35:41
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 48093 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 248.6244
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.1609
                       Mean reward: 377.79
               Mean episode length: 169.01
    Episode_Reward/reaching_object: 0.6011
     Episode_Reward/lifting_object: 77.0445
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.04s
                      Time elapsed: 00:35:43
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 46787 steps/s (collection: 2.004s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 250.5171
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.1622
                       Mean reward: 428.53
               Mean episode length: 193.74
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 79.8569
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.10s
                      Time elapsed: 00:35:45
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 47323 steps/s (collection: 1.976s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 273.5707
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.1631
                       Mean reward: 380.22
               Mean episode length: 177.15
    Episode_Reward/reaching_object: 0.6028
     Episode_Reward/lifting_object: 77.2223
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.08s
                      Time elapsed: 00:35:48
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 45231 steps/s (collection: 2.064s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 276.8392
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.1668
                       Mean reward: 412.63
               Mean episode length: 187.08
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 80.6376
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.17s
                      Time elapsed: 00:35:50
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 47118 steps/s (collection: 1.992s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 256.7762
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.1678
                       Mean reward: 424.37
               Mean episode length: 194.55
    Episode_Reward/reaching_object: 0.6035
     Episode_Reward/lifting_object: 76.7670
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.09s
                      Time elapsed: 00:35:52
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 44250 steps/s (collection: 2.027s, learning 0.195s)
             Mean action noise std: 1.86
          Mean value_function loss: 251.3644
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1695
                       Mean reward: 392.26
               Mean episode length: 185.42
    Episode_Reward/reaching_object: 0.6262
     Episode_Reward/lifting_object: 80.5700
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.22s
                      Time elapsed: 00:35:54
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 40344 steps/s (collection: 2.289s, learning 0.147s)
             Mean action noise std: 1.86
          Mean value_function loss: 226.6166
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.1704
                       Mean reward: 398.72
               Mean episode length: 179.59
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 80.3978
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.44s
                      Time elapsed: 00:35:56
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 41385 steps/s (collection: 2.272s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 246.5104
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.1682
                       Mean reward: 449.27
               Mean episode length: 200.38
    Episode_Reward/reaching_object: 0.6367
     Episode_Reward/lifting_object: 82.4479
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.38s
                      Time elapsed: 00:35:59
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 47227 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 276.3444
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.1713
                       Mean reward: 422.64
               Mean episode length: 193.37
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 81.4148
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.08s
                      Time elapsed: 00:36:01
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.926s, learning 0.125s)
             Mean action noise std: 1.86
          Mean value_function loss: 259.2211
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.1748
                       Mean reward: 408.26
               Mean episode length: 186.45
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 79.5721
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.05s
                      Time elapsed: 00:36:03
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 46720 steps/s (collection: 1.998s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 259.9700
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.1788
                       Mean reward: 387.53
               Mean episode length: 178.06
    Episode_Reward/reaching_object: 0.5879
     Episode_Reward/lifting_object: 75.6650
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.10s
                      Time elapsed: 00:36:05
                               ETA: 00:45:03

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 45783 steps/s (collection: 2.030s, learning 0.117s)
             Mean action noise std: 1.86
          Mean value_function loss: 222.7287
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.1803
                       Mean reward: 384.01
               Mean episode length: 177.16
    Episode_Reward/reaching_object: 0.6151
     Episode_Reward/lifting_object: 78.9606
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.15s
                      Time elapsed: 00:36:07
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 45350 steps/s (collection: 2.053s, learning 0.115s)
             Mean action noise std: 1.86
          Mean value_function loss: 265.0616
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.1810
                       Mean reward: 383.37
               Mean episode length: 171.95
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 79.3043
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.17s
                      Time elapsed: 00:36:09
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 45869 steps/s (collection: 2.025s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 279.8418
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.1805
                       Mean reward: 353.71
               Mean episode length: 160.70
    Episode_Reward/reaching_object: 0.5965
     Episode_Reward/lifting_object: 76.3882
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.14s
                      Time elapsed: 00:36:12
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.966s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 270.1314
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.1803
                       Mean reward: 385.42
               Mean episode length: 175.60
    Episode_Reward/reaching_object: 0.6017
     Episode_Reward/lifting_object: 77.5311
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.06s
                      Time elapsed: 00:36:14
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 45079 steps/s (collection: 2.008s, learning 0.173s)
             Mean action noise std: 1.87
          Mean value_function loss: 269.9008
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.1855
                       Mean reward: 404.00
               Mean episode length: 182.02
    Episode_Reward/reaching_object: 0.6168
     Episode_Reward/lifting_object: 79.7246
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.18s
                      Time elapsed: 00:36:16
                               ETA: 00:44:49

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 45920 steps/s (collection: 2.054s, learning 0.087s)
             Mean action noise std: 1.87
          Mean value_function loss: 273.4864
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.1929
                       Mean reward: 403.08
               Mean episode length: 178.51
    Episode_Reward/reaching_object: 0.6019
     Episode_Reward/lifting_object: 78.1297
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.14s
                      Time elapsed: 00:36:18
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 48001 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 274.1186
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.1983
                       Mean reward: 424.79
               Mean episode length: 188.02
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 81.6244
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.05s
                      Time elapsed: 00:36:20
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 48286 steps/s (collection: 1.941s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 289.1719
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.2021
                       Mean reward: 403.64
               Mean episode length: 184.58
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: 79.2734
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.04s
                      Time elapsed: 00:36:22
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 48182 steps/s (collection: 1.933s, learning 0.107s)
             Mean action noise std: 1.87
          Mean value_function loss: 273.2430
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.2016
                       Mean reward: 431.17
               Mean episode length: 187.68
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: 82.1605
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.04s
                      Time elapsed: 00:36:24
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 48291 steps/s (collection: 1.933s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 266.8423
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2043
                       Mean reward: 415.17
               Mean episode length: 186.76
    Episode_Reward/reaching_object: 0.6355
     Episode_Reward/lifting_object: 82.0558
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.04s
                      Time elapsed: 00:36:26
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 48452 steps/s (collection: 1.925s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 254.5777
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2101
                       Mean reward: 389.10
               Mean episode length: 174.16
    Episode_Reward/reaching_object: 0.6141
     Episode_Reward/lifting_object: 79.4907
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.03s
                      Time elapsed: 00:36:28
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 47196 steps/s (collection: 1.985s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 294.7842
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2145
                       Mean reward: 382.41
               Mean episode length: 172.65
    Episode_Reward/reaching_object: 0.6205
     Episode_Reward/lifting_object: 80.4447
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.08s
                      Time elapsed: 00:36:30
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 47729 steps/s (collection: 1.968s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 226.9691
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.2156
                       Mean reward: 387.21
               Mean episode length: 176.07
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 78.2225
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.06s
                      Time elapsed: 00:36:32
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 48417 steps/s (collection: 1.935s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 274.4889
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.2145
                       Mean reward: 400.79
               Mean episode length: 178.42
    Episode_Reward/reaching_object: 0.5946
     Episode_Reward/lifting_object: 77.7467
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.03s
                      Time elapsed: 00:36:34
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 46101 steps/s (collection: 2.007s, learning 0.125s)
             Mean action noise std: 1.87
          Mean value_function loss: 260.8800
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2099
                       Mean reward: 409.31
               Mean episode length: 184.58
    Episode_Reward/reaching_object: 0.6212
     Episode_Reward/lifting_object: 80.5997
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.13s
                      Time elapsed: 00:36:36
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 40262 steps/s (collection: 2.296s, learning 0.146s)
             Mean action noise std: 1.87
          Mean value_function loss: 267.2644
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.2068
                       Mean reward: 418.46
               Mean episode length: 185.74
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 80.7203
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.44s
                      Time elapsed: 00:36:39
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 42744 steps/s (collection: 2.160s, learning 0.140s)
             Mean action noise std: 1.87
          Mean value_function loss: 229.6996
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2080
                       Mean reward: 399.08
               Mean episode length: 180.86
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 79.0722
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.30s
                      Time elapsed: 00:36:41
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 44192 steps/s (collection: 2.117s, learning 0.108s)
             Mean action noise std: 1.87
          Mean value_function loss: 242.3662
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.2093
                       Mean reward: 406.87
               Mean episode length: 180.49
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 80.8638
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.22s
                      Time elapsed: 00:36:43
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 45265 steps/s (collection: 2.042s, learning 0.130s)
             Mean action noise std: 1.87
          Mean value_function loss: 230.7663
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.2110
                       Mean reward: 422.13
               Mean episode length: 189.04
    Episode_Reward/reaching_object: 0.6078
     Episode_Reward/lifting_object: 79.3538
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.17s
                      Time elapsed: 00:36:46
                               ETA: 00:44:10

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 44114 steps/s (collection: 2.120s, learning 0.108s)
             Mean action noise std: 1.87
          Mean value_function loss: 231.0709
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.2113
                       Mean reward: 414.06
               Mean episode length: 184.58
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 82.9761
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.23s
                      Time elapsed: 00:36:48
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 47387 steps/s (collection: 1.976s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 248.2813
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.2146
                       Mean reward: 394.91
               Mean episode length: 180.59
    Episode_Reward/reaching_object: 0.6287
     Episode_Reward/lifting_object: 81.8243
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.07s
                      Time elapsed: 00:36:50
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 48115 steps/s (collection: 1.950s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 240.6839
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.2199
                       Mean reward: 423.26
               Mean episode length: 189.03
    Episode_Reward/reaching_object: 0.6222
     Episode_Reward/lifting_object: 81.6037
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.04s
                      Time elapsed: 00:36:52
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 47905 steps/s (collection: 1.942s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 271.8516
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.2251
                       Mean reward: 423.64
               Mean episode length: 186.90
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 84.7875
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.05s
                      Time elapsed: 00:36:54
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 47900 steps/s (collection: 1.955s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 271.5965
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.2272
                       Mean reward: 382.79
               Mean episode length: 174.36
    Episode_Reward/reaching_object: 0.6203
     Episode_Reward/lifting_object: 80.4269
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.05s
                      Time elapsed: 00:36:56
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 48500 steps/s (collection: 1.928s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 218.8761
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2298
                       Mean reward: 412.61
               Mean episode length: 183.17
    Episode_Reward/reaching_object: 0.6357
     Episode_Reward/lifting_object: 83.2671
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.03s
                      Time elapsed: 00:36:58
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 47563 steps/s (collection: 1.960s, learning 0.107s)
             Mean action noise std: 1.87
          Mean value_function loss: 271.7450
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.2308
                       Mean reward: 417.64
               Mean episode length: 182.18
    Episode_Reward/reaching_object: 0.6321
     Episode_Reward/lifting_object: 82.5604
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.07s
                      Time elapsed: 00:37:00
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 49219 steps/s (collection: 1.901s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 280.5976
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.2314
                       Mean reward: 418.87
               Mean episode length: 185.81
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 82.6061
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.00s
                      Time elapsed: 00:37:02
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 48801 steps/s (collection: 1.929s, learning 0.086s)
             Mean action noise std: 1.87
          Mean value_function loss: 270.2615
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.2341
                       Mean reward: 451.53
               Mean episode length: 197.80
    Episode_Reward/reaching_object: 0.6272
     Episode_Reward/lifting_object: 82.2703
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.01s
                      Time elapsed: 00:37:04
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 49119 steps/s (collection: 1.913s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 242.3823
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.2361
                       Mean reward: 400.28
               Mean episode length: 176.86
    Episode_Reward/reaching_object: 0.6264
     Episode_Reward/lifting_object: 81.9915
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.00s
                      Time elapsed: 00:37:06
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 48606 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 245.2928
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.2375
                       Mean reward: 418.78
               Mean episode length: 186.12
    Episode_Reward/reaching_object: 0.6192
     Episode_Reward/lifting_object: 80.8149
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.02s
                      Time elapsed: 00:37:08
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.940s, learning 0.096s)
             Mean action noise std: 1.88
          Mean value_function loss: 244.3787
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2384
                       Mean reward: 439.56
               Mean episode length: 192.18
    Episode_Reward/reaching_object: 0.6393
     Episode_Reward/lifting_object: 84.4164
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.04s
                      Time elapsed: 00:37:10
                               ETA: 00:43:35

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 47752 steps/s (collection: 1.960s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 248.6275
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.2436
                       Mean reward: 455.44
               Mean episode length: 196.10
    Episode_Reward/reaching_object: 0.6426
     Episode_Reward/lifting_object: 85.2343
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.06s
                      Time elapsed: 00:37:12
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 48141 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 261.7051
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.2469
                       Mean reward: 386.69
               Mean episode length: 171.15
    Episode_Reward/reaching_object: 0.6161
     Episode_Reward/lifting_object: 81.3274
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.04s
                      Time elapsed: 00:37:14
                               ETA: 00:43:30

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 47715 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 254.6503
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.2485
                       Mean reward: 427.59
               Mean episode length: 187.63
    Episode_Reward/reaching_object: 0.6474
     Episode_Reward/lifting_object: 85.7685
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.06s
                      Time elapsed: 00:37:16
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 47854 steps/s (collection: 1.964s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 257.8103
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.2507
                       Mean reward: 407.31
               Mean episode length: 184.22
    Episode_Reward/reaching_object: 0.6328
     Episode_Reward/lifting_object: 83.2036
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.05s
                      Time elapsed: 00:37:18
                               ETA: 00:43:24

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 48197 steps/s (collection: 1.942s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 239.4735
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.2563
                       Mean reward: 411.44
               Mean episode length: 184.46
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 81.8472
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.04s
                      Time elapsed: 00:37:20
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 46916 steps/s (collection: 1.994s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 241.0119
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2594
                       Mean reward: 396.96
               Mean episode length: 180.36
    Episode_Reward/reaching_object: 0.6315
     Episode_Reward/lifting_object: 83.3093
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.10s
                      Time elapsed: 00:37:22
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 46827 steps/s (collection: 1.986s, learning 0.113s)
             Mean action noise std: 1.88
          Mean value_function loss: 231.5487
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.2617
                       Mean reward: 456.97
               Mean episode length: 196.61
    Episode_Reward/reaching_object: 0.6609
     Episode_Reward/lifting_object: 88.2573
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.10s
                      Time elapsed: 00:37:25
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 46057 steps/s (collection: 2.013s, learning 0.122s)
             Mean action noise std: 1.88
          Mean value_function loss: 237.3937
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.2640
                       Mean reward: 450.34
               Mean episode length: 194.89
    Episode_Reward/reaching_object: 0.6318
     Episode_Reward/lifting_object: 83.3672
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.13s
                      Time elapsed: 00:37:27
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 44980 steps/s (collection: 2.078s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 272.8999
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.2659
                       Mean reward: 443.40
               Mean episode length: 196.11
    Episode_Reward/reaching_object: 0.6476
     Episode_Reward/lifting_object: 85.7823
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.19s
                      Time elapsed: 00:37:29
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 46095 steps/s (collection: 2.033s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 242.3615
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.2701
                       Mean reward: 424.77
               Mean episode length: 184.64
    Episode_Reward/reaching_object: 0.6325
     Episode_Reward/lifting_object: 83.6371
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.13s
                      Time elapsed: 00:37:31
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 45949 steps/s (collection: 2.029s, learning 0.111s)
             Mean action noise std: 1.88
          Mean value_function loss: 199.7830
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.2714
                       Mean reward: 434.74
               Mean episode length: 186.40
    Episode_Reward/reaching_object: 0.6483
     Episode_Reward/lifting_object: 86.0534
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.14s
                      Time elapsed: 00:37:33
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 43739 steps/s (collection: 2.126s, learning 0.121s)
             Mean action noise std: 1.88
          Mean value_function loss: 235.3590
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.2728
                       Mean reward: 461.30
               Mean episode length: 198.54
    Episode_Reward/reaching_object: 0.6623
     Episode_Reward/lifting_object: 87.9376
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.25s
                      Time elapsed: 00:37:35
                               ETA: 00:43:02

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 45246 steps/s (collection: 2.070s, learning 0.103s)
             Mean action noise std: 1.88
          Mean value_function loss: 284.9341
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2771
                       Mean reward: 380.37
               Mean episode length: 171.80
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 82.6714
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.17s
                      Time elapsed: 00:37:38
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 45280 steps/s (collection: 2.062s, learning 0.109s)
             Mean action noise std: 1.88
          Mean value_function loss: 274.7273
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.2844
                       Mean reward: 426.69
               Mean episode length: 186.28
    Episode_Reward/reaching_object: 0.6271
     Episode_Reward/lifting_object: 82.3944
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.17s
                      Time elapsed: 00:37:40
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 43776 steps/s (collection: 2.136s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 277.9203
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.2908
                       Mean reward: 437.63
               Mean episode length: 190.01
    Episode_Reward/reaching_object: 0.6427
     Episode_Reward/lifting_object: 85.4536
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.25s
                      Time elapsed: 00:37:42
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 42956 steps/s (collection: 2.163s, learning 0.126s)
             Mean action noise std: 1.88
          Mean value_function loss: 273.3670
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.2930
                       Mean reward: 409.90
               Mean episode length: 179.63
    Episode_Reward/reaching_object: 0.6308
     Episode_Reward/lifting_object: 82.7796
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.29s
                      Time elapsed: 00:37:44
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 45913 steps/s (collection: 2.043s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 256.5529
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.2951
                       Mean reward: 434.95
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 0.6472
     Episode_Reward/lifting_object: 84.8994
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.14s
                      Time elapsed: 00:37:46
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 46205 steps/s (collection: 2.031s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 236.6409
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.2987
                       Mean reward: 389.73
               Mean episode length: 176.03
    Episode_Reward/reaching_object: 0.6480
     Episode_Reward/lifting_object: 84.7723
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.13s
                      Time elapsed: 00:37:49
                               ETA: 00:42:46

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.043s, learning 0.109s)
             Mean action noise std: 1.89
          Mean value_function loss: 281.4525
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.3023
                       Mean reward: 409.58
               Mean episode length: 178.74
    Episode_Reward/reaching_object: 0.6294
     Episode_Reward/lifting_object: 83.5885
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.15s
                      Time elapsed: 00:37:51
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 46675 steps/s (collection: 2.001s, learning 0.105s)
             Mean action noise std: 1.89
          Mean value_function loss: 250.9865
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.3072
                       Mean reward: 391.43
               Mean episode length: 174.60
    Episode_Reward/reaching_object: 0.6417
     Episode_Reward/lifting_object: 83.8576
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.11s
                      Time elapsed: 00:37:53
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 46403 steps/s (collection: 2.000s, learning 0.118s)
             Mean action noise std: 1.89
          Mean value_function loss: 287.0110
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.3116
                       Mean reward: 405.96
               Mean episode length: 179.32
    Episode_Reward/reaching_object: 0.6240
     Episode_Reward/lifting_object: 81.6155
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.12s
                      Time elapsed: 00:37:55
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 45744 steps/s (collection: 2.037s, learning 0.112s)
             Mean action noise std: 1.89
          Mean value_function loss: 231.7470
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.3163
                       Mean reward: 418.83
               Mean episode length: 184.50
    Episode_Reward/reaching_object: 0.6415
     Episode_Reward/lifting_object: 83.2193
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.15s
                      Time elapsed: 00:37:57
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 47136 steps/s (collection: 1.992s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 245.7486
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.3166
                       Mean reward: 413.89
               Mean episode length: 182.31
    Episode_Reward/reaching_object: 0.6428
     Episode_Reward/lifting_object: 84.7258
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.09s
                      Time elapsed: 00:37:59
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 46992 steps/s (collection: 1.986s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 247.0130
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.3162
                       Mean reward: 463.24
               Mean episode length: 199.50
    Episode_Reward/reaching_object: 0.6438
     Episode_Reward/lifting_object: 85.2062
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.09s
                      Time elapsed: 00:38:01
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 47057 steps/s (collection: 1.994s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 231.5650
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.3166
                       Mean reward: 447.22
               Mean episode length: 195.17
    Episode_Reward/reaching_object: 0.6564
     Episode_Reward/lifting_object: 86.0573
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.09s
                      Time elapsed: 00:38:03
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 47691 steps/s (collection: 1.969s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 217.6415
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.3183
                       Mean reward: 443.75
               Mean episode length: 194.22
    Episode_Reward/reaching_object: 0.6655
     Episode_Reward/lifting_object: 88.1035
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.06s
                      Time elapsed: 00:38:05
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 47406 steps/s (collection: 1.978s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 256.2183
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.3218
                       Mean reward: 458.83
               Mean episode length: 200.72
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 88.6017
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.07s
                      Time elapsed: 00:38:07
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 47416 steps/s (collection: 1.978s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 240.7716
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.3244
                       Mean reward: 441.32
               Mean episode length: 189.99
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 87.6211
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.07s
                      Time elapsed: 00:38:10
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 46887 steps/s (collection: 2.001s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 201.8022
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.3259
                       Mean reward: 438.41
               Mean episode length: 190.11
    Episode_Reward/reaching_object: 0.6680
     Episode_Reward/lifting_object: 88.5485
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.10s
                      Time elapsed: 00:38:12
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 46797 steps/s (collection: 2.003s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 254.0961
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.3296
                       Mean reward: 430.54
               Mean episode length: 191.36
    Episode_Reward/reaching_object: 0.6662
     Episode_Reward/lifting_object: 87.6958
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.10s
                      Time elapsed: 00:38:14
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 45317 steps/s (collection: 2.067s, learning 0.102s)
             Mean action noise std: 1.89
          Mean value_function loss: 257.1690
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.3365
                       Mean reward: 459.34
               Mean episode length: 201.70
    Episode_Reward/reaching_object: 0.6722
     Episode_Reward/lifting_object: 88.6960
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.17s
                      Time elapsed: 00:38:16
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 47038 steps/s (collection: 1.986s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 254.1517
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.3404
                       Mean reward: 414.67
               Mean episode length: 185.01
    Episode_Reward/reaching_object: 0.6520
     Episode_Reward/lifting_object: 85.6336
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.09s
                      Time elapsed: 00:38:18
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 46162 steps/s (collection: 2.026s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 232.2465
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.3410
                       Mean reward: 444.75
               Mean episode length: 194.94
    Episode_Reward/reaching_object: 0.6839
     Episode_Reward/lifting_object: 90.1226
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.13s
                      Time elapsed: 00:38:20
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.986s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 227.7092
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.3415
                       Mean reward: 454.77
               Mean episode length: 198.23
    Episode_Reward/reaching_object: 0.6637
     Episode_Reward/lifting_object: 87.6107
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.08s
                      Time elapsed: 00:38:22
                               ETA: 00:42:02

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 46143 steps/s (collection: 2.036s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 207.4059
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.3422
                       Mean reward: 433.89
               Mean episode length: 190.48
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: 89.2855
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.13s
                      Time elapsed: 00:38:24
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 45602 steps/s (collection: 2.044s, learning 0.112s)
             Mean action noise std: 1.89
          Mean value_function loss: 195.1843
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.3409
                       Mean reward: 446.80
               Mean episode length: 192.35
    Episode_Reward/reaching_object: 0.6893
     Episode_Reward/lifting_object: 91.5842
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.16s
                      Time elapsed: 00:38:27
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 45616 steps/s (collection: 2.042s, learning 0.114s)
             Mean action noise std: 1.89
          Mean value_function loss: 255.2444
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.3385
                       Mean reward: 451.92
               Mean episode length: 200.90
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 88.2526
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.16s
                      Time elapsed: 00:38:29
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 46560 steps/s (collection: 2.016s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 273.2699
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.3379
                       Mean reward: 447.61
               Mean episode length: 195.74
    Episode_Reward/reaching_object: 0.6618
     Episode_Reward/lifting_object: 87.9530
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.11s
                      Time elapsed: 00:38:31
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 45521 steps/s (collection: 2.045s, learning 0.115s)
             Mean action noise std: 1.89
          Mean value_function loss: 247.6715
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.3388
                       Mean reward: 448.29
               Mean episode length: 195.64
    Episode_Reward/reaching_object: 0.6761
     Episode_Reward/lifting_object: 89.7857
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.16s
                      Time elapsed: 00:38:33
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 46508 steps/s (collection: 2.008s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 237.3548
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.3398
                       Mean reward: 468.10
               Mean episode length: 201.80
    Episode_Reward/reaching_object: 0.6686
     Episode_Reward/lifting_object: 89.1923
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.11s
                      Time elapsed: 00:38:35
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 46621 steps/s (collection: 2.010s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 230.2769
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.3411
                       Mean reward: 462.65
               Mean episode length: 198.04
    Episode_Reward/reaching_object: 0.6674
     Episode_Reward/lifting_object: 88.8370
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.11s
                      Time elapsed: 00:38:37
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 46947 steps/s (collection: 1.994s, learning 0.100s)
             Mean action noise std: 1.89
          Mean value_function loss: 263.1720
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.3445
                       Mean reward: 482.71
               Mean episode length: 206.62
    Episode_Reward/reaching_object: 0.6862
     Episode_Reward/lifting_object: 91.9705
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.09s
                      Time elapsed: 00:38:39
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.040s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 224.5488
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.3460
                       Mean reward: 460.07
               Mean episode length: 202.24
    Episode_Reward/reaching_object: 0.6702
     Episode_Reward/lifting_object: 88.2230
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.13s
                      Time elapsed: 00:38:41
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 46208 steps/s (collection: 2.015s, learning 0.112s)
             Mean action noise std: 1.89
          Mean value_function loss: 239.6494
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 36.3469
                       Mean reward: 459.89
               Mean episode length: 198.04
    Episode_Reward/reaching_object: 0.6744
     Episode_Reward/lifting_object: 90.1488
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.13s
                      Time elapsed: 00:38:44
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 46648 steps/s (collection: 2.016s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 247.1469
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.3470
                       Mean reward: 410.72
               Mean episode length: 182.84
    Episode_Reward/reaching_object: 0.6491
     Episode_Reward/lifting_object: 85.5630
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.11s
                      Time elapsed: 00:38:46
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 46362 steps/s (collection: 2.023s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 240.0293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.3463
                       Mean reward: 458.17
               Mean episode length: 200.80
    Episode_Reward/reaching_object: 0.6673
     Episode_Reward/lifting_object: 88.6721
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.12s
                      Time elapsed: 00:38:48
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 46425 steps/s (collection: 2.016s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 224.6479
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.3467
                       Mean reward: 421.91
               Mean episode length: 184.91
    Episode_Reward/reaching_object: 0.6474
     Episode_Reward/lifting_object: 85.6821
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.12s
                      Time elapsed: 00:38:50
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 46071 steps/s (collection: 2.030s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 242.4487
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.3477
                       Mean reward: 455.96
               Mean episode length: 197.13
    Episode_Reward/reaching_object: 0.6789
     Episode_Reward/lifting_object: 90.4691
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.13s
                      Time elapsed: 00:38:52
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 46557 steps/s (collection: 1.999s, learning 0.112s)
             Mean action noise std: 1.90
          Mean value_function loss: 268.9679
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.3495
                       Mean reward: 453.61
               Mean episode length: 195.20
    Episode_Reward/reaching_object: 0.6777
     Episode_Reward/lifting_object: 90.4357
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.11s
                      Time elapsed: 00:38:54
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 46026 steps/s (collection: 2.005s, learning 0.131s)
             Mean action noise std: 1.90
          Mean value_function loss: 220.2613
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.3498
                       Mean reward: 453.43
               Mean episode length: 194.35
    Episode_Reward/reaching_object: 0.6793
     Episode_Reward/lifting_object: 90.7284
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.14s
                      Time elapsed: 00:38:56
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 46351 steps/s (collection: 1.999s, learning 0.122s)
             Mean action noise std: 1.90
          Mean value_function loss: 225.8265
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.3506
                       Mean reward: 462.31
               Mean episode length: 199.51
    Episode_Reward/reaching_object: 0.6814
     Episode_Reward/lifting_object: 91.1310
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.12s
                      Time elapsed: 00:38:58
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 46173 steps/s (collection: 2.016s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 249.1358
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.3522
                       Mean reward: 438.48
               Mean episode length: 187.25
    Episode_Reward/reaching_object: 0.6723
     Episode_Reward/lifting_object: 90.2826
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.13s
                      Time elapsed: 00:39:00
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 46194 steps/s (collection: 2.021s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 355.8174
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.3568
                       Mean reward: 438.77
               Mean episode length: 193.71
    Episode_Reward/reaching_object: 0.6762
     Episode_Reward/lifting_object: 90.0964
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.13s
                      Time elapsed: 00:39:03
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 46343 steps/s (collection: 2.027s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 348.1124
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.3603
                       Mean reward: 432.66
               Mean episode length: 192.26
    Episode_Reward/reaching_object: 0.6545
     Episode_Reward/lifting_object: 86.6509
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.12s
                      Time elapsed: 00:39:05
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 47080 steps/s (collection: 1.988s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 225.6733
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.3636
                       Mean reward: 449.93
               Mean episode length: 193.77
    Episode_Reward/reaching_object: 0.6574
     Episode_Reward/lifting_object: 88.6984
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.09s
                      Time elapsed: 00:39:07
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 46851 steps/s (collection: 1.997s, learning 0.102s)
             Mean action noise std: 1.90
          Mean value_function loss: 245.7648
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.3662
                       Mean reward: 490.83
               Mean episode length: 207.85
    Episode_Reward/reaching_object: 0.6803
     Episode_Reward/lifting_object: 91.3053
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.10s
                      Time elapsed: 00:39:09
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 46167 steps/s (collection: 2.016s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 239.6004
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.3677
                       Mean reward: 451.17
               Mean episode length: 193.09
    Episode_Reward/reaching_object: 0.6668
     Episode_Reward/lifting_object: 89.4626
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.13s
                      Time elapsed: 00:39:11
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 46205 steps/s (collection: 2.027s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 258.1325
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.3686
                       Mean reward: 462.41
               Mean episode length: 198.17
    Episode_Reward/reaching_object: 0.6594
     Episode_Reward/lifting_object: 88.5327
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.13s
                      Time elapsed: 00:39:13
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 46435 steps/s (collection: 2.013s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 212.0941
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.3705
                       Mean reward: 473.50
               Mean episode length: 202.49
    Episode_Reward/reaching_object: 0.6617
     Episode_Reward/lifting_object: 88.9812
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.12s
                      Time elapsed: 00:39:15
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 46463 steps/s (collection: 2.022s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 210.9118
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.3732
                       Mean reward: 435.72
               Mean episode length: 188.61
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 93.0341
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.12s
                      Time elapsed: 00:39:17
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 47177 steps/s (collection: 1.984s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 233.5134
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.3756
                       Mean reward: 454.33
               Mean episode length: 194.51
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 89.0160
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.08s
                      Time elapsed: 00:39:20
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 45515 steps/s (collection: 2.055s, learning 0.105s)
             Mean action noise std: 1.90
          Mean value_function loss: 234.5862
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.3766
                       Mean reward: 425.95
               Mean episode length: 182.52
    Episode_Reward/reaching_object: 0.6817
     Episode_Reward/lifting_object: 92.1465
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.16s
                      Time elapsed: 00:39:22
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 45718 steps/s (collection: 2.045s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 258.5446
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.3777
                       Mean reward: 457.70
               Mean episode length: 196.86
    Episode_Reward/reaching_object: 0.6832
     Episode_Reward/lifting_object: 92.5178
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.15s
                      Time elapsed: 00:39:24
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 45927 steps/s (collection: 2.034s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 227.5946
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.3817
                       Mean reward: 462.86
               Mean episode length: 198.40
    Episode_Reward/reaching_object: 0.6680
     Episode_Reward/lifting_object: 89.9994
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.14s
                      Time elapsed: 00:39:26
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 46224 steps/s (collection: 2.028s, learning 0.099s)
             Mean action noise std: 1.90
          Mean value_function loss: 262.7419
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.3857
                       Mean reward: 462.03
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 0.6456
     Episode_Reward/lifting_object: 87.0578
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.13s
                      Time elapsed: 00:39:28
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 45974 steps/s (collection: 2.024s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 259.2849
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.3917
                       Mean reward: 434.44
               Mean episode length: 186.20
    Episode_Reward/reaching_object: 0.6843
     Episode_Reward/lifting_object: 92.4976
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.14s
                      Time elapsed: 00:39:30
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 46576 steps/s (collection: 2.012s, learning 0.099s)
             Mean action noise std: 1.90
          Mean value_function loss: 245.5532
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.3950
                       Mean reward: 454.03
               Mean episode length: 192.30
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: 87.6465
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.11s
                      Time elapsed: 00:39:32
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.037s, learning 0.099s)
             Mean action noise std: 1.90
          Mean value_function loss: 255.4062
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.3954
                       Mean reward: 430.08
               Mean episode length: 183.60
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 86.6740
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.14s
                      Time elapsed: 00:39:34
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 46463 steps/s (collection: 2.018s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 228.6651
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.3958
                       Mean reward: 442.45
               Mean episode length: 186.96
    Episode_Reward/reaching_object: 0.6685
     Episode_Reward/lifting_object: 90.6129
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.12s
                      Time elapsed: 00:39:37
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 46121 steps/s (collection: 2.028s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 240.8859
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.3973
                       Mean reward: 452.84
               Mean episode length: 195.70
    Episode_Reward/reaching_object: 0.6679
     Episode_Reward/lifting_object: 90.3032
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.13s
                      Time elapsed: 00:39:39
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 45585 steps/s (collection: 2.061s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 241.6062
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.3979
                       Mean reward: 457.28
               Mean episode length: 191.06
    Episode_Reward/reaching_object: 0.6726
     Episode_Reward/lifting_object: 91.0882
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.16s
                      Time elapsed: 00:39:41
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 46645 steps/s (collection: 2.010s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 277.5930
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.3994
                       Mean reward: 442.58
               Mean episode length: 190.10
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 89.8949
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.11s
                      Time elapsed: 00:39:43
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 45725 steps/s (collection: 2.037s, learning 0.113s)
             Mean action noise std: 1.91
          Mean value_function loss: 262.0442
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.4020
                       Mean reward: 476.84
               Mean episode length: 201.67
    Episode_Reward/reaching_object: 0.6736
     Episode_Reward/lifting_object: 91.6837
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.15s
                      Time elapsed: 00:39:45
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 45949 steps/s (collection: 2.032s, learning 0.107s)
             Mean action noise std: 1.91
          Mean value_function loss: 254.9402
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.4018
                       Mean reward: 466.26
               Mean episode length: 200.09
    Episode_Reward/reaching_object: 0.6707
     Episode_Reward/lifting_object: 91.5190
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.14s
                      Time elapsed: 00:39:47
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 45791 steps/s (collection: 2.048s, learning 0.099s)
             Mean action noise std: 1.91
          Mean value_function loss: 228.2535
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.4007
                       Mean reward: 443.80
               Mean episode length: 188.59
    Episode_Reward/reaching_object: 0.6590
     Episode_Reward/lifting_object: 89.0821
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.15s
                      Time elapsed: 00:39:49
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 45102 steps/s (collection: 2.081s, learning 0.099s)
             Mean action noise std: 1.91
          Mean value_function loss: 267.4109
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.3995
                       Mean reward: 433.64
               Mean episode length: 185.29
    Episode_Reward/reaching_object: 0.6468
     Episode_Reward/lifting_object: 88.2459
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.18s
                      Time elapsed: 00:39:52
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 45234 steps/s (collection: 2.071s, learning 0.102s)
             Mean action noise std: 1.91
          Mean value_function loss: 232.0467
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.4040
                       Mean reward: 457.36
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 0.6752
     Episode_Reward/lifting_object: 91.4372
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.17s
                      Time elapsed: 00:39:54
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 45919 steps/s (collection: 2.040s, learning 0.101s)
             Mean action noise std: 1.91
          Mean value_function loss: 240.3338
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.4083
                       Mean reward: 498.71
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 0.6871
     Episode_Reward/lifting_object: 94.7807
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.14s
                      Time elapsed: 00:39:56
                               ETA: 00:40:03

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 46225 steps/s (collection: 2.015s, learning 0.112s)
             Mean action noise std: 1.91
          Mean value_function loss: 229.8972
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.4106
                       Mean reward: 453.74
               Mean episode length: 195.20
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 90.5833
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.13s
                      Time elapsed: 00:39:58
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14326 steps/s (collection: 6.748s, learning 0.114s)
             Mean action noise std: 1.91
          Mean value_function loss: 225.9898
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.4130
                       Mean reward: 459.73
               Mean episode length: 196.04
    Episode_Reward/reaching_object: 0.6587
     Episode_Reward/lifting_object: 89.4952
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.86s
                      Time elapsed: 00:40:05
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14217 steps/s (collection: 6.791s, learning 0.123s)
             Mean action noise std: 1.91
          Mean value_function loss: 225.0986
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.4190
                       Mean reward: 458.87
               Mean episode length: 192.49
    Episode_Reward/reaching_object: 0.6679
     Episode_Reward/lifting_object: 91.3770
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.91s
                      Time elapsed: 00:40:12
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14205 steps/s (collection: 6.805s, learning 0.115s)
             Mean action noise std: 1.91
          Mean value_function loss: 252.8566
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.4239
                       Mean reward: 479.85
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 0.6723
     Episode_Reward/lifting_object: 92.3363
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.92s
                      Time elapsed: 00:40:19
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14003 steps/s (collection: 6.903s, learning 0.117s)
             Mean action noise std: 1.91
          Mean value_function loss: 249.4507
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.4248
                       Mean reward: 447.21
               Mean episode length: 188.61
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: 90.8839
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.02s
                      Time elapsed: 00:40:26
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14052 steps/s (collection: 6.875s, learning 0.121s)
             Mean action noise std: 1.91
          Mean value_function loss: 228.9366
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.4261
                       Mean reward: 477.11
               Mean episode length: 197.72
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 91.2780
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.00s
                      Time elapsed: 00:40:33
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14023 steps/s (collection: 6.854s, learning 0.156s)
             Mean action noise std: 1.91
          Mean value_function loss: 232.4063
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.4279
                       Mean reward: 439.33
               Mean episode length: 182.59
    Episode_Reward/reaching_object: 0.6679
     Episode_Reward/lifting_object: 92.1473
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.01s
                      Time elapsed: 00:40:40
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13597 steps/s (collection: 7.114s, learning 0.116s)
             Mean action noise std: 1.91
          Mean value_function loss: 237.8301
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.4298
                       Mean reward: 451.86
               Mean episode length: 185.85
    Episode_Reward/reaching_object: 0.6678
     Episode_Reward/lifting_object: 91.9726
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.23s
                      Time elapsed: 00:40:47
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14301 steps/s (collection: 6.747s, learning 0.127s)
             Mean action noise std: 1.91
          Mean value_function loss: 274.4745
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.4345
                       Mean reward: 472.85
               Mean episode length: 198.11
    Episode_Reward/reaching_object: 0.6459
     Episode_Reward/lifting_object: 88.8519
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.87s
                      Time elapsed: 00:40:54
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17181 steps/s (collection: 5.628s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 235.4221
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.4360
                       Mean reward: 469.68
               Mean episode length: 198.45
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 91.7060
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.72s
                      Time elapsed: 00:41:00
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 1.91
          Mean value_function loss: 251.5832
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.4380
                       Mean reward: 464.67
               Mean episode length: 191.75
    Episode_Reward/reaching_object: 0.6386
     Episode_Reward/lifting_object: 87.5427
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.02s
                      Time elapsed: 00:41:02
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 43037 steps/s (collection: 2.075s, learning 0.210s)
             Mean action noise std: 1.91
          Mean value_function loss: 237.8336
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4442
                       Mean reward: 472.88
               Mean episode length: 197.37
    Episode_Reward/reaching_object: 0.6638
     Episode_Reward/lifting_object: 91.5205
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.28s
                      Time elapsed: 00:41:04
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 35978 steps/s (collection: 2.603s, learning 0.130s)
             Mean action noise std: 1.91
          Mean value_function loss: 231.1488
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.4454
                       Mean reward: 432.03
               Mean episode length: 183.85
    Episode_Reward/reaching_object: 0.6487
     Episode_Reward/lifting_object: 88.4802
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.73s
                      Time elapsed: 00:41:07
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 43340 steps/s (collection: 2.129s, learning 0.139s)
             Mean action noise std: 1.91
          Mean value_function loss: 244.2532
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.4483
                       Mean reward: 473.48
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 0.6694
     Episode_Reward/lifting_object: 92.0919
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.27s
                      Time elapsed: 00:41:09
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 48646 steps/s (collection: 1.931s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 215.0716
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.4502
                       Mean reward: 493.97
               Mean episode length: 203.25
    Episode_Reward/reaching_object: 0.6856
     Episode_Reward/lifting_object: 95.1811
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.02s
                      Time elapsed: 00:41:11
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 48911 steps/s (collection: 1.893s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 285.6945
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.4508
                       Mean reward: 439.11
               Mean episode length: 184.78
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: 91.2143
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.01s
                      Time elapsed: 00:41:13
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 48323 steps/s (collection: 1.923s, learning 0.112s)
             Mean action noise std: 1.92
          Mean value_function loss: 240.9467
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.4570
                       Mean reward: 453.48
               Mean episode length: 193.51
    Episode_Reward/reaching_object: 0.6690
     Episode_Reward/lifting_object: 92.4590
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.03s
                      Time elapsed: 00:41:15
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 49105 steps/s (collection: 1.888s, learning 0.114s)
             Mean action noise std: 1.92
          Mean value_function loss: 241.4851
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.4627
                       Mean reward: 479.94
               Mean episode length: 198.76
    Episode_Reward/reaching_object: 0.6610
     Episode_Reward/lifting_object: 90.9999
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.00s
                      Time elapsed: 00:41:17
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 49554 steps/s (collection: 1.894s, learning 0.089s)
             Mean action noise std: 1.92
          Mean value_function loss: 263.4203
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.4632
                       Mean reward: 485.23
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 0.6792
     Episode_Reward/lifting_object: 93.5399
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.98s
                      Time elapsed: 00:41:19
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 49785 steps/s (collection: 1.884s, learning 0.091s)
             Mean action noise std: 1.92
          Mean value_function loss: 309.6349
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4641
                       Mean reward: 494.04
               Mean episode length: 206.54
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 89.7524
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.97s
                      Time elapsed: 00:41:21
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 49712 steps/s (collection: 1.886s, learning 0.091s)
             Mean action noise std: 1.92
          Mean value_function loss: 257.8374
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.4675
                       Mean reward: 488.64
               Mean episode length: 202.17
    Episode_Reward/reaching_object: 0.6760
     Episode_Reward/lifting_object: 93.7059
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.98s
                      Time elapsed: 00:41:23
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 48860 steps/s (collection: 1.915s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 250.7012
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4702
                       Mean reward: 490.14
               Mean episode length: 203.06
    Episode_Reward/reaching_object: 0.6902
     Episode_Reward/lifting_object: 94.6121
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.01s
                      Time elapsed: 00:41:25
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 48840 steps/s (collection: 1.916s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 238.3307
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.4716
                       Mean reward: 463.32
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 0.6709
     Episode_Reward/lifting_object: 92.4547
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.01s
                      Time elapsed: 00:41:27
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 284.6691
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.4719
                       Mean reward: 493.33
               Mean episode length: 202.70
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 94.8049
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.99s
                      Time elapsed: 00:41:29
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 49434 steps/s (collection: 1.899s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 233.6312
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.4738
                       Mean reward: 495.15
               Mean episode length: 204.05
    Episode_Reward/reaching_object: 0.6697
     Episode_Reward/lifting_object: 92.6306
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.99s
                      Time elapsed: 00:41:31
                               ETA: 00:39:37

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 49871 steps/s (collection: 1.883s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 260.5296
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.4717
                       Mean reward: 466.59
               Mean episode length: 198.39
    Episode_Reward/reaching_object: 0.6817
     Episode_Reward/lifting_object: 93.5179
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.97s
                      Time elapsed: 00:41:33
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 49871 steps/s (collection: 1.884s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 265.9763
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.4675
                       Mean reward: 431.16
               Mean episode length: 181.74
    Episode_Reward/reaching_object: 0.6578
     Episode_Reward/lifting_object: 90.7406
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.97s
                      Time elapsed: 00:41:35
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 49458 steps/s (collection: 1.889s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 285.4120
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.4700
                       Mean reward: 479.34
               Mean episode length: 203.79
    Episode_Reward/reaching_object: 0.6512
     Episode_Reward/lifting_object: 88.9438
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.99s
                      Time elapsed: 00:41:37
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 49468 steps/s (collection: 1.888s, learning 0.100s)
             Mean action noise std: 1.92
          Mean value_function loss: 238.7908
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.4731
                       Mean reward: 453.97
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 94.1725
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.99s
                      Time elapsed: 00:41:39
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 50192 steps/s (collection: 1.857s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 245.4083
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.4751
                       Mean reward: 479.35
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 0.6785
     Episode_Reward/lifting_object: 94.0847
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.96s
                      Time elapsed: 00:41:41
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 49457 steps/s (collection: 1.877s, learning 0.111s)
             Mean action noise std: 1.92
          Mean value_function loss: 246.9494
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.4745
                       Mean reward: 464.54
               Mean episode length: 193.10
    Episode_Reward/reaching_object: 0.6708
     Episode_Reward/lifting_object: 92.8781
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.99s
                      Time elapsed: 00:41:43
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 49265 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 207.3251
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.4753
                       Mean reward: 462.05
               Mean episode length: 194.74
    Episode_Reward/reaching_object: 0.6789
     Episode_Reward/lifting_object: 92.8326
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.00s
                      Time elapsed: 00:41:45
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.886s, learning 0.105s)
             Mean action noise std: 1.92
          Mean value_function loss: 234.5890
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.4817
                       Mean reward: 475.66
               Mean episode length: 197.85
    Episode_Reward/reaching_object: 0.6884
     Episode_Reward/lifting_object: 96.1711
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.99s
                      Time elapsed: 00:41:47
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 49484 steps/s (collection: 1.891s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 226.7588
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.4901
                       Mean reward: 457.42
               Mean episode length: 190.12
    Episode_Reward/reaching_object: 0.6945
     Episode_Reward/lifting_object: 95.6177
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.99s
                      Time elapsed: 00:41:49
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 48993 steps/s (collection: 1.914s, learning 0.093s)
             Mean action noise std: 1.92
          Mean value_function loss: 271.2505
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.4932
                       Mean reward: 455.01
               Mean episode length: 193.19
    Episode_Reward/reaching_object: 0.6816
     Episode_Reward/lifting_object: 93.8946
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.01s
                      Time elapsed: 00:41:51
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 49257 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 252.9696
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.4939
                       Mean reward: 435.95
               Mean episode length: 187.42
    Episode_Reward/reaching_object: 0.6740
     Episode_Reward/lifting_object: 93.1308
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.00s
                      Time elapsed: 00:41:53
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 49262 steps/s (collection: 1.905s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 264.7045
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.4931
                       Mean reward: 455.53
               Mean episode length: 188.79
    Episode_Reward/reaching_object: 0.6810
     Episode_Reward/lifting_object: 93.5836
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.00s
                      Time elapsed: 00:41:55
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 49771 steps/s (collection: 1.882s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 273.1941
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.4955
                       Mean reward: 464.43
               Mean episode length: 193.14
    Episode_Reward/reaching_object: 0.6747
     Episode_Reward/lifting_object: 93.0670
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.98s
                      Time elapsed: 00:41:57
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 49541 steps/s (collection: 1.892s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 283.2187
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.5029
                       Mean reward: 431.32
               Mean episode length: 181.22
    Episode_Reward/reaching_object: 0.6487
     Episode_Reward/lifting_object: 88.2648
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.98s
                      Time elapsed: 00:41:59
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 49452 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 224.8621
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.5073
                       Mean reward: 459.56
               Mean episode length: 190.90
    Episode_Reward/reaching_object: 0.6745
     Episode_Reward/lifting_object: 92.8457
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 1.99s
                      Time elapsed: 00:42:01
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 49032 steps/s (collection: 1.911s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 251.2394
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.5071
                       Mean reward: 458.33
               Mean episode length: 194.33
    Episode_Reward/reaching_object: 0.6914
     Episode_Reward/lifting_object: 95.7820
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.00s
                      Time elapsed: 00:42:03
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 48989 steps/s (collection: 1.917s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 244.3847
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.5071
                       Mean reward: 441.06
               Mean episode length: 188.22
    Episode_Reward/reaching_object: 0.6726
     Episode_Reward/lifting_object: 92.7740
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.01s
                      Time elapsed: 00:42:05
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 49577 steps/s (collection: 1.894s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 226.1195
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.5078
                       Mean reward: 452.59
               Mean episode length: 188.07
    Episode_Reward/reaching_object: 0.6657
     Episode_Reward/lifting_object: 92.1607
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.98s
                      Time elapsed: 00:42:07
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 49164 steps/s (collection: 1.909s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 252.7447
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5091
                       Mean reward: 505.11
               Mean episode length: 206.88
    Episode_Reward/reaching_object: 0.6630
     Episode_Reward/lifting_object: 91.9722
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.00s
                      Time elapsed: 00:42:09
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 49018 steps/s (collection: 1.908s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 262.6941
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5127
                       Mean reward: 471.67
               Mean episode length: 197.20
    Episode_Reward/reaching_object: 0.6685
     Episode_Reward/lifting_object: 92.6730
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.01s
                      Time elapsed: 00:42:11
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 48525 steps/s (collection: 1.906s, learning 0.120s)
             Mean action noise std: 1.93
          Mean value_function loss: 243.5784
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.5130
                       Mean reward: 480.50
               Mean episode length: 200.44
    Episode_Reward/reaching_object: 0.6635
     Episode_Reward/lifting_object: 92.1074
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.03s
                      Time elapsed: 00:42:13
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 48739 steps/s (collection: 1.890s, learning 0.127s)
             Mean action noise std: 1.93
          Mean value_function loss: 272.0223
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.5122
                       Mean reward: 487.40
               Mean episode length: 198.87
    Episode_Reward/reaching_object: 0.6774
     Episode_Reward/lifting_object: 94.5643
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.02s
                      Time elapsed: 00:42:15
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 48570 steps/s (collection: 1.899s, learning 0.125s)
             Mean action noise std: 1.93
          Mean value_function loss: 275.8667
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.5149
                       Mean reward: 455.30
               Mean episode length: 189.91
    Episode_Reward/reaching_object: 0.6681
     Episode_Reward/lifting_object: 92.0547
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.02s
                      Time elapsed: 00:42:17
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 49229 steps/s (collection: 1.897s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 273.4582
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.5197
                       Mean reward: 497.57
               Mean episode length: 204.50
    Episode_Reward/reaching_object: 0.6836
     Episode_Reward/lifting_object: 95.4679
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.00s
                      Time elapsed: 00:42:19
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.889s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 268.6369
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.5233
                       Mean reward: 471.24
               Mean episode length: 194.85
    Episode_Reward/reaching_object: 0.6850
     Episode_Reward/lifting_object: 95.3646
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.98s
                      Time elapsed: 00:42:21
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 48727 steps/s (collection: 1.917s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 274.1413
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.5295
                       Mean reward: 452.56
               Mean episode length: 186.49
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 93.9847
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.02s
                      Time elapsed: 00:42:23
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 48331 steps/s (collection: 1.934s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 237.4221
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5335
                       Mean reward: 468.86
               Mean episode length: 196.35
    Episode_Reward/reaching_object: 0.6654
     Episode_Reward/lifting_object: 92.1624
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.03s
                      Time elapsed: 00:42:25
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 45865 steps/s (collection: 2.046s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 249.8177
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.5375
                       Mean reward: 478.15
               Mean episode length: 195.32
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 92.9328
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.14s
                      Time elapsed: 00:42:27
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 49081 steps/s (collection: 1.903s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 265.4289
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5410
                       Mean reward: 474.82
               Mean episode length: 193.37
    Episode_Reward/reaching_object: 0.6811
     Episode_Reward/lifting_object: 95.6461
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.00s
                      Time elapsed: 00:42:29
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 48637 steps/s (collection: 1.922s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 252.0047
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.5468
                       Mean reward: 455.52
               Mean episode length: 189.40
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 92.5436
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.02s
                      Time elapsed: 00:42:31
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 49453 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 258.1434
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5527
                       Mean reward: 515.44
               Mean episode length: 208.96
    Episode_Reward/reaching_object: 0.6760
     Episode_Reward/lifting_object: 94.2873
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.99s
                      Time elapsed: 00:42:33
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 49825 steps/s (collection: 1.885s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 228.5561
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5536
                       Mean reward: 473.49
               Mean episode length: 195.13
    Episode_Reward/reaching_object: 0.6878
     Episode_Reward/lifting_object: 96.0228
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 1.97s
                      Time elapsed: 00:42:35
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 49197 steps/s (collection: 1.908s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 235.8650
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.5543
                       Mean reward: 484.51
               Mean episode length: 198.95
    Episode_Reward/reaching_object: 0.6766
     Episode_Reward/lifting_object: 94.2725
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.00s
                      Time elapsed: 00:42:37
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 48564 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.94
          Mean value_function loss: 258.7822
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5570
                       Mean reward: 456.43
               Mean episode length: 191.59
    Episode_Reward/reaching_object: 0.6647
     Episode_Reward/lifting_object: 93.2013
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.02s
                      Time elapsed: 00:42:39
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 46996 steps/s (collection: 1.988s, learning 0.104s)
             Mean action noise std: 1.94
          Mean value_function loss: 269.3037
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5614
                       Mean reward: 489.75
               Mean episode length: 198.98
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 88.8110
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.09s
                      Time elapsed: 00:42:41
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 47284 steps/s (collection: 1.967s, learning 0.112s)
             Mean action noise std: 1.94
          Mean value_function loss: 280.2522
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.5665
                       Mean reward: 456.10
               Mean episode length: 185.35
    Episode_Reward/reaching_object: 0.6566
     Episode_Reward/lifting_object: 92.0756
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.08s
                      Time elapsed: 00:42:43
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 46770 steps/s (collection: 1.997s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 249.3851
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.5723
                       Mean reward: 510.35
               Mean episode length: 203.34
    Episode_Reward/reaching_object: 0.6932
     Episode_Reward/lifting_object: 97.4047
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.10s
                      Time elapsed: 00:42:45
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 41048 steps/s (collection: 2.264s, learning 0.130s)
             Mean action noise std: 1.94
          Mean value_function loss: 264.8261
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.5765
                       Mean reward: 475.90
               Mean episode length: 196.14
    Episode_Reward/reaching_object: 0.6703
     Episode_Reward/lifting_object: 93.8158
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.39s
                      Time elapsed: 00:42:48
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 48216 steps/s (collection: 1.940s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 218.6212
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.5786
                       Mean reward: 485.79
               Mean episode length: 198.55
    Episode_Reward/reaching_object: 0.6834
     Episode_Reward/lifting_object: 96.0306
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.04s
                      Time elapsed: 00:42:50
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 47507 steps/s (collection: 1.976s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 238.0967
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.5819
                       Mean reward: 507.65
               Mean episode length: 205.41
    Episode_Reward/reaching_object: 0.6713
     Episode_Reward/lifting_object: 94.4347
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.07s
                      Time elapsed: 00:42:52
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 47582 steps/s (collection: 1.956s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 253.4456
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5883
                       Mean reward: 517.89
               Mean episode length: 209.37
    Episode_Reward/reaching_object: 0.6907
     Episode_Reward/lifting_object: 97.1854
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.07s
                      Time elapsed: 00:42:54
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 44868 steps/s (collection: 2.096s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 258.6131
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.5940
                       Mean reward: 477.70
               Mean episode length: 192.42
    Episode_Reward/reaching_object: 0.6566
     Episode_Reward/lifting_object: 91.7888
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.19s
                      Time elapsed: 00:42:56
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 47087 steps/s (collection: 1.960s, learning 0.128s)
             Mean action noise std: 1.94
          Mean value_function loss: 253.3996
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5953
                       Mean reward: 467.62
               Mean episode length: 187.60
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 94.7538
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.09s
                      Time elapsed: 00:42:58
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 49062 steps/s (collection: 1.913s, learning 0.091s)
             Mean action noise std: 1.94
          Mean value_function loss: 275.9897
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.5985
                       Mean reward: 451.31
               Mean episode length: 187.16
    Episode_Reward/reaching_object: 0.6752
     Episode_Reward/lifting_object: 95.0262
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.00s
                      Time elapsed: 00:43:00
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 48451 steps/s (collection: 1.941s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 253.1271
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.6006
                       Mean reward: 491.08
               Mean episode length: 199.19
    Episode_Reward/reaching_object: 0.6746
     Episode_Reward/lifting_object: 94.7502
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.03s
                      Time elapsed: 00:43:02
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 48149 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 240.2401
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.6013
                       Mean reward: 483.58
               Mean episode length: 198.58
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 98.1796
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.04s
                      Time elapsed: 00:43:04
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 48899 steps/s (collection: 1.912s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 265.4418
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.6041
                       Mean reward: 477.24
               Mean episode length: 195.05
    Episode_Reward/reaching_object: 0.6774
     Episode_Reward/lifting_object: 96.0645
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.01s
                      Time elapsed: 00:43:06
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 48599 steps/s (collection: 1.929s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 249.2563
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.6062
                       Mean reward: 476.07
               Mean episode length: 194.75
    Episode_Reward/reaching_object: 0.6605
     Episode_Reward/lifting_object: 93.1257
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.02s
                      Time elapsed: 00:43:08
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 48813 steps/s (collection: 1.913s, learning 0.101s)
             Mean action noise std: 1.94
          Mean value_function loss: 271.9245
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.6074
                       Mean reward: 442.31
               Mean episode length: 181.38
    Episode_Reward/reaching_object: 0.6515
     Episode_Reward/lifting_object: 91.9386
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.01s
                      Time elapsed: 00:43:10
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 48401 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 285.4389
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.6088
                       Mean reward: 411.06
               Mean episode length: 170.90
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 90.0184
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.03s
                      Time elapsed: 00:43:12
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 48812 steps/s (collection: 1.915s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 258.8713
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.6124
                       Mean reward: 461.01
               Mean episode length: 190.18
    Episode_Reward/reaching_object: 0.6517
     Episode_Reward/lifting_object: 91.6870
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.01s
                      Time elapsed: 00:43:14
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 48415 steps/s (collection: 1.921s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 283.2461
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.6140
                       Mean reward: 482.01
               Mean episode length: 197.64
    Episode_Reward/reaching_object: 0.6752
     Episode_Reward/lifting_object: 95.4078
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.03s
                      Time elapsed: 00:43:16
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 48715 steps/s (collection: 1.921s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 236.9547
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.6135
                       Mean reward: 479.89
               Mean episode length: 200.45
    Episode_Reward/reaching_object: 0.6844
     Episode_Reward/lifting_object: 95.9323
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.02s
                      Time elapsed: 00:43:18
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 48979 steps/s (collection: 1.910s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 253.2939
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.6136
                       Mean reward: 497.59
               Mean episode length: 197.59
    Episode_Reward/reaching_object: 0.6795
     Episode_Reward/lifting_object: 96.1646
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.01s
                      Time elapsed: 00:43:20
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 48712 steps/s (collection: 1.924s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 271.7139
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.6137
                       Mean reward: 463.45
               Mean episode length: 188.20
    Episode_Reward/reaching_object: 0.6712
     Episode_Reward/lifting_object: 94.3195
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.02s
                      Time elapsed: 00:43:22
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 48256 steps/s (collection: 1.942s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 281.5921
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.6183
                       Mean reward: 467.37
               Mean episode length: 188.77
    Episode_Reward/reaching_object: 0.6534
     Episode_Reward/lifting_object: 91.9360
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.04s
                      Time elapsed: 00:43:24
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 47066 steps/s (collection: 1.996s, learning 0.093s)
             Mean action noise std: 1.95
          Mean value_function loss: 272.6138
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.6244
                       Mean reward: 477.37
               Mean episode length: 194.86
    Episode_Reward/reaching_object: 0.6734
     Episode_Reward/lifting_object: 94.8202
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.09s
                      Time elapsed: 00:43:26
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 48381 steps/s (collection: 1.936s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 269.0689
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6265
                       Mean reward: 464.30
               Mean episode length: 189.51
    Episode_Reward/reaching_object: 0.6642
     Episode_Reward/lifting_object: 93.3481
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.03s
                      Time elapsed: 00:43:28
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 48753 steps/s (collection: 1.919s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 233.7811
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.6315
                       Mean reward: 504.60
               Mean episode length: 205.17
    Episode_Reward/reaching_object: 0.7088
     Episode_Reward/lifting_object: 99.8173
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.02s
                      Time elapsed: 00:43:30
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 49179 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 225.5345
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.6373
                       Mean reward: 515.37
               Mean episode length: 208.76
    Episode_Reward/reaching_object: 0.6968
     Episode_Reward/lifting_object: 97.9351
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.00s
                      Time elapsed: 00:43:32
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 47882 steps/s (collection: 1.961s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 275.2456
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.6458
                       Mean reward: 479.08
               Mean episode length: 196.64
    Episode_Reward/reaching_object: 0.6777
     Episode_Reward/lifting_object: 95.3425
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.05s
                      Time elapsed: 00:43:35
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 46797 steps/s (collection: 2.006s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 214.9871
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.6494
                       Mean reward: 513.43
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 0.6961
     Episode_Reward/lifting_object: 97.8497
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.10s
                      Time elapsed: 00:43:37
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 46799 steps/s (collection: 1.999s, learning 0.102s)
             Mean action noise std: 1.95
          Mean value_function loss: 263.8135
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.6529
                       Mean reward: 474.99
               Mean episode length: 189.49
    Episode_Reward/reaching_object: 0.6843
     Episode_Reward/lifting_object: 96.5569
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.10s
                      Time elapsed: 00:43:39
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 44918 steps/s (collection: 2.037s, learning 0.151s)
             Mean action noise std: 1.95
          Mean value_function loss: 256.5468
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6545
                       Mean reward: 483.18
               Mean episode length: 197.38
    Episode_Reward/reaching_object: 0.6677
     Episode_Reward/lifting_object: 93.7135
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.19s
                      Time elapsed: 00:43:41
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 42537 steps/s (collection: 2.198s, learning 0.113s)
             Mean action noise std: 1.95
          Mean value_function loss: 262.9692
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.6549
                       Mean reward: 480.15
               Mean episode length: 194.47
    Episode_Reward/reaching_object: 0.6908
     Episode_Reward/lifting_object: 97.5872
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.31s
                      Time elapsed: 00:43:43
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 45510 steps/s (collection: 2.046s, learning 0.114s)
             Mean action noise std: 1.95
          Mean value_function loss: 255.1209
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.6554
                       Mean reward: 522.37
               Mean episode length: 210.21
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 98.7202
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.16s
                      Time elapsed: 00:43:45
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 43990 steps/s (collection: 2.141s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 239.8680
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.6585
                       Mean reward: 529.57
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 0.6852
     Episode_Reward/lifting_object: 96.5685
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.23s
                      Time elapsed: 00:43:48
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 44011 steps/s (collection: 2.118s, learning 0.116s)
             Mean action noise std: 1.95
          Mean value_function loss: 256.6204
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.6614
                       Mean reward: 495.79
               Mean episode length: 199.71
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 98.6410
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.23s
                      Time elapsed: 00:43:50
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 44702 steps/s (collection: 2.080s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 258.3742
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.6650
                       Mean reward: 490.94
               Mean episode length: 196.59
    Episode_Reward/reaching_object: 0.6892
     Episode_Reward/lifting_object: 97.2598
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.20s
                      Time elapsed: 00:43:52
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 43961 steps/s (collection: 2.091s, learning 0.145s)
             Mean action noise std: 1.95
          Mean value_function loss: 228.3636
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6703
                       Mean reward: 489.50
               Mean episode length: 196.25
    Episode_Reward/reaching_object: 0.6732
     Episode_Reward/lifting_object: 94.8923
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.24s
                      Time elapsed: 00:43:54
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 46099 steps/s (collection: 2.023s, learning 0.109s)
             Mean action noise std: 1.95
          Mean value_function loss: 232.4972
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.6727
                       Mean reward: 486.77
               Mean episode length: 196.49
    Episode_Reward/reaching_object: 0.6922
     Episode_Reward/lifting_object: 98.0847
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.13s
                      Time elapsed: 00:43:56
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 44129 steps/s (collection: 2.116s, learning 0.112s)
             Mean action noise std: 1.95
          Mean value_function loss: 217.1983
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.6740
                       Mean reward: 501.09
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 0.7013
     Episode_Reward/lifting_object: 98.6363
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.23s
                      Time elapsed: 00:43:59
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 45604 steps/s (collection: 2.053s, learning 0.103s)
             Mean action noise std: 1.96
          Mean value_function loss: 248.1273
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.6773
                       Mean reward: 472.67
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.6882
     Episode_Reward/lifting_object: 96.8289
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.16s
                      Time elapsed: 00:44:01
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 45016 steps/s (collection: 2.087s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 229.7733
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.6783
                       Mean reward: 483.14
               Mean episode length: 195.65
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 95.1863
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.18s
                      Time elapsed: 00:44:03
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 46099 steps/s (collection: 2.023s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 218.2442
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.6783
                       Mean reward: 520.03
               Mean episode length: 211.03
    Episode_Reward/reaching_object: 0.7096
     Episode_Reward/lifting_object: 100.4480
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.13s
                      Time elapsed: 00:44:05
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 46867 steps/s (collection: 1.992s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 205.9744
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.6810
                       Mean reward: 510.54
               Mean episode length: 206.74
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 101.1876
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.10s
                      Time elapsed: 00:44:07
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 45823 steps/s (collection: 2.015s, learning 0.130s)
             Mean action noise std: 1.96
          Mean value_function loss: 248.9171
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.6861
                       Mean reward: 504.98
               Mean episode length: 203.67
    Episode_Reward/reaching_object: 0.6915
     Episode_Reward/lifting_object: 97.9656
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.15s
                      Time elapsed: 00:44:09
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 47054 steps/s (collection: 1.990s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 231.9582
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.6905
                       Mean reward: 516.22
               Mean episode length: 208.62
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: 101.4309
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.09s
                      Time elapsed: 00:44:11
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 46487 steps/s (collection: 2.020s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 253.8663
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.6939
                       Mean reward: 508.81
               Mean episode length: 208.55
    Episode_Reward/reaching_object: 0.7156
     Episode_Reward/lifting_object: 101.0044
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.11s
                      Time elapsed: 00:44:14
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 47714 steps/s (collection: 1.948s, learning 0.113s)
             Mean action noise std: 1.96
          Mean value_function loss: 252.4916
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.6952
                       Mean reward: 467.96
               Mean episode length: 191.64
    Episode_Reward/reaching_object: 0.6761
     Episode_Reward/lifting_object: 94.4187
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.06s
                      Time elapsed: 00:44:16
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 48181 steps/s (collection: 1.940s, learning 0.101s)
             Mean action noise std: 1.96
          Mean value_function loss: 223.6079
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.6983
                       Mean reward: 517.15
               Mean episode length: 209.53
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 99.6349
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.04s
                      Time elapsed: 00:44:18
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 48157 steps/s (collection: 1.940s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 245.4224
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.7011
                       Mean reward: 501.39
               Mean episode length: 202.65
    Episode_Reward/reaching_object: 0.7027
     Episode_Reward/lifting_object: 99.9118
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.04s
                      Time elapsed: 00:44:20
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 49342 steps/s (collection: 1.899s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 234.8834
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.7030
                       Mean reward: 474.27
               Mean episode length: 193.03
    Episode_Reward/reaching_object: 0.6975
     Episode_Reward/lifting_object: 98.0146
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.99s
                      Time elapsed: 00:44:22
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 47638 steps/s (collection: 1.956s, learning 0.108s)
             Mean action noise std: 1.96
          Mean value_function loss: 254.9112
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.7050
                       Mean reward: 488.62
               Mean episode length: 199.23
    Episode_Reward/reaching_object: 0.7056
     Episode_Reward/lifting_object: 99.9392
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.06s
                      Time elapsed: 00:44:24
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 47580 steps/s (collection: 1.966s, learning 0.100s)
             Mean action noise std: 1.96
          Mean value_function loss: 238.5882
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.7064
                       Mean reward: 477.49
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 0.7030
     Episode_Reward/lifting_object: 99.8068
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.07s
                      Time elapsed: 00:44:26
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 47278 steps/s (collection: 1.981s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 247.0529
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.7081
                       Mean reward: 501.04
               Mean episode length: 201.68
    Episode_Reward/reaching_object: 0.6918
     Episode_Reward/lifting_object: 98.5452
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.08s
                      Time elapsed: 00:44:28
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 49145 steps/s (collection: 1.910s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 229.2733
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.7090
                       Mean reward: 503.26
               Mean episode length: 200.14
    Episode_Reward/reaching_object: 0.7034
     Episode_Reward/lifting_object: 100.0270
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.00s
                      Time elapsed: 00:44:30
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 49050 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 256.3889
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.7112
                       Mean reward: 465.83
               Mean episode length: 189.64
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 100.4844
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.00s
                      Time elapsed: 00:44:32
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 48442 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 248.2586
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.7123
                       Mean reward: 496.08
               Mean episode length: 197.16
    Episode_Reward/reaching_object: 0.7010
     Episode_Reward/lifting_object: 99.6969
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.03s
                      Time elapsed: 00:44:34
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 49204 steps/s (collection: 1.902s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 246.9014
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7181
                       Mean reward: 471.43
               Mean episode length: 194.40
    Episode_Reward/reaching_object: 0.6822
     Episode_Reward/lifting_object: 96.3608
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.00s
                      Time elapsed: 00:44:36
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 49115 steps/s (collection: 1.910s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 258.5194
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.7227
                       Mean reward: 512.50
               Mean episode length: 203.46
    Episode_Reward/reaching_object: 0.6983
     Episode_Reward/lifting_object: 100.4950
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.00s
                      Time elapsed: 00:44:38
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 48646 steps/s (collection: 1.933s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 288.5766
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.7226
                       Mean reward: 477.62
               Mean episode length: 193.72
    Episode_Reward/reaching_object: 0.6821
     Episode_Reward/lifting_object: 96.7544
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.02s
                      Time elapsed: 00:44:40
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 48177 steps/s (collection: 1.950s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 232.1606
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.7265
                       Mean reward: 510.11
               Mean episode length: 201.14
    Episode_Reward/reaching_object: 0.7006
     Episode_Reward/lifting_object: 99.5016
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.04s
                      Time elapsed: 00:44:42
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 48202 steps/s (collection: 1.942s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 266.7527
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.7291
                       Mean reward: 480.09
               Mean episode length: 191.04
    Episode_Reward/reaching_object: 0.6890
     Episode_Reward/lifting_object: 99.0699
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.04s
                      Time elapsed: 00:44:44
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 48606 steps/s (collection: 1.926s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 222.1575
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7295
                       Mean reward: 532.07
               Mean episode length: 209.75
    Episode_Reward/reaching_object: 0.6958
     Episode_Reward/lifting_object: 99.2976
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.02s
                      Time elapsed: 00:44:46
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 48456 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 1.97
          Mean value_function loss: 252.2247
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.7300
                       Mean reward: 482.33
               Mean episode length: 193.63
    Episode_Reward/reaching_object: 0.6944
     Episode_Reward/lifting_object: 99.2600
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.03s
                      Time elapsed: 00:44:48
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 47756 steps/s (collection: 1.949s, learning 0.110s)
             Mean action noise std: 1.97
          Mean value_function loss: 218.0836
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.7286
                       Mean reward: 507.66
               Mean episode length: 203.53
    Episode_Reward/reaching_object: 0.7032
     Episode_Reward/lifting_object: 100.2737
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.06s
                      Time elapsed: 00:44:50
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 46746 steps/s (collection: 1.992s, learning 0.111s)
             Mean action noise std: 1.97
          Mean value_function loss: 207.1639
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.7276
                       Mean reward: 492.16
               Mean episode length: 197.72
    Episode_Reward/reaching_object: 0.7062
     Episode_Reward/lifting_object: 101.1276
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.10s
                      Time elapsed: 00:44:52
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 47022 steps/s (collection: 1.957s, learning 0.133s)
             Mean action noise std: 1.97
          Mean value_function loss: 238.7625
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.7268
                       Mean reward: 476.55
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 99.1450
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.09s
                      Time elapsed: 00:44:54
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 45987 steps/s (collection: 2.008s, learning 0.130s)
             Mean action noise std: 1.97
          Mean value_function loss: 281.3740
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.7310
                       Mean reward: 510.32
               Mean episode length: 202.37
    Episode_Reward/reaching_object: 0.7062
     Episode_Reward/lifting_object: 100.6294
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.14s
                      Time elapsed: 00:44:56
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 43420 steps/s (collection: 2.151s, learning 0.113s)
             Mean action noise std: 1.97
          Mean value_function loss: 271.0455
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7368
                       Mean reward: 528.10
               Mean episode length: 208.09
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 99.5013
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.26s
                      Time elapsed: 00:44:59
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 44548 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 246.7253
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.7428
                       Mean reward: 486.51
               Mean episode length: 194.31
    Episode_Reward/reaching_object: 0.7083
     Episode_Reward/lifting_object: 101.3533
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.21s
                      Time elapsed: 00:45:01
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 47483 steps/s (collection: 1.980s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 244.4884
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.7431
                       Mean reward: 488.59
               Mean episode length: 195.61
    Episode_Reward/reaching_object: 0.7052
     Episode_Reward/lifting_object: 100.2626
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.07s
                      Time elapsed: 00:45:03
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 46744 steps/s (collection: 2.013s, learning 0.090s)
             Mean action noise std: 1.97
          Mean value_function loss: 250.0673
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.7444
                       Mean reward: 524.76
               Mean episode length: 206.90
    Episode_Reward/reaching_object: 0.7091
     Episode_Reward/lifting_object: 101.7175
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.10s
                      Time elapsed: 00:45:05
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 48194 steps/s (collection: 1.948s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 243.5829
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.7451
                       Mean reward: 463.49
               Mean episode length: 189.26
    Episode_Reward/reaching_object: 0.7045
     Episode_Reward/lifting_object: 99.8211
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.04s
                      Time elapsed: 00:45:07
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 48176 steps/s (collection: 1.948s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 221.6041
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.7456
                       Mean reward: 521.42
               Mean episode length: 205.11
    Episode_Reward/reaching_object: 0.7062
     Episode_Reward/lifting_object: 101.0092
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.04s
                      Time elapsed: 00:45:09
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 48569 steps/s (collection: 1.933s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 261.3913
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.7467
                       Mean reward: 489.67
               Mean episode length: 196.76
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 96.9951
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.02s
                      Time elapsed: 00:45:11
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 48817 steps/s (collection: 1.924s, learning 0.090s)
             Mean action noise std: 1.97
          Mean value_function loss: 214.2359
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.7487
                       Mean reward: 519.21
               Mean episode length: 204.49
    Episode_Reward/reaching_object: 0.7168
     Episode_Reward/lifting_object: 102.1955
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.01s
                      Time elapsed: 00:45:13
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 47863 steps/s (collection: 1.945s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 260.7067
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.7518
                       Mean reward: 500.03
               Mean episode length: 200.39
    Episode_Reward/reaching_object: 0.6913
     Episode_Reward/lifting_object: 98.0839
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.05s
                      Time elapsed: 00:45:15
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 47487 steps/s (collection: 1.954s, learning 0.116s)
             Mean action noise std: 1.97
          Mean value_function loss: 274.2140
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.7577
                       Mean reward: 512.43
               Mean episode length: 203.36
    Episode_Reward/reaching_object: 0.7045
     Episode_Reward/lifting_object: 100.4562
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.07s
                      Time elapsed: 00:45:17
                               ETA: 00:34:37

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 47840 steps/s (collection: 1.944s, learning 0.111s)
             Mean action noise std: 1.97
          Mean value_function loss: 231.9955
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.7617
                       Mean reward: 514.74
               Mean episode length: 207.44
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 98.3305
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.05s
                      Time elapsed: 00:45:19
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 48579 steps/s (collection: 1.933s, learning 0.090s)
             Mean action noise std: 1.97
          Mean value_function loss: 252.6135
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.7633
                       Mean reward: 512.09
               Mean episode length: 204.41
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 100.1821
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.02s
                      Time elapsed: 00:45:21
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 49233 steps/s (collection: 1.903s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 246.9333
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7666
                       Mean reward: 484.48
               Mean episode length: 197.07
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 100.0937
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.00s
                      Time elapsed: 00:45:23
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 47457 steps/s (collection: 1.966s, learning 0.106s)
             Mean action noise std: 1.98
          Mean value_function loss: 227.0129
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.7717
                       Mean reward: 505.11
               Mean episode length: 200.52
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 99.9348
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.07s
                      Time elapsed: 00:45:26
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 48245 steps/s (collection: 1.946s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 228.3811
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.7765
                       Mean reward: 489.71
               Mean episode length: 194.68
    Episode_Reward/reaching_object: 0.7169
     Episode_Reward/lifting_object: 102.6957
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.04s
                      Time elapsed: 00:45:28
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 48069 steps/s (collection: 1.954s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 216.8497
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.7768
                       Mean reward: 521.72
               Mean episode length: 208.92
    Episode_Reward/reaching_object: 0.7162
     Episode_Reward/lifting_object: 102.0762
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.05s
                      Time elapsed: 00:45:30
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 48635 steps/s (collection: 1.932s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 227.5445
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.7750
                       Mean reward: 546.95
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 0.7135
     Episode_Reward/lifting_object: 101.5614
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.02s
                      Time elapsed: 00:45:32
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 47732 steps/s (collection: 1.970s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 258.0315
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.7731
                       Mean reward: 477.64
               Mean episode length: 194.26
    Episode_Reward/reaching_object: 0.6798
     Episode_Reward/lifting_object: 95.8492
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.06s
                      Time elapsed: 00:45:34
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 47679 steps/s (collection: 1.972s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 223.7690
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7753
                       Mean reward: 495.70
               Mean episode length: 198.28
    Episode_Reward/reaching_object: 0.7187
     Episode_Reward/lifting_object: 102.3816
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.06s
                      Time elapsed: 00:45:36
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 48449 steps/s (collection: 1.925s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 258.9359
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.7779
                       Mean reward: 505.33
               Mean episode length: 204.62
    Episode_Reward/reaching_object: 0.7148
     Episode_Reward/lifting_object: 101.7907
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.03s
                      Time elapsed: 00:45:38
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 46799 steps/s (collection: 2.001s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 215.4718
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.7841
                       Mean reward: 519.92
               Mean episode length: 207.12
    Episode_Reward/reaching_object: 0.7150
     Episode_Reward/lifting_object: 102.1459
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.10s
                      Time elapsed: 00:45:40
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 47947 steps/s (collection: 1.958s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 216.5503
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.7884
                       Mean reward: 505.11
               Mean episode length: 200.67
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 102.2253
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.05s
                      Time elapsed: 00:45:42
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 47374 steps/s (collection: 1.979s, learning 0.097s)
             Mean action noise std: 1.98
          Mean value_function loss: 231.5401
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.7894
                       Mean reward: 509.60
               Mean episode length: 200.95
    Episode_Reward/reaching_object: 0.6961
     Episode_Reward/lifting_object: 99.2582
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.08s
                      Time elapsed: 00:45:44
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 46807 steps/s (collection: 1.988s, learning 0.113s)
             Mean action noise std: 1.98
          Mean value_function loss: 218.4737
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.7908
                       Mean reward: 524.66
               Mean episode length: 205.96
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: 100.7624
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.10s
                      Time elapsed: 00:45:46
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 46701 steps/s (collection: 1.990s, learning 0.115s)
             Mean action noise std: 1.98
          Mean value_function loss: 247.9479
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7960
                       Mean reward: 477.71
               Mean episode length: 191.74
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 100.1342
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.10s
                      Time elapsed: 00:45:48
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 47041 steps/s (collection: 1.999s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 257.3089
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.8036
                       Mean reward: 502.43
               Mean episode length: 198.33
    Episode_Reward/reaching_object: 0.6975
     Episode_Reward/lifting_object: 99.6668
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.09s
                      Time elapsed: 00:45:50
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 47973 steps/s (collection: 1.958s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 228.9765
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.8081
                       Mean reward: 516.36
               Mean episode length: 204.97
    Episode_Reward/reaching_object: 0.7055
     Episode_Reward/lifting_object: 100.6228
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.05s
                      Time elapsed: 00:45:52
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 46724 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 264.4276
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.8102
                       Mean reward: 507.96
               Mean episode length: 203.86
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 102.0430
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.10s
                      Time elapsed: 00:45:54
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 47247 steps/s (collection: 1.984s, learning 0.097s)
             Mean action noise std: 1.98
          Mean value_function loss: 213.7447
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.8115
                       Mean reward: 482.30
               Mean episode length: 193.08
    Episode_Reward/reaching_object: 0.7121
     Episode_Reward/lifting_object: 102.5450
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.08s
                      Time elapsed: 00:45:57
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 47972 steps/s (collection: 1.956s, learning 0.093s)
             Mean action noise std: 1.98
          Mean value_function loss: 222.6270
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.8134
                       Mean reward: 481.15
               Mean episode length: 192.25
    Episode_Reward/reaching_object: 0.7134
     Episode_Reward/lifting_object: 102.8067
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.05s
                      Time elapsed: 00:45:59
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 48088 steps/s (collection: 1.948s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 190.9504
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.8158
                       Mean reward: 559.45
               Mean episode length: 218.58
    Episode_Reward/reaching_object: 0.7303
     Episode_Reward/lifting_object: 105.6283
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.04s
                      Time elapsed: 00:46:01
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 47864 steps/s (collection: 1.962s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 217.0324
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.8149
                       Mean reward: 552.01
               Mean episode length: 215.59
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 103.1416
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.05s
                      Time elapsed: 00:46:03
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 47433 steps/s (collection: 1.949s, learning 0.124s)
             Mean action noise std: 1.98
          Mean value_function loss: 223.3268
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.8168
                       Mean reward: 508.98
               Mean episode length: 198.86
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 102.1996
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.07s
                      Time elapsed: 00:46:05
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 47691 steps/s (collection: 1.953s, learning 0.108s)
             Mean action noise std: 1.98
          Mean value_function loss: 252.3556
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.8193
                       Mean reward: 490.48
               Mean episode length: 192.72
    Episode_Reward/reaching_object: 0.7182
     Episode_Reward/lifting_object: 103.5659
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.06s
                      Time elapsed: 00:46:07
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 48452 steps/s (collection: 1.937s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 247.3750
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.8212
                       Mean reward: 522.36
               Mean episode length: 202.06
    Episode_Reward/reaching_object: 0.7128
     Episode_Reward/lifting_object: 103.4067
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.03s
                      Time elapsed: 00:46:09
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 48327 steps/s (collection: 1.942s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 242.6405
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.8227
                       Mean reward: 539.85
               Mean episode length: 209.61
    Episode_Reward/reaching_object: 0.7217
     Episode_Reward/lifting_object: 104.2342
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.03s
                      Time elapsed: 00:46:11
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 48991 steps/s (collection: 1.920s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 288.0100
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.8254
                       Mean reward: 518.35
               Mean episode length: 203.56
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 101.2353
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.01s
                      Time elapsed: 00:46:13
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 47848 steps/s (collection: 1.952s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 329.9920
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.8336
                       Mean reward: 537.63
               Mean episode length: 207.69
    Episode_Reward/reaching_object: 0.7057
     Episode_Reward/lifting_object: 101.6274
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.05s
                      Time elapsed: 00:46:15
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 48291 steps/s (collection: 1.917s, learning 0.119s)
             Mean action noise std: 1.99
          Mean value_function loss: 229.1975
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.8387
                       Mean reward: 512.14
               Mean episode length: 200.64
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: 105.9540
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.04s
                      Time elapsed: 00:46:17
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 47617 steps/s (collection: 1.962s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 256.2042
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.8394
                       Mean reward: 520.40
               Mean episode length: 202.52
    Episode_Reward/reaching_object: 0.7095
     Episode_Reward/lifting_object: 102.7553
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.06s
                      Time elapsed: 00:46:19
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 47582 steps/s (collection: 1.971s, learning 0.095s)
             Mean action noise std: 1.99
          Mean value_function loss: 271.9042
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.8405
                       Mean reward: 532.18
               Mean episode length: 208.71
    Episode_Reward/reaching_object: 0.7124
     Episode_Reward/lifting_object: 102.6716
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.07s
                      Time elapsed: 00:46:21
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 48105 steps/s (collection: 1.946s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 223.3846
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.8431
                       Mean reward: 554.22
               Mean episode length: 213.72
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: 106.8055
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.04s
                      Time elapsed: 00:46:23
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 47791 steps/s (collection: 1.950s, learning 0.107s)
             Mean action noise std: 1.99
          Mean value_function loss: 230.6544
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.8447
                       Mean reward: 502.01
               Mean episode length: 196.33
    Episode_Reward/reaching_object: 0.7103
     Episode_Reward/lifting_object: 102.9193
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.06s
                      Time elapsed: 00:46:25
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 47746 steps/s (collection: 1.971s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 248.6681
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.8461
                       Mean reward: 507.93
               Mean episode length: 198.85
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 102.4852
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.06s
                      Time elapsed: 00:46:27
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 48744 steps/s (collection: 1.922s, learning 0.095s)
             Mean action noise std: 1.99
          Mean value_function loss: 237.4602
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.8495
                       Mean reward: 534.20
               Mean episode length: 211.40
    Episode_Reward/reaching_object: 0.7311
     Episode_Reward/lifting_object: 105.8384
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.02s
                      Time elapsed: 00:46:29
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 48187 steps/s (collection: 1.954s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 223.1765
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.8509
                       Mean reward: 516.06
               Mean episode length: 200.20
    Episode_Reward/reaching_object: 0.7150
     Episode_Reward/lifting_object: 103.7345
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.04s
                      Time elapsed: 00:46:31
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 48460 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 233.4970
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.8523
                       Mean reward: 543.12
               Mean episode length: 211.49
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 104.8876
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.03s
                      Time elapsed: 00:46:33
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 48367 steps/s (collection: 1.945s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 255.5419
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.8561
                       Mean reward: 542.30
               Mean episode length: 212.59
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 107.7304
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.03s
                      Time elapsed: 00:46:35
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 48862 steps/s (collection: 1.921s, learning 0.091s)
             Mean action noise std: 1.99
          Mean value_function loss: 217.3203
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.8620
                       Mean reward: 532.07
               Mean episode length: 206.54
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 104.3003
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.01s
                      Time elapsed: 00:46:37
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 47654 steps/s (collection: 1.975s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 283.2842
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.8652
                       Mean reward: 486.83
               Mean episode length: 192.51
    Episode_Reward/reaching_object: 0.7096
     Episode_Reward/lifting_object: 103.1279
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.06s
                      Time elapsed: 00:46:39
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 48380 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 1.99
          Mean value_function loss: 258.7170
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.8645
                       Mean reward: 542.85
               Mean episode length: 209.01
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 106.5353
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.03s
                      Time elapsed: 00:46:41
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 47997 steps/s (collection: 1.947s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 243.3762
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.8633
                       Mean reward: 532.04
               Mean episode length: 209.04
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 102.0551
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.05s
                      Time elapsed: 00:46:44
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 48079 steps/s (collection: 1.943s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 276.7325
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.8712
                       Mean reward: 538.86
               Mean episode length: 209.03
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 104.0825
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.04s
                      Time elapsed: 00:46:46
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 47650 steps/s (collection: 1.953s, learning 0.111s)
             Mean action noise std: 1.99
          Mean value_function loss: 253.7668
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.8781
                       Mean reward: 516.14
               Mean episode length: 200.19
    Episode_Reward/reaching_object: 0.6755
     Episode_Reward/lifting_object: 97.9542
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.06s
                      Time elapsed: 00:46:48
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 48210 steps/s (collection: 1.928s, learning 0.111s)
             Mean action noise std: 1.99
          Mean value_function loss: 267.8608
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.8789
                       Mean reward: 487.74
               Mean episode length: 193.01
    Episode_Reward/reaching_object: 0.6996
     Episode_Reward/lifting_object: 101.4995
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.04s
                      Time elapsed: 00:46:50
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 47614 steps/s (collection: 1.963s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 276.4385
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.8791
                       Mean reward: 535.35
               Mean episode length: 207.13
    Episode_Reward/reaching_object: 0.7061
     Episode_Reward/lifting_object: 102.9961
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.06s
                      Time elapsed: 00:46:52
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 47550 steps/s (collection: 1.971s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 263.4696
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.8832
                       Mean reward: 505.10
               Mean episode length: 196.93
    Episode_Reward/reaching_object: 0.6852
     Episode_Reward/lifting_object: 100.2470
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.07s
                      Time elapsed: 00:46:54
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 48171 steps/s (collection: 1.939s, learning 0.102s)
             Mean action noise std: 2.00
          Mean value_function loss: 265.5503
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.8871
                       Mean reward: 544.94
               Mean episode length: 208.80
    Episode_Reward/reaching_object: 0.6886
     Episode_Reward/lifting_object: 100.8498
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.04s
                      Time elapsed: 00:46:56
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 45945 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 310.7089
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.8892
                       Mean reward: 507.73
               Mean episode length: 198.04
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 100.9762
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.14s
                      Time elapsed: 00:46:58
                               ETA: 00:32:28

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 46057 steps/s (collection: 2.043s, learning 0.092s)
             Mean action noise std: 2.00
          Mean value_function loss: 269.9914
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.8941
                       Mean reward: 516.66
               Mean episode length: 198.83
    Episode_Reward/reaching_object: 0.6892
     Episode_Reward/lifting_object: 100.3812
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.13s
                      Time elapsed: 00:47:00
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 47318 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 282.2135
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.8975
                       Mean reward: 482.13
               Mean episode length: 191.98
    Episode_Reward/reaching_object: 0.6927
     Episode_Reward/lifting_object: 100.0385
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.08s
                      Time elapsed: 00:47:02
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 48089 steps/s (collection: 1.949s, learning 0.095s)
             Mean action noise std: 2.00
          Mean value_function loss: 262.9799
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.8978
                       Mean reward: 475.31
               Mean episode length: 186.05
    Episode_Reward/reaching_object: 0.6891
     Episode_Reward/lifting_object: 100.6317
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.04s
                      Time elapsed: 00:47:04
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 47754 steps/s (collection: 1.966s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 267.4899
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.9007
                       Mean reward: 511.11
               Mean episode length: 195.14
    Episode_Reward/reaching_object: 0.6842
     Episode_Reward/lifting_object: 100.3974
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.06s
                      Time elapsed: 00:47:06
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 47792 steps/s (collection: 1.963s, learning 0.094s)
             Mean action noise std: 2.00
          Mean value_function loss: 282.4163
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.9060
                       Mean reward: 519.48
               Mean episode length: 197.71
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 104.5687
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.06s
                      Time elapsed: 00:47:08
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 48007 steps/s (collection: 1.961s, learning 0.087s)
             Mean action noise std: 2.00
          Mean value_function loss: 287.8750
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.9118
                       Mean reward: 517.24
               Mean episode length: 199.55
    Episode_Reward/reaching_object: 0.6960
     Episode_Reward/lifting_object: 101.8763
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.05s
                      Time elapsed: 00:47:10
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 47690 steps/s (collection: 1.969s, learning 0.092s)
             Mean action noise std: 2.00
          Mean value_function loss: 276.2904
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.9168
                       Mean reward: 523.06
               Mean episode length: 198.98
    Episode_Reward/reaching_object: 0.6948
     Episode_Reward/lifting_object: 101.6531
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.06s
                      Time elapsed: 00:47:12
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 46609 steps/s (collection: 2.006s, learning 0.103s)
             Mean action noise std: 2.00
          Mean value_function loss: 269.0907
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.9217
                       Mean reward: 484.83
               Mean episode length: 189.97
    Episode_Reward/reaching_object: 0.6929
     Episode_Reward/lifting_object: 101.3238
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.11s
                      Time elapsed: 00:47:15
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 46508 steps/s (collection: 2.004s, learning 0.110s)
             Mean action noise std: 2.00
          Mean value_function loss: 235.3032
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.9242
                       Mean reward: 528.14
               Mean episode length: 200.88
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 106.2687
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.11s
                      Time elapsed: 00:47:17
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 43274 steps/s (collection: 2.139s, learning 0.133s)
             Mean action noise std: 2.00
          Mean value_function loss: 262.9907
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.9247
                       Mean reward: 541.44
               Mean episode length: 204.75
    Episode_Reward/reaching_object: 0.7157
     Episode_Reward/lifting_object: 105.7161
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.27s
                      Time elapsed: 00:47:19
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 42951 steps/s (collection: 2.184s, learning 0.105s)
             Mean action noise std: 2.00
          Mean value_function loss: 289.5494
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.9288
                       Mean reward: 536.90
               Mean episode length: 207.00
    Episode_Reward/reaching_object: 0.6924
     Episode_Reward/lifting_object: 101.7895
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.29s
                      Time elapsed: 00:47:21
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 36665 steps/s (collection: 2.385s, learning 0.296s)
             Mean action noise std: 2.00
          Mean value_function loss: 311.6496
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.9360
                       Mean reward: 518.74
               Mean episode length: 198.68
    Episode_Reward/reaching_object: 0.6853
     Episode_Reward/lifting_object: 101.0629
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.68s
                      Time elapsed: 00:47:24
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 42025 steps/s (collection: 2.241s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 255.3068
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.9414
                       Mean reward: 511.66
               Mean episode length: 198.82
    Episode_Reward/reaching_object: 0.7000
     Episode_Reward/lifting_object: 102.3126
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.34s
                      Time elapsed: 00:47:26
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 45718 steps/s (collection: 2.051s, learning 0.099s)
             Mean action noise std: 2.01
          Mean value_function loss: 298.3538
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.9449
                       Mean reward: 550.93
               Mean episode length: 209.04
    Episode_Reward/reaching_object: 0.7095
     Episode_Reward/lifting_object: 104.7802
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.15s
                      Time elapsed: 00:47:28
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 46794 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 268.7913
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.9490
                       Mean reward: 529.46
               Mean episode length: 203.55
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: 105.9439
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.10s
                      Time elapsed: 00:47:31
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 47469 steps/s (collection: 1.977s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 307.8532
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.9516
                       Mean reward: 511.61
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 0.6847
     Episode_Reward/lifting_object: 99.9079
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.07s
                      Time elapsed: 00:47:33
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 47407 steps/s (collection: 1.986s, learning 0.088s)
             Mean action noise std: 2.01
          Mean value_function loss: 265.6159
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.9523
                       Mean reward: 525.70
               Mean episode length: 202.81
    Episode_Reward/reaching_object: 0.7320
     Episode_Reward/lifting_object: 108.1977
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.07s
                      Time elapsed: 00:47:35
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 48703 steps/s (collection: 1.928s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 282.7285
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.9545
                       Mean reward: 516.07
               Mean episode length: 197.68
    Episode_Reward/reaching_object: 0.7020
     Episode_Reward/lifting_object: 103.4971
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.02s
                      Time elapsed: 00:47:37
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 46614 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 267.5006
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.9583
                       Mean reward: 542.54
               Mean episode length: 205.19
    Episode_Reward/reaching_object: 0.6778
     Episode_Reward/lifting_object: 99.2120
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.11s
                      Time elapsed: 00:47:39
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 46454 steps/s (collection: 2.002s, learning 0.114s)
             Mean action noise std: 2.01
          Mean value_function loss: 278.2969
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.9603
                       Mean reward: 530.10
               Mean episode length: 202.95
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 101.0031
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.12s
                      Time elapsed: 00:47:41
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 46933 steps/s (collection: 1.997s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 283.2809
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.9621
                       Mean reward: 537.87
               Mean episode length: 205.62
    Episode_Reward/reaching_object: 0.6967
     Episode_Reward/lifting_object: 101.7156
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.09s
                      Time elapsed: 00:47:43
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 46169 steps/s (collection: 2.008s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 245.6859
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.9627
                       Mean reward: 500.04
               Mean episode length: 192.74
    Episode_Reward/reaching_object: 0.6931
     Episode_Reward/lifting_object: 101.2478
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.13s
                      Time elapsed: 00:47:45
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 44872 steps/s (collection: 2.064s, learning 0.127s)
             Mean action noise std: 2.01
          Mean value_function loss: 259.6681
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.9635
                       Mean reward: 523.58
               Mean episode length: 198.54
    Episode_Reward/reaching_object: 0.6984
     Episode_Reward/lifting_object: 103.4777
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.19s
                      Time elapsed: 00:47:47
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 45025 steps/s (collection: 2.067s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 263.2379
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.9666
                       Mean reward: 530.17
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 0.7238
     Episode_Reward/lifting_object: 105.9733
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.18s
                      Time elapsed: 00:47:49
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 44002 steps/s (collection: 2.118s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 236.9978
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.9707
                       Mean reward: 520.73
               Mean episode length: 197.39
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 106.5491
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.23s
                      Time elapsed: 00:47:52
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 46021 steps/s (collection: 2.017s, learning 0.119s)
             Mean action noise std: 2.01
          Mean value_function loss: 263.5839
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.9753
                       Mean reward: 542.34
               Mean episode length: 205.68
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 105.9624
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.14s
                      Time elapsed: 00:47:54
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 47642 steps/s (collection: 1.957s, learning 0.106s)
             Mean action noise std: 2.01
          Mean value_function loss: 272.2888
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.9760
                       Mean reward: 553.92
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 0.7145
     Episode_Reward/lifting_object: 105.4350
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.06s
                      Time elapsed: 00:47:56
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 46525 steps/s (collection: 2.009s, learning 0.104s)
             Mean action noise std: 2.01
          Mean value_function loss: 291.0966
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.9791
                       Mean reward: 512.09
               Mean episode length: 197.41
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 105.6064
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.11s
                      Time elapsed: 00:47:58
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 46324 steps/s (collection: 1.993s, learning 0.129s)
             Mean action noise std: 2.01
          Mean value_function loss: 270.5248
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.9812
                       Mean reward: 529.33
               Mean episode length: 202.54
    Episode_Reward/reaching_object: 0.7084
     Episode_Reward/lifting_object: 103.8694
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.12s
                      Time elapsed: 00:48:00
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 47710 steps/s (collection: 1.963s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 264.2821
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.9867
                       Mean reward: 549.30
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 0.7225
     Episode_Reward/lifting_object: 105.7460
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.06s
                      Time elapsed: 00:48:02
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 47810 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 2.01
          Mean value_function loss: 259.5677
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.9940
                       Mean reward: 563.56
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 109.9814
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.06s
                      Time elapsed: 00:48:04
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 46785 steps/s (collection: 1.986s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 253.6691
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.9976
                       Mean reward: 536.44
               Mean episode length: 206.42
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 108.0249
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.10s
                      Time elapsed: 00:48:06
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 44814 steps/s (collection: 2.087s, learning 0.107s)
             Mean action noise std: 2.01
          Mean value_function loss: 253.5086
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.9986
                       Mean reward: 560.87
               Mean episode length: 212.46
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 106.8558
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.19s
                      Time elapsed: 00:48:09
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 45975 steps/s (collection: 2.044s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 280.3670
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.0013
                       Mean reward: 549.66
               Mean episode length: 210.32
    Episode_Reward/reaching_object: 0.7347
     Episode_Reward/lifting_object: 107.6095
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.14s
                      Time elapsed: 00:48:11
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 45722 steps/s (collection: 1.998s, learning 0.152s)
             Mean action noise std: 2.02
          Mean value_function loss: 260.9075
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.0068
                       Mean reward: 514.31
               Mean episode length: 198.29
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 103.2665
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.15s
                      Time elapsed: 00:48:13
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 38377 steps/s (collection: 2.380s, learning 0.181s)
             Mean action noise std: 2.02
          Mean value_function loss: 297.8555
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.0088
                       Mean reward: 539.50
               Mean episode length: 204.78
    Episode_Reward/reaching_object: 0.7329
     Episode_Reward/lifting_object: 108.6261
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.56s
                      Time elapsed: 00:48:15
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 37267 steps/s (collection: 2.478s, learning 0.160s)
             Mean action noise std: 2.02
          Mean value_function loss: 214.7948
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0099
                       Mean reward: 498.16
               Mean episode length: 192.54
    Episode_Reward/reaching_object: 0.7018
     Episode_Reward/lifting_object: 103.0120
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.64s
                      Time elapsed: 00:48:18
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 44298 steps/s (collection: 2.107s, learning 0.112s)
             Mean action noise std: 2.02
          Mean value_function loss: 246.3130
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.0116
                       Mean reward: 542.16
               Mean episode length: 205.65
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 111.0219
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.22s
                      Time elapsed: 00:48:20
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 46876 steps/s (collection: 1.979s, learning 0.119s)
             Mean action noise std: 2.02
          Mean value_function loss: 241.6892
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 37.0139
                       Mean reward: 567.35
               Mean episode length: 214.44
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 110.7754
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.10s
                      Time elapsed: 00:48:22
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 46769 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 2.02
          Mean value_function loss: 233.7819
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.0153
                       Mean reward: 541.86
               Mean episode length: 204.69
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 107.7697
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.10s
                      Time elapsed: 00:48:24
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 46959 steps/s (collection: 1.985s, learning 0.109s)
             Mean action noise std: 2.02
          Mean value_function loss: 254.5232
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.0150
                       Mean reward: 531.45
               Mean episode length: 201.56
    Episode_Reward/reaching_object: 0.7291
     Episode_Reward/lifting_object: 107.4172
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.09s
                      Time elapsed: 00:48:27
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 44901 steps/s (collection: 2.088s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 236.2533
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 37.0137
                       Mean reward: 551.58
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 0.7179
     Episode_Reward/lifting_object: 106.2679
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.19s
                      Time elapsed: 00:48:29
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 45762 steps/s (collection: 2.043s, learning 0.106s)
             Mean action noise std: 2.02
          Mean value_function loss: 250.6244
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.0145
                       Mean reward: 560.45
               Mean episode length: 210.23
    Episode_Reward/reaching_object: 0.7175
     Episode_Reward/lifting_object: 105.7897
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.15s
                      Time elapsed: 00:48:31
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 41233 steps/s (collection: 2.271s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 223.4089
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.0165
                       Mean reward: 553.86
               Mean episode length: 208.83
    Episode_Reward/reaching_object: 0.7376
     Episode_Reward/lifting_object: 108.1228
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.38s
                      Time elapsed: 00:48:33
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 42341 steps/s (collection: 2.215s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 248.7703
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.0159
                       Mean reward: 555.35
               Mean episode length: 212.28
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 109.0340
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.32s
                      Time elapsed: 00:48:36
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 40528 steps/s (collection: 2.264s, learning 0.161s)
             Mean action noise std: 2.02
          Mean value_function loss: 271.1379
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.0152
                       Mean reward: 525.41
               Mean episode length: 201.19
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 107.8633
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.43s
                      Time elapsed: 00:48:38
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 46076 steps/s (collection: 2.037s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 240.6544
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.0165
                       Mean reward: 540.41
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 110.0629
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.13s
                      Time elapsed: 00:48:40
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 45803 steps/s (collection: 2.052s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 214.6228
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.0185
                       Mean reward: 538.27
               Mean episode length: 204.45
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 107.6340
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.15s
                      Time elapsed: 00:48:42
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 47148 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 245.2944
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.0189
                       Mean reward: 517.21
               Mean episode length: 196.67
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 108.2025
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.08s
                      Time elapsed: 00:48:44
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 46135 steps/s (collection: 2.027s, learning 0.104s)
             Mean action noise std: 2.02
          Mean value_function loss: 258.0625
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0201
                       Mean reward: 530.68
               Mean episode length: 203.50
    Episode_Reward/reaching_object: 0.7166
     Episode_Reward/lifting_object: 106.1470
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.13s
                      Time elapsed: 00:48:47
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 46823 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 2.02
          Mean value_function loss: 302.1646
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.0245
                       Mean reward: 537.24
               Mean episode length: 202.97
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 108.8331
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.10s
                      Time elapsed: 00:48:49
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 47089 steps/s (collection: 1.977s, learning 0.111s)
             Mean action noise std: 2.02
          Mean value_function loss: 259.4415
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 37.0296
                       Mean reward: 555.91
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 0.7233
     Episode_Reward/lifting_object: 106.7082
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.09s
                      Time elapsed: 00:48:51
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 47399 steps/s (collection: 1.967s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 269.8144
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.0297
                       Mean reward: 509.83
               Mean episode length: 192.77
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 108.6709
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.07s
                      Time elapsed: 00:48:53
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 46745 steps/s (collection: 2.006s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 330.0132
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.0291
                       Mean reward: 546.43
               Mean episode length: 205.11
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 107.0095
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.10s
                      Time elapsed: 00:48:55
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 47358 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 2.02
          Mean value_function loss: 246.1536
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 37.0284
                       Mean reward: 565.42
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 0.7179
     Episode_Reward/lifting_object: 106.6305
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.08s
                      Time elapsed: 00:48:57
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 46107 steps/s (collection: 2.026s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 238.7732
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 37.0297
                       Mean reward: 551.34
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 0.7318
     Episode_Reward/lifting_object: 109.5727
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.13s
                      Time elapsed: 00:48:59
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 46967 steps/s (collection: 1.998s, learning 0.095s)
             Mean action noise std: 2.02
          Mean value_function loss: 215.7571
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 37.0306
                       Mean reward: 555.34
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 110.9599
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.09s
                      Time elapsed: 00:49:01
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 47150 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 228.9822
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 37.0314
                       Mean reward: 574.42
               Mean episode length: 216.57
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 110.8553
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.08s
                      Time elapsed: 00:49:03
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 46773 steps/s (collection: 2.000s, learning 0.102s)
             Mean action noise std: 2.02
          Mean value_function loss: 254.0840
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.0317
                       Mean reward: 561.94
               Mean episode length: 212.08
    Episode_Reward/reaching_object: 0.7211
     Episode_Reward/lifting_object: 107.5957
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.10s
                      Time elapsed: 00:49:05
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 47438 steps/s (collection: 1.972s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 284.0151
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.0325
                       Mean reward: 535.66
               Mean episode length: 201.76
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 108.6651
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.07s
                      Time elapsed: 00:49:07
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 47188 steps/s (collection: 1.974s, learning 0.109s)
             Mean action noise std: 2.03
          Mean value_function loss: 253.4860
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.0354
                       Mean reward: 532.19
               Mean episode length: 199.90
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 110.9499
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.08s
                      Time elapsed: 00:49:10
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 47137 steps/s (collection: 1.981s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 246.0085
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 37.0394
                       Mean reward: 552.52
               Mean episode length: 206.85
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 111.9142
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.09s
                      Time elapsed: 00:49:12
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 47360 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 269.3701
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.0406
                       Mean reward: 572.94
               Mean episode length: 214.05
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 107.4678
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.08s
                      Time elapsed: 00:49:14
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 47289 steps/s (collection: 1.986s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 281.7376
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0441
                       Mean reward: 581.28
               Mean episode length: 214.76
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: 108.0940
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.08s
                      Time elapsed: 00:49:16
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 46958 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 248.7156
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.0462
                       Mean reward: 522.31
               Mean episode length: 197.41
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 105.8777
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.09s
                      Time elapsed: 00:49:18
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 46946 steps/s (collection: 1.996s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 250.2191
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.0462
                       Mean reward: 528.75
               Mean episode length: 197.46
    Episode_Reward/reaching_object: 0.7260
     Episode_Reward/lifting_object: 108.9507
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.09s
                      Time elapsed: 00:49:20
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 46591 steps/s (collection: 1.998s, learning 0.112s)
             Mean action noise std: 2.03
          Mean value_function loss: 236.6039
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.0469
                       Mean reward: 577.68
               Mean episode length: 210.61
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: 110.1317
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.11s
                      Time elapsed: 00:49:22
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 46498 steps/s (collection: 2.005s, learning 0.109s)
             Mean action noise std: 2.03
          Mean value_function loss: 256.6109
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 37.0472
                       Mean reward: 535.09
               Mean episode length: 201.15
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: 107.4009
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.11s
                      Time elapsed: 00:49:24
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 44657 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 252.5741
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 37.0476
                       Mean reward: 553.97
               Mean episode length: 208.02
    Episode_Reward/reaching_object: 0.7227
     Episode_Reward/lifting_object: 108.3958
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.20s
                      Time elapsed: 00:49:26
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 46973 steps/s (collection: 1.994s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 277.2664
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.0479
                       Mean reward: 516.23
               Mean episode length: 195.07
    Episode_Reward/reaching_object: 0.7145
     Episode_Reward/lifting_object: 106.0392
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.09s
                      Time elapsed: 00:49:28
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 46967 steps/s (collection: 1.997s, learning 0.096s)
             Mean action noise std: 2.03
          Mean value_function loss: 261.7637
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.0498
                       Mean reward: 561.48
               Mean episode length: 209.42
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 112.6838
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.09s
                      Time elapsed: 00:49:31
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 47069 steps/s (collection: 1.985s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 230.4400
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.0540
                       Mean reward: 554.76
               Mean episode length: 205.05
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 109.6383
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.09s
                      Time elapsed: 00:49:33
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 47138 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.03
          Mean value_function loss: 275.7574
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.0573
                       Mean reward: 528.97
               Mean episode length: 196.07
    Episode_Reward/reaching_object: 0.7184
     Episode_Reward/lifting_object: 107.5660
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.09s
                      Time elapsed: 00:49:35
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 47055 steps/s (collection: 1.990s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 230.4749
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.0587
                       Mean reward: 526.60
               Mean episode length: 197.94
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 110.4972
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.09s
                      Time elapsed: 00:49:37
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 46414 steps/s (collection: 2.018s, learning 0.100s)
             Mean action noise std: 2.03
          Mean value_function loss: 252.5194
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.0586
                       Mean reward: 580.44
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 111.6314
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.12s
                      Time elapsed: 00:49:39
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 46257 steps/s (collection: 2.034s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 286.8495
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.0612
                       Mean reward: 558.36
               Mean episode length: 204.37
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: 109.2948
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.13s
                      Time elapsed: 00:49:41
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 46326 steps/s (collection: 2.012s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 260.1709
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.0652
                       Mean reward: 566.57
               Mean episode length: 210.28
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 109.9131
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.12s
                      Time elapsed: 00:49:43
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 45533 steps/s (collection: 2.048s, learning 0.111s)
             Mean action noise std: 2.03
          Mean value_function loss: 288.2325
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.0706
                       Mean reward: 521.19
               Mean episode length: 197.03
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 109.3660
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.16s
                      Time elapsed: 00:49:45
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 45982 steps/s (collection: 2.030s, learning 0.108s)
             Mean action noise std: 2.03
          Mean value_function loss: 274.6904
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.0752
                       Mean reward: 562.47
               Mean episode length: 208.52
    Episode_Reward/reaching_object: 0.7365
     Episode_Reward/lifting_object: 110.1669
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.14s
                      Time elapsed: 00:49:48
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 46792 steps/s (collection: 1.994s, learning 0.107s)
             Mean action noise std: 2.03
          Mean value_function loss: 272.4698
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.0781
                       Mean reward: 566.62
               Mean episode length: 210.44
    Episode_Reward/reaching_object: 0.7213
     Episode_Reward/lifting_object: 108.4381
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.10s
                      Time elapsed: 00:49:50
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 46757 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 2.03
          Mean value_function loss: 281.3275
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 37.0793
                       Mean reward: 546.48
               Mean episode length: 202.34
    Episode_Reward/reaching_object: 0.7351
     Episode_Reward/lifting_object: 110.9131
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.10s
                      Time elapsed: 00:49:52
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 46033 steps/s (collection: 2.024s, learning 0.111s)
             Mean action noise std: 2.03
          Mean value_function loss: 260.0088
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 37.0821
                       Mean reward: 560.56
               Mean episode length: 210.02
    Episode_Reward/reaching_object: 0.7344
     Episode_Reward/lifting_object: 109.6129
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.14s
                      Time elapsed: 00:49:54
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 45323 steps/s (collection: 2.042s, learning 0.127s)
             Mean action noise std: 2.04
          Mean value_function loss: 268.9841
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0849
                       Mean reward: 506.62
               Mean episode length: 189.03
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 109.5053
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.17s
                      Time elapsed: 00:49:56
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 45381 steps/s (collection: 2.061s, learning 0.105s)
             Mean action noise std: 2.04
          Mean value_function loss: 244.5826
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.0907
                       Mean reward: 584.55
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 111.7784
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.17s
                      Time elapsed: 00:49:58
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 46830 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 2.04
          Mean value_function loss: 288.1461
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.0958
                       Mean reward: 544.53
               Mean episode length: 203.38
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 111.6416
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.10s
                      Time elapsed: 00:50:00
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 46441 steps/s (collection: 2.011s, learning 0.106s)
             Mean action noise std: 2.04
          Mean value_function loss: 259.0416
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1026
                       Mean reward: 552.26
               Mean episode length: 202.96
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 110.3013
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.12s
                      Time elapsed: 00:50:02
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 46601 steps/s (collection: 2.008s, learning 0.101s)
             Mean action noise std: 2.04
          Mean value_function loss: 268.6725
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.1082
                       Mean reward: 568.75
               Mean episode length: 208.08
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 112.4287
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.11s
                      Time elapsed: 00:50:05
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 47250 steps/s (collection: 1.976s, learning 0.104s)
             Mean action noise std: 2.04
          Mean value_function loss: 298.0483
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.1133
                       Mean reward: 568.16
               Mean episode length: 206.37
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 111.5374
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.08s
                      Time elapsed: 00:50:07
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 47369 steps/s (collection: 1.972s, learning 0.104s)
             Mean action noise std: 2.04
          Mean value_function loss: 259.4089
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.1189
                       Mean reward: 508.11
               Mean episode length: 192.20
    Episode_Reward/reaching_object: 0.7239
     Episode_Reward/lifting_object: 108.1331
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.08s
                      Time elapsed: 00:50:09
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 47064 steps/s (collection: 1.989s, learning 0.100s)
             Mean action noise std: 2.04
          Mean value_function loss: 241.8757
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.1206
                       Mean reward: 554.22
               Mean episode length: 205.22
    Episode_Reward/reaching_object: 0.7256
     Episode_Reward/lifting_object: 108.9984
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.09s
                      Time elapsed: 00:50:11
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 47008 steps/s (collection: 1.998s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 284.9699
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1218
                       Mean reward: 586.39
               Mean episode length: 213.01
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 112.8344
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.09s
                      Time elapsed: 00:50:13
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 46966 steps/s (collection: 1.999s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 283.1983
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.1232
                       Mean reward: 545.93
               Mean episode length: 201.57
    Episode_Reward/reaching_object: 0.7278
     Episode_Reward/lifting_object: 109.8070
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.09s
                      Time elapsed: 00:50:15
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 47330 steps/s (collection: 1.982s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 227.1571
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.1232
                       Mean reward: 554.97
               Mean episode length: 204.36
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 110.4429
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.08s
                      Time elapsed: 00:50:17
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 46892 steps/s (collection: 1.988s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 262.1381
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.1228
                       Mean reward: 560.05
               Mean episode length: 206.30
    Episode_Reward/reaching_object: 0.7212
     Episode_Reward/lifting_object: 109.2410
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.10s
                      Time elapsed: 00:50:19
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 46524 steps/s (collection: 2.000s, learning 0.113s)
             Mean action noise std: 2.04
          Mean value_function loss: 283.7925
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.1240
                       Mean reward: 580.38
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 113.8192
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.11s
                      Time elapsed: 00:50:21
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 47276 steps/s (collection: 1.955s, learning 0.125s)
             Mean action noise std: 2.04
          Mean value_function loss: 283.2805
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.1244
                       Mean reward: 555.26
               Mean episode length: 201.30
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 112.6871
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.08s
                      Time elapsed: 00:50:23
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 45783 steps/s (collection: 2.025s, learning 0.123s)
             Mean action noise std: 2.04
          Mean value_function loss: 264.8233
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.1261
                       Mean reward: 585.64
               Mean episode length: 212.74
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 112.3378
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.15s
                      Time elapsed: 00:50:25
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 46544 steps/s (collection: 2.018s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 275.2752
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.1304
                       Mean reward: 547.08
               Mean episode length: 204.52
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 111.1932
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.11s
                      Time elapsed: 00:50:28
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 47075 steps/s (collection: 1.968s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 338.0642
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.1354
                       Mean reward: 534.06
               Mean episode length: 197.54
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: 109.3076
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.09s
                      Time elapsed: 00:50:30
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 46338 steps/s (collection: 2.011s, learning 0.111s)
             Mean action noise std: 2.05
          Mean value_function loss: 295.7757
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.1424
                       Mean reward: 545.64
               Mean episode length: 199.87
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 109.5241
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.12s
                      Time elapsed: 00:50:32
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 46841 steps/s (collection: 1.992s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 281.9013
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.1447
                       Mean reward: 540.29
               Mean episode length: 197.71
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 111.1072
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.10s
                      Time elapsed: 00:50:34
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 47764 steps/s (collection: 1.963s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 295.7171
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.1436
                       Mean reward: 549.81
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 0.7318
     Episode_Reward/lifting_object: 111.0035
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.06s
                      Time elapsed: 00:50:36
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 46372 steps/s (collection: 2.021s, learning 0.099s)
             Mean action noise std: 2.05
          Mean value_function loss: 288.4757
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.1442
                       Mean reward: 512.93
               Mean episode length: 188.12
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 108.2700
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.12s
                      Time elapsed: 00:50:38
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 46664 steps/s (collection: 1.983s, learning 0.123s)
             Mean action noise std: 2.05
          Mean value_function loss: 433.0703
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.1497
                       Mean reward: 543.86
               Mean episode length: 200.31
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: 109.0770
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.11s
                      Time elapsed: 00:50:40
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 47336 steps/s (collection: 1.973s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 369.5382
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 37.1571
                       Mean reward: 572.53
               Mean episode length: 206.68
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 111.6852
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.08s
                      Time elapsed: 00:50:42
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 43324 steps/s (collection: 2.152s, learning 0.117s)
             Mean action noise std: 2.05
          Mean value_function loss: 262.2251
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.1633
                       Mean reward: 570.83
               Mean episode length: 207.73
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 109.4373
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.27s
                      Time elapsed: 00:50:44
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 43855 steps/s (collection: 2.139s, learning 0.103s)
             Mean action noise std: 2.05
          Mean value_function loss: 249.6691
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1692
                       Mean reward: 553.06
               Mean episode length: 203.17
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 111.4988
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.24s
                      Time elapsed: 00:50:47
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 44268 steps/s (collection: 2.086s, learning 0.135s)
             Mean action noise std: 2.05
          Mean value_function loss: 263.2359
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.1726
                       Mean reward: 570.67
               Mean episode length: 207.69
    Episode_Reward/reaching_object: 0.7390
     Episode_Reward/lifting_object: 111.8979
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.22s
                      Time elapsed: 00:50:49
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 43531 steps/s (collection: 2.130s, learning 0.128s)
             Mean action noise std: 2.05
          Mean value_function loss: 229.5276
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1735
                       Mean reward: 570.76
               Mean episode length: 206.97
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 112.6974
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.26s
                      Time elapsed: 00:50:51
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 45942 steps/s (collection: 2.033s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 237.7142
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.1745
                       Mean reward: 558.48
               Mean episode length: 204.48
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 113.3738
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.14s
                      Time elapsed: 00:50:53
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 38413 steps/s (collection: 2.401s, learning 0.158s)
             Mean action noise std: 2.05
          Mean value_function loss: 236.5285
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.1774
                       Mean reward: 599.65
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 112.8465
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.56s
                      Time elapsed: 00:50:56
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 36634 steps/s (collection: 2.488s, learning 0.195s)
             Mean action noise std: 2.05
          Mean value_function loss: 270.2934
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.1802
                       Mean reward: 566.45
               Mean episode length: 206.16
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 114.1685
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.68s
                      Time elapsed: 00:50:59
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 44943 steps/s (collection: 2.093s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 291.4140
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.1810
                       Mean reward: 559.50
               Mean episode length: 204.31
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 111.8554
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.19s
                      Time elapsed: 00:51:01
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 47014 steps/s (collection: 1.988s, learning 0.103s)
             Mean action noise std: 2.05
          Mean value_function loss: 281.3284
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.1817
                       Mean reward: 547.94
               Mean episode length: 203.39
    Episode_Reward/reaching_object: 0.7380
     Episode_Reward/lifting_object: 110.3920
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.09s
                      Time elapsed: 00:51:03
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 46621 steps/s (collection: 2.002s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 252.8886
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.1861
                       Mean reward: 570.42
               Mean episode length: 206.99
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 114.6438
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.11s
                      Time elapsed: 00:51:05
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 47291 steps/s (collection: 1.982s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 286.4439
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.1921
                       Mean reward: 561.44
               Mean episode length: 203.68
    Episode_Reward/reaching_object: 0.7339
     Episode_Reward/lifting_object: 111.3209
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.08s
                      Time elapsed: 00:51:07
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 47911 steps/s (collection: 1.959s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 252.1576
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.1949
                       Mean reward: 542.62
               Mean episode length: 200.38
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 111.8405
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.05s
                      Time elapsed: 00:51:09
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 46993 steps/s (collection: 1.998s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 266.1187
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.1991
                       Mean reward: 573.96
               Mean episode length: 206.27
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 113.3312
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.09s
                      Time elapsed: 00:51:11
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 46033 steps/s (collection: 2.038s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 247.7128
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.2053
                       Mean reward: 553.39
               Mean episode length: 205.56
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 113.8384
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.14s
                      Time elapsed: 00:51:13
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 47666 steps/s (collection: 1.965s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 287.9032
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.2092
                       Mean reward: 527.83
               Mean episode length: 196.01
    Episode_Reward/reaching_object: 0.7303
     Episode_Reward/lifting_object: 110.5648
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.06s
                      Time elapsed: 00:51:15
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 46574 steps/s (collection: 1.995s, learning 0.116s)
             Mean action noise std: 2.06
          Mean value_function loss: 277.7501
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.2109
                       Mean reward: 565.35
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 113.8629
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.11s
                      Time elapsed: 00:51:18
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 46344 steps/s (collection: 1.997s, learning 0.125s)
             Mean action noise std: 2.06
          Mean value_function loss: 313.0472
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 37.2138
                       Mean reward: 598.67
               Mean episode length: 216.27
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 114.2092
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.12s
                      Time elapsed: 00:51:20
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 46099 steps/s (collection: 2.022s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 301.9568
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.2151
                       Mean reward: 557.60
               Mean episode length: 200.93
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 112.8649
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.13s
                      Time elapsed: 00:51:22
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 46992 steps/s (collection: 1.992s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 295.6648
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.2167
                       Mean reward: 607.54
               Mean episode length: 215.17
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 117.0576
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.09s
                      Time elapsed: 00:51:24
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 46206 steps/s (collection: 1.995s, learning 0.133s)
             Mean action noise std: 2.06
          Mean value_function loss: 314.5023
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.2169
                       Mean reward: 569.41
               Mean episode length: 206.93
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 111.1969
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.13s
                      Time elapsed: 00:51:26
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 47932 steps/s (collection: 1.950s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 312.2143
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2195
                       Mean reward: 581.86
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 0.7381
     Episode_Reward/lifting_object: 111.8703
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.05s
                      Time elapsed: 00:51:28
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 47131 steps/s (collection: 1.991s, learning 0.095s)
             Mean action noise std: 2.06
          Mean value_function loss: 268.2814
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.2241
                       Mean reward: 554.66
               Mean episode length: 201.58
    Episode_Reward/reaching_object: 0.7194
     Episode_Reward/lifting_object: 108.3450
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.09s
                      Time elapsed: 00:51:30
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 46772 steps/s (collection: 2.007s, learning 0.095s)
             Mean action noise std: 2.06
          Mean value_function loss: 303.5417
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.2245
                       Mean reward: 590.87
               Mean episode length: 212.19
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 112.1769
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.10s
                      Time elapsed: 00:51:32
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 47012 steps/s (collection: 1.984s, learning 0.107s)
             Mean action noise std: 2.06
          Mean value_function loss: 314.1411
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.2226
                       Mean reward: 552.59
               Mean episode length: 202.90
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 112.7011
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.09s
                      Time elapsed: 00:51:34
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 46596 steps/s (collection: 2.005s, learning 0.105s)
             Mean action noise std: 2.06
          Mean value_function loss: 262.6585
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 37.2224
                       Mean reward: 570.77
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 110.4130
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.11s
                      Time elapsed: 00:51:36
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 46436 steps/s (collection: 2.019s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 284.6980
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.2248
                       Mean reward: 571.65
               Mean episode length: 206.02
    Episode_Reward/reaching_object: 0.7105
     Episode_Reward/lifting_object: 107.1886
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.12s
                      Time elapsed: 00:51:39
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 46911 steps/s (collection: 1.983s, learning 0.113s)
             Mean action noise std: 2.06
          Mean value_function loss: 264.5243
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.2287
                       Mean reward: 574.01
               Mean episode length: 207.87
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 115.5032
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.10s
                      Time elapsed: 00:51:41
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 44334 steps/s (collection: 2.117s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 254.6032
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.2298
                       Mean reward: 557.79
               Mean episode length: 198.45
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 115.2465
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.22s
                      Time elapsed: 00:51:43
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 46871 steps/s (collection: 1.999s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 276.2353
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.2280
                       Mean reward: 581.17
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 114.7497
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.10s
                      Time elapsed: 00:51:45
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 46671 steps/s (collection: 2.000s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 293.7438
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.2279
                       Mean reward: 589.16
               Mean episode length: 213.79
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 114.8311
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.11s
                      Time elapsed: 00:51:47
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 47266 steps/s (collection: 1.973s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 234.6700
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.2306
                       Mean reward: 586.38
               Mean episode length: 209.55
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 116.8700
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.08s
                      Time elapsed: 00:51:49
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 47022 steps/s (collection: 1.990s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 295.8276
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 37.2356
                       Mean reward: 552.60
               Mean episode length: 197.14
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 117.6823
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.09s
                      Time elapsed: 00:51:51
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 46840 steps/s (collection: 1.995s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 223.1606
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.2382
                       Mean reward: 599.91
               Mean episode length: 214.72
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 115.3617
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.10s
                      Time elapsed: 00:51:53
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 47197 steps/s (collection: 1.987s, learning 0.096s)
             Mean action noise std: 2.06
          Mean value_function loss: 281.9583
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.2403
                       Mean reward: 574.42
               Mean episode length: 210.09
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 116.0341
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.08s
                      Time elapsed: 00:51:55
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 46843 steps/s (collection: 1.995s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 240.2030
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.2405
                       Mean reward: 594.30
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 115.9409
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.10s
                      Time elapsed: 00:51:58
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 46547 steps/s (collection: 2.014s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 306.0824
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 37.2406
                       Mean reward: 590.56
               Mean episode length: 213.36
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 117.1612
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.11s
                      Time elapsed: 00:52:00
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 47240 steps/s (collection: 1.971s, learning 0.110s)
             Mean action noise std: 2.06
          Mean value_function loss: 286.4911
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.2419
                       Mean reward: 556.77
               Mean episode length: 200.40
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 115.0903
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.08s
                      Time elapsed: 00:52:02
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 47548 steps/s (collection: 1.967s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 302.4357
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.2441
                       Mean reward: 595.41
               Mean episode length: 216.78
    Episode_Reward/reaching_object: 0.7854
     Episode_Reward/lifting_object: 118.4606
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.07s
                      Time elapsed: 00:52:04
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 47732 steps/s (collection: 1.955s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 265.0585
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.2451
                       Mean reward: 588.70
               Mean episode length: 214.68
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 115.4731
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.06s
                      Time elapsed: 00:52:06
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 46737 steps/s (collection: 1.988s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 227.4838
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2452
                       Mean reward: 593.32
               Mean episode length: 212.67
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 116.1724
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.10s
                      Time elapsed: 00:52:08
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 47408 steps/s (collection: 1.968s, learning 0.106s)
             Mean action noise std: 2.07
          Mean value_function loss: 280.3210
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.2471
                       Mean reward: 583.78
               Mean episode length: 208.87
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 117.0441
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.07s
                      Time elapsed: 00:52:10
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 47516 steps/s (collection: 1.964s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 300.3336
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 37.2516
                       Mean reward: 536.48
               Mean episode length: 196.87
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 114.0570
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.07s
                      Time elapsed: 00:52:12
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 47322 steps/s (collection: 1.983s, learning 0.095s)
             Mean action noise std: 2.07
          Mean value_function loss: 273.7701
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.2551
                       Mean reward: 564.98
               Mean episode length: 202.77
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 115.7850
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.08s
                      Time elapsed: 00:52:14
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 47911 steps/s (collection: 1.949s, learning 0.103s)
             Mean action noise std: 2.07
          Mean value_function loss: 276.2434
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2597
                       Mean reward: 579.93
               Mean episode length: 207.95
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 113.8931
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.05s
                      Time elapsed: 00:52:16
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 47682 steps/s (collection: 1.957s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 260.3299
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2618
                       Mean reward: 579.84
               Mean episode length: 205.32
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 113.2138
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.06s
                      Time elapsed: 00:52:18
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19435 steps/s (collection: 4.937s, learning 0.121s)
             Mean action noise std: 2.07
          Mean value_function loss: 258.8945
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.2659
                       Mean reward: 567.66
               Mean episode length: 204.91
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 115.5816
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.06s
                      Time elapsed: 00:52:23
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14570 steps/s (collection: 6.629s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 319.0634
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.2720
                       Mean reward: 597.02
               Mean episode length: 210.93
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 116.8495
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.75s
                      Time elapsed: 00:52:30
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14679 steps/s (collection: 6.580s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 300.2231
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 37.2763
                       Mean reward: 588.06
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 116.0505
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.70s
                      Time elapsed: 00:52:37
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14097 steps/s (collection: 6.857s, learning 0.116s)
             Mean action noise std: 2.07
          Mean value_function loss: 336.6942
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.2783
                       Mean reward: 543.75
               Mean episode length: 197.55
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 115.3404
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.97s
                      Time elapsed: 00:52:44
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14649 steps/s (collection: 6.591s, learning 0.119s)
             Mean action noise std: 2.07
          Mean value_function loss: 317.0827
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.2823
                       Mean reward: 563.23
               Mean episode length: 205.05
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 115.3537
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.71s
                      Time elapsed: 00:52:50
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14692 steps/s (collection: 6.555s, learning 0.135s)
             Mean action noise std: 2.07
          Mean value_function loss: 304.4565
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.2852
                       Mean reward: 589.36
               Mean episode length: 209.24
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 114.0293
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.69s
                      Time elapsed: 00:52:57
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14340 steps/s (collection: 6.724s, learning 0.131s)
             Mean action noise std: 2.07
          Mean value_function loss: 295.6426
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2890
                       Mean reward: 555.35
               Mean episode length: 200.18
    Episode_Reward/reaching_object: 0.7449
     Episode_Reward/lifting_object: 113.0894
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.85s
                      Time elapsed: 00:53:04
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14202 steps/s (collection: 6.797s, learning 0.124s)
             Mean action noise std: 2.07
          Mean value_function loss: 257.4442
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.2948
                       Mean reward: 564.68
               Mean episode length: 200.05
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 115.4677
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.92s
                      Time elapsed: 00:53:11
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13085 steps/s (collection: 7.409s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 294.8145
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.2987
                       Mean reward: 572.39
               Mean episode length: 204.70
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 116.0516
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.51s
                      Time elapsed: 00:53:18
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 49429 steps/s (collection: 1.901s, learning 0.088s)
             Mean action noise std: 2.08
          Mean value_function loss: 295.9628
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.3020
                       Mean reward: 558.22
               Mean episode length: 202.26
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 113.7504
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.99s
                      Time elapsed: 00:53:20
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 49573 steps/s (collection: 1.872s, learning 0.111s)
             Mean action noise std: 2.08
          Mean value_function loss: 294.3922
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3044
                       Mean reward: 597.56
               Mean episode length: 215.64
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 118.8295
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 1.98s
                      Time elapsed: 00:53:22
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 49843 steps/s (collection: 1.870s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 298.3194
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.3067
                       Mean reward: 578.26
               Mean episode length: 206.84
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 117.2999
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.97s
                      Time elapsed: 00:53:24
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 49196 steps/s (collection: 1.905s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 286.0455
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.3080
                       Mean reward: 605.71
               Mean episode length: 213.02
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 119.2844
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.00s
                      Time elapsed: 00:53:26
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 50042 steps/s (collection: 1.867s, learning 0.098s)
             Mean action noise std: 2.08
          Mean value_function loss: 289.9301
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.3096
                       Mean reward: 565.56
               Mean episode length: 200.82
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 119.3363
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.96s
                      Time elapsed: 00:53:28
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 49899 steps/s (collection: 1.882s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 237.8435
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3092
                       Mean reward: 631.07
               Mean episode length: 221.03
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 121.4438
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.97s
                      Time elapsed: 00:53:30
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 50445 steps/s (collection: 1.862s, learning 0.087s)
             Mean action noise std: 2.08
          Mean value_function loss: 321.5439
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.3115
                       Mean reward: 566.94
               Mean episode length: 202.45
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 116.5614
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.95s
                      Time elapsed: 00:53:32
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 49497 steps/s (collection: 1.872s, learning 0.114s)
             Mean action noise std: 2.08
          Mean value_function loss: 237.2785
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 37.3135
                       Mean reward: 573.37
               Mean episode length: 203.90
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 119.1268
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.99s
                      Time elapsed: 00:53:34
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 48791 steps/s (collection: 1.900s, learning 0.115s)
             Mean action noise std: 2.08
          Mean value_function loss: 247.3805
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.3139
                       Mean reward: 625.92
               Mean episode length: 220.07
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 120.7601
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.01s
                      Time elapsed: 00:53:36
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 47707 steps/s (collection: 1.947s, learning 0.114s)
             Mean action noise std: 2.08
          Mean value_function loss: 264.9037
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.3139
                       Mean reward: 580.78
               Mean episode length: 206.80
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 115.6451
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.06s
                      Time elapsed: 00:53:38
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 50317 steps/s (collection: 1.864s, learning 0.090s)
             Mean action noise std: 2.08
          Mean value_function loss: 288.6348
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.3194
                       Mean reward: 563.56
               Mean episode length: 200.39
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 115.3411
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.95s
                      Time elapsed: 00:53:40
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 50440 steps/s (collection: 1.861s, learning 0.088s)
             Mean action noise std: 2.08
          Mean value_function loss: 315.2925
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.3268
                       Mean reward: 602.42
               Mean episode length: 211.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 120.4268
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.95s
                      Time elapsed: 00:53:42
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 50009 steps/s (collection: 1.860s, learning 0.106s)
             Mean action noise std: 2.08
          Mean value_function loss: 312.0289
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.3302
                       Mean reward: 581.33
               Mean episode length: 203.13
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 114.2521
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.97s
                      Time elapsed: 00:53:44
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 48825 steps/s (collection: 1.918s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 311.2781
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.3348
                       Mean reward: 600.34
               Mean episode length: 211.02
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 120.2863
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.01s
                      Time elapsed: 00:53:46
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 49521 steps/s (collection: 1.883s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 292.9067
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.3376
                       Mean reward: 598.30
               Mean episode length: 210.39
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 120.1163
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.99s
                      Time elapsed: 00:53:48
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 49855 steps/s (collection: 1.872s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 293.6145
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.3382
                       Mean reward: 607.41
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 116.7144
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.97s
                      Time elapsed: 00:53:50
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 50226 steps/s (collection: 1.864s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 291.9821
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 37.3405
                       Mean reward: 584.34
               Mean episode length: 208.77
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 115.5942
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.96s
                      Time elapsed: 00:53:52
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 50497 steps/s (collection: 1.856s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 331.1478
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.3402
                       Mean reward: 572.74
               Mean episode length: 199.10
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 113.8689
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 1.95s
                      Time elapsed: 00:53:54
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 50391 steps/s (collection: 1.862s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 279.9395
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3401
                       Mean reward: 593.06
               Mean episode length: 210.62
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 116.5485
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.95s
                      Time elapsed: 00:53:56
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 50570 steps/s (collection: 1.852s, learning 0.092s)
             Mean action noise std: 2.08
          Mean value_function loss: 284.3501
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.3415
                       Mean reward: 596.64
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 116.2776
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.94s
                      Time elapsed: 00:53:58
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 50121 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 257.8252
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.3413
                       Mean reward: 613.26
               Mean episode length: 214.86
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 118.8213
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.96s
                      Time elapsed: 00:54:00
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 49672 steps/s (collection: 1.885s, learning 0.094s)
             Mean action noise std: 2.09
          Mean value_function loss: 291.4385
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.3405
                       Mean reward: 623.69
               Mean episode length: 216.79
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 120.6772
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.98s
                      Time elapsed: 00:54:02
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 50377 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 309.2267
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.3440
                       Mean reward: 627.65
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 119.7036
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.95s
                      Time elapsed: 00:54:04
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 50103 steps/s (collection: 1.857s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 335.9390
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.3475
                       Mean reward: 577.83
               Mean episode length: 206.53
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 116.9493
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.96s
                      Time elapsed: 00:54:06
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 50483 steps/s (collection: 1.855s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 265.2664
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 37.3509
                       Mean reward: 617.42
               Mean episode length: 217.54
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 117.1268
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.95s
                      Time elapsed: 00:54:08
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 50403 steps/s (collection: 1.853s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 285.7266
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.3516
                       Mean reward: 627.50
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 122.4563
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 1.95s
                      Time elapsed: 00:54:10
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 49015 steps/s (collection: 1.907s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 272.5418
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.3521
                       Mean reward: 580.36
               Mean episode length: 211.96
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 118.6762
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.01s
                      Time elapsed: 00:54:12
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 50243 steps/s (collection: 1.868s, learning 0.089s)
             Mean action noise std: 2.09
          Mean value_function loss: 283.3206
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.3549
                       Mean reward: 584.69
               Mean episode length: 207.95
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 119.0679
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.96s
                      Time elapsed: 00:54:14
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 50090 steps/s (collection: 1.864s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 275.9594
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.3593
                       Mean reward: 600.56
               Mean episode length: 209.22
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 119.1747
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.96s
                      Time elapsed: 00:54:16
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 48491 steps/s (collection: 1.932s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 301.7152
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.3679
                       Mean reward: 614.08
               Mean episode length: 216.40
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 116.3093
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.03s
                      Time elapsed: 00:54:18
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 48344 steps/s (collection: 1.941s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 296.0060
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 37.3750
                       Mean reward: 599.03
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 117.7584
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.03s
                      Time elapsed: 00:54:20
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 47733 steps/s (collection: 1.915s, learning 0.144s)
             Mean action noise std: 2.09
          Mean value_function loss: 278.5310
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.3782
                       Mean reward: 604.90
               Mean episode length: 213.72
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 122.9884
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.06s
                      Time elapsed: 00:54:22
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 42795 steps/s (collection: 2.174s, learning 0.123s)
             Mean action noise std: 2.09
          Mean value_function loss: 321.9161
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.3810
                       Mean reward: 562.25
               Mean episode length: 203.13
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 120.6196
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.30s
                      Time elapsed: 00:54:24
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 41960 steps/s (collection: 2.183s, learning 0.160s)
             Mean action noise std: 2.09
          Mean value_function loss: 321.4977
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.3819
                       Mean reward: 596.99
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 116.8100
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.34s
                      Time elapsed: 00:54:26
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 45335 steps/s (collection: 2.054s, learning 0.114s)
             Mean action noise std: 2.09
          Mean value_function loss: 307.4735
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.3848
                       Mean reward: 582.98
               Mean episode length: 203.69
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 118.1089
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.17s
                      Time elapsed: 00:54:29
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 47728 steps/s (collection: 1.955s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 278.5213
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.3868
                       Mean reward: 569.40
               Mean episode length: 203.17
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 120.8755
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.06s
                      Time elapsed: 00:54:31
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 49025 steps/s (collection: 1.905s, learning 0.100s)
             Mean action noise std: 2.09
          Mean value_function loss: 286.6011
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.3879
                       Mean reward: 573.57
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 118.7765
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.01s
                      Time elapsed: 00:54:33
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 47089 steps/s (collection: 1.995s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 288.9649
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 37.3933
                       Mean reward: 617.38
               Mean episode length: 214.81
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 118.7960
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.09s
                      Time elapsed: 00:54:35
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 46578 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 303.3755
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.3969
                       Mean reward: 570.53
               Mean episode length: 201.24
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 118.0593
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.11s
                      Time elapsed: 00:54:37
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 47852 steps/s (collection: 1.939s, learning 0.116s)
             Mean action noise std: 2.10
          Mean value_function loss: 309.6354
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.4058
                       Mean reward: 555.61
               Mean episode length: 196.53
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 114.2064
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.05s
                      Time elapsed: 00:54:39
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 50218 steps/s (collection: 1.867s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 321.2980
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 37.4122
                       Mean reward: 580.23
               Mean episode length: 203.32
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 118.1830
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.96s
                      Time elapsed: 00:54:41
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 49634 steps/s (collection: 1.889s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 343.4709
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.4117
                       Mean reward: 594.51
               Mean episode length: 206.77
    Episode_Reward/reaching_object: 0.7441
     Episode_Reward/lifting_object: 115.4127
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.98s
                      Time elapsed: 00:54:43
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 49702 steps/s (collection: 1.890s, learning 0.087s)
             Mean action noise std: 2.10
          Mean value_function loss: 306.8585
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.4097
                       Mean reward: 572.78
               Mean episode length: 204.10
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 117.6629
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.98s
                      Time elapsed: 00:54:45
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 50137 steps/s (collection: 1.871s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 270.5215
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.4091
                       Mean reward: 596.98
               Mean episode length: 208.54
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 120.2940
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.96s
                      Time elapsed: 00:54:47
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 49504 steps/s (collection: 1.894s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 340.1869
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 37.4109
                       Mean reward: 611.59
               Mean episode length: 213.11
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 118.3651
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 1.99s
                      Time elapsed: 00:54:49
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 49420 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 332.9302
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.4143
                       Mean reward: 595.52
               Mean episode length: 209.92
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 112.8673
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.99s
                      Time elapsed: 00:54:51
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 49876 steps/s (collection: 1.881s, learning 0.090s)
             Mean action noise std: 2.10
          Mean value_function loss: 292.7970
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.4172
                       Mean reward: 601.63
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 122.0317
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 1.97s
                      Time elapsed: 00:54:53
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 49614 steps/s (collection: 1.888s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 268.3990
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 37.4198
                       Mean reward: 588.53
               Mean episode length: 204.01
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 119.0391
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.98s
                      Time elapsed: 00:54:55
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 49319 steps/s (collection: 1.903s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 262.4819
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.4223
                       Mean reward: 597.98
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 121.5824
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.99s
                      Time elapsed: 00:54:57
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 49787 steps/s (collection: 1.878s, learning 0.096s)
             Mean action noise std: 2.10
          Mean value_function loss: 288.0676
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.4258
                       Mean reward: 619.54
               Mean episode length: 214.10
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 120.6274
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 1.97s
                      Time elapsed: 00:54:59
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 49279 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 2.10
          Mean value_function loss: 280.1690
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.4302
                       Mean reward: 645.30
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 121.8313
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.99s
                      Time elapsed: 00:55:01
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 50173 steps/s (collection: 1.866s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 273.8647
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.4334
                       Mean reward: 600.27
               Mean episode length: 210.49
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 123.7216
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.96s
                      Time elapsed: 00:55:03
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 49507 steps/s (collection: 1.896s, learning 0.090s)
             Mean action noise std: 2.10
          Mean value_function loss: 276.8863
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.4368
                       Mean reward: 615.82
               Mean episode length: 210.45
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 121.9939
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.99s
                      Time elapsed: 00:55:05
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 49405 steps/s (collection: 1.882s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 270.2540
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.4404
                       Mean reward: 559.22
               Mean episode length: 195.71
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 118.7854
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.99s
                      Time elapsed: 00:55:07
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 49677 steps/s (collection: 1.876s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 244.1696
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.4418
                       Mean reward: 620.92
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 120.4471
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.98s
                      Time elapsed: 00:55:09
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 49299 steps/s (collection: 1.886s, learning 0.108s)
             Mean action noise std: 2.10
          Mean value_function loss: 266.4015
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.4451
                       Mean reward: 635.69
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.8084
     Episode_Reward/lifting_object: 127.3785
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.99s
                      Time elapsed: 00:55:11
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 48906 steps/s (collection: 1.915s, learning 0.095s)
             Mean action noise std: 2.10
          Mean value_function loss: 264.5365
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.4452
                       Mean reward: 630.13
               Mean episode length: 217.20
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 122.5153
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.01s
                      Time elapsed: 00:55:13
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 49306 steps/s (collection: 1.904s, learning 0.090s)
             Mean action noise std: 2.10
          Mean value_function loss: 243.7132
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.4418
                       Mean reward: 604.26
               Mean episode length: 206.62
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 121.8986
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.99s
                      Time elapsed: 00:55:15
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 49401 steps/s (collection: 1.891s, learning 0.099s)
             Mean action noise std: 2.11
          Mean value_function loss: 217.3009
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.4415
                       Mean reward: 601.48
               Mean episode length: 208.70
    Episode_Reward/reaching_object: 0.7954
     Episode_Reward/lifting_object: 124.1846
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.99s
                      Time elapsed: 00:55:17
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 48595 steps/s (collection: 1.928s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 244.6294
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.4410
                       Mean reward: 632.40
               Mean episode length: 217.28
    Episode_Reward/reaching_object: 0.8056
     Episode_Reward/lifting_object: 126.7428
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.02s
                      Time elapsed: 00:55:19
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 49744 steps/s (collection: 1.885s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 260.5211
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.4423
                       Mean reward: 633.51
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 123.7253
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.98s
                      Time elapsed: 00:55:21
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 48895 steps/s (collection: 1.912s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 295.3356
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.4427
                       Mean reward: 583.84
               Mean episode length: 200.46
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 119.9321
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.01s
                      Time elapsed: 00:55:23
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 48353 steps/s (collection: 1.929s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 315.3788
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.4464
                       Mean reward: 637.56
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 119.8950
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.03s
                      Time elapsed: 00:55:25
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 49279 steps/s (collection: 1.901s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 290.2467
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.4497
                       Mean reward: 575.06
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 121.5188
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.99s
                      Time elapsed: 00:55:27
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 49743 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 273.4290
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.4527
                       Mean reward: 602.85
               Mean episode length: 210.19
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 119.9251
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.98s
                      Time elapsed: 00:55:29
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 47816 steps/s (collection: 1.965s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 276.4597
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.4560
                       Mean reward: 600.89
               Mean episode length: 208.09
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 122.2184
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.06s
                      Time elapsed: 00:55:31
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 49352 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 300.2414
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.4609
                       Mean reward: 631.26
               Mean episode length: 212.39
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 121.4595
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.99s
                      Time elapsed: 00:55:33
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 48983 steps/s (collection: 1.914s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 279.2351
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.4656
                       Mean reward: 642.67
               Mean episode length: 219.99
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 121.0307
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.01s
                      Time elapsed: 00:55:35
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 49081 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 2.11
          Mean value_function loss: 274.6696
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.4708
                       Mean reward: 617.09
               Mean episode length: 210.26
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 122.7603
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.00s
                      Time elapsed: 00:55:37
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 49865 steps/s (collection: 1.882s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 248.9592
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.4729
                       Mean reward: 602.07
               Mean episode length: 208.76
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 123.9450
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.97s
                      Time elapsed: 00:55:39
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 48809 steps/s (collection: 1.923s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 301.6280
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.4744
                       Mean reward: 623.10
               Mean episode length: 213.03
    Episode_Reward/reaching_object: 0.7965
     Episode_Reward/lifting_object: 125.8447
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.01s
                      Time elapsed: 00:55:41
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 48918 steps/s (collection: 1.891s, learning 0.119s)
             Mean action noise std: 2.11
          Mean value_function loss: 235.1678
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.4773
                       Mean reward: 624.64
               Mean episode length: 216.50
    Episode_Reward/reaching_object: 0.7957
     Episode_Reward/lifting_object: 124.7796
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.01s
                      Time elapsed: 00:55:43
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 48155 steps/s (collection: 1.928s, learning 0.113s)
             Mean action noise std: 2.11
          Mean value_function loss: 283.0274
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.4789
                       Mean reward: 641.79
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 122.1290
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.04s
                      Time elapsed: 00:55:45
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 47802 steps/s (collection: 1.943s, learning 0.114s)
             Mean action noise std: 2.11
          Mean value_function loss: 251.3927
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.4791
                       Mean reward: 633.63
               Mean episode length: 215.55
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 125.0918
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.06s
                      Time elapsed: 00:55:47
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 48413 steps/s (collection: 1.929s, learning 0.101s)
             Mean action noise std: 2.11
          Mean value_function loss: 288.3960
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.4798
                       Mean reward: 600.22
               Mean episode length: 203.96
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 121.3254
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.03s
                      Time elapsed: 00:55:49
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 48535 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 255.7299
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.4810
                       Mean reward: 612.11
               Mean episode length: 209.02
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 121.6625
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.03s
                      Time elapsed: 00:55:51
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 49211 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 248.8858
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.4816
                       Mean reward: 633.56
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.7903
     Episode_Reward/lifting_object: 125.4814
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.00s
                      Time elapsed: 00:55:53
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 48763 steps/s (collection: 1.924s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 257.7377
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 37.4822
                       Mean reward: 601.69
               Mean episode length: 208.22
    Episode_Reward/reaching_object: 0.7824
     Episode_Reward/lifting_object: 123.8299
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.02s
                      Time elapsed: 00:55:55
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 49797 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 242.9180
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.4846
                       Mean reward: 633.25
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 0.8022
     Episode_Reward/lifting_object: 126.6696
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.97s
                      Time elapsed: 00:55:57
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 48580 steps/s (collection: 1.932s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 251.4936
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.4861
                       Mean reward: 656.62
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 0.8054
     Episode_Reward/lifting_object: 127.6068
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.02s
                      Time elapsed: 00:55:59
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 47874 steps/s (collection: 1.959s, learning 0.095s)
             Mean action noise std: 2.12
          Mean value_function loss: 238.5123
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.4877
                       Mean reward: 595.26
               Mean episode length: 204.53
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 123.2505
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.05s
                      Time elapsed: 00:56:01
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 48868 steps/s (collection: 1.915s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 259.5038
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.4942
                       Mean reward: 618.78
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 0.8021
     Episode_Reward/lifting_object: 127.4761
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.01s
                      Time elapsed: 00:56:03
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 47468 steps/s (collection: 1.979s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 254.9473
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.5032
                       Mean reward: 636.17
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 122.6366
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.07s
                      Time elapsed: 00:56:05
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 48486 steps/s (collection: 1.919s, learning 0.108s)
             Mean action noise std: 2.12
          Mean value_function loss: 266.9240
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.5064
                       Mean reward: 627.32
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 0.7955
     Episode_Reward/lifting_object: 124.9896
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.03s
                      Time elapsed: 00:56:07
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 49233 steps/s (collection: 1.890s, learning 0.107s)
             Mean action noise std: 2.12
          Mean value_function loss: 283.9617
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.5066
                       Mean reward: 645.98
               Mean episode length: 219.67
    Episode_Reward/reaching_object: 0.8032
     Episode_Reward/lifting_object: 127.2669
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.00s
                      Time elapsed: 00:56:09
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 49316 steps/s (collection: 1.886s, learning 0.107s)
             Mean action noise std: 2.12
          Mean value_function loss: 243.5415
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.5075
                       Mean reward: 611.89
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 0.7973
     Episode_Reward/lifting_object: 124.7980
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.99s
                      Time elapsed: 00:56:11
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 49591 steps/s (collection: 1.888s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 247.6195
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.5087
                       Mean reward: 608.78
               Mean episode length: 208.38
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 122.4588
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.98s
                      Time elapsed: 00:56:13
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 49941 steps/s (collection: 1.881s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 257.8063
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 37.5107
                       Mean reward: 632.35
               Mean episode length: 216.24
    Episode_Reward/reaching_object: 0.7892
     Episode_Reward/lifting_object: 125.9195
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.97s
                      Time elapsed: 00:56:15
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 49720 steps/s (collection: 1.888s, learning 0.089s)
             Mean action noise std: 2.12
          Mean value_function loss: 255.4863
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.5098
                       Mean reward: 616.43
               Mean episode length: 208.63
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 121.7706
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.98s
                      Time elapsed: 00:56:17
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 49533 steps/s (collection: 1.892s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 260.9359
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.5106
                       Mean reward: 624.38
               Mean episode length: 215.69
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 123.9347
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.98s
                      Time elapsed: 00:56:19
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 49114 steps/s (collection: 1.912s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 261.1226
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 37.5142
                       Mean reward: 621.58
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 124.7436
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.00s
                      Time elapsed: 00:56:21
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 49022 steps/s (collection: 1.911s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 232.6305
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.5150
                       Mean reward: 634.03
               Mean episode length: 215.59
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 125.4839
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.01s
                      Time elapsed: 00:56:23
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 48248 steps/s (collection: 1.946s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 245.8035
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.5153
                       Mean reward: 617.48
               Mean episode length: 209.43
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 125.8877
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.04s
                      Time elapsed: 00:56:25
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 49591 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 278.3775
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.5146
                       Mean reward: 635.91
               Mean episode length: 215.32
    Episode_Reward/reaching_object: 0.8012
     Episode_Reward/lifting_object: 128.7627
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.98s
                      Time elapsed: 00:56:27
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 48781 steps/s (collection: 1.917s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 277.1286
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 37.5155
                       Mean reward: 657.07
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 125.8165
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.02s
                      Time elapsed: 00:56:29
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 49025 steps/s (collection: 1.899s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 269.9552
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.5155
                       Mean reward: 622.48
               Mean episode length: 211.72
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 123.3661
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.01s
                      Time elapsed: 00:56:31
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 50053 steps/s (collection: 1.877s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 251.1144
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.5180
                       Mean reward: 649.00
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 123.3239
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.96s
                      Time elapsed: 00:56:33
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 49795 steps/s (collection: 1.884s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 234.8181
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.5251
                       Mean reward: 663.60
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 0.8019
     Episode_Reward/lifting_object: 128.7827
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.97s
                      Time elapsed: 00:56:35
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 48512 steps/s (collection: 1.926s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 230.3611
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.5277
                       Mean reward: 649.99
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 0.8060
     Episode_Reward/lifting_object: 129.5491
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.03s
                      Time elapsed: 00:56:37
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 266.2973
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.5299
                       Mean reward: 633.01
               Mean episode length: 212.11
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 125.5903
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.03s
                      Time elapsed: 00:56:39
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 49029 steps/s (collection: 1.900s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 273.8304
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.5343
                       Mean reward: 631.42
               Mean episode length: 212.24
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 126.1602
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.00s
                      Time elapsed: 00:56:41
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 49635 steps/s (collection: 1.884s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 295.5910
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.5375
                       Mean reward: 615.71
               Mean episode length: 209.40
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 121.8344
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.98s
                      Time elapsed: 00:56:43
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 49549 steps/s (collection: 1.884s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 249.1927
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.5403
                       Mean reward: 649.86
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 0.8012
     Episode_Reward/lifting_object: 129.4378
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.98s
                      Time elapsed: 00:56:45
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.913s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 287.3224
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.5414
                       Mean reward: 611.49
               Mean episode length: 208.33
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 126.5882
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.00s
                      Time elapsed: 00:56:47
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.900s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 259.3417
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.5414
                       Mean reward: 662.29
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 0.7956
     Episode_Reward/lifting_object: 127.9940
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.99s
                      Time elapsed: 00:56:49
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 48828 steps/s (collection: 1.915s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 300.7140
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.5426
                       Mean reward: 656.71
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 0.7952
     Episode_Reward/lifting_object: 128.3216
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.01s
                      Time elapsed: 00:56:51
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 49425 steps/s (collection: 1.895s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 313.1460
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.5510
                       Mean reward: 628.92
               Mean episode length: 208.51
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 124.7450
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.99s
                      Time elapsed: 00:56:53
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 49025 steps/s (collection: 1.912s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 254.2921
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.5559
                       Mean reward: 651.10
               Mean episode length: 218.60
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 126.6446
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.01s
                      Time elapsed: 00:56:55
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 48832 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 251.8607
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.5574
                       Mean reward: 658.67
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 127.4861
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.01s
                      Time elapsed: 00:56:57
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 48400 steps/s (collection: 1.936s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 316.9110
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 37.5611
                       Mean reward: 617.07
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 125.6478
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.03s
                      Time elapsed: 00:56:59
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.898s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 273.6619
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.5645
                       Mean reward: 615.72
               Mean episode length: 206.33
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 125.2417
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.01s
                      Time elapsed: 00:57:01
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 49304 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 295.5907
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.5677
                       Mean reward: 610.00
               Mean episode length: 206.90
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 124.6479
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 1.99s
                      Time elapsed: 00:57:03
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 49626 steps/s (collection: 1.887s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 282.0095
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.5697
                       Mean reward: 651.72
               Mean episode length: 214.88
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 124.7669
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.98s
                      Time elapsed: 00:57:05
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 49127 steps/s (collection: 1.908s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 330.1401
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 37.5711
                       Mean reward: 635.40
               Mean episode length: 211.09
    Episode_Reward/reaching_object: 0.7409
     Episode_Reward/lifting_object: 119.6139
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.00s
                      Time elapsed: 00:57:07
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 50165 steps/s (collection: 1.864s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 260.2144
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.5732
                       Mean reward: 642.85
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 0.7962
     Episode_Reward/lifting_object: 129.0478
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.96s
                      Time elapsed: 00:57:09
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 49036 steps/s (collection: 1.886s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 253.0668
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.5765
                       Mean reward: 643.76
               Mean episode length: 214.04
    Episode_Reward/reaching_object: 0.8012
     Episode_Reward/lifting_object: 130.2815
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.00s
                      Time elapsed: 00:57:11
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 49483 steps/s (collection: 1.897s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 264.2488
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.5785
                       Mean reward: 625.14
               Mean episode length: 207.94
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 128.4407
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.99s
                      Time elapsed: 00:57:13
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 49450 steps/s (collection: 1.898s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 284.9274
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.5830
                       Mean reward: 636.50
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 126.5402
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.99s
                      Time elapsed: 00:57:15
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 49039 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 309.5472
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.5874
                       Mean reward: 639.80
               Mean episode length: 214.48
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 127.2564
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.00s
                      Time elapsed: 00:57:17
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 48617 steps/s (collection: 1.925s, learning 0.097s)
             Mean action noise std: 2.14
          Mean value_function loss: 367.5749
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.5945
                       Mean reward: 610.52
               Mean episode length: 204.10
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 125.6473
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.02s
                      Time elapsed: 00:57:19
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 49488 steps/s (collection: 1.883s, learning 0.104s)
             Mean action noise std: 2.14
          Mean value_function loss: 301.5054
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.6009
                       Mean reward: 677.98
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 126.3605
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.99s
                      Time elapsed: 00:57:21
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 48852 steps/s (collection: 1.920s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 317.8406
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.6046
                       Mean reward: 652.93
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 128.9631
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.01s
                      Time elapsed: 00:57:23
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 49125 steps/s (collection: 1.911s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 279.9659
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.6078
                       Mean reward: 594.31
               Mean episode length: 199.60
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 127.8176
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.00s
                      Time elapsed: 00:57:25
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 48729 steps/s (collection: 1.910s, learning 0.107s)
             Mean action noise std: 2.14
          Mean value_function loss: 277.5449
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.6121
                       Mean reward: 618.35
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 0.7960
     Episode_Reward/lifting_object: 130.2433
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.02s
                      Time elapsed: 00:57:27
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 268.7208
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.6165
                       Mean reward: 667.64
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 0.7968
     Episode_Reward/lifting_object: 131.3129
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.99s
                      Time elapsed: 00:57:29
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 49412 steps/s (collection: 1.901s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 349.6082
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.6206
                       Mean reward: 651.80
               Mean episode length: 214.31
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 125.2857
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.99s
                      Time elapsed: 00:57:31
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.917s, learning 0.110s)
             Mean action noise std: 2.14
          Mean value_function loss: 304.0721
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.6242
                       Mean reward: 646.54
               Mean episode length: 214.27
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 125.3428
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.03s
                      Time elapsed: 00:57:33
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 49276 steps/s (collection: 1.904s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 267.0350
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 37.6268
                       Mean reward: 662.82
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 0.7981
     Episode_Reward/lifting_object: 130.6495
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.99s
                      Time elapsed: 00:57:35
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 48245 steps/s (collection: 1.923s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 230.6186
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 37.6278
                       Mean reward: 658.07
               Mean episode length: 214.10
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 130.5079
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.04s
                      Time elapsed: 00:57:37
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 49343 steps/s (collection: 1.887s, learning 0.105s)
             Mean action noise std: 2.14
          Mean value_function loss: 325.8893
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.6291
                       Mean reward: 652.47
               Mean episode length: 213.28
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 128.2779
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.99s
                      Time elapsed: 00:57:39
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 48612 steps/s (collection: 1.916s, learning 0.106s)
             Mean action noise std: 2.14
          Mean value_function loss: 343.7343
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.6338
                       Mean reward: 693.79
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 0.8033
     Episode_Reward/lifting_object: 132.6636
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.02s
                      Time elapsed: 00:57:41
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 49083 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 305.9711
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.6406
                       Mean reward: 647.53
               Mean episode length: 214.93
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 125.1737
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.00s
                      Time elapsed: 00:57:43
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 49327 steps/s (collection: 1.899s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 300.9344
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.6445
                       Mean reward: 630.23
               Mean episode length: 207.02
    Episode_Reward/reaching_object: 0.7908
     Episode_Reward/lifting_object: 130.5091
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.99s
                      Time elapsed: 00:57:45
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 49647 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 269.7079
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.6499
                       Mean reward: 678.71
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 0.8002
     Episode_Reward/lifting_object: 131.8056
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.98s
                      Time elapsed: 00:57:47
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 49904 steps/s (collection: 1.867s, learning 0.103s)
             Mean action noise std: 2.15
          Mean value_function loss: 342.0638
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.6563
                       Mean reward: 679.99
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 125.3988
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 1.97s
                      Time elapsed: 00:57:49
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 48672 steps/s (collection: 1.925s, learning 0.095s)
             Mean action noise std: 2.15
          Mean value_function loss: 316.8553
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.6623
                       Mean reward: 594.83
               Mean episode length: 197.10
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 127.7038
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.02s
                      Time elapsed: 00:57:51
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 49055 steps/s (collection: 1.907s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 342.8660
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.6688
                       Mean reward: 689.06
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 0.8021
     Episode_Reward/lifting_object: 132.0747
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.00s
                      Time elapsed: 00:57:53
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 48943 steps/s (collection: 1.912s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 318.5900
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.6729
                       Mean reward: 667.71
               Mean episode length: 213.85
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 130.2430
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.01s
                      Time elapsed: 00:57:55
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 49013 steps/s (collection: 1.909s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 284.5372
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.6771
                       Mean reward: 650.28
               Mean episode length: 210.10
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 127.4180
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.01s
                      Time elapsed: 00:57:57
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 49234 steps/s (collection: 1.899s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 341.4655
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.6823
                       Mean reward: 689.27
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 127.8240
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.00s
                      Time elapsed: 00:57:59
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 49316 steps/s (collection: 1.892s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 295.0839
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.6860
                       Mean reward: 671.55
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 131.0335
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.99s
                      Time elapsed: 00:58:01
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 49333 steps/s (collection: 1.901s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 252.3180
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 37.6855
                       Mean reward: 635.86
               Mean episode length: 206.90
    Episode_Reward/reaching_object: 0.8033
     Episode_Reward/lifting_object: 133.0694
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.99s
                      Time elapsed: 00:58:03
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 48634 steps/s (collection: 1.925s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 249.4070
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.6853
                       Mean reward: 682.86
               Mean episode length: 217.75
    Episode_Reward/reaching_object: 0.8021
     Episode_Reward/lifting_object: 132.7834
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.02s
                      Time elapsed: 00:58:05
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 49231 steps/s (collection: 1.888s, learning 0.109s)
             Mean action noise std: 2.15
          Mean value_function loss: 339.4787
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.6844
                       Mean reward: 640.69
               Mean episode length: 210.64
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 129.5242
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.00s
                      Time elapsed: 00:58:07
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 48775 steps/s (collection: 1.901s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 341.7905
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.6848
                       Mean reward: 678.38
               Mean episode length: 218.52
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 130.0867
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.02s
                      Time elapsed: 00:58:09
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 49039 steps/s (collection: 1.894s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 369.9221
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.6870
                       Mean reward: 645.20
               Mean episode length: 205.94
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 130.0619
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.00s
                      Time elapsed: 00:58:11
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 49093 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 273.9584
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 37.6907
                       Mean reward: 673.65
               Mean episode length: 218.08
    Episode_Reward/reaching_object: 0.7968
     Episode_Reward/lifting_object: 132.2874
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.00s
                      Time elapsed: 00:58:13
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 48993 steps/s (collection: 1.915s, learning 0.091s)
             Mean action noise std: 2.15
          Mean value_function loss: 266.3952
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.6921
                       Mean reward: 666.02
               Mean episode length: 215.20
    Episode_Reward/reaching_object: 0.7942
     Episode_Reward/lifting_object: 132.7155
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.01s
                      Time elapsed: 00:58:15
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 48847 steps/s (collection: 1.919s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 357.6308
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.6949
                       Mean reward: 647.37
               Mean episode length: 211.57
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 128.0364
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.01s
                      Time elapsed: 00:58:17
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 48898 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 423.3897
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.6996
                       Mean reward: 638.90
               Mean episode length: 206.82
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 126.3873
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.01s
                      Time elapsed: 00:58:19
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 49309 steps/s (collection: 1.905s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 380.0085
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.7047
                       Mean reward: 587.66
               Mean episode length: 194.04
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 126.2445
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.99s
                      Time elapsed: 00:58:21
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 48664 steps/s (collection: 1.924s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 320.8774
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.7086
                       Mean reward: 671.86
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 0.7973
     Episode_Reward/lifting_object: 134.0650
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.02s
                      Time elapsed: 00:58:23
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 44674 steps/s (collection: 2.087s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 320.0740
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.7119
                       Mean reward: 659.58
               Mean episode length: 210.86
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 127.9794
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.20s
                      Time elapsed: 00:58:25
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 46965 steps/s (collection: 1.991s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 344.8512
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.7122
                       Mean reward: 650.47
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 0.8137
     Episode_Reward/lifting_object: 137.6552
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.09s
                      Time elapsed: 00:58:27
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 42830 steps/s (collection: 2.146s, learning 0.149s)
             Mean action noise std: 2.16
          Mean value_function loss: 346.0314
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 37.7133
                       Mean reward: 653.24
               Mean episode length: 209.80
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 132.6848
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.30s
                      Time elapsed: 00:58:30
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 37879 steps/s (collection: 2.453s, learning 0.142s)
             Mean action noise std: 2.16
          Mean value_function loss: 334.4997
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.7167
                       Mean reward: 711.45
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 0.8001
     Episode_Reward/lifting_object: 136.2055
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.60s
                      Time elapsed: 00:58:32
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 44745 steps/s (collection: 2.077s, learning 0.120s)
             Mean action noise std: 2.16
          Mean value_function loss: 313.2583
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.7204
                       Mean reward: 691.59
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 132.4402
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.20s
                      Time elapsed: 00:58:34
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 45115 steps/s (collection: 2.076s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 383.2562
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.7237
                       Mean reward: 632.57
               Mean episode length: 200.67
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 132.8053
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.18s
                      Time elapsed: 00:58:37
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 47946 steps/s (collection: 1.937s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 365.8956
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.7299
                       Mean reward: 645.24
               Mean episode length: 207.59
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 129.8342
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.05s
                      Time elapsed: 00:58:39
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 47102 steps/s (collection: 1.969s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 275.8735
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.7352
                       Mean reward: 646.03
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 0.7992
     Episode_Reward/lifting_object: 136.8614
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.09s
                      Time elapsed: 00:58:41
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 47718 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 332.1024
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.7409
                       Mean reward: 639.66
               Mean episode length: 201.46
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 134.9816
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.06s
                      Time elapsed: 00:58:43
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 44116 steps/s (collection: 2.103s, learning 0.125s)
             Mean action noise std: 2.16
          Mean value_function loss: 301.0779
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.7456
                       Mean reward: 723.16
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 0.7997
     Episode_Reward/lifting_object: 136.5091
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.23s
                      Time elapsed: 00:58:45
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 47377 steps/s (collection: 1.976s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 306.6104
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 37.7500
                       Mean reward: 699.11
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 0.7949
     Episode_Reward/lifting_object: 135.5965
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.07s
                      Time elapsed: 00:58:47
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 46627 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 310.0552
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.7516
                       Mean reward: 673.99
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 132.8771
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.11s
                      Time elapsed: 00:58:49
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 47866 steps/s (collection: 1.950s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 327.7579
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.7532
                       Mean reward: 685.34
               Mean episode length: 216.35
    Episode_Reward/reaching_object: 0.8066
     Episode_Reward/lifting_object: 136.5151
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.05s
                      Time elapsed: 00:58:51
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 48263 steps/s (collection: 1.931s, learning 0.106s)
             Mean action noise std: 2.17
          Mean value_function loss: 272.7007
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.7573
                       Mean reward: 674.44
               Mean episode length: 211.60
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 132.7292
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.04s
                      Time elapsed: 00:58:53
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 48126 steps/s (collection: 1.942s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 303.1191
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.7606
                       Mean reward: 711.56
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 0.7981
     Episode_Reward/lifting_object: 135.9484
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.04s
                      Time elapsed: 00:58:55
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 50162 steps/s (collection: 1.869s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 300.5103
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.7624
                       Mean reward: 687.82
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 0.8124
     Episode_Reward/lifting_object: 138.6489
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 1.96s
                      Time elapsed: 00:58:57
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 49546 steps/s (collection: 1.895s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 276.3308
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.7654
                       Mean reward: 689.06
               Mean episode length: 216.52
    Episode_Reward/reaching_object: 0.7964
     Episode_Reward/lifting_object: 134.8083
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 1.98s
                      Time elapsed: 00:58:59
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.910s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 343.1571
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.7671
                       Mean reward: 680.34
               Mean episode length: 211.81
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 137.2943
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.00s
                      Time elapsed: 00:59:01
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 48299 steps/s (collection: 1.938s, learning 0.097s)
             Mean action noise std: 2.17
          Mean value_function loss: 307.2268
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.7714
                       Mean reward: 665.45
               Mean episode length: 209.91
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: 134.6555
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.04s
                      Time elapsed: 00:59:03
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 49285 steps/s (collection: 1.902s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 272.4726
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.7729
                       Mean reward: 698.05
               Mean episode length: 217.79
    Episode_Reward/reaching_object: 0.8137
     Episode_Reward/lifting_object: 139.6049
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 1.99s
                      Time elapsed: 00:59:05
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 49070 steps/s (collection: 1.909s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 287.4118
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.7764
                       Mean reward: 685.08
               Mean episode length: 213.28
    Episode_Reward/reaching_object: 0.8006
     Episode_Reward/lifting_object: 137.4593
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.00s
                      Time elapsed: 00:59:07
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 48901 steps/s (collection: 1.896s, learning 0.115s)
             Mean action noise std: 2.17
          Mean value_function loss: 306.1390
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.7823
                       Mean reward: 713.03
               Mean episode length: 221.06
    Episode_Reward/reaching_object: 0.8059
     Episode_Reward/lifting_object: 138.4359
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.01s
                      Time elapsed: 00:59:09
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 48098 steps/s (collection: 1.925s, learning 0.119s)
             Mean action noise std: 2.17
          Mean value_function loss: 313.5348
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.7859
                       Mean reward: 724.66
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 0.7928
     Episode_Reward/lifting_object: 136.9903
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.04s
                      Time elapsed: 00:59:11
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 49217 steps/s (collection: 1.877s, learning 0.121s)
             Mean action noise std: 2.17
          Mean value_function loss: 264.1439
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.7900
                       Mean reward: 721.64
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 0.8109
     Episode_Reward/lifting_object: 140.4567
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.00s
                      Time elapsed: 00:59:13
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 49300 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 296.7170
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7925
                       Mean reward: 689.48
               Mean episode length: 215.92
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 135.1733
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 1.99s
                      Time elapsed: 00:59:15
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 49285 steps/s (collection: 1.896s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 281.2080
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.7944
                       Mean reward: 701.30
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 0.8013
     Episode_Reward/lifting_object: 139.0741
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 1.99s
                      Time elapsed: 00:59:17
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 48787 steps/s (collection: 1.917s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 355.8083
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 37.7953
                       Mean reward: 710.21
               Mean episode length: 217.65
    Episode_Reward/reaching_object: 0.8044
     Episode_Reward/lifting_object: 139.2570
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.01s
                      Time elapsed: 00:59:19
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 49496 steps/s (collection: 1.885s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 354.5721
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.7986
                       Mean reward: 710.01
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 0.7943
     Episode_Reward/lifting_object: 138.3676
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 1.99s
                      Time elapsed: 00:59:21
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 48822 steps/s (collection: 1.911s, learning 0.102s)
             Mean action noise std: 2.17
          Mean value_function loss: 352.1906
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.8034
                       Mean reward: 726.61
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 136.5009
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.01s
                      Time elapsed: 00:59:23
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 48929 steps/s (collection: 1.920s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 328.2067
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.8092
                       Mean reward: 704.94
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 0.8115
     Episode_Reward/lifting_object: 140.5499
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.01s
                      Time elapsed: 00:59:25
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 49930 steps/s (collection: 1.879s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 313.7788
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 37.8157
                       Mean reward: 699.51
               Mean episode length: 214.26
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 135.9650
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.97s
                      Time elapsed: 00:59:27
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 48657 steps/s (collection: 1.923s, learning 0.097s)
             Mean action noise std: 2.18
          Mean value_function loss: 332.8047
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.8174
                       Mean reward: 685.14
               Mean episode length: 211.94
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 134.1436
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.02s
                      Time elapsed: 00:59:29
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 49743 steps/s (collection: 1.880s, learning 0.096s)
             Mean action noise std: 2.18
          Mean value_function loss: 303.7549
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.8184
                       Mean reward: 678.40
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 0.7927
     Episode_Reward/lifting_object: 138.2833
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.98s
                      Time elapsed: 00:59:31
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 49085 steps/s (collection: 1.914s, learning 0.088s)
             Mean action noise std: 2.18
          Mean value_function loss: 286.3419
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.8204
                       Mean reward: 710.35
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 0.8029
     Episode_Reward/lifting_object: 139.8747
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.00s
                      Time elapsed: 00:59:33
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 48454 steps/s (collection: 1.938s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 251.4549
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 37.8227
                       Mean reward: 730.16
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 0.8079
     Episode_Reward/lifting_object: 142.0825
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.03s
                      Time elapsed: 00:59:35
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 49823 steps/s (collection: 1.883s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 280.1842
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.8241
                       Mean reward: 733.90
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 0.8006
     Episode_Reward/lifting_object: 140.2193
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 1.97s
                      Time elapsed: 00:59:37
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 49368 steps/s (collection: 1.880s, learning 0.111s)
             Mean action noise std: 2.18
          Mean value_function loss: 306.5232
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.8275
                       Mean reward: 700.21
               Mean episode length: 213.13
    Episode_Reward/reaching_object: 0.7934
     Episode_Reward/lifting_object: 140.0618
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 1.99s
                      Time elapsed: 00:59:39
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 49199 steps/s (collection: 1.882s, learning 0.116s)
             Mean action noise std: 2.18
          Mean value_function loss: 317.2876
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.8304
                       Mean reward: 724.81
               Mean episode length: 221.79
    Episode_Reward/reaching_object: 0.8056
     Episode_Reward/lifting_object: 141.1683
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.00s
                      Time elapsed: 00:59:41
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 48873 steps/s (collection: 1.899s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 271.2213
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.8327
                       Mean reward: 705.39
               Mean episode length: 216.67
    Episode_Reward/reaching_object: 0.8217
     Episode_Reward/lifting_object: 143.3555
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.01s
                      Time elapsed: 00:59:43
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 49928 steps/s (collection: 1.877s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 345.4838
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.8367
                       Mean reward: 718.09
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 0.8049
     Episode_Reward/lifting_object: 141.1865
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 1.97s
                      Time elapsed: 00:59:45
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 49460 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 2.18
          Mean value_function loss: 304.3092
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.8413
                       Mean reward: 723.55
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 0.8047
     Episode_Reward/lifting_object: 139.2618
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 1.99s
                      Time elapsed: 00:59:47
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 49827 steps/s (collection: 1.879s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 319.6888
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.8442
                       Mean reward: 718.60
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 0.8011
     Episode_Reward/lifting_object: 140.1022
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 1.97s
                      Time elapsed: 00:59:49
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 49368 steps/s (collection: 1.898s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 308.3932
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.8475
                       Mean reward: 694.62
               Mean episode length: 211.28
    Episode_Reward/reaching_object: 0.8004
     Episode_Reward/lifting_object: 140.8824
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.99s
                      Time elapsed: 00:59:51
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 49932 steps/s (collection: 1.877s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 325.0820
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.8521
                       Mean reward: 721.88
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 140.6103
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 1.97s
                      Time elapsed: 00:59:53
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 50216 steps/s (collection: 1.867s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 292.5558
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.8564
                       Mean reward: 676.61
               Mean episode length: 206.79
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 137.3654
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 1.96s
                      Time elapsed: 00:59:55
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 49735 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 289.1542
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.8601
                       Mean reward: 722.65
               Mean episode length: 215.87
    Episode_Reward/reaching_object: 0.8183
     Episode_Reward/lifting_object: 146.4966
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 1.98s
                      Time elapsed: 00:59:57
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 49994 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 268.3798
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.8671
                       Mean reward: 714.41
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 0.7876
     Episode_Reward/lifting_object: 139.8069
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 1.97s
                      Time elapsed: 00:59:59
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 49918 steps/s (collection: 1.879s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 334.4437
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.8683
                       Mean reward: 746.07
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 0.8239
     Episode_Reward/lifting_object: 147.3984
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 1.97s
                      Time elapsed: 01:00:01
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 50141 steps/s (collection: 1.871s, learning 0.089s)
             Mean action noise std: 2.19
          Mean value_function loss: 346.2045
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.8696
                       Mean reward: 701.18
               Mean episode length: 209.97
    Episode_Reward/reaching_object: 0.7938
     Episode_Reward/lifting_object: 139.5925
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 1.96s
                      Time elapsed: 01:00:03
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 48992 steps/s (collection: 1.904s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 271.8947
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.8735
                       Mean reward: 699.98
               Mean episode length: 210.55
    Episode_Reward/reaching_object: 0.8055
     Episode_Reward/lifting_object: 142.5263
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.01s
                      Time elapsed: 01:00:05
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 49470 steps/s (collection: 1.885s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 233.1869
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.8755
                       Mean reward: 711.01
               Mean episode length: 211.14
    Episode_Reward/reaching_object: 0.8326
     Episode_Reward/lifting_object: 148.9620
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 1.99s
                      Time elapsed: 01:00:07
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 49532 steps/s (collection: 1.875s, learning 0.110s)
             Mean action noise std: 2.19
          Mean value_function loss: 305.7650
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 37.8752
                       Mean reward: 721.39
               Mean episode length: 214.54
    Episode_Reward/reaching_object: 0.7920
     Episode_Reward/lifting_object: 141.5761
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 1.98s
                      Time elapsed: 01:00:09
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 49457 steps/s (collection: 1.874s, learning 0.114s)
             Mean action noise std: 2.19
          Mean value_function loss: 304.3192
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.8769
                       Mean reward: 752.54
               Mean episode length: 222.44
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 146.2685
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 1.99s
                      Time elapsed: 01:00:11
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 49339 steps/s (collection: 1.883s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 289.2411
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.8799
                       Mean reward: 732.54
               Mean episode length: 218.30
    Episode_Reward/reaching_object: 0.8005
     Episode_Reward/lifting_object: 143.4970
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 1.99s
                      Time elapsed: 01:00:13
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 49627 steps/s (collection: 1.886s, learning 0.095s)
             Mean action noise std: 2.19
          Mean value_function loss: 270.1339
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.8866
                       Mean reward: 709.91
               Mean episode length: 211.86
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: 144.4467
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 1.98s
                      Time elapsed: 01:00:15
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 49931 steps/s (collection: 1.873s, learning 0.096s)
             Mean action noise std: 2.19
          Mean value_function loss: 273.7407
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.8945
                       Mean reward: 727.62
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 0.8241
     Episode_Reward/lifting_object: 147.8927
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 1.97s
                      Time elapsed: 01:00:17
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 49439 steps/s (collection: 1.900s, learning 0.089s)
             Mean action noise std: 2.19
          Mean value_function loss: 273.6530
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 37.8966
                       Mean reward: 740.69
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 0.7985
     Episode_Reward/lifting_object: 143.2805
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.99s
                      Time elapsed: 01:00:19
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 48754 steps/s (collection: 1.923s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 302.4150
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.8986
                       Mean reward: 696.92
               Mean episode length: 208.27
    Episode_Reward/reaching_object: 0.7974
     Episode_Reward/lifting_object: 142.5954
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.02s
                      Time elapsed: 01:00:21
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 49077 steps/s (collection: 1.912s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 266.0349
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.9053
                       Mean reward: 767.60
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 0.8107
     Episode_Reward/lifting_object: 144.2638
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.00s
                      Time elapsed: 01:00:23
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 48701 steps/s (collection: 1.922s, learning 0.096s)
             Mean action noise std: 2.19
          Mean value_function loss: 259.9516
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.9083
                       Mean reward: 714.89
               Mean episode length: 213.04
    Episode_Reward/reaching_object: 0.8175
     Episode_Reward/lifting_object: 147.5004
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.02s
                      Time elapsed: 01:00:25
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 49004 steps/s (collection: 1.916s, learning 0.090s)
             Mean action noise std: 2.19
          Mean value_function loss: 302.8203
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.9090
                       Mean reward: 708.52
               Mean episode length: 212.01
    Episode_Reward/reaching_object: 0.8170
     Episode_Reward/lifting_object: 147.5525
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.01s
                      Time elapsed: 01:00:27
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 49516 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 288.5468
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.9106
                       Mean reward: 731.28
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 0.8179
     Episode_Reward/lifting_object: 146.8210
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 1.99s
                      Time elapsed: 01:00:29
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 49072 steps/s (collection: 1.898s, learning 0.106s)
             Mean action noise std: 2.19
          Mean value_function loss: 261.4313
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.9120
                       Mean reward: 733.85
               Mean episode length: 219.67
    Episode_Reward/reaching_object: 0.8201
     Episode_Reward/lifting_object: 148.1676
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.00s
                      Time elapsed: 01:00:31
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 48937 steps/s (collection: 1.899s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 245.9013
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.9132
                       Mean reward: 704.71
               Mean episode length: 211.05
    Episode_Reward/reaching_object: 0.8183
     Episode_Reward/lifting_object: 147.1158
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.01s
                      Time elapsed: 01:00:33
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 48409 steps/s (collection: 1.929s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 264.7229
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 37.9188
                       Mean reward: 731.68
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 146.4730
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.03s
                      Time elapsed: 01:00:35
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 47412 steps/s (collection: 1.963s, learning 0.110s)
             Mean action noise std: 2.20
          Mean value_function loss: 348.2392
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.9245
                       Mean reward: 721.63
               Mean episode length: 213.05
    Episode_Reward/reaching_object: 0.8163
     Episode_Reward/lifting_object: 146.7307
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.07s
                      Time elapsed: 01:00:37
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 48865 steps/s (collection: 1.898s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 319.1134
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.9337
                       Mean reward: 729.19
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 0.7965
     Episode_Reward/lifting_object: 142.5292
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.01s
                      Time elapsed: 01:00:39
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 48175 steps/s (collection: 1.930s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 288.8486
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.9382
                       Mean reward: 739.08
               Mean episode length: 217.84
    Episode_Reward/reaching_object: 0.8127
     Episode_Reward/lifting_object: 147.2656
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.04s
                      Time elapsed: 01:00:41
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 48498 steps/s (collection: 1.919s, learning 0.108s)
             Mean action noise std: 2.20
          Mean value_function loss: 295.1592
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 37.9426
                       Mean reward: 714.91
               Mean episode length: 206.38
    Episode_Reward/reaching_object: 0.8069
     Episode_Reward/lifting_object: 147.1236
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.03s
                      Time elapsed: 01:00:43
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 49009 steps/s (collection: 1.900s, learning 0.106s)
             Mean action noise std: 2.20
          Mean value_function loss: 275.9700
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.9457
                       Mean reward: 719.59
               Mean episode length: 213.01
    Episode_Reward/reaching_object: 0.8152
     Episode_Reward/lifting_object: 147.6395
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.01s
                      Time elapsed: 01:00:45
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 48245 steps/s (collection: 1.927s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 282.7017
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.9499
                       Mean reward: 751.56
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 0.8159
     Episode_Reward/lifting_object: 147.9430
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.04s
                      Time elapsed: 01:00:47
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 47307 steps/s (collection: 1.971s, learning 0.107s)
             Mean action noise std: 2.20
          Mean value_function loss: 294.3769
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 37.9567
                       Mean reward: 744.16
               Mean episode length: 214.84
    Episode_Reward/reaching_object: 0.8142
     Episode_Reward/lifting_object: 148.7056
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.08s
                      Time elapsed: 01:00:49
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 47504 steps/s (collection: 1.966s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 325.4581
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.9601
                       Mean reward: 745.51
               Mean episode length: 217.20
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 142.0232
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.07s
                      Time elapsed: 01:00:51
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 46936 steps/s (collection: 1.992s, learning 0.102s)
             Mean action noise std: 2.20
          Mean value_function loss: 320.2430
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9679
                       Mean reward: 730.82
               Mean episode length: 211.82
    Episode_Reward/reaching_object: 0.7976
     Episode_Reward/lifting_object: 144.9319
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.09s
                      Time elapsed: 01:00:53
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 46771 steps/s (collection: 1.994s, learning 0.108s)
             Mean action noise std: 2.20
          Mean value_function loss: 366.7780
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.9759
                       Mean reward: 709.17
               Mean episode length: 210.22
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 141.0125
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.10s
                      Time elapsed: 01:00:56
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 47961 steps/s (collection: 1.948s, learning 0.102s)
             Mean action noise std: 2.20
          Mean value_function loss: 325.8929
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.9805
                       Mean reward: 745.59
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 0.8035
     Episode_Reward/lifting_object: 146.4435
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.05s
                      Time elapsed: 01:00:58
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 48177 steps/s (collection: 1.929s, learning 0.112s)
             Mean action noise std: 2.20
          Mean value_function loss: 282.3232
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.9843
                       Mean reward: 745.02
               Mean episode length: 214.83
    Episode_Reward/reaching_object: 0.8107
     Episode_Reward/lifting_object: 148.1756
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.04s
                      Time elapsed: 01:01:00
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 48848 steps/s (collection: 1.915s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 285.5422
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.9867
                       Mean reward: 691.92
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 0.8032
     Episode_Reward/lifting_object: 146.2114
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.01s
                      Time elapsed: 01:01:02
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 48749 steps/s (collection: 1.920s, learning 0.097s)
             Mean action noise std: 2.21
          Mean value_function loss: 339.4623
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.9924
                       Mean reward: 751.26
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 0.8007
     Episode_Reward/lifting_object: 146.2308
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.02s
                      Time elapsed: 01:01:04
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 48459 steps/s (collection: 1.928s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 301.8206
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.0005
                       Mean reward: 698.71
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 141.7164
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.03s
                      Time elapsed: 01:01:06
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 48725 steps/s (collection: 1.914s, learning 0.104s)
             Mean action noise std: 2.21
          Mean value_function loss: 289.6522
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.0029
                       Mean reward: 731.98
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 145.0746
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.02s
                      Time elapsed: 01:01:08
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 48113 steps/s (collection: 1.934s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 301.1801
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.0021
                       Mean reward: 697.70
               Mean episode length: 206.97
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 143.5399
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.04s
                      Time elapsed: 01:01:10
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 48589 steps/s (collection: 1.918s, learning 0.105s)
             Mean action noise std: 2.21
          Mean value_function loss: 323.4044
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.0052
                       Mean reward: 771.03
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 0.8226
     Episode_Reward/lifting_object: 150.9205
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.02s
                      Time elapsed: 01:01:12
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 48389 steps/s (collection: 1.913s, learning 0.119s)
             Mean action noise std: 2.21
          Mean value_function loss: 327.5189
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.0121
                       Mean reward: 718.25
               Mean episode length: 208.02
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 142.8655
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.03s
                      Time elapsed: 01:01:14
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 47747 steps/s (collection: 1.956s, learning 0.103s)
             Mean action noise std: 2.21
          Mean value_function loss: 334.1306
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.0208
                       Mean reward: 741.66
               Mean episode length: 215.72
    Episode_Reward/reaching_object: 0.8045
     Episode_Reward/lifting_object: 147.8511
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.06s
                      Time elapsed: 01:01:16
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 48241 steps/s (collection: 1.940s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 308.2126
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.0305
                       Mean reward: 734.79
               Mean episode length: 210.45
    Episode_Reward/reaching_object: 0.8143
     Episode_Reward/lifting_object: 150.3761
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.04s
                      Time elapsed: 01:01:18
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 47287 steps/s (collection: 1.959s, learning 0.120s)
             Mean action noise std: 2.21
          Mean value_function loss: 264.8908
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 38.0363
                       Mean reward: 738.65
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 0.8182
     Episode_Reward/lifting_object: 151.1406
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.08s
                      Time elapsed: 01:01:20
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 48863 steps/s (collection: 1.915s, learning 0.097s)
             Mean action noise std: 2.21
          Mean value_function loss: 278.2685
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.0410
                       Mean reward: 710.22
               Mean episode length: 206.67
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 142.6903
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.01s
                      Time elapsed: 01:01:22
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 47376 steps/s (collection: 1.977s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 297.9842
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.0464
                       Mean reward: 765.16
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 0.8167
     Episode_Reward/lifting_object: 152.2390
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.07s
                      Time elapsed: 01:01:24
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 47765 steps/s (collection: 1.958s, learning 0.100s)
             Mean action noise std: 2.22
          Mean value_function loss: 282.1778
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.0547
                       Mean reward: 740.44
               Mean episode length: 214.09
    Episode_Reward/reaching_object: 0.8016
     Episode_Reward/lifting_object: 148.8814
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.06s
                      Time elapsed: 01:01:26
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 48534 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 2.22
          Mean value_function loss: 287.0310
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 38.0614
                       Mean reward: 778.84
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 0.8066
     Episode_Reward/lifting_object: 149.7754
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.03s
                      Time elapsed: 01:01:28
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 47827 steps/s (collection: 1.964s, learning 0.091s)
             Mean action noise std: 2.22
          Mean value_function loss: 276.8666
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.0660
                       Mean reward: 747.88
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 0.7997
     Episode_Reward/lifting_object: 148.5185
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.06s
                      Time elapsed: 01:01:30
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 48070 steps/s (collection: 1.950s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 301.7722
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.0711
                       Mean reward: 745.84
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 148.5676
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.05s
                      Time elapsed: 01:01:32
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 47479 steps/s (collection: 1.971s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 326.0912
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.0779
                       Mean reward: 715.25
               Mean episode length: 208.44
    Episode_Reward/reaching_object: 0.7892
     Episode_Reward/lifting_object: 145.1160
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.07s
                      Time elapsed: 01:01:34
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 48165 steps/s (collection: 1.944s, learning 0.097s)
             Mean action noise std: 2.22
          Mean value_function loss: 271.8625
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 38.0858
                       Mean reward: 791.52
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 153.9548
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.04s
                      Time elapsed: 01:01:36
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 47696 steps/s (collection: 1.966s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 399.7798
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.0889
                       Mean reward: 735.68
               Mean episode length: 211.21
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 147.9244
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.06s
                      Time elapsed: 01:01:38
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 47661 steps/s (collection: 1.964s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 296.4249
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.0927
                       Mean reward: 711.35
               Mean episode length: 209.78
    Episode_Reward/reaching_object: 0.8251
     Episode_Reward/lifting_object: 151.5448
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.06s
                      Time elapsed: 01:01:40
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 48190 steps/s (collection: 1.934s, learning 0.106s)
             Mean action noise std: 2.22
          Mean value_function loss: 323.4569
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.1027
                       Mean reward: 740.86
               Mean episode length: 215.28
    Episode_Reward/reaching_object: 0.8078
     Episode_Reward/lifting_object: 149.7068
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.04s
                      Time elapsed: 01:01:42
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 48440 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 2.22
          Mean value_function loss: 352.7684
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 38.1129
                       Mean reward: 689.52
               Mean episode length: 203.65
    Episode_Reward/reaching_object: 0.7938
     Episode_Reward/lifting_object: 146.6671
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.03s
                      Time elapsed: 01:01:45
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 46976 steps/s (collection: 1.976s, learning 0.117s)
             Mean action noise std: 2.23
          Mean value_function loss: 332.5817
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.1191
                       Mean reward: 768.74
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 0.8233
     Episode_Reward/lifting_object: 153.0070
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.09s
                      Time elapsed: 01:01:47
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 47216 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 272.1719
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.1240
                       Mean reward: 763.03
               Mean episode length: 219.53
    Episode_Reward/reaching_object: 0.8224
     Episode_Reward/lifting_object: 152.5641
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.08s
                      Time elapsed: 01:01:49
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 47664 steps/s (collection: 1.957s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 300.3547
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.1269
                       Mean reward: 793.04
               Mean episode length: 225.59
    Episode_Reward/reaching_object: 0.8201
     Episode_Reward/lifting_object: 153.1526
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.06s
                      Time elapsed: 01:01:51
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 48141 steps/s (collection: 1.940s, learning 0.102s)
             Mean action noise std: 2.23
          Mean value_function loss: 273.4748
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 38.1308
                       Mean reward: 777.09
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 0.8040
     Episode_Reward/lifting_object: 149.6614
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.04s
                      Time elapsed: 01:01:53
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.947s, learning 0.103s)
             Mean action noise std: 2.23
          Mean value_function loss: 298.2870
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.1319
                       Mean reward: 762.28
               Mean episode length: 217.40
    Episode_Reward/reaching_object: 0.8122
     Episode_Reward/lifting_object: 152.1809
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.05s
                      Time elapsed: 01:01:55
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 48024 steps/s (collection: 1.936s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 335.7272
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.1358
                       Mean reward: 728.13
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 0.7960
     Episode_Reward/lifting_object: 147.8780
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.05s
                      Time elapsed: 01:01:57
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 2.23
          Mean value_function loss: 262.1874
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.1386
                       Mean reward: 766.26
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: 153.3144
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.02s
                      Time elapsed: 01:01:59
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 48198 steps/s (collection: 1.929s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 275.5292
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.1414
                       Mean reward: 775.55
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 0.8192
     Episode_Reward/lifting_object: 153.1834
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.04s
                      Time elapsed: 01:02:01
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 46439 steps/s (collection: 1.995s, learning 0.122s)
             Mean action noise std: 2.23
          Mean value_function loss: 307.8859
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.1492
                       Mean reward: 769.55
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 0.8027
     Episode_Reward/lifting_object: 150.6115
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.12s
                      Time elapsed: 01:02:03
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 47890 steps/s (collection: 1.939s, learning 0.114s)
             Mean action noise std: 2.23
          Mean value_function loss: 241.2507
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 38.1553
                       Mean reward: 767.24
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 0.8266
     Episode_Reward/lifting_object: 154.5687
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.05s
                      Time elapsed: 01:02:05
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 48434 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 252.5687
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.1579
                       Mean reward: 760.24
               Mean episode length: 216.57
    Episode_Reward/reaching_object: 0.8173
     Episode_Reward/lifting_object: 151.7853
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.03s
                      Time elapsed: 01:02:07
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 48618 steps/s (collection: 1.911s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 316.6946
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.1580
                       Mean reward: 769.84
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 0.8211
     Episode_Reward/lifting_object: 152.5373
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.02s
                      Time elapsed: 01:02:09
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 47095 steps/s (collection: 1.975s, learning 0.113s)
             Mean action noise std: 2.23
          Mean value_function loss: 274.9464
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.1626
                       Mean reward: 765.14
               Mean episode length: 219.31
    Episode_Reward/reaching_object: 0.8253
     Episode_Reward/lifting_object: 153.2467
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.09s
                      Time elapsed: 01:02:11
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 48259 steps/s (collection: 1.929s, learning 0.108s)
             Mean action noise std: 2.23
          Mean value_function loss: 293.6448
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.1720
                       Mean reward: 715.49
               Mean episode length: 209.57
    Episode_Reward/reaching_object: 0.8021
     Episode_Reward/lifting_object: 148.2166
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.04s
                      Time elapsed: 01:02:13
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 47743 steps/s (collection: 1.950s, learning 0.109s)
             Mean action noise std: 2.24
          Mean value_function loss: 299.1987
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.1767
                       Mean reward: 720.27
               Mean episode length: 207.52
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 150.6525
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.06s
                      Time elapsed: 01:02:15
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 47971 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 288.2015
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.1836
                       Mean reward: 800.22
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 156.3978
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.05s
                      Time elapsed: 01:02:17
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 47646 steps/s (collection: 1.962s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 304.6261
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.1925
                       Mean reward: 729.12
               Mean episode length: 206.95
    Episode_Reward/reaching_object: 0.8091
     Episode_Reward/lifting_object: 149.6630
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.06s
                      Time elapsed: 01:02:19
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 48124 steps/s (collection: 1.940s, learning 0.102s)
             Mean action noise std: 2.24
          Mean value_function loss: 320.7198
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 38.1987
                       Mean reward: 774.16
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 0.8046
     Episode_Reward/lifting_object: 150.1403
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.04s
                      Time elapsed: 01:02:22
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 47504 steps/s (collection: 1.964s, learning 0.105s)
             Mean action noise std: 2.24
          Mean value_function loss: 258.8921
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.2007
                       Mean reward: 749.19
               Mean episode length: 216.08
    Episode_Reward/reaching_object: 0.7978
     Episode_Reward/lifting_object: 148.2724
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.07s
                      Time elapsed: 01:02:24
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 47730 steps/s (collection: 1.955s, learning 0.105s)
             Mean action noise std: 2.24
          Mean value_function loss: 319.4704
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.2024
                       Mean reward: 732.81
               Mean episode length: 209.75
    Episode_Reward/reaching_object: 0.8131
     Episode_Reward/lifting_object: 152.2341
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.06s
                      Time elapsed: 01:02:26
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 47397 steps/s (collection: 1.966s, learning 0.109s)
             Mean action noise std: 2.24
          Mean value_function loss: 271.8563
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.2090
                       Mean reward: 800.65
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 155.6412
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.07s
                      Time elapsed: 01:02:28
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 47476 steps/s (collection: 1.945s, learning 0.125s)
             Mean action noise std: 2.24
          Mean value_function loss: 309.0315
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 38.2138
                       Mean reward: 735.33
               Mean episode length: 211.43
    Episode_Reward/reaching_object: 0.8053
     Episode_Reward/lifting_object: 150.5653
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.07s
                      Time elapsed: 01:02:30
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 47198 steps/s (collection: 1.966s, learning 0.117s)
             Mean action noise std: 2.24
          Mean value_function loss: 276.7707
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.2177
                       Mean reward: 746.12
               Mean episode length: 212.67
    Episode_Reward/reaching_object: 0.8073
     Episode_Reward/lifting_object: 150.5515
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.08s
                      Time elapsed: 01:02:32
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 47763 steps/s (collection: 1.952s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 307.0948
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.2240
                       Mean reward: 757.35
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 0.8071
     Episode_Reward/lifting_object: 150.9162
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.06s
                      Time elapsed: 01:02:34
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 46847 steps/s (collection: 1.998s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 294.5850
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.2268
                       Mean reward: 809.73
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 0.8141
     Episode_Reward/lifting_object: 152.0968
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.10s
                      Time elapsed: 01:02:36
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 46873 steps/s (collection: 1.979s, learning 0.118s)
             Mean action noise std: 2.24
          Mean value_function loss: 301.6732
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.2290
                       Mean reward: 787.75
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 0.8101
     Episode_Reward/lifting_object: 150.9571
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.10s
                      Time elapsed: 01:02:38
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 45787 steps/s (collection: 2.018s, learning 0.129s)
             Mean action noise std: 2.24
          Mean value_function loss: 318.0024
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.2319
                       Mean reward: 732.36
               Mean episode length: 209.19
    Episode_Reward/reaching_object: 0.8009
     Episode_Reward/lifting_object: 149.4882
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.15s
                      Time elapsed: 01:02:40
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 47276 steps/s (collection: 1.974s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 313.2953
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.2382
                       Mean reward: 723.69
               Mean episode length: 207.42
    Episode_Reward/reaching_object: 0.7896
     Episode_Reward/lifting_object: 146.8424
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.08s
                      Time elapsed: 01:02:42
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 47357 steps/s (collection: 1.959s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 356.7072
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.2433
                       Mean reward: 689.95
               Mean episode length: 199.50
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 144.4267
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.08s
                      Time elapsed: 01:02:44
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 46854 steps/s (collection: 1.975s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 311.8006
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.2450
                       Mean reward: 743.57
               Mean episode length: 211.78
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 148.7447
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.10s
                      Time elapsed: 01:02:47
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 47608 steps/s (collection: 1.948s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 308.9783
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.2483
                       Mean reward: 736.09
               Mean episode length: 211.41
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 146.2070
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.06s
                      Time elapsed: 01:02:49
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 48470 steps/s (collection: 1.927s, learning 0.101s)
             Mean action noise std: 2.25
          Mean value_function loss: 294.7248
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.2487
                       Mean reward: 700.72
               Mean episode length: 205.17
    Episode_Reward/reaching_object: 0.8103
     Episode_Reward/lifting_object: 152.2001
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.03s
                      Time elapsed: 01:02:51
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 48007 steps/s (collection: 1.945s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 308.6030
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.2493
                       Mean reward: 766.66
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 151.9366
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.05s
                      Time elapsed: 01:02:53
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 41649 steps/s (collection: 2.190s, learning 0.171s)
             Mean action noise std: 2.25
          Mean value_function loss: 292.2837
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.2523
                       Mean reward: 738.72
               Mean episode length: 210.77
    Episode_Reward/reaching_object: 0.8065
     Episode_Reward/lifting_object: 150.7241
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.36s
                      Time elapsed: 01:02:55
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 39027 steps/s (collection: 2.353s, learning 0.166s)
             Mean action noise std: 2.25
          Mean value_function loss: 265.6302
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.2558
                       Mean reward: 744.74
               Mean episode length: 211.72
    Episode_Reward/reaching_object: 0.7996
     Episode_Reward/lifting_object: 150.5836
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.52s
                      Time elapsed: 01:02:58
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 43325 steps/s (collection: 2.115s, learning 0.154s)
             Mean action noise std: 2.25
          Mean value_function loss: 285.7862
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.2616
                       Mean reward: 756.36
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: 155.1135
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.27s
                      Time elapsed: 01:03:00
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 43558 steps/s (collection: 2.095s, learning 0.162s)
             Mean action noise std: 2.25
          Mean value_function loss: 301.0310
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.2698
                       Mean reward: 794.06
               Mean episode length: 224.18
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: 157.2251
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.26s
                      Time elapsed: 01:03:02
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 42088 steps/s (collection: 2.212s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 310.1779
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.2766
                       Mean reward: 781.25
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 0.8169
     Episode_Reward/lifting_object: 153.0071
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.34s
                      Time elapsed: 01:03:04
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 41226 steps/s (collection: 2.265s, learning 0.120s)
             Mean action noise std: 2.25
          Mean value_function loss: 314.0706
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.2823
                       Mean reward: 751.73
               Mean episode length: 215.42
    Episode_Reward/reaching_object: 0.8234
     Episode_Reward/lifting_object: 154.8088
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.38s
                      Time elapsed: 01:03:07
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 42771 steps/s (collection: 2.039s, learning 0.260s)
             Mean action noise std: 2.25
          Mean value_function loss: 305.6870
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.2875
                       Mean reward: 795.30
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 0.8246
     Episode_Reward/lifting_object: 155.6138
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.30s
                      Time elapsed: 01:03:09
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 44619 steps/s (collection: 2.029s, learning 0.174s)
             Mean action noise std: 2.25
          Mean value_function loss: 310.5278
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.2977
                       Mean reward: 749.67
               Mean episode length: 211.17
    Episode_Reward/reaching_object: 0.7883
     Episode_Reward/lifting_object: 147.8423
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.20s
                      Time elapsed: 01:03:11
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 42647 steps/s (collection: 2.206s, learning 0.099s)
             Mean action noise std: 2.26
          Mean value_function loss: 283.6917
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 38.3101
                       Mean reward: 804.09
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 157.7408
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.31s
                      Time elapsed: 01:03:14
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 44189 steps/s (collection: 2.044s, learning 0.181s)
             Mean action noise std: 2.26
          Mean value_function loss: 289.5964
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 38.3156
                       Mean reward: 731.82
               Mean episode length: 207.08
    Episode_Reward/reaching_object: 0.8066
     Episode_Reward/lifting_object: 151.9673
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.22s
                      Time elapsed: 01:03:16
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 41269 steps/s (collection: 2.233s, learning 0.149s)
             Mean action noise std: 2.26
          Mean value_function loss: 310.9334
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.3166
                       Mean reward: 785.98
               Mean episode length: 221.48
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: 153.1227
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.38s
                      Time elapsed: 01:03:18
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 40525 steps/s (collection: 2.309s, learning 0.117s)
             Mean action noise std: 2.26
          Mean value_function loss: 247.3358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.3215
                       Mean reward: 753.79
               Mean episode length: 212.37
    Episode_Reward/reaching_object: 0.8106
     Episode_Reward/lifting_object: 153.7138
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.43s
                      Time elapsed: 01:03:21
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 39146 steps/s (collection: 2.329s, learning 0.183s)
             Mean action noise std: 2.26
          Mean value_function loss: 261.0941
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 38.3300
                       Mean reward: 779.60
               Mean episode length: 219.14
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: 158.7397
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.51s
                      Time elapsed: 01:03:23
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 36564 steps/s (collection: 2.510s, learning 0.179s)
             Mean action noise std: 2.26
          Mean value_function loss: 277.3907
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.3350
                       Mean reward: 757.56
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 0.8157
     Episode_Reward/lifting_object: 156.0681
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.69s
                      Time elapsed: 01:03:26
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 38437 steps/s (collection: 2.341s, learning 0.217s)
             Mean action noise std: 2.26
          Mean value_function loss: 298.1525
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.3381
                       Mean reward: 747.75
               Mean episode length: 209.79
    Episode_Reward/reaching_object: 0.8071
     Episode_Reward/lifting_object: 152.8623
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.56s
                      Time elapsed: 01:03:28
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 37963 steps/s (collection: 2.332s, learning 0.257s)
             Mean action noise std: 2.26
          Mean value_function loss: 280.9820
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.3436
                       Mean reward: 804.48
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 156.5442
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.59s
                      Time elapsed: 01:03:31
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 37786 steps/s (collection: 2.419s, learning 0.183s)
             Mean action noise std: 2.26
          Mean value_function loss: 267.0058
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.3478
                       Mean reward: 832.73
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 0.8219
     Episode_Reward/lifting_object: 155.5163
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.60s
                      Time elapsed: 01:03:34
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 38050 steps/s (collection: 2.361s, learning 0.223s)
             Mean action noise std: 2.26
          Mean value_function loss: 286.4424
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.3506
                       Mean reward: 741.75
               Mean episode length: 210.26
    Episode_Reward/reaching_object: 0.8124
     Episode_Reward/lifting_object: 153.8929
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.58s
                      Time elapsed: 01:03:36
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 40720 steps/s (collection: 2.261s, learning 0.153s)
             Mean action noise std: 2.26
          Mean value_function loss: 249.1888
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.3540
                       Mean reward: 816.73
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 0.8280
     Episode_Reward/lifting_object: 156.7480
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.41s
                      Time elapsed: 01:03:39
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 42970 steps/s (collection: 2.116s, learning 0.172s)
             Mean action noise std: 2.26
          Mean value_function loss: 211.6664
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.3569
                       Mean reward: 805.19
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 0.8403
     Episode_Reward/lifting_object: 159.1011
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.29s
                      Time elapsed: 01:03:41
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 42084 steps/s (collection: 2.133s, learning 0.203s)
             Mean action noise std: 2.26
          Mean value_function loss: 230.0294
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.3606
                       Mean reward: 787.22
               Mean episode length: 222.03
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 159.1476
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.34s
                      Time elapsed: 01:03:43
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 42941 steps/s (collection: 2.125s, learning 0.164s)
             Mean action noise std: 2.26
          Mean value_function loss: 222.4592
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.3650
                       Mean reward: 792.97
               Mean episode length: 223.95
    Episode_Reward/reaching_object: 0.8241
     Episode_Reward/lifting_object: 155.5987
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.29s
                      Time elapsed: 01:03:45
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 42428 steps/s (collection: 2.115s, learning 0.202s)
             Mean action noise std: 2.27
          Mean value_function loss: 217.6707
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.3702
                       Mean reward: 788.54
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 161.0473
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.32s
                      Time elapsed: 01:03:48
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 45533 steps/s (collection: 2.005s, learning 0.154s)
             Mean action noise std: 2.27
          Mean value_function loss: 263.3673
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 38.3768
                       Mean reward: 807.84
               Mean episode length: 224.56
    Episode_Reward/reaching_object: 0.8256
     Episode_Reward/lifting_object: 156.6435
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.16s
                      Time elapsed: 01:03:50
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 44500 steps/s (collection: 2.109s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 291.6243
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.3782
                       Mean reward: 762.90
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 0.8168
     Episode_Reward/lifting_object: 154.4770
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.21s
                      Time elapsed: 01:03:52
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 45119 steps/s (collection: 2.064s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 245.6466
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.3809
                       Mean reward: 827.82
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.8461
     Episode_Reward/lifting_object: 159.1335
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.18s
                      Time elapsed: 01:03:54
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 46025 steps/s (collection: 2.033s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 225.5082
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 38.3905
                       Mean reward: 801.56
               Mean episode length: 222.66
    Episode_Reward/reaching_object: 0.8342
     Episode_Reward/lifting_object: 158.0667
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.14s
                      Time elapsed: 01:03:56
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 47154 steps/s (collection: 1.985s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 263.3234
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.3949
                       Mean reward: 796.49
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 158.5776
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.08s
                      Time elapsed: 01:03:59
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 48747 steps/s (collection: 1.918s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 265.4413
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 38.3983
                       Mean reward: 772.26
               Mean episode length: 218.13
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: 156.4928
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.02s
                      Time elapsed: 01:04:01
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 47660 steps/s (collection: 1.972s, learning 0.091s)
             Mean action noise std: 2.27
          Mean value_function loss: 216.3702
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.4030
                       Mean reward: 790.96
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 0.8316
     Episode_Reward/lifting_object: 157.4324
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.06s
                      Time elapsed: 01:04:03
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 48035 steps/s (collection: 1.955s, learning 0.091s)
             Mean action noise std: 2.27
          Mean value_function loss: 267.9928
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.4079
                       Mean reward: 769.13
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 0.8120
     Episode_Reward/lifting_object: 153.1559
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.05s
                      Time elapsed: 01:04:05
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 46977 steps/s (collection: 1.993s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 300.3580
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4105
                       Mean reward: 779.42
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 0.8256
     Episode_Reward/lifting_object: 155.9639
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.09s
                      Time elapsed: 01:04:07
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 48282 steps/s (collection: 1.926s, learning 0.110s)
             Mean action noise std: 2.27
          Mean value_function loss: 265.0329
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.4144
                       Mean reward: 780.86
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 0.8132
     Episode_Reward/lifting_object: 153.2464
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.04s
                      Time elapsed: 01:04:09
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.905s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 255.8216
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.4165
                       Mean reward: 764.50
               Mean episode length: 216.07
    Episode_Reward/reaching_object: 0.8134
     Episode_Reward/lifting_object: 152.6321
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.00s
                      Time elapsed: 01:04:11
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 48007 steps/s (collection: 1.952s, learning 0.096s)
             Mean action noise std: 2.27
          Mean value_function loss: 243.2659
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.4211
                       Mean reward: 778.41
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 158.4664
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.05s
                      Time elapsed: 01:04:13
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 47549 steps/s (collection: 1.944s, learning 0.123s)
             Mean action noise std: 2.28
          Mean value_function loss: 272.0285
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.4274
                       Mean reward: 802.65
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 0.8407
     Episode_Reward/lifting_object: 158.3964
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.07s
                      Time elapsed: 01:04:15
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 47878 steps/s (collection: 1.952s, learning 0.102s)
             Mean action noise std: 2.28
          Mean value_function loss: 319.0868
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.4350
                       Mean reward: 754.34
               Mean episode length: 216.24
    Episode_Reward/reaching_object: 0.8297
     Episode_Reward/lifting_object: 156.9633
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.05s
                      Time elapsed: 01:04:17
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 48804 steps/s (collection: 1.920s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 350.0013
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 38.4457
                       Mean reward: 802.21
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 0.8314
     Episode_Reward/lifting_object: 156.4726
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.01s
                      Time elapsed: 01:04:19
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 49262 steps/s (collection: 1.895s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 227.2232
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.4505
                       Mean reward: 842.39
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: 160.5736
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.00s
                      Time elapsed: 01:04:21
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 49839 steps/s (collection: 1.881s, learning 0.091s)
             Mean action noise std: 2.28
          Mean value_function loss: 313.6373
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.4570
                       Mean reward: 798.11
               Mean episode length: 221.79
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: 153.9775
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 1.97s
                      Time elapsed: 01:04:23
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 27839 steps/s (collection: 3.428s, learning 0.103s)
             Mean action noise std: 2.28
          Mean value_function loss: 250.6887
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.4631
                       Mean reward: 837.70
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 0.8167
     Episode_Reward/lifting_object: 155.6618
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.53s
                      Time elapsed: 01:04:27
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14249 steps/s (collection: 6.750s, learning 0.149s)
             Mean action noise std: 2.28
          Mean value_function loss: 268.8804
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.4673
                       Mean reward: 785.64
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 0.8326
     Episode_Reward/lifting_object: 158.2332
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.90s
                      Time elapsed: 01:04:33
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14095 steps/s (collection: 6.844s, learning 0.130s)
             Mean action noise std: 2.28
          Mean value_function loss: 254.5278
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 38.4690
                       Mean reward: 796.75
               Mean episode length: 221.49
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: 157.1148
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.97s
                      Time elapsed: 01:04:40
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13742 steps/s (collection: 7.031s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 259.9341
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.4707
                       Mean reward: 774.77
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 159.5335
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.15s
                      Time elapsed: 01:04:48
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14173 steps/s (collection: 6.778s, learning 0.157s)
             Mean action noise std: 2.28
          Mean value_function loss: 228.9184
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.4746
                       Mean reward: 779.92
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 160.6971
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.94s
                      Time elapsed: 01:04:54
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14421 steps/s (collection: 6.706s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 227.9843
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.4827
                       Mean reward: 833.46
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 0.8412
     Episode_Reward/lifting_object: 161.5432
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.82s
                      Time elapsed: 01:05:01
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14100 steps/s (collection: 6.850s, learning 0.122s)
             Mean action noise std: 2.29
          Mean value_function loss: 225.6193
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.4934
                       Mean reward: 812.44
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 0.8455
     Episode_Reward/lifting_object: 161.7629
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.97s
                      Time elapsed: 01:05:08
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14156 steps/s (collection: 6.825s, learning 0.119s)
             Mean action noise std: 2.29
          Mean value_function loss: 219.3149
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.5016
                       Mean reward: 809.78
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 0.8511
     Episode_Reward/lifting_object: 163.4783
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.94s
                      Time elapsed: 01:05:15
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14070 steps/s (collection: 6.871s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 231.3963
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.5062
                       Mean reward: 848.84
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 0.8490
     Episode_Reward/lifting_object: 161.9153
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.99s
                      Time elapsed: 01:05:22
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 22256 steps/s (collection: 4.298s, learning 0.119s)
             Mean action noise std: 2.29
          Mean value_function loss: 254.8011
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.5086
                       Mean reward: 813.26
               Mean episode length: 225.70
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 158.8913
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.42s
                      Time elapsed: 01:05:27
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 50966 steps/s (collection: 1.817s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 209.9698
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.5146
                       Mean reward: 738.37
               Mean episode length: 206.91
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 158.7447
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.93s
                      Time elapsed: 01:05:29
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 51975 steps/s (collection: 1.793s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 219.7590
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.5179
                       Mean reward: 800.36
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 157.7269
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.89s
                      Time elapsed: 01:05:30
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 48824 steps/s (collection: 1.872s, learning 0.142s)
             Mean action noise std: 2.29
          Mean value_function loss: 261.9805
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.5219
                       Mean reward: 835.55
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 161.5113
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.01s
                      Time elapsed: 01:05:32
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 45220 steps/s (collection: 2.032s, learning 0.142s)
             Mean action noise std: 2.29
          Mean value_function loss: 224.1387
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.5266
                       Mean reward: 842.81
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 0.8351
     Episode_Reward/lifting_object: 159.1066
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.17s
                      Time elapsed: 01:05:35
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 47018 steps/s (collection: 1.918s, learning 0.173s)
             Mean action noise std: 2.29
          Mean value_function loss: 201.4546
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.5300
                       Mean reward: 870.03
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 0.8553
     Episode_Reward/lifting_object: 163.6721
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.09s
                      Time elapsed: 01:05:37
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 50107 steps/s (collection: 1.827s, learning 0.135s)
             Mean action noise std: 2.29
          Mean value_function loss: 237.8026
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.5341
                       Mean reward: 773.97
               Mean episode length: 216.95
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 157.8472
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.96s
                      Time elapsed: 01:05:39
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 50762 steps/s (collection: 1.829s, learning 0.107s)
             Mean action noise std: 2.29
          Mean value_function loss: 230.4706
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.5385
                       Mean reward: 796.02
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 0.8190
     Episode_Reward/lifting_object: 156.3398
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 1.94s
                      Time elapsed: 01:05:41
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 48586 steps/s (collection: 1.851s, learning 0.173s)
             Mean action noise std: 2.29
          Mean value_function loss: 216.7756
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.5424
                       Mean reward: 803.88
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 162.4678
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.02s
                      Time elapsed: 01:05:43
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 49625 steps/s (collection: 1.849s, learning 0.132s)
             Mean action noise std: 2.29
          Mean value_function loss: 217.9227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.5478
                       Mean reward: 791.64
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 159.5137
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.98s
                      Time elapsed: 01:05:45
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 49019 steps/s (collection: 1.865s, learning 0.140s)
             Mean action noise std: 2.30
          Mean value_function loss: 238.8487
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.5568
                       Mean reward: 827.21
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 163.8203
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.01s
                      Time elapsed: 01:05:47
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 45146 steps/s (collection: 1.981s, learning 0.197s)
             Mean action noise std: 2.30
          Mean value_function loss: 194.7273
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.5632
                       Mean reward: 827.51
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 162.7526
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.18s
                      Time elapsed: 01:05:49
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 46866 steps/s (collection: 1.998s, learning 0.099s)
             Mean action noise std: 2.30
          Mean value_function loss: 225.1328
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.5650
                       Mean reward: 848.90
               Mean episode length: 234.27
    Episode_Reward/reaching_object: 0.8465
     Episode_Reward/lifting_object: 162.4533
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.10s
                      Time elapsed: 01:05:51
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 50889 steps/s (collection: 1.841s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 255.8843
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.5685
                       Mean reward: 801.55
               Mean episode length: 221.90
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 161.1733
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 1.93s
                      Time elapsed: 01:05:53
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 50428 steps/s (collection: 1.835s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 244.8821
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.5769
                       Mean reward: 807.58
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 0.8233
     Episode_Reward/lifting_object: 158.0313
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.95s
                      Time elapsed: 01:05:55
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 49198 steps/s (collection: 1.845s, learning 0.154s)
             Mean action noise std: 2.30
          Mean value_function loss: 234.1549
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.5864
                       Mean reward: 765.89
               Mean episode length: 212.37
    Episode_Reward/reaching_object: 0.8292
     Episode_Reward/lifting_object: 159.4223
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.00s
                      Time elapsed: 01:05:57
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 48844 steps/s (collection: 1.899s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 214.2650
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.5938
                       Mean reward: 834.84
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.8559
     Episode_Reward/lifting_object: 163.5042
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.01s
                      Time elapsed: 01:05:59
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 49247 steps/s (collection: 1.905s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 222.6119
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.6003
                       Mean reward: 823.65
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 161.2457
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.00s
                      Time elapsed: 01:06:01
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 50610 steps/s (collection: 1.839s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 212.1522
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.6051
                       Mean reward: 839.00
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 0.8460
     Episode_Reward/lifting_object: 162.4362
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.94s
                      Time elapsed: 01:06:03
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 50739 steps/s (collection: 1.847s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 203.4837
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.6071
                       Mean reward: 778.76
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 162.0861
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.94s
                      Time elapsed: 01:06:05
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 50134 steps/s (collection: 1.875s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 257.2756
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.6113
                       Mean reward: 808.76
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 0.8316
     Episode_Reward/lifting_object: 160.1563
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.96s
                      Time elapsed: 01:06:07
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 49426 steps/s (collection: 1.856s, learning 0.133s)
             Mean action noise std: 2.30
          Mean value_function loss: 224.0167
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6157
                       Mean reward: 788.16
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 160.7112
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.99s
                      Time elapsed: 01:06:09
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 49724 steps/s (collection: 1.855s, learning 0.122s)
             Mean action noise std: 2.30
          Mean value_function loss: 213.1403
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.6213
                       Mean reward: 836.68
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 0.8479
     Episode_Reward/lifting_object: 163.5984
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.98s
                      Time elapsed: 01:06:11
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 49299 steps/s (collection: 1.862s, learning 0.132s)
             Mean action noise std: 2.30
          Mean value_function loss: 217.7724
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.6235
                       Mean reward: 810.90
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 0.8455
     Episode_Reward/lifting_object: 162.7805
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.99s
                      Time elapsed: 01:06:13
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 48018 steps/s (collection: 1.911s, learning 0.137s)
             Mean action noise std: 2.31
          Mean value_function loss: 235.5384
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6239
                       Mean reward: 781.16
               Mean episode length: 218.40
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 159.8295
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.05s
                      Time elapsed: 01:06:15
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 49737 steps/s (collection: 1.863s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 235.7085
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.6251
                       Mean reward: 807.02
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 0.8403
     Episode_Reward/lifting_object: 161.0600
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 1.98s
                      Time elapsed: 01:06:17
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 45145 steps/s (collection: 2.010s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 221.9287
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.6304
                       Mean reward: 821.69
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 161.6464
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.18s
                      Time elapsed: 01:06:19
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 48384 steps/s (collection: 1.880s, learning 0.152s)
             Mean action noise std: 2.31
          Mean value_function loss: 212.7050
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.6352
                       Mean reward: 828.92
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 0.8592
     Episode_Reward/lifting_object: 166.0166
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.03s
                      Time elapsed: 01:06:21
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 48477 steps/s (collection: 1.877s, learning 0.151s)
             Mean action noise std: 2.31
          Mean value_function loss: 246.2633
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 38.6382
                       Mean reward: 818.10
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.8457
     Episode_Reward/lifting_object: 163.8963
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.03s
                      Time elapsed: 01:06:23
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 49997 steps/s (collection: 1.828s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 261.7382
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.6406
                       Mean reward: 830.56
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.8386
     Episode_Reward/lifting_object: 161.3528
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.97s
                      Time elapsed: 01:06:25
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 50456 steps/s (collection: 1.839s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 261.9554
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.6421
                       Mean reward: 807.44
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 0.8259
     Episode_Reward/lifting_object: 158.7745
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.95s
                      Time elapsed: 01:06:27
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 50568 steps/s (collection: 1.834s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 236.8794
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.6458
                       Mean reward: 829.18
               Mean episode length: 227.03
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 164.9880
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.94s
                      Time elapsed: 01:06:29
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 51117 steps/s (collection: 1.835s, learning 0.088s)
             Mean action noise std: 2.31
          Mean value_function loss: 271.4307
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.6505
                       Mean reward: 822.11
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 0.8371
     Episode_Reward/lifting_object: 161.3687
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.92s
                      Time elapsed: 01:06:31
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 51578 steps/s (collection: 1.815s, learning 0.091s)
             Mean action noise std: 2.31
          Mean value_function loss: 290.5108
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 38.6541
                       Mean reward: 799.17
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 162.2751
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.91s
                      Time elapsed: 01:06:33
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 49114 steps/s (collection: 1.913s, learning 0.089s)
             Mean action noise std: 2.31
          Mean value_function loss: 259.3790
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.6568
                       Mean reward: 809.62
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 164.0832
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.00s
                      Time elapsed: 01:06:35
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 50852 steps/s (collection: 1.837s, learning 0.096s)
             Mean action noise std: 2.31
          Mean value_function loss: 207.1883
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.6616
                       Mean reward: 832.01
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 161.1529
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.93s
                      Time elapsed: 01:06:36
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 50247 steps/s (collection: 1.852s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 269.8753
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.6681
                       Mean reward: 788.03
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 160.2807
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.96s
                      Time elapsed: 01:06:38
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 51063 steps/s (collection: 1.839s, learning 0.086s)
             Mean action noise std: 2.31
          Mean value_function loss: 273.8675
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.6704
                       Mean reward: 803.19
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 0.8335
     Episode_Reward/lifting_object: 160.3713
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.93s
                      Time elapsed: 01:06:40
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 50087 steps/s (collection: 1.862s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 241.4526
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.6740
                       Mean reward: 838.38
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 0.8361
     Episode_Reward/lifting_object: 160.4496
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.96s
                      Time elapsed: 01:06:42
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 50302 steps/s (collection: 1.850s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 200.9770
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.6788
                       Mean reward: 837.58
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.8763
     Episode_Reward/lifting_object: 169.0678
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 1.95s
                      Time elapsed: 01:06:44
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 50074 steps/s (collection: 1.854s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 222.5679
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.6852
                       Mean reward: 823.28
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 0.8635
     Episode_Reward/lifting_object: 166.8033
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 1.96s
                      Time elapsed: 01:06:46
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 50244 steps/s (collection: 1.850s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 235.3007
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 38.6915
                       Mean reward: 837.40
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 0.8684
     Episode_Reward/lifting_object: 168.2177
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.96s
                      Time elapsed: 01:06:48
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 50436 steps/s (collection: 1.839s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 219.3357
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 38.6961
                       Mean reward: 850.62
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 0.8663
     Episode_Reward/lifting_object: 167.3089
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.95s
                      Time elapsed: 01:06:50
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 50637 steps/s (collection: 1.843s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 228.7185
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.7016
                       Mean reward: 816.08
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 160.6518
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.94s
                      Time elapsed: 01:06:52
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 50626 steps/s (collection: 1.852s, learning 0.090s)
             Mean action noise std: 2.32
          Mean value_function loss: 262.4689
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.7093
                       Mean reward: 830.34
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 0.8698
     Episode_Reward/lifting_object: 166.8395
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.94s
                      Time elapsed: 01:06:54
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 50005 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 212.9345
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.7206
                       Mean reward: 826.39
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 0.8576
     Episode_Reward/lifting_object: 163.5358
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 1.97s
                      Time elapsed: 01:06:56
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 48938 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 209.3252
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.7277
                       Mean reward: 847.98
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 0.8733
     Episode_Reward/lifting_object: 167.0720
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.01s
                      Time elapsed: 01:06:58
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 49728 steps/s (collection: 1.848s, learning 0.129s)
             Mean action noise std: 2.32
          Mean value_function loss: 203.7037
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.7301
                       Mean reward: 839.23
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.8696
     Episode_Reward/lifting_object: 166.3690
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 1.98s
                      Time elapsed: 01:07:00
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 49826 steps/s (collection: 1.888s, learning 0.085s)
             Mean action noise std: 2.32
          Mean value_function loss: 205.0050
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.7305
                       Mean reward: 827.80
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.8620
     Episode_Reward/lifting_object: 164.3713
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 1.97s
                      Time elapsed: 01:07:02
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 49058 steps/s (collection: 1.867s, learning 0.137s)
             Mean action noise std: 2.32
          Mean value_function loss: 205.5456
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.7343
                       Mean reward: 824.36
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 0.8722
     Episode_Reward/lifting_object: 166.2668
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.00s
                      Time elapsed: 01:07:04
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 50425 steps/s (collection: 1.845s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 207.8704
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.7369
                       Mean reward: 821.73
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 0.8786
     Episode_Reward/lifting_object: 167.8037
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 1.95s
                      Time elapsed: 01:07:06
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 51077 steps/s (collection: 1.816s, learning 0.109s)
             Mean action noise std: 2.32
          Mean value_function loss: 184.7346
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.7389
                       Mean reward: 870.42
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.8687
     Episode_Reward/lifting_object: 166.3100
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 1.92s
                      Time elapsed: 01:07:08
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 50649 steps/s (collection: 1.834s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 185.8081
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.7415
                       Mean reward: 879.06
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 0.8682
     Episode_Reward/lifting_object: 165.3916
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 1.94s
                      Time elapsed: 01:07:10
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 50313 steps/s (collection: 1.862s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 176.9769
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.7466
                       Mean reward: 793.33
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 0.8678
     Episode_Reward/lifting_object: 164.8739
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 1.95s
                      Time elapsed: 01:07:12
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 48335 steps/s (collection: 1.906s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 170.1290
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.7512
                       Mean reward: 825.99
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 0.8758
     Episode_Reward/lifting_object: 166.4860
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.03s
                      Time elapsed: 01:07:14
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 50189 steps/s (collection: 1.872s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 205.9616
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.7553
                       Mean reward: 811.75
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 0.8638
     Episode_Reward/lifting_object: 164.6536
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.96s
                      Time elapsed: 01:07:16
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 50145 steps/s (collection: 1.873s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 234.1169
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.7567
                       Mean reward: 816.98
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 0.8640
     Episode_Reward/lifting_object: 163.9115
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.96s
                      Time elapsed: 01:07:18
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 50419 steps/s (collection: 1.838s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 154.2512
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.7607
                       Mean reward: 778.94
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 0.8631
     Episode_Reward/lifting_object: 163.6528
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 1.95s
                      Time elapsed: 01:07:20
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 50246 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 191.1631
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.7644
                       Mean reward: 809.94
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 0.8606
     Episode_Reward/lifting_object: 164.3227
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 1.96s
                      Time elapsed: 01:07:22
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 50533 steps/s (collection: 1.859s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 219.7955
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.7681
                       Mean reward: 791.30
               Mean episode length: 218.43
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 163.0821
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 1.95s
                      Time elapsed: 01:07:24
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 50512 steps/s (collection: 1.830s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 187.5048
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.7728
                       Mean reward: 841.75
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 0.8851
     Episode_Reward/lifting_object: 170.1961
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.95s
                      Time elapsed: 01:07:25
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 49645 steps/s (collection: 1.879s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 209.2747
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.7763
                       Mean reward: 818.05
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 0.8738
     Episode_Reward/lifting_object: 166.8317
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.98s
                      Time elapsed: 01:07:27
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 49867 steps/s (collection: 1.865s, learning 0.106s)
             Mean action noise std: 2.33
          Mean value_function loss: 177.6511
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.7802
                       Mean reward: 826.74
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 0.8765
     Episode_Reward/lifting_object: 167.8093
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 1.97s
                      Time elapsed: 01:07:29
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 48965 steps/s (collection: 1.906s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 212.1712
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.7831
                       Mean reward: 855.55
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 0.8669
     Episode_Reward/lifting_object: 166.4391
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.01s
                      Time elapsed: 01:07:31
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 49175 steps/s (collection: 1.902s, learning 0.098s)
             Mean action noise std: 2.33
          Mean value_function loss: 192.3280
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.7861
                       Mean reward: 840.28
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 0.8625
     Episode_Reward/lifting_object: 165.3921
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.00s
                      Time elapsed: 01:07:33
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 49314 steps/s (collection: 1.890s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 187.9946
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.7889
                       Mean reward: 816.11
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 163.9692
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 1.99s
                      Time elapsed: 01:07:35
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 50478 steps/s (collection: 1.836s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 175.0048
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.7921
                       Mean reward: 834.56
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 0.8785
     Episode_Reward/lifting_object: 168.0340
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 1.95s
                      Time elapsed: 01:07:37
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 48836 steps/s (collection: 1.895s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 204.7372
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 38.7979
                       Mean reward: 852.07
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.8782
     Episode_Reward/lifting_object: 168.7437
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.01s
                      Time elapsed: 01:07:39
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 49751 steps/s (collection: 1.890s, learning 0.086s)
             Mean action noise std: 2.34
          Mean value_function loss: 206.5769
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.8035
                       Mean reward: 786.90
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 164.9226
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.98s
                      Time elapsed: 01:07:41
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 50206 steps/s (collection: 1.869s, learning 0.089s)
             Mean action noise std: 2.34
          Mean value_function loss: 199.4157
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.8094
                       Mean reward: 850.22
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 0.8662
     Episode_Reward/lifting_object: 166.8841
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.96s
                      Time elapsed: 01:07:43
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 49189 steps/s (collection: 1.900s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 187.2273
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.8121
                       Mean reward: 874.20
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.8726
     Episode_Reward/lifting_object: 168.2283
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.00s
                      Time elapsed: 01:07:45
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 50154 steps/s (collection: 1.848s, learning 0.112s)
             Mean action noise std: 2.34
          Mean value_function loss: 198.6572
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.8163
                       Mean reward: 827.56
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 166.3746
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 1.96s
                      Time elapsed: 01:07:47
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 49777 steps/s (collection: 1.870s, learning 0.105s)
             Mean action noise std: 2.34
          Mean value_function loss: 222.8837
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.8239
                       Mean reward: 799.58
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 164.9796
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 1.97s
                      Time elapsed: 01:07:49
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 47954 steps/s (collection: 1.934s, learning 0.116s)
             Mean action noise std: 2.34
          Mean value_function loss: 183.6643
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.8292
                       Mean reward: 846.15
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 0.8834
     Episode_Reward/lifting_object: 169.9427
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.05s
                      Time elapsed: 01:07:51
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 49965 steps/s (collection: 1.880s, learning 0.088s)
             Mean action noise std: 2.34
          Mean value_function loss: 168.9211
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.8362
                       Mean reward: 836.74
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 0.8673
     Episode_Reward/lifting_object: 166.8012
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 1.97s
                      Time elapsed: 01:07:53
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 48872 steps/s (collection: 1.914s, learning 0.098s)
             Mean action noise std: 2.34
          Mean value_function loss: 180.8169
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 38.8425
                       Mean reward: 816.20
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.8575
     Episode_Reward/lifting_object: 165.5463
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.01s
                      Time elapsed: 01:07:55
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 48178 steps/s (collection: 1.944s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 173.8964
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.8457
                       Mean reward: 865.46
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 0.8774
     Episode_Reward/lifting_object: 169.4881
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.04s
                      Time elapsed: 01:07:57
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 49762 steps/s (collection: 1.862s, learning 0.114s)
             Mean action noise std: 2.34
          Mean value_function loss: 245.3856
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.8510
                       Mean reward: 809.66
               Mean episode length: 224.01
    Episode_Reward/reaching_object: 0.8635
     Episode_Reward/lifting_object: 166.8107
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.98s
                      Time elapsed: 01:07:59
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 49546 steps/s (collection: 1.888s, learning 0.097s)
             Mean action noise std: 2.34
          Mean value_function loss: 168.8219
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.8553
                       Mean reward: 808.18
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 163.8067
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.98s
                      Time elapsed: 01:08:01
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 49124 steps/s (collection: 1.894s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 187.7893
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.8573
                       Mean reward: 809.75
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 0.8668
     Episode_Reward/lifting_object: 167.3663
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.00s
                      Time elapsed: 01:08:03
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 48571 steps/s (collection: 1.928s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 204.9403
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.8617
                       Mean reward: 809.55
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 0.8649
     Episode_Reward/lifting_object: 166.7717
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.02s
                      Time elapsed: 01:08:05
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 50608 steps/s (collection: 1.855s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 183.2975
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.8693
                       Mean reward: 850.00
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.8689
     Episode_Reward/lifting_object: 168.1921
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 1.94s
                      Time elapsed: 01:08:07
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 50597 steps/s (collection: 1.847s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 218.6768
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.8721
                       Mean reward: 830.06
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 0.8679
     Episode_Reward/lifting_object: 168.1368
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 1.94s
                      Time elapsed: 01:08:09
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 49610 steps/s (collection: 1.890s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 216.3346
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.8756
                       Mean reward: 851.78
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.8673
     Episode_Reward/lifting_object: 168.5332
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 1.98s
                      Time elapsed: 01:08:11
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 50021 steps/s (collection: 1.843s, learning 0.122s)
             Mean action noise std: 2.35
          Mean value_function loss: 211.1457
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 38.8787
                       Mean reward: 803.28
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 0.8636
     Episode_Reward/lifting_object: 167.5080
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 1.97s
                      Time elapsed: 01:08:13
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 50669 steps/s (collection: 1.842s, learning 0.099s)
             Mean action noise std: 2.35
          Mean value_function loss: 208.9881
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.8806
                       Mean reward: 816.87
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 0.8516
     Episode_Reward/lifting_object: 164.9483
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 1.94s
                      Time elapsed: 01:08:15
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 49449 steps/s (collection: 1.870s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 228.5169
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8842
                       Mean reward: 812.36
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 0.8564
     Episode_Reward/lifting_object: 166.2245
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 1.99s
                      Time elapsed: 01:08:17
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 49758 steps/s (collection: 1.856s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 144.5162
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.8877
                       Mean reward: 854.56
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 0.8839
     Episode_Reward/lifting_object: 171.5083
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 1.98s
                      Time elapsed: 01:08:19
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 50767 steps/s (collection: 1.832s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 178.0896
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 38.8919
                       Mean reward: 881.29
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.8837
     Episode_Reward/lifting_object: 170.9468
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 1.94s
                      Time elapsed: 01:08:21
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 50063 steps/s (collection: 1.862s, learning 0.102s)
             Mean action noise std: 2.35
          Mean value_function loss: 190.6294
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.8976
                       Mean reward: 856.60
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 0.8691
     Episode_Reward/lifting_object: 168.3776
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 1.96s
                      Time elapsed: 01:08:23
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 50249 steps/s (collection: 1.868s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 197.6793
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.9064
                       Mean reward: 823.40
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 0.8726
     Episode_Reward/lifting_object: 168.4654
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.96s
                      Time elapsed: 01:08:25
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 49916 steps/s (collection: 1.878s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 157.4610
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 38.9133
                       Mean reward: 871.30
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.8921
     Episode_Reward/lifting_object: 172.5328
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.97s
                      Time elapsed: 01:08:27
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 50673 steps/s (collection: 1.849s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 171.1905
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.9158
                       Mean reward: 811.07
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 0.8650
     Episode_Reward/lifting_object: 167.2725
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 1.94s
                      Time elapsed: 01:08:29
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 50180 steps/s (collection: 1.868s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 208.1364
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.9171
                       Mean reward: 860.62
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 0.8768
     Episode_Reward/lifting_object: 169.5591
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 1.96s
                      Time elapsed: 01:08:31
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 49378 steps/s (collection: 1.882s, learning 0.109s)
             Mean action noise std: 2.35
          Mean value_function loss: 202.0735
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.9196
                       Mean reward: 827.46
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 166.4949
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 1.99s
                      Time elapsed: 01:08:33
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 49859 steps/s (collection: 1.870s, learning 0.102s)
             Mean action noise std: 2.36
          Mean value_function loss: 228.4654
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.9253
                       Mean reward: 787.57
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 164.6756
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.97s
                      Time elapsed: 01:08:35
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 49912 steps/s (collection: 1.871s, learning 0.098s)
             Mean action noise std: 2.36
          Mean value_function loss: 201.8182
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.9352
                       Mean reward: 863.08
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.8631
     Episode_Reward/lifting_object: 166.3799
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.97s
                      Time elapsed: 01:08:37
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 50458 steps/s (collection: 1.848s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 190.5302
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 38.9420
                       Mean reward: 846.45
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 0.8710
     Episode_Reward/lifting_object: 168.6201
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.95s
                      Time elapsed: 01:08:39
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 50781 steps/s (collection: 1.845s, learning 0.091s)
             Mean action noise std: 2.36
          Mean value_function loss: 197.6935
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.9437
                       Mean reward: 874.84
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.8830
     Episode_Reward/lifting_object: 170.8222
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.94s
                      Time elapsed: 01:08:41
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 49798 steps/s (collection: 1.882s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 209.3469
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.9463
                       Mean reward: 799.92
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 0.8554
     Episode_Reward/lifting_object: 164.6883
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 1.97s
                      Time elapsed: 01:08:43
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 50205 steps/s (collection: 1.870s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 224.1234
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.9516
                       Mean reward: 811.27
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 0.8554
     Episode_Reward/lifting_object: 165.4792
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 1.96s
                      Time elapsed: 01:08:45
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 50087 steps/s (collection: 1.870s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 219.8355
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.9561
                       Mean reward: 864.13
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.8814
     Episode_Reward/lifting_object: 170.7627
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 1.96s
                      Time elapsed: 01:08:46
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 49031 steps/s (collection: 1.896s, learning 0.109s)
             Mean action noise std: 2.36
          Mean value_function loss: 221.5506
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.9584
                       Mean reward: 829.19
               Mean episode length: 228.39
    Episode_Reward/reaching_object: 0.8685
     Episode_Reward/lifting_object: 166.4710
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.00s
                      Time elapsed: 01:08:48
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 49099 steps/s (collection: 1.899s, learning 0.103s)
             Mean action noise std: 2.36
          Mean value_function loss: 212.3878
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.9630
                       Mean reward: 807.56
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 0.8757
     Episode_Reward/lifting_object: 168.3655
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.00s
                      Time elapsed: 01:08:50
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 49324 steps/s (collection: 1.896s, learning 0.097s)
             Mean action noise std: 2.36
          Mean value_function loss: 215.9430
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 38.9710
                       Mean reward: 836.14
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 0.8585
     Episode_Reward/lifting_object: 165.3994
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 1.99s
                      Time elapsed: 01:08:52
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 49563 steps/s (collection: 1.892s, learning 0.091s)
             Mean action noise std: 2.36
          Mean value_function loss: 201.2639
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.9754
                       Mean reward: 853.72
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.8752
     Episode_Reward/lifting_object: 167.8460
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 1.98s
                      Time elapsed: 01:08:54
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 49147 steps/s (collection: 1.899s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 209.2188
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.9788
                       Mean reward: 822.01
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 0.8708
     Episode_Reward/lifting_object: 167.4856
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.00s
                      Time elapsed: 01:08:56
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 50824 steps/s (collection: 1.831s, learning 0.103s)
             Mean action noise std: 2.36
          Mean value_function loss: 213.5073
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.9820
                       Mean reward: 824.88
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.8616
     Episode_Reward/lifting_object: 165.4231
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.93s
                      Time elapsed: 01:08:58
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 50320 steps/s (collection: 1.834s, learning 0.120s)
             Mean action noise std: 2.36
          Mean value_function loss: 209.5267
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 38.9839
                       Mean reward: 860.05
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.8685
     Episode_Reward/lifting_object: 167.3003
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 1.95s
                      Time elapsed: 01:09:00
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 50136 steps/s (collection: 1.839s, learning 0.122s)
             Mean action noise std: 2.36
          Mean value_function loss: 181.6307
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.9860
                       Mean reward: 845.48
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: 167.6625
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 1.96s
                      Time elapsed: 01:09:02
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 49708 steps/s (collection: 1.865s, learning 0.113s)
             Mean action noise std: 2.36
          Mean value_function loss: 185.0071
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.9899
                       Mean reward: 866.94
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.8864
     Episode_Reward/lifting_object: 170.5817
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 1.98s
                      Time elapsed: 01:09:04
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 50954 steps/s (collection: 1.827s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 179.3367
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 38.9935
                       Mean reward: 824.47
               Mean episode length: 224.60
    Episode_Reward/reaching_object: 0.8790
     Episode_Reward/lifting_object: 170.2544
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 1.93s
                      Time elapsed: 01:09:06
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 48070 steps/s (collection: 1.861s, learning 0.184s)
             Mean action noise std: 2.37
          Mean value_function loss: 198.6893
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.9969
                       Mean reward: 856.02
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 0.8628
     Episode_Reward/lifting_object: 165.6676
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.04s
                      Time elapsed: 01:09:08
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 48901 steps/s (collection: 1.869s, learning 0.141s)
             Mean action noise std: 2.37
          Mean value_function loss: 171.8333
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0003
                       Mean reward: 841.34
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 0.8788
     Episode_Reward/lifting_object: 169.0402
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.01s
                      Time elapsed: 01:09:10
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 44985 steps/s (collection: 2.037s, learning 0.149s)
             Mean action noise std: 2.37
          Mean value_function loss: 164.7105
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.0054
                       Mean reward: 868.50
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 0.8877
     Episode_Reward/lifting_object: 171.1038
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.19s
                      Time elapsed: 01:09:12
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 49201 steps/s (collection: 1.904s, learning 0.094s)
             Mean action noise std: 2.37
          Mean value_function loss: 159.4938
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.0091
                       Mean reward: 845.06
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 0.8895
     Episode_Reward/lifting_object: 170.9399
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.00s
                      Time elapsed: 01:09:14
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.883s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 174.5117
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.0124
                       Mean reward: 861.92
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 0.8826
     Episode_Reward/lifting_object: 169.6121
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 1.99s
                      Time elapsed: 01:09:16
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 49472 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 2.37
          Mean value_function loss: 181.5865
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.0163
                       Mean reward: 832.88
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 0.8767
     Episode_Reward/lifting_object: 168.2305
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.99s
                      Time elapsed: 01:09:18
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 50138 steps/s (collection: 1.866s, learning 0.095s)
             Mean action noise std: 2.37
          Mean value_function loss: 165.6287
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 39.0206
                       Mean reward: 870.46
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.8894
     Episode_Reward/lifting_object: 171.7129
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.96s
                      Time elapsed: 01:09:20
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 49128 steps/s (collection: 1.871s, learning 0.130s)
             Mean action noise std: 2.37
          Mean value_function loss: 176.4943
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.0249
                       Mean reward: 838.53
               Mean episode length: 228.80
    Episode_Reward/reaching_object: 0.8914
     Episode_Reward/lifting_object: 171.3634
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.00s
                      Time elapsed: 01:09:22
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 49929 steps/s (collection: 1.867s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 163.7224
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.0310
                       Mean reward: 860.90
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.8812
     Episode_Reward/lifting_object: 169.8611
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 1.97s
                      Time elapsed: 01:09:24
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 48585 steps/s (collection: 1.892s, learning 0.132s)
             Mean action noise std: 2.37
          Mean value_function loss: 194.7092
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.0391
                       Mean reward: 821.93
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.8696
     Episode_Reward/lifting_object: 167.3226
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.02s
                      Time elapsed: 01:09:26
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 50062 steps/s (collection: 1.864s, learning 0.100s)
             Mean action noise std: 2.37
          Mean value_function loss: 174.4411
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 39.0486
                       Mean reward: 873.53
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.8849
     Episode_Reward/lifting_object: 169.7337
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 1.96s
                      Time elapsed: 01:09:28
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 48979 steps/s (collection: 1.879s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 165.6136
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0531
                       Mean reward: 833.52
               Mean episode length: 227.81
    Episode_Reward/reaching_object: 0.8818
     Episode_Reward/lifting_object: 169.2364
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.01s
                      Time elapsed: 01:09:30
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 47440 steps/s (collection: 1.883s, learning 0.189s)
             Mean action noise std: 2.37
          Mean value_function loss: 162.9418
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.0586
                       Mean reward: 844.49
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.8978
     Episode_Reward/lifting_object: 172.5671
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.07s
                      Time elapsed: 01:09:32
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 50755 steps/s (collection: 1.851s, learning 0.086s)
             Mean action noise std: 2.38
          Mean value_function loss: 143.9538
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0627
                       Mean reward: 844.31
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 0.8947
     Episode_Reward/lifting_object: 171.8700
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.94s
                      Time elapsed: 01:09:34
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 47403 steps/s (collection: 1.953s, learning 0.121s)
             Mean action noise std: 2.38
          Mean value_function loss: 152.9016
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.0656
                       Mean reward: 872.77
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.8924
     Episode_Reward/lifting_object: 171.4032
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.07s
                      Time elapsed: 01:09:36
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 48203 steps/s (collection: 1.892s, learning 0.148s)
             Mean action noise std: 2.38
          Mean value_function loss: 195.5854
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 39.0674
                       Mean reward: 894.18
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: 172.1027
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.04s
                      Time elapsed: 01:09:38
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 49459 steps/s (collection: 1.897s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 158.3100
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 39.0694
                       Mean reward: 826.47
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 0.8908
     Episode_Reward/lifting_object: 170.6833
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 1.99s
                      Time elapsed: 01:09:40
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 49404 steps/s (collection: 1.898s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 187.2084
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.0719
                       Mean reward: 817.67
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 0.8847
     Episode_Reward/lifting_object: 170.4001
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 1.99s
                      Time elapsed: 01:09:42
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 50445 steps/s (collection: 1.851s, learning 0.098s)
             Mean action noise std: 2.38
          Mean value_function loss: 147.7587
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.0758
                       Mean reward: 827.57
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 171.5984
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.95s
                      Time elapsed: 01:09:44
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 49917 steps/s (collection: 1.839s, learning 0.131s)
             Mean action noise std: 2.38
          Mean value_function loss: 139.9906
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 39.0810
                       Mean reward: 887.03
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.8974
     Episode_Reward/lifting_object: 173.0560
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 1.97s
                      Time elapsed: 01:09:46
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 47975 steps/s (collection: 1.914s, learning 0.136s)
             Mean action noise std: 2.38
          Mean value_function loss: 177.2308
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0847
                       Mean reward: 871.28
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.8952
     Episode_Reward/lifting_object: 172.5345
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.05s
                      Time elapsed: 01:09:48
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 49509 steps/s (collection: 1.861s, learning 0.125s)
             Mean action noise std: 2.38
          Mean value_function loss: 203.3688
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.0904
                       Mean reward: 848.17
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 0.8966
     Episode_Reward/lifting_object: 173.5352
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 1.99s
                      Time elapsed: 01:09:50
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 48991 steps/s (collection: 1.902s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 133.0536
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.1009
                       Mean reward: 851.86
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.8892
     Episode_Reward/lifting_object: 170.7697
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.01s
                      Time elapsed: 01:09:52
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 49604 steps/s (collection: 1.872s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 165.5890
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 39.1067
                       Mean reward: 865.72
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.8954
     Episode_Reward/lifting_object: 172.9673
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 1.98s
                      Time elapsed: 01:09:54
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 48521 steps/s (collection: 1.913s, learning 0.113s)
             Mean action noise std: 2.38
          Mean value_function loss: 165.8773
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.1113
                       Mean reward: 842.42
               Mean episode length: 230.91
    Episode_Reward/reaching_object: 0.8920
     Episode_Reward/lifting_object: 171.8008
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.03s
                      Time elapsed: 01:09:56
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 50719 steps/s (collection: 1.843s, learning 0.095s)
             Mean action noise std: 2.38
          Mean value_function loss: 130.5249
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.1169
                       Mean reward: 879.75
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 0.9020
     Episode_Reward/lifting_object: 173.0458
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 1.94s
                      Time elapsed: 01:09:58
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 50186 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 148.6965
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 39.1211
                       Mean reward: 869.24
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.8993
     Episode_Reward/lifting_object: 173.7105
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 1.96s
                      Time elapsed: 01:10:00
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 48985 steps/s (collection: 1.907s, learning 0.100s)
             Mean action noise std: 2.38
          Mean value_function loss: 160.0224
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 39.1226
                       Mean reward: 875.65
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 0.8900
     Episode_Reward/lifting_object: 171.6381
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.01s
                      Time elapsed: 01:10:02
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 47998 steps/s (collection: 1.924s, learning 0.124s)
             Mean action noise std: 2.38
          Mean value_function loss: 192.0470
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1246
                       Mean reward: 832.79
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 0.8737
     Episode_Reward/lifting_object: 167.9992
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.05s
                      Time elapsed: 01:10:04
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 49945 steps/s (collection: 1.860s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 170.0861
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.1317
                       Mean reward: 823.60
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.8728
     Episode_Reward/lifting_object: 167.8237
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 1.97s
                      Time elapsed: 01:10:06
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 50628 steps/s (collection: 1.841s, learning 0.101s)
             Mean action noise std: 2.39
          Mean value_function loss: 159.4409
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.1433
                       Mean reward: 833.27
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 0.8800
     Episode_Reward/lifting_object: 169.5851
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 1.94s
                      Time elapsed: 01:10:08
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 49939 steps/s (collection: 1.869s, learning 0.099s)
             Mean action noise std: 2.39
          Mean value_function loss: 132.0836
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.1527
                       Mean reward: 882.82
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 175.6216
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.97s
                      Time elapsed: 01:10:10
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 47718 steps/s (collection: 1.910s, learning 0.150s)
             Mean action noise std: 2.39
          Mean value_function loss: 134.6156
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.1564
                       Mean reward: 838.43
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 0.8997
     Episode_Reward/lifting_object: 173.4133
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.06s
                      Time elapsed: 01:10:12
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 48180 steps/s (collection: 1.891s, learning 0.149s)
             Mean action noise std: 2.39
          Mean value_function loss: 142.1707
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.1601
                       Mean reward: 840.65
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 0.8945
     Episode_Reward/lifting_object: 171.5786
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.04s
                      Time elapsed: 01:10:14
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 49965 steps/s (collection: 1.846s, learning 0.122s)
             Mean action noise std: 2.39
          Mean value_function loss: 136.7953
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.1621
                       Mean reward: 890.41
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 0.9068
     Episode_Reward/lifting_object: 175.6076
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 1.97s
                      Time elapsed: 01:10:16
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.876s, learning 0.099s)
             Mean action noise std: 2.39
          Mean value_function loss: 123.6562
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1655
                       Mean reward: 896.19
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 174.7278
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.97s
                      Time elapsed: 01:10:18
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 49147 steps/s (collection: 1.905s, learning 0.095s)
             Mean action noise std: 2.39
          Mean value_function loss: 139.2610
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.1703
                       Mean reward: 854.05
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 0.8964
     Episode_Reward/lifting_object: 173.0195
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.00s
                      Time elapsed: 01:10:20
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 49671 steps/s (collection: 1.881s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 108.4291
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 39.1759
                       Mean reward: 876.44
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 0.9085
     Episode_Reward/lifting_object: 175.4239
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 1.98s
                      Time elapsed: 01:10:22
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 50619 steps/s (collection: 1.850s, learning 0.093s)
             Mean action noise std: 2.39
          Mean value_function loss: 102.8002
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.1778
                       Mean reward: 884.70
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.9097
     Episode_Reward/lifting_object: 175.4042
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 1.94s
                      Time elapsed: 01:10:24
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 146.8643
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.1812
                       Mean reward: 907.27
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.9059
     Episode_Reward/lifting_object: 174.8384
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 1.98s
                      Time elapsed: 01:10:26
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 48837 steps/s (collection: 1.901s, learning 0.112s)
             Mean action noise std: 2.39
          Mean value_function loss: 137.2409
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1875
                       Mean reward: 860.31
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.8975
     Episode_Reward/lifting_object: 172.7292
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.01s
                      Time elapsed: 01:10:28
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 48735 steps/s (collection: 1.930s, learning 0.087s)
             Mean action noise std: 2.39
          Mean value_function loss: 144.3154
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.1932
                       Mean reward: 870.52
               Mean episode length: 234.51
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 175.5711
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.02s
                      Time elapsed: 01:10:30
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 49279 steps/s (collection: 1.859s, learning 0.136s)
             Mean action noise std: 2.40
          Mean value_function loss: 104.8016
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.1990
                       Mean reward: 860.13
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 175.4032
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 1.99s
                      Time elapsed: 01:10:32
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 49496 steps/s (collection: 1.889s, learning 0.098s)
             Mean action noise std: 2.40
          Mean value_function loss: 135.3124
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.2056
                       Mean reward: 884.85
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.9050
     Episode_Reward/lifting_object: 174.3320
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 1.99s
                      Time elapsed: 01:10:34
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 48357 steps/s (collection: 1.926s, learning 0.107s)
             Mean action noise std: 2.40
          Mean value_function loss: 131.5613
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.2109
                       Mean reward: 870.72
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.8996
     Episode_Reward/lifting_object: 173.5768
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.03s
                      Time elapsed: 01:10:36
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 48875 steps/s (collection: 1.922s, learning 0.089s)
             Mean action noise std: 2.40
          Mean value_function loss: 108.5934
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.2149
                       Mean reward: 894.73
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 177.4091
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.01s
                      Time elapsed: 01:10:38
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 49965 steps/s (collection: 1.877s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 124.4332
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.2187
                       Mean reward: 905.68
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.9191
     Episode_Reward/lifting_object: 176.6578
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.97s
                      Time elapsed: 01:10:40
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 50550 steps/s (collection: 1.851s, learning 0.094s)
             Mean action noise std: 2.40
          Mean value_function loss: 136.8068
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.2238
                       Mean reward: 854.54
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 0.9024
     Episode_Reward/lifting_object: 173.0027
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.94s
                      Time elapsed: 01:10:42
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.922s, learning 0.093s)
             Mean action noise std: 2.40
          Mean value_function loss: 138.8075
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.2332
                       Mean reward: 845.65
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.9008
     Episode_Reward/lifting_object: 172.8479
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.02s
                      Time elapsed: 01:10:44
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 49461 steps/s (collection: 1.898s, learning 0.090s)
             Mean action noise std: 2.40
          Mean value_function loss: 144.9322
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.2448
                       Mean reward: 881.72
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.9082
     Episode_Reward/lifting_object: 174.3061
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.99s
                      Time elapsed: 01:10:46
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 49401 steps/s (collection: 1.898s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 168.5104
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.2521
                       Mean reward: 852.28
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 0.8969
     Episode_Reward/lifting_object: 171.6846
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.99s
                      Time elapsed: 01:10:48
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 48205 steps/s (collection: 1.939s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 127.2424
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.2576
                       Mean reward: 877.42
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.9074
     Episode_Reward/lifting_object: 173.9391
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.04s
                      Time elapsed: 01:10:50
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 45833 steps/s (collection: 2.048s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 149.9078
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.2631
                       Mean reward: 876.97
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 0.9057
     Episode_Reward/lifting_object: 173.6097
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.14s
                      Time elapsed: 01:10:52
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 47763 steps/s (collection: 1.941s, learning 0.117s)
             Mean action noise std: 2.41
          Mean value_function loss: 135.1374
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.2700
                       Mean reward: 839.73
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.8876
     Episode_Reward/lifting_object: 169.5088
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.06s
                      Time elapsed: 01:10:54
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.920s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 112.4117
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.2736
                       Mean reward: 871.86
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 175.5622
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.01s
                      Time elapsed: 01:10:56
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 47450 steps/s (collection: 1.926s, learning 0.146s)
             Mean action noise std: 2.41
          Mean value_function loss: 104.2553
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.2789
                       Mean reward: 873.39
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.9178
     Episode_Reward/lifting_object: 176.2783
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.07s
                      Time elapsed: 01:10:58
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 46332 steps/s (collection: 1.910s, learning 0.212s)
             Mean action noise std: 2.41
          Mean value_function loss: 148.0701
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.2835
                       Mean reward: 887.78
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 0.8940
     Episode_Reward/lifting_object: 170.9837
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.12s
                      Time elapsed: 01:11:01
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 45590 steps/s (collection: 2.057s, learning 0.100s)
             Mean action noise std: 2.41
          Mean value_function loss: 163.9327
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.2899
                       Mean reward: 851.00
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 0.9001
     Episode_Reward/lifting_object: 172.4305
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.16s
                      Time elapsed: 01:11:03
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 45982 steps/s (collection: 2.027s, learning 0.111s)
             Mean action noise std: 2.41
          Mean value_function loss: 174.6123
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.2953
                       Mean reward: 871.15
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.9045
     Episode_Reward/lifting_object: 173.3224
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.14s
                      Time elapsed: 01:11:05
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 47725 steps/s (collection: 1.952s, learning 0.108s)
             Mean action noise std: 2.41
          Mean value_function loss: 169.1358
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.3002
                       Mean reward: 863.84
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.8979
     Episode_Reward/lifting_object: 172.2240
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.06s
                      Time elapsed: 01:11:07
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 50469 steps/s (collection: 1.854s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 161.4142
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.3087
                       Mean reward: 840.81
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 0.8941
     Episode_Reward/lifting_object: 171.9305
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.95s
                      Time elapsed: 01:11:09
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 49913 steps/s (collection: 1.877s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 153.5448
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 39.3162
                       Mean reward: 877.38
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 0.9231
     Episode_Reward/lifting_object: 177.6700
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.97s
                      Time elapsed: 01:11:11
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 48954 steps/s (collection: 1.913s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 160.9857
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.3214
                       Mean reward: 890.23
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.9207
     Episode_Reward/lifting_object: 176.7148
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.01s
                      Time elapsed: 01:11:13
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 48488 steps/s (collection: 1.928s, learning 0.100s)
             Mean action noise std: 2.41
          Mean value_function loss: 191.6419
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.3254
                       Mean reward: 832.61
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 0.8771
     Episode_Reward/lifting_object: 168.3195
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.03s
                      Time elapsed: 01:11:15
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 47739 steps/s (collection: 1.957s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 142.0109
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.3290
                       Mean reward: 892.64
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.9112
     Episode_Reward/lifting_object: 174.7754
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.06s
                      Time elapsed: 01:11:17
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 48426 steps/s (collection: 1.923s, learning 0.106s)
             Mean action noise std: 2.41
          Mean value_function loss: 166.8256
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 39.3329
                       Mean reward: 866.17
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 0.9034
     Episode_Reward/lifting_object: 173.5812
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.03s
                      Time elapsed: 01:11:19
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 49618 steps/s (collection: 1.890s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 126.8477
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.3356
                       Mean reward: 872.09
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 0.9035
     Episode_Reward/lifting_object: 173.2157
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.98s
                      Time elapsed: 01:11:21
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 49296 steps/s (collection: 1.902s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 128.7731
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.3410
                       Mean reward: 904.26
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.9170
     Episode_Reward/lifting_object: 176.5911
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.99s
                      Time elapsed: 01:11:23
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 50123 steps/s (collection: 1.870s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 124.9207
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.3477
                       Mean reward: 892.93
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 0.9089
     Episode_Reward/lifting_object: 175.1226
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 1.96s
                      Time elapsed: 01:11:25
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 45583 steps/s (collection: 2.047s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 155.1203
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.3534
                       Mean reward: 881.38
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.9113
     Episode_Reward/lifting_object: 175.1969
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.16s
                      Time elapsed: 01:11:27
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 48025 steps/s (collection: 1.946s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 126.8022
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.3603
                       Mean reward: 880.83
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.9137
     Episode_Reward/lifting_object: 175.9911
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.05s
                      Time elapsed: 01:11:29
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 49505 steps/s (collection: 1.890s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 117.2208
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.3662
                       Mean reward: 909.20
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: 178.2030
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.99s
                      Time elapsed: 01:11:31
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 48710 steps/s (collection: 1.893s, learning 0.126s)
             Mean action noise std: 2.42
          Mean value_function loss: 117.4834
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.3682
                       Mean reward: 905.47
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 175.3810
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.02s
                      Time elapsed: 01:11:33
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 44625 steps/s (collection: 2.079s, learning 0.124s)
             Mean action noise std: 2.42
          Mean value_function loss: 143.2209
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 39.3692
                       Mean reward: 908.50
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 172.6117
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.20s
                      Time elapsed: 01:11:35
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 48794 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 132.6645
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.3709
                       Mean reward: 913.53
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.9169
     Episode_Reward/lifting_object: 176.5854
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.01s
                      Time elapsed: 01:11:37
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 48798 steps/s (collection: 1.910s, learning 0.104s)
             Mean action noise std: 2.42
          Mean value_function loss: 114.9806
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.3731
                       Mean reward: 850.60
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.9195
     Episode_Reward/lifting_object: 176.6593
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.01s
                      Time elapsed: 01:11:39
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 43687 steps/s (collection: 2.142s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 125.6020
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.3762
                       Mean reward: 876.79
               Mean episode length: 236.54
    Episode_Reward/reaching_object: 0.9197
     Episode_Reward/lifting_object: 177.0000
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.25s
                      Time elapsed: 01:11:42
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 49322 steps/s (collection: 1.899s, learning 0.094s)
             Mean action noise std: 2.42
          Mean value_function loss: 104.1299
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.3807
                       Mean reward: 900.57
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.9168
     Episode_Reward/lifting_object: 175.4841
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 1.99s
                      Time elapsed: 01:11:44
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 49642 steps/s (collection: 1.885s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 145.0154
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.3829
                       Mean reward: 879.03
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.9139
     Episode_Reward/lifting_object: 175.6687
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.98s
                      Time elapsed: 01:11:46
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 50201 steps/s (collection: 1.865s, learning 0.094s)
             Mean action noise std: 2.42
          Mean value_function loss: 144.5056
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.3857
                       Mean reward: 867.44
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 0.9013
     Episode_Reward/lifting_object: 173.1475
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 1.96s
                      Time elapsed: 01:11:48
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 50030 steps/s (collection: 1.867s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 138.3512
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.3911
                       Mean reward: 869.83
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 0.9086
     Episode_Reward/lifting_object: 174.9162
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 1.96s
                      Time elapsed: 01:11:50
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 49466 steps/s (collection: 1.893s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 112.2282
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.4002
                       Mean reward: 876.23
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 0.9169
     Episode_Reward/lifting_object: 177.0348
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 1.99s
                      Time elapsed: 01:11:52
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.031s, learning 0.129s)
             Mean action noise std: 2.43
          Mean value_function loss: 148.3254
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.4066
                       Mean reward: 880.34
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.9068
     Episode_Reward/lifting_object: 174.6072
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.16s
                      Time elapsed: 01:11:54
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 49381 steps/s (collection: 1.904s, learning 0.087s)
             Mean action noise std: 2.43
          Mean value_function loss: 137.0986
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.4092
                       Mean reward: 886.20
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.9089
     Episode_Reward/lifting_object: 174.7717
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.99s
                      Time elapsed: 01:11:56
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.940s, learning 0.087s)
             Mean action noise std: 2.43
          Mean value_function loss: 148.8563
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4124
                       Mean reward: 843.22
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.9120
     Episode_Reward/lifting_object: 175.9874
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.03s
                      Time elapsed: 01:11:58
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 49221 steps/s (collection: 1.900s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 135.5985
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4186
                       Mean reward: 884.72
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.9076
     Episode_Reward/lifting_object: 175.3113
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.00s
                      Time elapsed: 01:12:00
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 48913 steps/s (collection: 1.894s, learning 0.116s)
             Mean action noise std: 2.43
          Mean value_function loss: 121.9357
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.4258
                       Mean reward: 886.33
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.9178
     Episode_Reward/lifting_object: 176.7765
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.01s
                      Time elapsed: 01:12:02
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 48519 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 2.43
          Mean value_function loss: 137.4294
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.4339
                       Mean reward: 869.88
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 0.9099
     Episode_Reward/lifting_object: 175.2387
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.03s
                      Time elapsed: 01:12:04
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 49138 steps/s (collection: 1.893s, learning 0.108s)
             Mean action noise std: 2.43
          Mean value_function loss: 117.5964
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.4420
                       Mean reward: 883.84
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 177.6018
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.00s
                      Time elapsed: 01:12:06
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 49895 steps/s (collection: 1.865s, learning 0.105s)
             Mean action noise std: 2.43
          Mean value_function loss: 134.2736
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.4530
                       Mean reward: 888.03
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.9170
     Episode_Reward/lifting_object: 176.6926
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.97s
                      Time elapsed: 01:12:08
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 49927 steps/s (collection: 1.876s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 145.9576
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.4639
                       Mean reward: 868.45
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 0.9120
     Episode_Reward/lifting_object: 175.3181
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.97s
                      Time elapsed: 01:12:10
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 50108 steps/s (collection: 1.873s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 144.6317
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.4724
                       Mean reward: 860.35
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 0.9017
     Episode_Reward/lifting_object: 173.7449
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.96s
                      Time elapsed: 01:12:12
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.907s, learning 0.087s)
             Mean action noise std: 2.44
          Mean value_function loss: 122.0664
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.4777
                       Mean reward: 879.49
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.9205
     Episode_Reward/lifting_object: 177.6199
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.99s
                      Time elapsed: 01:12:14
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 50628 steps/s (collection: 1.851s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 140.8210
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4843
                       Mean reward: 899.15
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.9018
     Episode_Reward/lifting_object: 173.4599
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.94s
                      Time elapsed: 01:12:16
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 50763 steps/s (collection: 1.847s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 109.2741
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.4913
                       Mean reward: 895.86
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.9190
     Episode_Reward/lifting_object: 177.1143
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.94s
                      Time elapsed: 01:12:17
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 49244 steps/s (collection: 1.902s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 100.2375
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4973
                       Mean reward: 894.90
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 178.0432
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.00s
                      Time elapsed: 01:12:19
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 49350 steps/s (collection: 1.885s, learning 0.107s)
             Mean action noise std: 2.44
          Mean value_function loss: 109.6584
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.5042
                       Mean reward: 889.39
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 177.0806
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.99s
                      Time elapsed: 01:12:21
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 50442 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 112.6259
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.5110
                       Mean reward: 889.83
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 174.6330
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 1.95s
                      Time elapsed: 01:12:23
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 50271 steps/s (collection: 1.867s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 107.8863
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.5162
                       Mean reward: 915.83
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.9412
     Episode_Reward/lifting_object: 181.5479
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.96s
                      Time elapsed: 01:12:25
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.875s, learning 0.100s)
             Mean action noise std: 2.44
          Mean value_function loss: 120.1333
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.5232
                       Mean reward: 895.01
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.9227
     Episode_Reward/lifting_object: 178.3071
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 1.97s
                      Time elapsed: 01:12:27
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 49871 steps/s (collection: 1.867s, learning 0.105s)
             Mean action noise std: 2.44
          Mean value_function loss: 119.9527
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.5326
                       Mean reward: 883.48
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 0.9140
     Episode_Reward/lifting_object: 175.5614
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 1.97s
                      Time elapsed: 01:12:29
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 49560 steps/s (collection: 1.873s, learning 0.110s)
             Mean action noise std: 2.44
          Mean value_function loss: 114.9606
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.5378
                       Mean reward: 894.87
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 177.9756
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.98s
                      Time elapsed: 01:12:31
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 49383 steps/s (collection: 1.882s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 123.2697
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.5460
                       Mean reward: 894.90
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.9195
     Episode_Reward/lifting_object: 176.8687
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 1.99s
                      Time elapsed: 01:12:33
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 50109 steps/s (collection: 1.856s, learning 0.106s)
             Mean action noise std: 2.45
          Mean value_function loss: 135.5471
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.5568
                       Mean reward: 894.70
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 0.9203
     Episode_Reward/lifting_object: 177.0312
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.96s
                      Time elapsed: 01:12:35
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 49233 steps/s (collection: 1.890s, learning 0.106s)
             Mean action noise std: 2.45
          Mean value_function loss: 137.7608
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.5718
                       Mean reward: 863.38
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 0.9134
     Episode_Reward/lifting_object: 175.6883
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.00s
                      Time elapsed: 01:12:37
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 48467 steps/s (collection: 1.922s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 145.7840
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.5828
                       Mean reward: 902.03
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.9185
     Episode_Reward/lifting_object: 176.1857
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.03s
                      Time elapsed: 01:12:39
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 48519 steps/s (collection: 1.918s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 164.6892
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.5891
                       Mean reward: 878.31
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.9230
     Episode_Reward/lifting_object: 177.7646
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.03s
                      Time elapsed: 01:12:41
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 49286 steps/s (collection: 1.875s, learning 0.120s)
             Mean action noise std: 2.45
          Mean value_function loss: 170.6160
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.5958
                       Mean reward: 885.18
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 175.1748
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 1.99s
                      Time elapsed: 01:12:43
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 49289 steps/s (collection: 1.900s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 171.3469
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.5993
                       Mean reward: 864.89
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 0.8864
     Episode_Reward/lifting_object: 170.1404
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 1.99s
                      Time elapsed: 01:12:45
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 49605 steps/s (collection: 1.887s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 147.9343
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.6014
                       Mean reward: 897.87
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.9196
     Episode_Reward/lifting_object: 176.5298
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 1.98s
                      Time elapsed: 01:12:47
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 49779 steps/s (collection: 1.871s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 116.6545
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.6038
                       Mean reward: 898.51
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.9200
     Episode_Reward/lifting_object: 176.8766
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 1.97s
                      Time elapsed: 01:12:49
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 48141 steps/s (collection: 1.939s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 128.6506
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.6089
                       Mean reward: 903.74
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.9224
     Episode_Reward/lifting_object: 177.7476
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.04s
                      Time elapsed: 01:12:51
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 46116 steps/s (collection: 2.001s, learning 0.131s)
             Mean action noise std: 2.45
          Mean value_function loss: 108.3809
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 39.6116
                       Mean reward: 918.84
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.9198
     Episode_Reward/lifting_object: 177.0710
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.13s
                      Time elapsed: 01:12:53
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 47761 steps/s (collection: 1.945s, learning 0.114s)
             Mean action noise std: 2.45
          Mean value_function loss: 130.7625
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.6154
                       Mean reward: 885.62
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.8983
     Episode_Reward/lifting_object: 172.2651
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.06s
                      Time elapsed: 01:12:55
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 46432 steps/s (collection: 2.018s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 123.3381
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.6203
                       Mean reward: 873.96
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.9097
     Episode_Reward/lifting_object: 175.7733
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.12s
                      Time elapsed: 01:12:58
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 45448 steps/s (collection: 2.062s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 97.3428
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.6218
                       Mean reward: 891.42
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 0.9092
     Episode_Reward/lifting_object: 174.8356
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.16s
                      Time elapsed: 01:13:00
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 49992 steps/s (collection: 1.869s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 103.5254
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.6249
                       Mean reward: 896.77
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: 177.6849
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 1.97s
                      Time elapsed: 01:13:02
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 49602 steps/s (collection: 1.878s, learning 0.104s)
             Mean action noise std: 2.46
          Mean value_function loss: 91.0187
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6301
                       Mean reward: 903.23
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 0.9164
     Episode_Reward/lifting_object: 176.4801
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 1.98s
                      Time elapsed: 01:13:04
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 48492 steps/s (collection: 1.908s, learning 0.119s)
             Mean action noise std: 2.46
          Mean value_function loss: 107.9933
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.6331
                       Mean reward: 866.87
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 0.9202
     Episode_Reward/lifting_object: 177.1898
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.03s
                      Time elapsed: 01:13:06
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 48342 steps/s (collection: 1.921s, learning 0.112s)
             Mean action noise std: 2.46
          Mean value_function loss: 93.8040
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.6377
                       Mean reward: 908.05
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 178.9635
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.03s
                      Time elapsed: 01:13:08
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 50007 steps/s (collection: 1.870s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 97.0124
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.6434
                       Mean reward: 883.00
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.9076
     Episode_Reward/lifting_object: 175.0455
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 1.97s
                      Time elapsed: 01:13:10
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 49731 steps/s (collection: 1.879s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 90.9380
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.6492
                       Mean reward: 924.15
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: 177.9395
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 1.98s
                      Time elapsed: 01:13:12
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 49564 steps/s (collection: 1.889s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 98.0560
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 39.6563
                       Mean reward: 893.27
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.9190
     Episode_Reward/lifting_object: 177.0497
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 1.98s
                      Time elapsed: 01:13:14
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 49877 steps/s (collection: 1.874s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 124.5713
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.6584
                       Mean reward: 892.95
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 175.2348
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 1.97s
                      Time elapsed: 01:13:16
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 49382 steps/s (collection: 1.890s, learning 0.101s)
             Mean action noise std: 2.46
          Mean value_function loss: 96.3851
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.6632
                       Mean reward: 879.92
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 0.9284
     Episode_Reward/lifting_object: 178.7040
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 1.99s
                      Time elapsed: 01:13:18
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 48952 steps/s (collection: 1.909s, learning 0.100s)
             Mean action noise std: 2.46
          Mean value_function loss: 105.8735
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 39.6666
                       Mean reward: 902.53
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.9304
     Episode_Reward/lifting_object: 179.3402
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.01s
                      Time elapsed: 01:13:20
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 48499 steps/s (collection: 1.930s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 118.4276
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.6676
                       Mean reward: 898.68
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 0.9251
     Episode_Reward/lifting_object: 178.0896
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.03s
                      Time elapsed: 01:13:22
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 49153 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 163.1720
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.6704
                       Mean reward: 897.62
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.9054
     Episode_Reward/lifting_object: 173.9199
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.00s
                      Time elapsed: 01:13:24
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 48584 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 124.6689
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6743
                       Mean reward: 884.53
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 0.9086
     Episode_Reward/lifting_object: 174.5869
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.02s
                      Time elapsed: 01:13:26
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 48939 steps/s (collection: 1.907s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 145.9237
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.6807
                       Mean reward: 880.90
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.9117
     Episode_Reward/lifting_object: 174.5153
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.01s
                      Time elapsed: 01:13:28
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 48759 steps/s (collection: 1.919s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 170.7084
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.6891
                       Mean reward: 856.89
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 174.4631
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.02s
                      Time elapsed: 01:13:30
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 48618 steps/s (collection: 1.926s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 183.4182
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.6979
                       Mean reward: 882.00
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 0.9048
     Episode_Reward/lifting_object: 173.7581
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.02s
                      Time elapsed: 01:13:32
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 49843 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 165.0044
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.7082
                       Mean reward: 878.66
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 176.0278
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 1.97s
                      Time elapsed: 01:13:34
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 48060 steps/s (collection: 1.946s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 126.1266
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.7153
                       Mean reward: 898.77
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.9210
     Episode_Reward/lifting_object: 176.5971
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.05s
                      Time elapsed: 01:13:36
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 47793 steps/s (collection: 1.955s, learning 0.102s)
             Mean action noise std: 2.47
          Mean value_function loss: 149.1099
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.7238
                       Mean reward: 855.76
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 0.9076
     Episode_Reward/lifting_object: 174.4629
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.06s
                      Time elapsed: 01:13:38
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 49314 steps/s (collection: 1.879s, learning 0.114s)
             Mean action noise std: 2.47
          Mean value_function loss: 154.1519
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.7299
                       Mean reward: 888.39
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.9079
     Episode_Reward/lifting_object: 174.3718
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 1.99s
                      Time elapsed: 01:13:40
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 48959 steps/s (collection: 1.893s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 134.5822
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.7356
                       Mean reward: 889.87
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.9188
     Episode_Reward/lifting_object: 176.1430
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.01s
                      Time elapsed: 01:13:42
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 47576 steps/s (collection: 1.948s, learning 0.119s)
             Mean action noise std: 2.47
          Mean value_function loss: 131.4178
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 39.7455
                       Mean reward: 907.37
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 0.9328
     Episode_Reward/lifting_object: 179.0570
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.07s
                      Time elapsed: 01:13:44
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 49271 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 2.47
          Mean value_function loss: 105.8058
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.7570
                       Mean reward: 899.08
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.9153
     Episode_Reward/lifting_object: 175.3092
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.00s
                      Time elapsed: 01:13:46
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 48049 steps/s (collection: 1.940s, learning 0.106s)
             Mean action noise std: 2.47
          Mean value_function loss: 98.1134
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.7654
                       Mean reward: 877.78
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.9312
     Episode_Reward/lifting_object: 178.6180
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.05s
                      Time elapsed: 01:13:48
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 49319 steps/s (collection: 1.894s, learning 0.100s)
             Mean action noise std: 2.48
          Mean value_function loss: 130.7256
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.7715
                       Mean reward: 890.05
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.9241
     Episode_Reward/lifting_object: 176.8625
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 1.99s
                      Time elapsed: 01:13:50
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 48754 steps/s (collection: 1.915s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 117.2359
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.7806
                       Mean reward: 895.97
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 0.9285
     Episode_Reward/lifting_object: 177.8613
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.02s
                      Time elapsed: 01:13:52
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 48616 steps/s (collection: 1.914s, learning 0.109s)
             Mean action noise std: 2.48
          Mean value_function loss: 142.7919
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.7907
                       Mean reward: 910.48
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.9334
     Episode_Reward/lifting_object: 179.2017
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.02s
                      Time elapsed: 01:13:54
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 48136 steps/s (collection: 1.946s, learning 0.096s)
             Mean action noise std: 2.48
          Mean value_function loss: 105.6311
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.7995
                       Mean reward: 914.40
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.9235
     Episode_Reward/lifting_object: 177.0774
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.04s
                      Time elapsed: 01:13:56
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 49059 steps/s (collection: 1.911s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 123.5950
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.8062
                       Mean reward: 915.12
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.9359
     Episode_Reward/lifting_object: 179.5867
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.00s
                      Time elapsed: 01:13:58
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 48021 steps/s (collection: 1.946s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 120.8199
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.8107
                       Mean reward: 905.01
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 0.9296
     Episode_Reward/lifting_object: 178.3313
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.05s
                      Time elapsed: 01:14:00
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 48824 steps/s (collection: 1.915s, learning 0.099s)
             Mean action noise std: 2.48
          Mean value_function loss: 83.3741
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.8178
                       Mean reward: 907.08
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.9335
     Episode_Reward/lifting_object: 178.7336
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.01s
                      Time elapsed: 01:14:02
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 48737 steps/s (collection: 1.893s, learning 0.124s)
             Mean action noise std: 2.48
          Mean value_function loss: 130.5079
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 39.8234
                       Mean reward: 914.29
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 0.9305
     Episode_Reward/lifting_object: 178.1208
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.02s
                      Time elapsed: 01:14:04
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 47529 steps/s (collection: 1.943s, learning 0.125s)
             Mean action noise std: 2.48
          Mean value_function loss: 131.3540
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.8250
                       Mean reward: 878.51
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.9131
     Episode_Reward/lifting_object: 174.8423
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.07s
                      Time elapsed: 01:14:06
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 49858 steps/s (collection: 1.856s, learning 0.116s)
             Mean action noise std: 2.48
          Mean value_function loss: 105.7446
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8297
                       Mean reward: 900.84
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.9168
     Episode_Reward/lifting_object: 175.1480
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 1.97s
                      Time elapsed: 01:14:08
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 47260 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 77.0286
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.8392
                       Mean reward: 913.81
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.9337
     Episode_Reward/lifting_object: 178.8553
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.08s
                      Time elapsed: 01:14:10
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 48959 steps/s (collection: 1.913s, learning 0.095s)
             Mean action noise std: 2.49
          Mean value_function loss: 109.3540
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.8495
                       Mean reward: 889.42
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.9136
     Episode_Reward/lifting_object: 174.8194
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.01s
                      Time elapsed: 01:14:12
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 48857 steps/s (collection: 1.911s, learning 0.102s)
             Mean action noise std: 2.49
          Mean value_function loss: 90.5112
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.8567
                       Mean reward: 917.82
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.9459
     Episode_Reward/lifting_object: 180.8494
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.01s
                      Time elapsed: 01:14:14
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 49231 steps/s (collection: 1.898s, learning 0.099s)
             Mean action noise std: 2.49
          Mean value_function loss: 111.0605
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.8612
                       Mean reward: 882.19
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 0.9292
     Episode_Reward/lifting_object: 177.2137
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.00s
                      Time elapsed: 01:14:16
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 48939 steps/s (collection: 1.915s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 130.4052
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.8655
                       Mean reward: 850.99
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 0.9217
     Episode_Reward/lifting_object: 175.3278
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.01s
                      Time elapsed: 01:14:18
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 49395 steps/s (collection: 1.898s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 97.3794
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.8717
                       Mean reward: 876.92
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 0.9261
     Episode_Reward/lifting_object: 176.7692
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 1.99s
                      Time elapsed: 01:14:20
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 49225 steps/s (collection: 1.905s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 109.0558
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.8752
                       Mean reward: 889.49
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 0.9339
     Episode_Reward/lifting_object: 179.5937
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.00s
                      Time elapsed: 01:14:22
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 48543 steps/s (collection: 1.900s, learning 0.125s)
             Mean action noise std: 2.49
          Mean value_function loss: 88.8925
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.8776
                       Mean reward: 869.85
               Mean episode length: 232.27
    Episode_Reward/reaching_object: 0.9286
     Episode_Reward/lifting_object: 177.3393
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.03s
                      Time elapsed: 01:14:24
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 47425 steps/s (collection: 1.951s, learning 0.122s)
             Mean action noise std: 2.49
          Mean value_function loss: 71.5728
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.8805
                       Mean reward: 912.32
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.9380
     Episode_Reward/lifting_object: 179.3038
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.07s
                      Time elapsed: 01:14:26
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 48811 steps/s (collection: 1.894s, learning 0.120s)
             Mean action noise std: 2.49
          Mean value_function loss: 82.6282
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.8873
                       Mean reward: 885.29
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 0.9384
     Episode_Reward/lifting_object: 179.0700
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.01s
                      Time elapsed: 01:14:28
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 47631 steps/s (collection: 1.939s, learning 0.125s)
             Mean action noise std: 2.49
          Mean value_function loss: 91.3276
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.8953
                       Mean reward: 919.25
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.9339
     Episode_Reward/lifting_object: 178.8651
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.06s
                      Time elapsed: 01:14:30
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 47241 steps/s (collection: 1.961s, learning 0.120s)
             Mean action noise std: 2.49
          Mean value_function loss: 81.6385
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.9025
                       Mean reward: 911.31
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.9334
     Episode_Reward/lifting_object: 178.4035
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.08s
                      Time elapsed: 01:14:32
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 48846 steps/s (collection: 1.902s, learning 0.111s)
             Mean action noise std: 2.49
          Mean value_function loss: 73.6077
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.9060
                       Mean reward: 877.00
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.9485
     Episode_Reward/lifting_object: 182.0639
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.01s
                      Time elapsed: 01:14:35
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 48790 steps/s (collection: 1.893s, learning 0.122s)
             Mean action noise std: 2.49
          Mean value_function loss: 107.5496
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9139
                       Mean reward: 884.38
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 0.9407
     Episode_Reward/lifting_object: 180.6446
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.01s
                      Time elapsed: 01:14:37
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 46891 steps/s (collection: 1.979s, learning 0.117s)
             Mean action noise std: 2.49
          Mean value_function loss: 100.3869
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.9214
                       Mean reward: 918.08
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.9376
     Episode_Reward/lifting_object: 179.5485
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.10s
                      Time elapsed: 01:14:39
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 47781 steps/s (collection: 1.936s, learning 0.121s)
             Mean action noise std: 2.50
          Mean value_function loss: 92.8108
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.9247
                       Mean reward: 916.27
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.9366
     Episode_Reward/lifting_object: 178.6039
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.06s
                      Time elapsed: 01:14:41
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 48126 steps/s (collection: 1.937s, learning 0.106s)
             Mean action noise std: 2.50
          Mean value_function loss: 107.1583
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.9301
                       Mean reward: 904.02
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.9335
     Episode_Reward/lifting_object: 178.5508
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.04s
                      Time elapsed: 01:14:43
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 48076 steps/s (collection: 1.920s, learning 0.125s)
             Mean action noise std: 2.50
          Mean value_function loss: 132.6587
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.9350
                       Mean reward: 857.09
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 0.9127
     Episode_Reward/lifting_object: 174.6678
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.04s
                      Time elapsed: 01:14:45
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 47266 steps/s (collection: 1.979s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 120.3417
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.9423
                       Mean reward: 898.58
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 0.9216
     Episode_Reward/lifting_object: 176.1379
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.08s
                      Time elapsed: 01:14:47
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.898s, learning 0.097s)
             Mean action noise std: 2.50
          Mean value_function loss: 145.9175
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.9490
                       Mean reward: 893.96
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.9292
     Episode_Reward/lifting_object: 177.3394
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 1.99s
                      Time elapsed: 01:14:49
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 48383 steps/s (collection: 1.916s, learning 0.116s)
             Mean action noise std: 2.50
          Mean value_function loss: 135.0837
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 39.9571
                       Mean reward: 909.69
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.9332
     Episode_Reward/lifting_object: 178.8677
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.03s
                      Time elapsed: 01:14:51
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 48341 steps/s (collection: 1.932s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 115.8445
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9656
                       Mean reward: 905.40
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.9351
     Episode_Reward/lifting_object: 179.5634
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.03s
                      Time elapsed: 01:14:53
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 48654 steps/s (collection: 1.922s, learning 0.099s)
             Mean action noise std: 2.50
          Mean value_function loss: 94.8025
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.9764
                       Mean reward: 897.23
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: 177.9052
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.02s
                      Time elapsed: 01:14:55
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 46522 steps/s (collection: 2.015s, learning 0.099s)
             Mean action noise std: 2.50
          Mean value_function loss: 103.5921
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.9815
                       Mean reward: 906.40
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.9388
     Episode_Reward/lifting_object: 179.9332
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.11s
                      Time elapsed: 01:14:57
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 48963 steps/s (collection: 1.907s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 81.8802
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.9856
                       Mean reward: 928.78
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.9414
     Episode_Reward/lifting_object: 181.0097
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.01s
                      Time elapsed: 01:14:59
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 48533 steps/s (collection: 1.921s, learning 0.104s)
             Mean action noise std: 2.50
          Mean value_function loss: 116.1748
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.9935
                       Mean reward: 883.72
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 177.6460
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.03s
                      Time elapsed: 01:15:01
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 48891 steps/s (collection: 1.907s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 99.9694
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0005
                       Mean reward: 886.31
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.9269
     Episode_Reward/lifting_object: 178.6491
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.01s
                      Time elapsed: 01:15:03
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 47629 steps/s (collection: 1.928s, learning 0.136s)
             Mean action noise std: 2.51
          Mean value_function loss: 111.8763
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0066
                       Mean reward: 865.22
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 0.9182
     Episode_Reward/lifting_object: 176.6646
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.06s
                      Time elapsed: 01:15:05
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 47348 steps/s (collection: 1.968s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 95.0383
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.0161
                       Mean reward: 893.85
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.9294
     Episode_Reward/lifting_object: 178.7134
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.08s
                      Time elapsed: 01:15:07
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 48933 steps/s (collection: 1.909s, learning 0.100s)
             Mean action noise std: 2.51
          Mean value_function loss: 107.2787
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.0236
                       Mean reward: 905.32
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.9320
     Episode_Reward/lifting_object: 179.8771
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.01s
                      Time elapsed: 01:15:09
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 48461 steps/s (collection: 1.924s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 77.9589
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.0321
                       Mean reward: 921.08
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.9309
     Episode_Reward/lifting_object: 179.1139
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.03s
                      Time elapsed: 01:15:11
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 48521 steps/s (collection: 1.925s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 83.1733
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0413
                       Mean reward: 930.19
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.9363
     Episode_Reward/lifting_object: 180.7734
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.03s
                      Time elapsed: 01:15:13
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 48633 steps/s (collection: 1.911s, learning 0.111s)
             Mean action noise std: 2.51
          Mean value_function loss: 103.5133
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.0480
                       Mean reward: 897.14
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 0.9241
     Episode_Reward/lifting_object: 177.9022
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.02s
                      Time elapsed: 01:15:15
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 48145 steps/s (collection: 1.924s, learning 0.118s)
             Mean action noise std: 2.51
          Mean value_function loss: 91.7218
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.0547
                       Mean reward: 915.10
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.9390
     Episode_Reward/lifting_object: 181.3632
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.04s
                      Time elapsed: 01:15:17
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 46936 steps/s (collection: 1.981s, learning 0.113s)
             Mean action noise std: 2.51
          Mean value_function loss: 102.7213
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.0601
                       Mean reward: 887.39
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 178.1557
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.09s
                      Time elapsed: 01:15:19
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 48665 steps/s (collection: 1.910s, learning 0.110s)
             Mean action noise std: 2.51
          Mean value_function loss: 94.1269
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.0674
                       Mean reward: 891.12
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.9229
     Episode_Reward/lifting_object: 178.2877
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.02s
                      Time elapsed: 01:15:21
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 49067 steps/s (collection: 1.901s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 92.1456
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.0748
                       Mean reward: 921.70
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.9328
     Episode_Reward/lifting_object: 180.2719
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.00s
                      Time elapsed: 01:15:23
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 48363 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 105.6706
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.0816
                       Mean reward: 900.17
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.9329
     Episode_Reward/lifting_object: 179.9334
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.03s
                      Time elapsed: 01:15:26
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 48565 steps/s (collection: 1.926s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 72.8705
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.0889
                       Mean reward: 898.16
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.9344
     Episode_Reward/lifting_object: 180.0844
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.02s
                      Time elapsed: 01:15:28
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 48462 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 96.5073
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.0962
                       Mean reward: 914.28
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.9294
     Episode_Reward/lifting_object: 179.3228
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.03s
                      Time elapsed: 01:15:30
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 46539 steps/s (collection: 2.012s, learning 0.100s)
             Mean action noise std: 2.52
          Mean value_function loss: 110.9281
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.1006
                       Mean reward: 870.04
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 0.9267
     Episode_Reward/lifting_object: 179.1111
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.11s
                      Time elapsed: 01:15:32
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.926s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 97.5493
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.1029
                       Mean reward: 887.15
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.9300
     Episode_Reward/lifting_object: 179.6004
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.02s
                      Time elapsed: 01:15:34
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 48107 steps/s (collection: 1.930s, learning 0.114s)
             Mean action noise std: 2.52
          Mean value_function loss: 94.2692
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.1068
                       Mean reward: 921.00
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.9415
     Episode_Reward/lifting_object: 181.7508
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.04s
                      Time elapsed: 01:15:36
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 48361 steps/s (collection: 1.926s, learning 0.106s)
             Mean action noise std: 2.52
          Mean value_function loss: 93.3939
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 40.1132
                       Mean reward: 894.83
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 0.9303
     Episode_Reward/lifting_object: 178.7061
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.03s
                      Time elapsed: 01:15:38
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 48065 steps/s (collection: 1.936s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 100.1061
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1168
                       Mean reward: 899.57
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.9306
     Episode_Reward/lifting_object: 179.5411
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.05s
                      Time elapsed: 01:15:40
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 47920 steps/s (collection: 1.948s, learning 0.104s)
             Mean action noise std: 2.52
          Mean value_function loss: 101.9126
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.1216
                       Mean reward: 905.63
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.9249
     Episode_Reward/lifting_object: 177.9517
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.05s
                      Time elapsed: 01:15:42
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 47664 steps/s (collection: 1.948s, learning 0.114s)
             Mean action noise std: 2.52
          Mean value_function loss: 89.7505
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1292
                       Mean reward: 920.37
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.9474
     Episode_Reward/lifting_object: 182.8172
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.06s
                      Time elapsed: 01:15:44
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 47193 steps/s (collection: 1.975s, learning 0.108s)
             Mean action noise std: 2.52
          Mean value_function loss: 113.9774
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 40.1381
                       Mean reward: 897.07
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 177.4330
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.08s
                      Time elapsed: 01:15:46
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.981s, learning 0.111s)
             Mean action noise std: 2.52
          Mean value_function loss: 93.4728
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.1420
                       Mean reward: 903.89
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.9343
     Episode_Reward/lifting_object: 179.8892
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.09s
                      Time elapsed: 01:15:48
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 45331 steps/s (collection: 2.027s, learning 0.142s)
             Mean action noise std: 2.53
          Mean value_function loss: 112.1769
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.1476
                       Mean reward: 901.60
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 0.9280
     Episode_Reward/lifting_object: 178.9417
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.17s
                      Time elapsed: 01:15:50
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 46657 steps/s (collection: 1.990s, learning 0.117s)
             Mean action noise std: 2.53
          Mean value_function loss: 97.4041
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.1526
                       Mean reward: 884.11
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 0.9366
     Episode_Reward/lifting_object: 180.1408
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.11s
                      Time elapsed: 01:15:52
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 47467 steps/s (collection: 1.971s, learning 0.100s)
             Mean action noise std: 2.53
          Mean value_function loss: 120.2191
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1568
                       Mean reward: 917.09
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.9062
     Episode_Reward/lifting_object: 174.4686
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.07s
                      Time elapsed: 01:15:54
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 49167 steps/s (collection: 1.906s, learning 0.093s)
             Mean action noise std: 2.53
          Mean value_function loss: 111.3129
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.1630
                       Mean reward: 903.85
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.9254
     Episode_Reward/lifting_object: 178.4513
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.00s
                      Time elapsed: 01:15:56
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 47907 steps/s (collection: 1.940s, learning 0.112s)
             Mean action noise std: 2.53
          Mean value_function loss: 112.3954
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.1689
                       Mean reward: 882.74
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.9337
     Episode_Reward/lifting_object: 180.5702
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.05s
                      Time elapsed: 01:15:58
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 43760 steps/s (collection: 2.128s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 149.9699
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.1756
                       Mean reward: 908.90
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.9235
     Episode_Reward/lifting_object: 177.9536
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.25s
                      Time elapsed: 01:16:01
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 46508 steps/s (collection: 2.001s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 117.1142
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.1796
                       Mean reward: 865.87
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.9305
     Episode_Reward/lifting_object: 179.8052
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.11s
                      Time elapsed: 01:16:03
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 46025 steps/s (collection: 2.009s, learning 0.127s)
             Mean action noise std: 2.53
          Mean value_function loss: 66.5368
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.1857
                       Mean reward: 886.35
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 0.9357
     Episode_Reward/lifting_object: 181.2234
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.14s
                      Time elapsed: 01:16:05
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 46412 steps/s (collection: 2.005s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 99.2367
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.1939
                       Mean reward: 898.58
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.9360
     Episode_Reward/lifting_object: 180.5837
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.12s
                      Time elapsed: 01:16:07
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 49001 steps/s (collection: 1.909s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 93.0805
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.1989
                       Mean reward: 902.50
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.9210
     Episode_Reward/lifting_object: 177.6185
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.01s
                      Time elapsed: 01:16:09
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.961s, learning 0.123s)
             Mean action noise std: 2.53
          Mean value_function loss: 100.3073
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.2038
                       Mean reward: 902.02
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 0.9243
     Episode_Reward/lifting_object: 178.5314
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.08s
                      Time elapsed: 01:16:11
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 44668 steps/s (collection: 2.099s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 97.9511
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.2126
                       Mean reward: 907.00
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.9295
     Episode_Reward/lifting_object: 180.2376
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.20s
                      Time elapsed: 01:16:13
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 46207 steps/s (collection: 2.004s, learning 0.124s)
             Mean action noise std: 2.53
          Mean value_function loss: 72.4509
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.2187
                       Mean reward: 914.15
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.9436
     Episode_Reward/lifting_object: 182.3170
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.13s
                      Time elapsed: 01:16:16
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 46597 steps/s (collection: 2.011s, learning 0.099s)
             Mean action noise std: 2.54
          Mean value_function loss: 89.7305
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.2246
                       Mean reward: 900.33
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: 180.4901
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.11s
                      Time elapsed: 01:16:18
                               ETA: 00:00:02

